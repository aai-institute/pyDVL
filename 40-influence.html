<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="Examples" href="examples/index.html" /><link rel="prev" title="Computing data values" href="30-data-valuation.html" />

    <!-- Generated with Sphinx 5.3.0 and Furo 2023.03.27 -->
        <title>Computing influence values - pyDVL 0.6.2.dev0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="_static/css/tooltipster.custom.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/tooltipster.bundle.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/tooltipster-sideTip-shadow.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/tooltipster-sideTip-punk.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/tooltipster-sideTip-noir.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/tooltipster-sideTip-light.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/tooltipster-sideTip-borderless.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/micromodal.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<div class="announcement">
  <aside class="announcement-content">
     pyDVL is in an early stage of development. Expect changes to functionality and the API until version 1.0.0. 
  </aside>
</div>

<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">pyDVL 0.6.2.dev0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand centered" href="index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="_static/logo.svg" alt="Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="10-getting-started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="20-install.html">Installing pyDVL</a></li>
<li class="toctree-l1"><a class="reference internal" href="30-data-valuation.html">Computing data values</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Computing influence values</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="examples/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="examples/shapley_basic_spotify.html">Shapley for data valuation</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/shapley_knn_flowers.html">KNN Shapley</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/shapley_utility_learning.html">Data Utility Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/least_core_basic.html">Least Core for Data Valuation</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/influence_imagenet.html">Influence functions for image classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/influence_synthetic.html">Influence functions for data mislabeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/influence_wine.html">Influence functions for outlier detection</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="pydvl/index.html">API Reference</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="pydvl/influence.html">influence</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="pydvl/influence/general.html">general</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/influence/inversion.html">inversion</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pydvl/influence/torch.html">torch</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pydvl/influence/torch/functional.html">functional</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/influence/torch/torch_differentiable.html">torch_differentiable</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/influence/torch/util.html">util</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/influence/twice_differentiable.html">twice_differentiable</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pydvl/reporting.html">reporting</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="pydvl/reporting/plots.html">plots</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/reporting/scores.html">scores</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pydvl/utils.html">utils</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="pydvl/utils/caching.html">caching</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/utils/config.html">config</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/utils/dataset.html">dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/utils/numeric.html">numeric</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pydvl/utils/parallel.html">parallel</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pydvl/utils/parallel/backend.html">backend</a></li>
<li class="toctree-l4 has-children"><a class="reference internal" href="pydvl/utils/parallel/backends.html">backends</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l5"><a class="reference internal" href="pydvl/utils/parallel/backends/joblib.html">joblib</a></li>
<li class="toctree-l5"><a class="reference internal" href="pydvl/utils/parallel/backends/ray.html">ray</a></li>
</ul>
</li>
<li class="toctree-l4 has-children"><a class="reference internal" href="pydvl/utils/parallel/futures.html">futures</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l5"><a class="reference internal" href="pydvl/utils/parallel/futures/ray.html">ray</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/utils/parallel/map_reduce.html">map_reduce</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/utils/progress.html">progress</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/utils/score.html">score</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/utils/status.html">status</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/utils/types.html">types</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/utils/utility.html">utility</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pydvl/value.html">value</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="pydvl/value/least_core.html">least_core</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/least_core/common.html">common</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/least_core/montecarlo.html">montecarlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/least_core/naive.html">naive</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pydvl/value/loo.html">loo</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/loo/loo.html">loo</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/loo/naive.html">naive</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/value/result.html">result</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/value/sampler.html">sampler</a></li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/value/semivalues.html">semivalues</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pydvl/value/shapley.html">shapley</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/shapley/common.html">common</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/shapley/gt.html">gt</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/shapley/knn.html">knn</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/shapley/montecarlo.html">montecarlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/shapley/naive.html">naive</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/shapley/owen.html">owen</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/shapley/truncated.html">truncated</a></li>
<li class="toctree-l4"><a class="reference internal" href="pydvl/value/shapley/types.html">types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pydvl/value/stopping.html">stopping</a></li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="computing-influence-values">
<span id="influence"></span><h1>Computing influence values<a class="headerlink" href="#computing-influence-values" title="Permalink to this heading">#</a></h1>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Much of the code in the package <a class="reference internal" href="pydvl/influence.html#module-pydvl.influence" title="pydvl.influence"><code class="xref py py-mod docutils literal notranslate"><span class="pre">pydvl.influence</span></code></a> is experimental.
Package structure and basic API are planned to change extensively before
v1.0.0</p>
</div>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<dl class="simple">
<dt>This section needs rewriting:</dt><dd><ul class="simple">
<li><p>Document each approximation method and explain how they differ</p></li>
<li><p>Add example for <code class="docutils literal notranslate"><span class="pre">TwiceDifferentiable</span></code></p></li>
</ul>
</dd>
</dl>
</div>
<p>The influence function (IF) is a method to quantify the effect (influence) that
each training point has on the parameters of a model, and by extension on any
function thereof. In particular, it allows to estimate how much each training
sample affects the error on a test point, making the IF useful for understanding
and debugging models.</p>
<p>pyDVL implements several methods for the efficient computation of the IF for
machine learning.</p>
<section id="the-influence-function">
<span id="id2"></span><h2>The Influence Function<a class="headerlink" href="#the-influence-function" title="Permalink to this heading">#</a></h2>
<p>First introduced in the context of robust statistics in
Hampel<a class="footnote-reference brackets" href="#footcite-hampel-influence-1974" id="id3">1</a>, the IF was popularized in the context of
machine learning in Koh and Liang<a class="footnote-reference brackets" href="#footcite-koh-understanding-2017" id="id4">2</a>. Following their
formulation, consider an input space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (e.g. images) and an output
space <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> (e.g. labels). Let’s take <span class="math notranslate nohighlight">\(z_i = (x_i, y_i)\)</span>, for <span class="math notranslate nohighlight">\(i \in
\{1,...,n\}\)</span> to be the <span class="math notranslate nohighlight">\(i\)</span>-th training point, and <span class="math notranslate nohighlight">\(\theta\)</span> to be the
(potentially highly) multi-dimensional parameters of a model (e.g. <span class="math notranslate nohighlight">\(\theta\)</span> is a
big array with all of a neural network’s parameters, including biases and/or
dropout rates). We will denote with <span class="math notranslate nohighlight">\(L(z, \theta)\)</span> the loss of the model for
point <span class="math notranslate nohighlight">\(z\)</span> when the parameters are <span class="math notranslate nohighlight">\(\theta.\)</span></p>
<p>To train a model, we typically minimize the loss over all <span class="math notranslate nohighlight">\(z_i\)</span>, i.e. the
optimal parameters are</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\hat{\theta} = \arg \min_\theta \sum_{i=1}^n L(z_i, \theta).\]</div>
</div>
</p>
<p>In practice, lack of convexity means that one doesn’t really obtain the
minimizer of the loss, and the training is stopped when the validation loss
stops decreasing.</p>
<p>For notational convenience, let’s define</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\hat{\theta}_{-z} = \arg \min_\theta \sum_{z_i \ne z} L(z_i, \theta), \]</div>
</div>
</p>
<p>i.e. <span class="math notranslate nohighlight">\(\hat{\theta}_{-z}\)</span> are the model parameters that minimize the total loss
when <span class="math notranslate nohighlight">\(z\)</span> is not in the training dataset.</p>
<p>In order to compute the impact of each training point on the model, we would
need to calculate <span class="math notranslate nohighlight">\(\hat{\theta}_{-z}\)</span> for each <span class="math notranslate nohighlight">\(z\)</span> in the training dataset, thus
re-training the model at least ~<span class="math notranslate nohighlight">\(n\)</span> times (more if model training is
stochastic). This is computationally very expensive, especially for big neural
networks. To circumvent this problem, we can just calculate a first order
approximation of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. This can be done through single backpropagation
and without re-training the full model.</p>
<section id="approximating-the-influence-of-a-point">
<span id="approximating-influence-of-a-point"></span><h3>Approximating the influence of a point<a class="headerlink" href="#approximating-the-influence-of-a-point" title="Permalink to this heading">#</a></h3>
<p>Let’s define</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\hat{\theta}_{\epsilon, z} = \arg \min_\theta \frac{1}{n}\sum_{i=1}^n L(z_i,
\theta) + \epsilon L(z, \theta), \]</div>
</div>
</p>
<p>which is the optimal <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> when we up-weight <span class="math notranslate nohighlight">\(z\)</span> by an amount <span class="math notranslate nohighlight">\(\epsilon
\gt 0\)</span>.</p>
<p>From a classical result (a simple derivation is available in Appendix A of
Koh and Liang<a class="footnote-reference brackets" href="#footcite-koh-understanding-2017" id="id5">2</a>), we know that:</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{d \ \hat{\theta}_{\epsilon, z}}{d \epsilon} \Big|_{\epsilon=0} =
-H_{\hat{\theta}}^{-1} \nabla_\theta L(z, \hat{\theta}), \]</div>
</div>
</p>
<p>where <span class="math notranslate nohighlight">\(H_{\hat{\theta}} = \frac{1}{n} \sum_{i=1}^n \nabla_\theta^2 L(z_i,
\hat{\theta})\)</span> is the Hessian of <span class="math notranslate nohighlight">\(L\)</span>. These quantities are also knows as
<strong>influence factors</strong>.</p>
<p>Importantly, notice that this expression is only valid when <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is a
minimum of <span class="math notranslate nohighlight">\(L\)</span>, or otherwise <span class="math notranslate nohighlight">\(H_{\hat{\theta}}\)</span> cannot be inverted! At the same
time, in machine learning full convergence is rarely achieved, so direct Hessian
inversion is not possible. Approximations need to be developed that circumvent
the problem of inverting the Hessian of the model in all those (frequent) cases
where it is not positive definite.</p>
<p>The influence of training point <span class="math notranslate nohighlight">\(z\)</span> on test point <span class="math notranslate nohighlight">\(z_{\text{test}}\)</span> is defined
as:</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{I}(z, z_{\text{test}}) =  L(z_{\text{test}}, \hat{\theta}_{-z}) -
L(z_{\text{test}}, \hat{\theta}). \]</div>
</div>
</p>
<p>Notice that <span class="math notranslate nohighlight">\(\mathcal{I}\)</span> is higher for points <span class="math notranslate nohighlight">\(z\)</span> which positively impact the
model score, since the loss is higher when they are excluded from training. In
practice, one needs to rely on the following infinitesimal approximation:</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{I}_{up}(z, z_{\text{test}}) = - \frac{d L(z_{\text{test}},
\hat{\theta}_{\epsilon, z})}{d \epsilon} \Big|_{\epsilon=0} \]</div>
</div>
</p>
<p>Using the chain rule and the results calculated above, we get:</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{I}_{up}(z, z_{\text{test}}) = - \nabla_\theta L(z_{\text{test}},
\hat{\theta})^\top \ \frac{d \hat{\theta}_{\epsilon, z}}{d \epsilon}
\Big|_{\epsilon=0} = \nabla_\theta L(z_{\text{test}}, \hat{\theta})^\top \
H_{\hat{\theta}}^{-1} \ \nabla_\theta L(z, \hat{\theta}) \]</div>
</div>
</p>
<p>All the resulting factors are gradients of the loss wrt. the model parameters
<span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. This can be easily computed through one or more backpropagation
passes.</p>
</section>
<section id="perturbation-definition-of-the-influence-score">
<span id="id6"></span><h3>Perturbation definition of the influence score<a class="headerlink" href="#perturbation-definition-of-the-influence-score" title="Permalink to this heading">#</a></h3>
<p>How would the loss of the model change if, instead of up-weighting an individual
point <span class="math notranslate nohighlight">\(z\)</span>, we were to up-weight only a single feature of that point? Given <span class="math notranslate nohighlight">\(z =
(x, y)\)</span>, we can define <span class="math notranslate nohighlight">\(z_{\delta} = (x+\delta, y)\)</span>, where <span class="math notranslate nohighlight">\(\delta\)</span> is a vector
of zeros except for a 1 in the position of the feature we want to up-weight. In
order to approximate the effect of modifying a single feature of a single point
on the model score we can define</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\hat{\theta}_{\epsilon, z_{\delta} ,-z} = \arg \min_\theta
\frac{1}{n}\sum_{i=1}^n L(z_{i}, \theta) + \epsilon L(z_{\delta}, \theta) -
\epsilon L(z, \theta), \]</div>
</div>
</p>
<p>Similarly to what was done above, we up-weight point <span class="math notranslate nohighlight">\(z_{\delta}\)</span>, but then we
also remove the up-weighting for all the features that are not modified by
<span class="math notranslate nohighlight">\(\delta\)</span>. From the calculations in <a class="reference internal" href="#approximating-influence-of-a-point"><span class="std std-ref">the previous section</span></a>, it is then easy to see that</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{d \ \hat{\theta}_{\epsilon, z_{\delta} ,-z}}{d \epsilon}
\Big|_{\epsilon=0} = -H_{\hat{\theta}}^{-1} \nabla_\theta \Big( L(z_{\delta},
\hat{\theta}) - L(z, \hat{\theta}) \Big) \]</div>
</div>
</p>
<p>and if the feature space is continuous and as <span class="math notranslate nohighlight">\(\delta \to 0\)</span> we can write</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{d \ \hat{\theta}_{\epsilon, z_{\delta} ,-z}}{d \epsilon}
\Big|_{\epsilon=0} = -H_{\hat{\theta}}^{-1} \ \nabla_x \nabla_\theta L(z,
\hat{\theta}) \delta + \mathcal{o}(\delta) \]</div>
</div>
</p>
<p>The influence of each feature of <span class="math notranslate nohighlight">\(z\)</span> on the loss of the model can therefore be
estimated through the following quantity:</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{I}_{pert}(z, z_{\text{test}}) = - \lim_{\delta \to 0} \
\frac{1}{\delta} \frac{d L(z_{\text{test}}, \hat{\theta}_{\epsilon, \
z_{\delta}, \ -z})}{d \epsilon} \Big|_{\epsilon=0} \]</div>
</div>
</p>
<p>which, using the chain rule and the results calculated above, is equal to</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{I}_{pert}(z, z_{\text{test}}) = - \nabla_\theta L(z_{\text{test}},
\hat{\theta})^\top \ \frac{d \hat{\theta}_{\epsilon, z_{\delta} ,-z}}{d
\epsilon} \Big|_{\epsilon=0} = \nabla_\theta L(z_{\text{test}},
\hat{\theta})^\top \ H_{\hat{\theta}}^{-1} \ \nabla_x \nabla_\theta L(z,
\hat{\theta}) \]</div>
</div>
</p>
<p>The perturbation definition of the influence score is not straightforward to
understand, but it has a simple interpretation: it tells how much the loss of
the model changes when a certain feature of point z is up-weighted. A positive
perturbation influence score indicates that the feature might have a positive
effect on the accuracy of the model.</p>
<p>It is worth noting that the perturbation influence score is a very rough
estimate of the impact of a point on the models loss and it is subject to large
approximation errors. It can nonetheless be used to build training-set attacks,
as done in Koh and Liang<a class="footnote-reference brackets" href="#footcite-koh-understanding-2017" id="id7">2</a>.</p>
</section>
</section>
<section id="computing-influences">
<h2>Computing influences<a class="headerlink" href="#computing-influences" title="Permalink to this heading">#</a></h2>
<p>The main entry point of the library for influence calculation is
<a class="reference internal" href="pydvl/influence/general.html#pydvl.influence.general.compute_influences" title="pydvl.influence.general.compute_influences"><code class="xref py py-func docutils literal notranslate"><span class="pre">compute_influences()</span></code></a>. Given a pre-trained pytorch
model with a loss, first an instance of
<code class="xref py py-func docutils literal notranslate"><span class="pre">TorchTwiceDifferentiable()</span></code> needs to be created.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pydvl.influence</span> <span class="kn">import</span> <span class="n">TorchTwiceDifferentiable</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span>  <span class="n">TorchTwiceDifferentiable</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>The device specifies where influence calculation will be run.</p>
<p>Given training and test data loaders, the influence of each training point on
each test point can be calculated via:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pydvl.influence</span> <span class="kn">import</span> <span class="n">influences</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compute_influences</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">model</span><span class="p">:</span> <span class="n">TorchTwiceDifferentiable</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">training_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">test_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The result is a tensor with one row per test point and one column per training
point. Thus, each entry <span class="math notranslate nohighlight">\((i, j)\)</span> represents the influence of training point <span class="math notranslate nohighlight">\(j\)</span>
on test point <span class="math notranslate nohighlight">\(i\)</span>. A large positive influence indicates that training point <span class="math notranslate nohighlight">\(j\)</span>
tends to improve the performance of the model on test point <span class="math notranslate nohighlight">\(i\)</span>, and vice versa,
a large negative influence indicates that training point <span class="math notranslate nohighlight">\(j\)</span> tends to worsen the
performance of the model on test point <span class="math notranslate nohighlight">\(i\)</span>.</p>
<section id="perturbation-influences">
<h3>Perturbation influences<a class="headerlink" href="#perturbation-influences" title="Permalink to this heading">#</a></h3>
<p>The method of empirical influence computation can be selected in
<a class="reference internal" href="pydvl/influence/general.html#pydvl.influence.general.compute_influences" title="pydvl.influence.general.compute_influences"><code class="xref py py-func docutils literal notranslate"><span class="pre">compute_influences()</span></code></a> with the parameter
<cite>influence_type</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pydvl.influence</span> <span class="kn">import</span> <span class="n">compute_influences</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compute_influences</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">model</span><span class="p">:</span> <span class="n">TorchTwiceDifferentiable</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">training_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">test_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">influence_type</span><span class="o">=</span><span class="s2">&quot;perturbation&quot;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The result is a tensor with at least three dimensions. The first two dimensions
are the same as in the case of <cite>influence_type=up</cite> case, i.e. one row per test
point and one column per training point. The remaining dimensions are the same
as the number of input features in the data. Therefore, each entry in the tensor
represents the influence of each feature of each training point on each test
point.</p>
</section>
<section id="approximate-matrix-inversion">
<h3>Approximate matrix inversion<a class="headerlink" href="#approximate-matrix-inversion" title="Permalink to this heading">#</a></h3>
<p>In almost every practical application it is not possible to construct, even less
invert the complete Hessian in memory. pyDVL offers several approximate
algorithms to invert it by setting the parameter <cite>inversion_method</cite> of
<a class="reference internal" href="pydvl/influence/general.html#pydvl.influence.general.compute_influences" title="pydvl.influence.general.compute_influences"><code class="xref py py-func docutils literal notranslate"><span class="pre">compute_influences()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pydvl.influence</span> <span class="kn">import</span> <span class="n">compute_influences</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compute_influences</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">model</span><span class="p">:</span> <span class="n">TorchTwiceDifferentiable</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">training_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">test_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">inversion_method</span><span class="o">=</span><span class="s2">&quot;cg&quot;</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>Each inversion method has its own set of parameters that can be tuned to improve
the final result. These parameters can be passed directly to
<a class="reference internal" href="pydvl/influence/general.html#pydvl.influence.general.compute_influences" title="pydvl.influence.general.compute_influences"><code class="xref py py-func docutils literal notranslate"><span class="pre">compute_influences()</span></code></a> as keyword arguments. For
example, the following code sets the maximum number of iterations for conjugate
gradient to <span class="math notranslate nohighlight">\(100\)</span> and the miximum relative error to <span class="math notranslate nohighlight">\(0.01\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pydvl.influence</span> <span class="kn">import</span> <span class="n">compute_influences</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compute_influences</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">model</span><span class="p">:</span> <span class="n">TorchTwiceDifferentiable</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">training_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">test_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">inversion_method</span><span class="o">=</span><span class="s2">&quot;cg&quot;</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">hessian_regularization</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">maxiter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">rtol</span><span class="o">=</span><span class="mf">0.01</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="hessian-regularization">
<h3>Hessian regularization<a class="headerlink" href="#hessian-regularization" title="Permalink to this heading">#</a></h3>
<p>Additionally, and as discussed in <a class="reference internal" href="#the-influence-function"><span class="std std-ref">the introduction</span></a>, in machine learning training rarely converges to a
global minimum of the loss. Despite good apparent convergence, <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>
might be located in a region with flat curvature or close to a saddle point. In
particular, the Hessian might have vanishing eigenvalues making its direct
inversion impossible. Certain methods, such as the <a class="reference internal" href="#arnoldi-solver"><span class="std std-ref">Arnoldi method</span></a> are robust against these problems, but most are not.</p>
<p>To circumvent this problem, many approximate methods can be implemented. The simplest
adds a small <em>hessian perturbation term</em>, i.e. <span class="math notranslate nohighlight">\(H_{\hat{\theta}} + \lambda
\mathbb{I}\)</span>, with <span class="math notranslate nohighlight">\(\mathbb{I}\)</span> being the identity matrix. This standard trick
ensures that the eigenvalues of <span class="math notranslate nohighlight">\(H_{\hat{\theta}}\)</span> are bounded away from zero
and therefore the matrix is invertible. In order for this regularization not to
corrupt the outcome too much, the parameter <span class="math notranslate nohighlight">\(\lambda\)</span> should be as small as
possible while still allowing a reliable inversion of <span class="math notranslate nohighlight">\(H_{\hat{\theta}} +
\lambda \mathbb{I}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pydvl.influence</span> <span class="kn">import</span> <span class="n">compute_influences</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compute_influences</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">model</span><span class="p">:</span> <span class="n">TorchTwiceDifferentiable</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">training_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">test_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">inversion_method</span><span class="o">=</span><span class="s2">&quot;cg&quot;</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">hessian_regularization</span><span class="o">=</span><span class="mf">1e-4</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="influence-factors">
<h3>Influence factors<a class="headerlink" href="#influence-factors" title="Permalink to this heading">#</a></h3>
<p>The <a class="reference internal" href="pydvl/influence/general.html#pydvl.influence.general.compute_influences" title="pydvl.influence.general.compute_influences"><code class="xref py py-func docutils literal notranslate"><span class="pre">compute_influences()</span></code></a> method offers a fast way
to obtain the influence scores given a model and a dataset. Nevertheless, it is
often more convenient to inspect and save some of the intermediate results of
influence calculation for later use.</p>
<p>The influence factors(refer to <a class="reference internal" href="#approximating-influence-of-a-point"><span class="std std-ref">the previous paragraph</span></a> for a definition) are typically the most
computationally demanding part of influence calculation. They can be obtained
via the <a class="reference internal" href="pydvl/influence/general.html#pydvl.influence.general.compute_influence_factors" title="pydvl.influence.general.compute_influence_factors"><code class="xref py py-func docutils literal notranslate"><span class="pre">compute_influence_factors()</span></code></a> method,
saved, and later used for influence calculation on different subsets of the
training dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pydvl.influence</span> <span class="kn">import</span> <span class="n">compute_influence_factors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">influence_factors</span> <span class="o">=</span> <span class="n">compute_influence_factors</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">model</span><span class="p">:</span> <span class="n">TorchTwiceDifferentiable</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">training_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">test_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">inversion_method</span><span class="o">=</span><span class="s2">&quot;cg&quot;</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The result is an object of type <code class="xref py py-class docutils literal notranslate"><span class="pre">InverseHvpResult</span></code>,
which holds the calculated influence factors (<cite>influence_factors.x</cite>) and a
dictionary with the info on the inversion process (<cite>influence_factors.info</cite>).</p>
</section>
</section>
<section id="methods-for-inverse-hvp-calculation">
<span id="methods-for-inverse-hessian-vector-product-calculation"></span><h2>Methods for inverse HVP calculation<a class="headerlink" href="#methods-for-inverse-hvp-calculation" title="Permalink to this heading">#</a></h2>
<p>In order to calculate influence values, pydvl implements several methods for the
calculation of the inverse Hessian vector product (iHVP). More precisely, given
a model, training data and a tensor <span class="math notranslate nohighlight">\(b\)</span>, the function
<a class="reference internal" href="pydvl/influence/inversion.html#pydvl.influence.inversion.solve_hvp" title="pydvl.influence.inversion.solve_hvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">solve_hvp()</span></code></a> will find <span class="math notranslate nohighlight">\(x\)</span> such that <span class="math notranslate nohighlight">\(H x = b\)</span>,
with <span class="math notranslate nohighlight">\(H\)</span> is the hessian of model.</p>
<p>Many different inversion methods can be selected selected via the parameter
<cite>inversion_method</cite> of <a class="reference internal" href="pydvl/influence/general.html#pydvl.influence.general.compute_influences" title="pydvl.influence.general.compute_influences"><code class="xref py py-func docutils literal notranslate"><span class="pre">compute_influences()</span></code></a>.
The following paragraphs will offer more detailed exaple of each method.</p>
<section id="direct-inversion">
<span id="id8"></span><h3>Direct inversion<a class="headerlink" href="#direct-inversion" title="Permalink to this heading">#</a></h3>
<p>With <cite>inversion_method = “direct”</cite> pyDVL will calculate the inverse Hessian
using the direct matrix inversion. This means that the Hessian will first be
explicitly created and then inverted. This method is the most accurate, but also
the most computationally demanding. It is therefore not recommended for large
datasets or models with many parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pydvl.influence.inversion</span> <span class="kn">import</span> <span class="n">solve_hvp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">solve_hvp</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">inversion_method</span><span class="o">=</span><span class="s2">&quot;direct&quot;</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">model</span><span class="p">:</span> <span class="n">TorchTwiceDifferentiable</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">training_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The result, an object of type <code class="xref py py-class docutils literal notranslate"><span class="pre">InverseHvpResult</span></code>,
which holds two objects: <cite>influence_factors.x</cite> and <cite>influence_factors.info</cite>. The
first one is the inverse Hessian vector product, while the second one is a
dictionary with the info on the inversion process. For this method, the info
consists of the Hessian matrix itself.</p>
</section>
<section id="conjugate-gradient">
<span id="id9"></span><h3>Conjugate Gradient<a class="headerlink" href="#conjugate-gradient" title="Permalink to this heading">#</a></h3>
<p>A classical method for solving linear systems of equations is the conjugate
gradient method. It is an iterative method that does not require the explicit
inversion of the Hessian matrix. Instead, it only requires the calculation of
the Hessian vector product. This makes it a good choice for large datasets or
models with many parameters. It is Nevertheless much slower than the direct
inversion method and not as accurate. More info on the theory of conjugate
gradient can be found
<a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">here</a></p>
<p>In pyDVL, you can select conjugate gradient with <cite>inversion_method = “cg”</cite>, like
this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pydvl.influence.inversion</span> <span class="kn">import</span> <span class="n">solve_hvp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">solve_hvp</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">inversion_method</span><span class="o">=</span><span class="s2">&quot;cg&quot;</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">mode</span><span class="p">:</span> <span class="n">TorchTwiceDifferentiable</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">training_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">x0</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-7</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-7</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">maxiter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>The addinal optional parameters <cite>x0</cite>, <cite>rtol</cite>, <cite>atol</cite>, and <cite>maxiter</cite> are passed
to the <code class="xref py py-func docutils literal notranslate"><span class="pre">solve_batch_cg()</span></code>
function, and are respecively the initial guess for the solution, the relative
tolerance, the absolute tolerance, and the maximum number of iterations.</p>
<p>The resulting <code class="xref py py-class docutils literal notranslate"><span class="pre">InverseHvpResult</span></code>
holds the solution of the iHVP, <cite>influence_factors.x</cite>, and some info on the
inversion process <cite>influence_factors.info</cite>. More specifically, for each batch
the infos will report the number of iterations, a boolean indicating if the
inversion converged, and the residual of the inversion.</p>
</section>
<section id="linear-time-stochastic-second-order-approximation-lissa">
<span id="lissa-solver"></span><h3>Linear time Stochastic Second-Order Approximation (LiSSA)<a class="headerlink" href="#linear-time-stochastic-second-order-approximation-lissa" title="Permalink to this heading">#</a></h3>
<p>The LiSSA method is a stochastic approximation of the inverse Hessian vector
product. Compared to <a class="reference internal" href="#conjugate-gradient"><span class="std std-ref">conjugate gradient</span></a> it is faster but less accurate and typically suffers from
instability.</p>
<p>In order to find the solution of the HVP, LiSSA iteratively approximates the
inverse of the Hessian matrix with the following update:</p>
<p><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[H^{-1}_{j+1} b = b + (I - d) \ H - \frac{H^{-1}_j b}{s},\]</div>
</div>
</p>
<p>where <span class="math notranslate nohighlight">\(d\)</span> and <span class="math notranslate nohighlight">\(s\)</span> are a dampening and a scaling factor, which are essential
for the convergence of the method and they need to be chosen carefully, and I
is the identity matrix. More info on the theory of LiSSA can be found in the
original paper Agarwal <em>et al.</em><a class="footnote-reference brackets" href="#footcite-agarwal-2017-second" id="id10">3</a>.</p>
<p>In pyDVL, you can select LiSSA with <cite>inversion_method = “lissa”</cite>, like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pydvl.influence.inversion</span> <span class="kn">import</span> <span class="n">solve_hvp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">solve_hvp</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">inversion_method</span><span class="o">=</span><span class="s2">&quot;lissa&quot;</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">model</span><span class="p">:</span> <span class="n">TorchTwiceDifferentiable</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">training_data_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">dampen</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">h0</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>   <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>with the additional optional parameters <cite>maxiter</cite>, <cite>dampen</cite>, <cite>scale</cite>, <cite>h0</cite>, and
<cite>rtol</cite>, which are passed to the
<code class="xref py py-func docutils literal notranslate"><span class="pre">solve_lissa()</span></code> function,
being the maximum number of iterations, the dampening factor, the scaling
factor, the initial guess for the solution and the relative tolerance,
respectively.</p>
<p>The resulting <code class="xref py py-class docutils literal notranslate"><span class="pre">InverseHvpResult</span></code> holds the solution
of the iHVP, <cite>influence_factors.x</cite>, and, within <cite>influence_factors.info</cite>, the
maximum percentage error and the mean percentage error of the approximation.</p>
</section>
<section id="arnoldi-solver">
<span id="id11"></span><h3>Arnoldi solver<a class="headerlink" href="#arnoldi-solver" title="Permalink to this heading">#</a></h3>
<p>The Arnoldi method is a Krylov subspace method for approximating the action of a
matrix on a vector. It is a generalization of the power method for finding
eigenvectors of a matrix.</p>
<div class="docutils container" id="id12">
<section id="references">
<h4>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h4>
</section>
<dl class="footnote brackets">
<dt class="label" id="footcite-hampel-influence-1974"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p>Frank R. Hampel. The Influence Curve and Its Role in Robust Estimation. <em>Journal of the American Statistical Association</em>, 69(346):383–393, 1974. URL: <a class="reference external" href="https://www.jstor.org/stable/2285666">https://www.jstor.org/stable/2285666</a> (visited on 2022-05-09), <a class="reference external" href="https://arxiv.org/abs/2285666">arXiv:2285666</a>.</p>
</dd>
<dt class="label" id="footcite-koh-understanding-2017"><span class="brackets">2</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id5">2</a>,<a href="#id7">3</a>)</span></dt>
<dd><p>Pang Wei Koh and Percy Liang. Understanding Black-box Predictions via Influence Functions. In <em>Proceedings of the 34th International Conference on Machine Learning</em>, 1885–1894. PMLR, July 2017. URL: <a class="reference external" href="https://proceedings.mlr.press/v70/koh17a.html">https://proceedings.mlr.press/v70/koh17a.html</a> (visited on 2022-05-09), <a class="reference external" href="https://arxiv.org/abs/1703.04730">arXiv:1703.04730</a>.</p>
</dd>
<dt class="label" id="footcite-agarwal-2017-second"><span class="brackets"><a class="fn-backref" href="#id10">3</a></span></dt>
<dd><p>Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine learning in linear time. <em>The Journal of Machine Learning Research</em>, 18(1):4148–4187, 2017.</p>
</dd>
</dl>
</div>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="examples/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Examples</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="30-data-valuation.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Computing data values</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; AppliedAI Institute gGmbH
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/appliedAI-Initiative/pyDVL" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Computing influence values</a><ul>
<li><a class="reference internal" href="#the-influence-function">The Influence Function</a><ul>
<li><a class="reference internal" href="#approximating-the-influence-of-a-point">Approximating the influence of a point</a></li>
<li><a class="reference internal" href="#perturbation-definition-of-the-influence-score">Perturbation definition of the influence score</a></li>
</ul>
</li>
<li><a class="reference internal" href="#computing-influences">Computing influences</a><ul>
<li><a class="reference internal" href="#perturbation-influences">Perturbation influences</a></li>
<li><a class="reference internal" href="#approximate-matrix-inversion">Approximate matrix inversion</a></li>
<li><a class="reference internal" href="#hessian-regularization">Hessian regularization</a></li>
<li><a class="reference internal" href="#influence-factors">Influence factors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#methods-for-inverse-hvp-calculation">Methods for inverse HVP calculation</a><ul>
<li><a class="reference internal" href="#direct-inversion">Direct inversion</a></li>
<li><a class="reference internal" href="#conjugate-gradient">Conjugate Gradient</a></li>
<li><a class="reference internal" href="#linear-time-stochastic-second-order-approximation-lissa">Linear time Stochastic Second-Order Approximation (LiSSA)</a></li>
<li><a class="reference internal" href="#arnoldi-solver">Arnoldi solver</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/scripts/furo.js"></script>
    <script src="_static/js/hoverxref.js"></script>
    <script src="_static/js/tooltipster.bundle.min.js"></script>
    <script src="_static/js/micromodal.min.js"></script>
    <script src="_static/design-tabs.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["\\(", "\\)"]], "processEscapes": true, "displayMath": [["\\[", "\\]"]]}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>
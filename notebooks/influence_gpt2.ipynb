{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabio/miniconda3/envs/pydvl_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"dpv/finetuned-gpt2-tiny\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"dpv/finetuned-gpt2-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mlp parameters size: 56.7M parameters\n"
     ]
    }
   ],
   "source": [
    "mlp_model_size = sum(t.numel() for name, t in model.named_parameters() if \"mlp\" in name)\n",
    "print(f\"Mlp parameters size: {mlp_model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, t in model.named_parameters():\n",
    "    if \"mlp\" not in name:\n",
    "        t.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.h.6.ln_1.weight\n",
      "transformer.h.6.ln_1.bias\n",
      "transformer.h.6.attn.c_attn.weight\n",
      "transformer.h.6.attn.c_attn.bias\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "transformer.h.6.ln_2.weight\n",
      "transformer.h.6.ln_2.bias\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "transformer.h.7.ln_1.weight\n",
      "transformer.h.7.ln_1.bias\n",
      "transformer.h.7.attn.c_attn.weight\n",
      "transformer.h.7.attn.c_attn.bias\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "transformer.h.7.ln_2.weight\n",
      "transformer.h.7.ln_2.bias\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "transformer.h.8.ln_1.weight\n",
      "transformer.h.8.ln_1.bias\n",
      "transformer.h.8.attn.c_attn.weight\n",
      "transformer.h.8.attn.c_attn.bias\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "transformer.h.8.ln_2.weight\n",
      "transformer.h.8.ln_2.bias\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "transformer.h.9.ln_1.weight\n",
      "transformer.h.9.ln_1.bias\n",
      "transformer.h.9.attn.c_attn.weight\n",
      "transformer.h.9.attn.c_attn.bias\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "transformer.h.9.ln_2.weight\n",
      "transformer.h.9.ln_2.bias\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "transformer.h.10.ln_1.weight\n",
      "transformer.h.10.ln_1.bias\n",
      "transformer.h.10.attn.c_attn.weight\n",
      "transformer.h.10.attn.c_attn.bias\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "transformer.h.10.ln_2.weight\n",
      "transformer.h.10.ln_2.bias\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "transformer.h.11.ln_1.weight\n",
      "transformer.h.11.ln_1.bias\n",
      "transformer.h.11.attn.c_attn.weight\n",
      "transformer.h.11.attn.c_attn.bias\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "transformer.h.11.ln_2.weight\n",
      "transformer.h.11.ln_2.bias\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "for name, t in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In a near future, in a little human colony, the human race\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "# output_base = model(**encoded_input)\n",
    "output = model.generate(\n",
    "    **encoded_input, max_length=25, pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a near future, in a little human colony, the human race will be able to create a new species of life.\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  818,   257,  1474,  2003,    11,   287,   257,  1310,  1692, 18815,\n",
       "            11,   262,  1692,  3234,   481,   307,  1498,   284,  2251,   257,\n",
       "           649,  4693,   286,  1204,    13]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directly with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the statement-, the the world over-, the human race will'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "model_output = model(**encoded_input, labels=encoded_input[\"input_ids\"])\n",
    "tokenizer.decode(model_output.logits.argmax(dim=2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' will'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits = model_output.logits[:, -1, :]\n",
    "next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "tokenizer.decode(next_token, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 262, 2643,   12,   11,  262,  262,  995,  625,   12,   11,  262, 1692,\n",
       "         3234,  481]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model_output.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the statement-, the the world over-, the human race will'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    torch.argmax(model_output.logits, dim=-1)[0],\n",
    "    skip_special_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sentence(sentence_tokens, embedding_dim=50257):\n",
    "    inputs_ids = []\n",
    "    label = []\n",
    "    for label_token_idx in range(1, len(sentence_tokens)):\n",
    "        input_ids = [tokenizer.eos_token_id] * (\n",
    "            len(sentence_tokens) - 1 - label_token_idx\n",
    "        ) + sentence_tokens[:label_token_idx]\n",
    "        inputs_ids.append(input_ids)\n",
    "        label.append(sentence_tokens[label_token_idx])\n",
    "    return torch.as_tensor(inputs_ids), torch.as_tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_input = len(encoded_input[\"input_ids\"][0])\n",
    "batch_input_tokens, labels = batch_sentence(output[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(batch_input_tokens)\n",
    "next_token_logits = model_output.logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the recent-, the which world over-, a world race will be able to create a new species of life.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_tokens = torch.argmax(model_output.logits[:, -1, :], dim=-1)\n",
    "tokenizer.decode(\n",
    "    next_tokens,\n",
    "    skip_special_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare batch_input to model output with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In \"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "# output_base = model(**encoded_input)\n",
    "output = model.generate(\n",
    "    **encoded_input, max_length=25, pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_tokens, labels = batch_sentence(output[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = []\n",
    "for input_ in batch_input_tokens:\n",
    "    model_output = model(torch.as_tensor(input_))\n",
    "    next_token_logits.append(model_output.logits[-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration roneneldan--TinyStories-6ac769f186d7da53\n",
      "Found cached dataset parquet (/Users/fabio/.cache/huggingface/datasets/roneneldan___parquet/roneneldan--TinyStories-6ac769f186d7da53/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "\n",
    "train_ds = train_dataset.shuffle().select(range(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-574ae6c787a6bd21\n",
      "Found cached dataset generator (/Users/fabio/.cache/huggingface/datasets/generator/default-574ae6c787a6bd21/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "\n",
    "def gen():\n",
    "    yield {\"text\": output_text}\n",
    "\n",
    "\n",
    "test_ds = HFDataset.from_generator(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, input_len=None):\n",
    "        self.tokenized_ds = dataset.map(\n",
    "            lambda x: tokenizer(x[\"text\"], return_tensors=\"pt\")\n",
    "        )\n",
    "        if input_len is None:\n",
    "            self.input_len = (\n",
    "                min([len(x[\"input_ids\"][0]) for x in self.tokenized_ds]) - 1\n",
    "            )\n",
    "        else:\n",
    "            self.input_len = input_len\n",
    "        self.last_sentence_id = None\n",
    "        self.last_batched_sentence = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_ds) * (self.input_len - 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_id = idx // self.input_len\n",
    "        if sentence_id != self.last_sentence_id:\n",
    "            self.last_sentence_id = sentence_id\n",
    "            self.last_batched_sentence, self.sentence_labels = batch_sentence(\n",
    "                self.tokenized_ds[sentence_id][\"input_ids\"][0][: self.input_len + 1]\n",
    "            )\n",
    "        return (\n",
    "            self.last_batched_sentence[idx % self.input_len],\n",
    "            self.sentence_labels[idx % self.input_len],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 112.75ex/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 400.79ex/s]\n"
     ]
    }
   ],
   "source": [
    "batched_train_ds = TinyStoriesDataset(train_ds, tokenizer, 3)\n",
    "batched_test_ds = TinyStoriesDataset(test_ds, tokenizer, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(batched_train_ds, batch_size=3)\n",
    "test_dataloader = DataLoader(batched_test_ds, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchedGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    class PatchedGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "        def __init__(self, config):\n",
    "            super().__init__(config)\n",
    "            self.conv1d = torch.nn.Conv1d(\n",
    "                config.hidden_size, config.hidden_size, kernel_size=3, padding=1\n",
    "            )\n",
    "\n",
    "        def forward(self, input_ids, **kwargs):\n",
    "            outputs = super().forward(input_ids, **kwargs)\n",
    "            hidden_states = outputs[0]\n",
    "            hidden_states = hidden_states.transpose(\n",
    "                1, 2\n",
    "            )  # Transpose to (batch_size, hidden_size, sequence_length)\n",
    "            conv_output = self.conv1d(hidden_states)\n",
    "            conv_output = conv_output.transpose(\n",
    "                1, 2\n",
    "            )  # Transpose back to (batch_size, sequence_length, hidden_size)\n",
    "            logits = self.lm_head(conv_output)\n",
    "            return logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Test Gradients: 100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "t() expects a tensor with <= 2 dimensions, but self is 3D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb Cell 30\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydvl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfluence\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m TorchTwiceDifferentiable\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m PatchedGPT2LMHeadModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mdpv/finetuned-gpt2-tiny\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ekfac_train_influences \u001b[39m=\u001b[39m compute_influences(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     TorchTwiceDifferentiable(model, torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mcross_entropy),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     training_data\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     test_data\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     influence_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mup\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     inversion_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mekfac\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     hessian_regularization\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     on_layers \u001b[39m=\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39mlm_head\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/influence_gpt2.ipynb#X41sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m mean_ekfac_train_influences \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(ekfac_train_influences\u001b[39m.\u001b[39mnumpy(), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/valuation/src/pydvl/influence/general.py:317\u001b[0m, in \u001b[0;36mcompute_influences\u001b[0;34m(differentiable_model, training_data, test_data, input_data, inversion_method, influence_type, hessian_regularization, progress, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39mif\u001b[39;00m test_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     test_data \u001b[39m=\u001b[39m deepcopy(training_data)\n\u001b[0;32m--> 317\u001b[0m influence_factors, _ \u001b[39m=\u001b[39m compute_influence_factors(\n\u001b[1;32m    318\u001b[0m     differentiable_model,\n\u001b[1;32m    319\u001b[0m     training_data,\n\u001b[1;32m    320\u001b[0m     test_data,\n\u001b[1;32m    321\u001b[0m     inversion_method,\n\u001b[1;32m    322\u001b[0m     hessian_perturbation\u001b[39m=\u001b[39;49mhessian_regularization,\n\u001b[1;32m    323\u001b[0m     progress\u001b[39m=\u001b[39;49mprogress,\n\u001b[1;32m    324\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    325\u001b[0m )\n\u001b[1;32m    327\u001b[0m \u001b[39mreturn\u001b[39;00m influence_type_registry[influence_type](\n\u001b[1;32m    328\u001b[0m     differentiable_model,\n\u001b[1;32m    329\u001b[0m     input_data,\n\u001b[1;32m    330\u001b[0m     influence_factors,\n\u001b[1;32m    331\u001b[0m     progress\u001b[39m=\u001b[39mprogress,\n\u001b[1;32m    332\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/valuation/src/pydvl/influence/general.py:126\u001b[0m, in \u001b[0;36mcompute_influence_factors\u001b[0;34m(model, training_data, test_data, inversion_method, hessian_perturbation, progress, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    121\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to pre-allocate result tensor: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEvaluate all resulting tensor and concatenate\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     rhs \u001b[39m=\u001b[39m cat(\u001b[39mlist\u001b[39m(test_grads()))\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m solve_hvp(\n\u001b[1;32m    127\u001b[0m     inversion_method,\n\u001b[1;32m    128\u001b[0m     model,\n\u001b[1;32m    129\u001b[0m     training_data,\n\u001b[1;32m    130\u001b[0m     rhs,\n\u001b[1;32m    131\u001b[0m     hessian_perturbation\u001b[39m=\u001b[39;49mhessian_perturbation,\n\u001b[1;32m    132\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    133\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/valuation/src/pydvl/influence/inversion.py:68\u001b[0m, in \u001b[0;36msolve_hvp\u001b[0;34m(inversion_method, model, training_data, b, hessian_perturbation, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msolve_hvp\u001b[39m(\n\u001b[1;32m     39\u001b[0m     inversion_method: InversionMethod,\n\u001b[1;32m     40\u001b[0m     model: TwiceDifferentiable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     46\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InverseHvpResult:\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m    Finds \\( x \\) such that \\( Ax = b \\), where \\( A \\) is the hessian of the model,\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m    and \\( b \\) a vector. Depending on the inversion method, the hessian is either\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m            and a dictionary containing information about the inversion process.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m InversionRegistry\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m     69\u001b[0m         inversion_method,\n\u001b[1;32m     70\u001b[0m         model,\n\u001b[1;32m     71\u001b[0m         training_data,\n\u001b[1;32m     72\u001b[0m         b,\n\u001b[1;32m     73\u001b[0m         hessian_perturbation\u001b[39m=\u001b[39;49mhessian_perturbation,\n\u001b[1;32m     74\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     75\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/valuation/src/pydvl/influence/inversion.py:204\u001b[0m, in \u001b[0;36mInversionRegistry.call\u001b[0;34m(cls, inversion_method, model, training_data, b, hessian_perturbation, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[1;32m    178\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    185\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InverseHvpResult:\n\u001b[1;32m    186\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m    Call a registered function with the provided parameters.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m            about the inversion process.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget(\u001b[39mtype\u001b[39;49m(model), inversion_method)(\n\u001b[1;32m    205\u001b[0m         model, training_data, b, hessian_perturbation, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/valuation/src/pydvl/influence/inversion.py:157\u001b[0m, in \u001b[0;36mInversionRegistry.register.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/valuation/src/pydvl/influence/torch/torch_differentiable.py:792\u001b[0m, in \u001b[0;36msolve_ekfac\u001b[0;34m(model, training_data, b, hessian_perturbation, update_diag, full_calculation, on_layers)\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m on_layers:\n\u001b[1;32m    786\u001b[0m         active_layers\u001b[39m.\u001b[39madd_layer(\n\u001b[1;32m    787\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (name, \u001b[39mstr\u001b[39m(module)),\n\u001b[1;32m    788\u001b[0m             LayerCollection\u001b[39m.\u001b[39m_module_to_layer(module),\n\u001b[1;32m    789\u001b[0m         )\n\u001b[0;32m--> 792\u001b[0m hessian_repr: PMatEKFAC \u001b[39m=\u001b[39m FIM(\n\u001b[1;32m    793\u001b[0m     model\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    794\u001b[0m     training_data,\n\u001b[1;32m    795\u001b[0m     representation\u001b[39m=\u001b[39;49mPMatEKFAC,\n\u001b[1;32m    796\u001b[0m     n_output\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    797\u001b[0m     function\u001b[39m=\u001b[39;49mbatch_loss_fn,\n\u001b[1;32m    798\u001b[0m     \u001b[39m# HACK: in order to pass the correct function,\u001b[39;49;00m\n\u001b[1;32m    799\u001b[0m     \u001b[39m# we need to set this to \"regression\", even though we are doing classification.\u001b[39;49;00m\n\u001b[1;32m    800\u001b[0m     \u001b[39m# Maybe open issue in nngeometry\u001b[39;49;00m\n\u001b[1;32m    801\u001b[0m     variant\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mregression\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    802\u001b[0m     layer_collection\u001b[39m=\u001b[39;49mactive_layers,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39mif\u001b[39;00m update_diag:\n\u001b[1;32m    806\u001b[0m     hessian_repr\u001b[39m.\u001b[39mupdate_diag(training_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pydvl_env/lib/python3.9/site-packages/nngeometry/metrics.py:168\u001b[0m, in \u001b[0;36mFIM\u001b[0;34m(model, loader, representation, n_output, variant, device, function, layer_collection)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m    164\u001b[0m generator \u001b[39m=\u001b[39m Jacobian(layer_collection\u001b[39m=\u001b[39mlayer_collection,\n\u001b[1;32m    165\u001b[0m                      model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    166\u001b[0m                      function\u001b[39m=\u001b[39mfunction_fim,\n\u001b[1;32m    167\u001b[0m                      n_output\u001b[39m=\u001b[39mn_output)\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m representation(generator\u001b[39m=\u001b[39;49mgenerator, examples\u001b[39m=\u001b[39;49mloader)\n",
      "File \u001b[0;32m~/miniconda3/envs/pydvl_env/lib/python3.9/site-packages/nngeometry/object/pspace.py:623\u001b[0m, in \u001b[0;36mPMatEKFAC.__init__\u001b[0;34m(self, generator, data, examples)\u001b[0m\n\u001b[1;32m    620\u001b[0m evecs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    621\u001b[0m diags \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m--> 623\u001b[0m kfac_blocks \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39;49mget_kfac_blocks(examples)\n\u001b[1;32m    624\u001b[0m \u001b[39mfor\u001b[39;00m layer_id, layer \u001b[39min\u001b[39;00m \\\n\u001b[1;32m    625\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator\u001b[39m.\u001b[39mlayer_collection\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    626\u001b[0m     a, g \u001b[39m=\u001b[39m kfac_blocks[layer_id]\n",
      "File \u001b[0;32m~/miniconda3/envs/pydvl_env/lib/python3.9/site-packages/nngeometry/generator/jacobian/__init__.py:248\u001b[0m, in \u001b[0;36mJacobian.get_kfac_blocks\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    245\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39md)\u001b[39m.\u001b[39mview(bs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_output) \\\n\u001b[1;32m    246\u001b[0m         \u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    247\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mi_output \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_output):\n\u001b[0;32m--> 248\u001b[0m         output\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    249\u001b[0m         \u001b[39m# retain_graph = self.i_output < self.n_output - 1\u001b[39;00m\n\u001b[1;32m    250\u001b[0m         \u001b[39m# torch.autograd.grad(output[self.i_output], [inputs],\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[39m#                     retain_graph=retain_graph,\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[39m#                     only_inputs=True)\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39mfor\u001b[39;00m layer_id \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_collection\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/miniconda3/envs/pydvl_env/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pydvl_env/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pydvl_env/lib/python3.9/site-packages/nngeometry/generator/jacobian/__init__.py:630\u001b[0m, in \u001b[0;36mJacobian._add_hooks.<locals>._hook_x.<locals>.<lambda>\u001b[0;34m(g_o)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_hook_x\u001b[39m(mod, i, o):\n\u001b[1;32m    629\u001b[0m     hook_x(mod, i)\n\u001b[0;32m--> 630\u001b[0m     o\u001b[39m.\u001b[39mregister_hook(\u001b[39mlambda\u001b[39;00m g_o: hook_gy(mod, g_o))\n",
      "File \u001b[0;32m~/miniconda3/envs/pydvl_env/lib/python3.9/site-packages/nngeometry/generator/jacobian/__init__.py:687\u001b[0m, in \u001b[0;36mJacobian._hook_compute_kfac_blocks\u001b[0;34m(self, mod, gy)\u001b[0m\n\u001b[1;32m    685\u001b[0m block \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocks[layer_id]\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m mod_class \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mLinear\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mConv2d\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m--> 687\u001b[0m     FactoryMap[layer\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m]\u001b[39m.\u001b[39;49mkfac_gg(block[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m    688\u001b[0m         mod, layer, x, gy)\n\u001b[1;32m    689\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mi_output \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    690\u001b[0m         \u001b[39m# do this only once if n_output > 1\u001b[39;00m\n\u001b[1;32m    691\u001b[0m         FactoryMap[layer\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m]\u001b[39m.\u001b[39mkfac_xx(block[\u001b[39m0\u001b[39m],\n\u001b[1;32m    692\u001b[0m             mod, layer, x, gy)\n",
      "File \u001b[0;32m~/miniconda3/envs/pydvl_env/lib/python3.9/site-packages/nngeometry/generator/jacobian/grads.py:96\u001b[0m, in \u001b[0;36mLinearJacobianFactory.kfac_gg\u001b[0;34m(cls, buffer, mod, layer, x, gy)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mkfac_gg\u001b[39m(\u001b[39mcls\u001b[39m, buffer, mod, layer, x, gy):\n\u001b[0;32m---> 96\u001b[0m     buffer\u001b[39m.\u001b[39madd_(torch\u001b[39m.\u001b[39mmm(gy\u001b[39m.\u001b[39;49mt(), gy))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: t() expects a tensor with <= 2 dimensions, but self is 3D"
     ]
    }
   ],
   "source": [
    "from pydvl.influence import compute_influences\n",
    "from pydvl.influence.torch import TorchTwiceDifferentiable\n",
    "\n",
    "model = PatchedGPT2LMHeadModel.from_pretrained(\"dpv/finetuned-gpt2-tiny\")\n",
    "\n",
    "ekfac_train_influences = compute_influences(\n",
    "    TorchTwiceDifferentiable(model, torch.nn.functional.cross_entropy),\n",
    "    training_data=train_dataloader,\n",
    "    test_data=test_dataloader,\n",
    "    influence_type=\"up\",\n",
    "    inversion_method=\"ekfac\",\n",
    "    hessian_regularization=0.1,\n",
    "    on_layers=[\"lm_head\"],\n",
    "    progress=True,\n",
    ")\n",
    "mean_ekfac_train_influences = np.mean(ekfac_train_influences.numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydvl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

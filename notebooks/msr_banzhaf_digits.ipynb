{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banzhaf Values for data valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Shapley values notebook, this notebook shows how to compute Banzhaf values using pyDVL.\n",
    "Banzhaf semivalues achieve a higher safety margin when compared with other methods like Shapley or LOO semivalues.\n",
    "\n",
    "This notebook compares two sampling techniques for computing Banzhaf semivalues, a simple Monte Carlo sampling technique\n",
    "and another sampling technique using the MSR (Maximum Sample Reuse) principle. We also compare both techniques in this notebook.\n",
    "\n",
    "In this notebook, we will use a CNN to classify handwritten digits from the [scikit-learn toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html#optical-recognition-of-handwritten-digits-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing the main libraries and setting some defaults.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "plt.ioff()  # Prevent jupyter from automatically plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 6)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10\n",
    "plt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\n",
    "plt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\n",
    "\n",
    "is_CI = os.environ.get(\"CI\")\n",
    "random_state = 24\n",
    "n_jobs = 16\n",
    "random.seed(random_state)\n",
    "is_CI = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the following functions from pyDVL. The main entry point is the function `compute_banzhaf_semivalues()`.\n",
    "In order to use it we need the classes [Dataset](../../api/pydvl/utils/dataset/#pydvl.utils.dataset.Dataset), [Utility](../../api/pydvl/utils/utility/#pydvl.utils.utility.Utility) and [Scorer](../../api/pydvl/utils/score/#pydvl.utils.score.Scorer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from pydvl.reporting.plots import plot_shapley\n",
    "from support.banzhaf import load_digits_dataset\n",
    "from pydvl.value import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and grouping the dataset\n",
    "\n",
    "pyDVL provides a support function for this notebook, `load_digits_dataset()`, which downloads the data and prepares it for usage. The data consists of greyscale images of shape 8x8 pixels with 16 shades of grey. These images contain handwritten digits from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data, _, test_data = load_digits_dataset(\n",
    "    test_size=0.3, random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# In CI we only use a subset of the training set\n",
    "training_data = list(training_data)\n",
    "if is_CI:\n",
    "    training_data[0] = training_data[0][:10]\n",
    "    training_data[1] = training_data[1][:10]\n",
    "    max_checks = 1\n",
    "else:\n",
    "    training_data[0] = training_data[0][:200]\n",
    "    training_data[1] = training_data[1][:200]\n",
    "    max_checks = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(4, 4))\n",
    "for i in range(4):\n",
    "    ax = axes[i % 2, i // 2]\n",
    "    ax.imshow(np.reshape(training_data[0][i], (8, 8)), cmap=\"grey\")\n",
    "    ax.set_xlabel(f\"Label: {training_data[1][i]}\")\n",
    "plt.suptitle(\"Example images from the dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test data are then used to instantiate a [Dataset](../../api/pydvl/utils/dataset/#pydvl.utils.dataset.Dataset) object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(*training_data, *test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the utility and computing Banzhaf semivalues\n",
    "\n",
    "Now we can calculate the contribution of each group to the model performance.\n",
    "\n",
    "As a model, we use scikit-learn's [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), but pyDVL can work with any model from sklearn, xgboost or lightgbm. More precisely, any model that implements the protocol [pydvl.utils.types.SupervisedModel](../../api/pydvl/utils/types/#pydvl.utils.types.SupervisedModel), which is just the standard sklearn interface of `fit()`,`predict()` and `score()` can be used to construct the utility.\n",
    "\n",
    "The third and final component is the scoring function. It can be anything like accuracy or $R^2$, and is set with a string from the [standard sklearn scoring methods](https://scikit-learn.org/stable/modules/model_evaluation.html). Please refer to that documentation on information on how to define your own scoring function.\n",
    "\n",
    "We group dataset, model and scoring function into an instance of [Utility](../../api/pydvl/utils/utility/#pydvl.utils.utility.Utility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from numpy.typing import NDArray\n",
    "from pydvl.utils.types import SupervisedModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class TorchCNNModel(SupervisedModel):\n",
    "    def __init__(self, lr=0.001, epochs=40, batch_size=32):\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                out_channels=8, in_channels=1, kernel_size=(3, 3), padding=\"same\"\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                out_channels=4, in_channels=8, kernel_size=(3, 3), padding=\"same\"\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.Linear(in_features=32, out_features=10),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.model.to(device)\n",
    "\n",
    "    def fit(self, x: NDArray, y: NDArray) -> None:\n",
    "        torch_dataset = TensorDataset(\n",
    "            torch.tensor(\n",
    "                np.reshape(x, (x.shape[0], 1, 8, 8)), dtype=torch.float, device=device\n",
    "            ),\n",
    "            torch.tensor(y, device=device),\n",
    "        )\n",
    "        torch_dataloader = DataLoader(torch_dataset, batch_size=self.batch_size)\n",
    "        for epoch in range(self.epochs):\n",
    "            for features, labels in torch_dataloader:\n",
    "                pred = self.model(features)\n",
    "                loss = self.loss(pred, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def predict(self, x: NDArray) -> NDArray:\n",
    "        pred = self.model(\n",
    "            torch.tensor(\n",
    "                np.reshape(x, (x.shape[0], 1, 8, 8)), dtype=torch.float, device=device\n",
    "            )\n",
    "        )\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        return pred.cpu().numpy()\n",
    "\n",
    "    def score(self, x: NDArray, y: NDArray) -> float:\n",
    "        pred = self.predict(x)\n",
    "        acc = accuracy_score(pred, y)\n",
    "        return acc\n",
    "\n",
    "    def get_params(self, deep: bool = False):\n",
    "        return {\"lr\": self.lr, \"epochs\": self.epochs}\n",
    "\n",
    "\n",
    "model = TorchCNNModel(lr=0.001, epochs=40)\n",
    "model.fit(x=training_data[0], y=training_data[1])\n",
    "print(f\"Train Accuracy: {model.score(x=training_data[0], y=training_data[1]):.3f}\")\n",
    "print(f\"Test Accuracy: {model.score(x=test_data[0], y=test_data[1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute regular Banzhaf semivalue\n",
    "utility = Utility(\n",
    "    model=TorchCNNModel(),\n",
    "    data=dataset,\n",
    "    scorer=Scorer(\"accuracy\", default=0.0, range=(0, 1)),\n",
    ")\n",
    "values = compute_banzhaf_semivalues(\n",
    "    utility, done=MaxChecks(max_checks), n_jobs=n_jobs, progress=True\n",
    ")\n",
    "values.sort(key=\"value\")\n",
    "df = values.to_dataframe(column=\"data_value\", use_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the returned dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the results. In the next cell we will take the 30 images with the lowest score and plot their values with 95% Normal confidence intervals. Keep in mind that Monte Carlo Banzhaf is typically very noisy, and it can take many steps to arrive at a clean estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "low_dvl = df.iloc[:30].copy()\n",
    "low_dvl.index = low_dvl.index.map(str)\n",
    "plot_shapley(\n",
    "    low_dvl,\n",
    "    level=0.05,\n",
    "    title=\"Images with low values\",\n",
    "    xlabel=\"Image\",\n",
    "    ylabel=\"Banzhaf value\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on anomalous data\n",
    "\n",
    "An interesting usecase for data valuation is finding anomalous data. Maybe some of the data is really noisy or has been mislabeled. To simulate this, we will change some of the labels of our dataset and add noise to some others. Intuitively, these anomalous data points should then have a lower value.\n",
    "\n",
    "To evaluate this, let us first check the average value of the first 10 data points, as these will be the ones that we modify. Currently, these are the 10 data points with the highest semivalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "high_dvl = df.iloc[-10:].copy()\n",
    "print(f\"Average value of first 10 data points: {high_dvl['data_value'].mean()}\")\n",
    "print(f\"Exact values:\\n{high_dvl['data_value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first 5 images, we will falsify their label, for images 6-10, we will add some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_anomalous = training_data[0].copy()\n",
    "y_train_anomalous = training_data[1].copy()\n",
    "anomalous_indices = high_dvl.index.map(int).values[:10]\n",
    "\n",
    "# Set label of first 5 images to 0\n",
    "y_train_anomalous[high_dvl.index.map(int).values[:5]] = 0\n",
    "\n",
    "# Add noise to images 6-10\n",
    "indices = high_dvl.index.values[5:10].astype(int)\n",
    "current_images = x_train_anomalous[indices]\n",
    "noisy_images = current_images + 0.5 * np.random.randn(*current_images.shape)\n",
    "noisy_images[noisy_images < 0] = 0.0\n",
    "noisy_images[noisy_images > 1] = 1.0\n",
    "x_train_anomalous[indices] = noisy_images\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(9, 5))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(np.reshape(current_images[i], (8, 8)), cmap=\"grey\")\n",
    "    axes[1, i].imshow(np.reshape(noisy_images[i], (8, 8)), cmap=\"grey\")\n",
    "    axes[0, i].set_xlabel(f\"Original: {training_data[1][indices[i]]}\")\n",
    "    axes[1, i].set_xlabel(f\"Noisy: {training_data[1][indices[i]]}\")\n",
    "plt.suptitle(\"Original and noisy versions of images 6-10\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_dataset = Dataset(\n",
    "    x_train=x_train_anomalous,\n",
    "    y_train=y_train_anomalous,\n",
    "    x_test=test_data[0],\n",
    "    y_test=test_data[1],\n",
    ")\n",
    "\n",
    "anomalous_utility = Utility(\n",
    "    model=TorchCNNModel(),\n",
    "    data=anomalous_dataset,\n",
    "    scorer=Scorer(\"accuracy\", default=0.0, range=(0, 1)),\n",
    ")\n",
    "anomalous_values = compute_banzhaf_semivalues(\n",
    "    anomalous_utility, done=MaxChecks(max_checks), n_jobs=n_jobs, progress=True\n",
    ")\n",
    "anomalous_values.sort(key=\"value\")\n",
    "anomalous_df = anomalous_values.to_dataframe(column=\"data_value\", use_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now take a look at the low-value images and check how many of our anomalous images are part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "plot_data = anomalous_df.loc[anomalous_indices].copy()\n",
    "plot_data[\"original_data_value\"] = df.loc[anomalous_indices][\"data_value\"]\n",
    "plot_data[\"original_data_value_stderr\"] = df.loc[anomalous_indices][\"data_value_stderr\"]\n",
    "plot_data.index = plot_data.index.map(str)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "yerr = norm.ppf(1 - 0.05 / 2) * plot_data[\"data_value_stderr\"]\n",
    "original_yerr = norm.ppf(1 - 0.05 / 2) * plot_data[\"original_data_value_stderr\"]\n",
    "ax.errorbar(\n",
    "    x=plot_data.index,\n",
    "    y=plot_data[\"data_value\"],\n",
    "    yerr=yerr,\n",
    "    fmt=\"o\",\n",
    "    capsize=6,\n",
    "    color=\"red\",\n",
    "    label=\"Anomalous\",\n",
    ")\n",
    "ax.errorbar(\n",
    "    x=plot_data.index,\n",
    "    y=plot_data[\"original_data_value\"],\n",
    "    yerr=original_yerr,\n",
    "    fmt=\"o\",\n",
    "    capsize=6,\n",
    "    color=\"green\",\n",
    "    label=\"Original\",\n",
    ")\n",
    "ax.set_xlabel(\"Image\")\n",
    "ax.set_ylabel(\"Banzhaf Value\")\n",
    "ax.set_title(\"Data valuation scores of anomalous images\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in this figure, the valuation of the data points has decreased significantly by adding noise or falsifying their labels.\n",
    "This shows the potential of using Banzhaf values or other data valuation methods to detect mislabeled data points or noisy input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Average value of original data points: {plot_data['original_data_value'].mean()}\"\n",
    ")\n",
    "print(\n",
    "    f\"Average value of modified, anomalous data points: {plot_data['data_value'].mean()}\"\n",
    ")\n",
    "print(\n",
    "    \"For reference, these are the average data values of all data points used for training (anomalous):\"\n",
    ")\n",
    "print(anomalous_df.mean())\n",
    "print(\"These are the average data values of all points (original data):\")\n",
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Sample Reuse Banzhaf\n",
    "\n",
    "Now, we want to use a method called Maximum Sample Reuse (MSR) which reuses samples for updating the Banzhaf values.\n",
    "The method is introduced by Wang et al. in the paper *Data Banzhaf: A Robust Data Valuation Framework for Machine Learning*.\n",
    "It promises a higher sample efficiency by using a principle called Maximum Sample Reuse for sampling and updating semivalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSR Banzhaf semivalue\n",
    "utility = Utility(\n",
    "    model=TorchCNNModel(),\n",
    "    data=dataset,\n",
    "    scorer=Scorer(\"accuracy\", default=0.0, range=(0, 1)),\n",
    ")\n",
    "values = compute_msr_banzhaf_semivalues(\n",
    "    utility,\n",
    "    done=RankStability(0.0001),\n",
    "    n_jobs=n_jobs,\n",
    "    progress=True,\n",
    ")\n",
    "values.sort(key=\"value\")\n",
    "msr_df = values.to_dataframe(column=\"data_value\", use_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dvl = df.iloc[-30:]\n",
    "high_dvl.index = high_dvl.index.map(str)\n",
    "ax = plot_shapley(\n",
    "    high_dvl,\n",
    "    title=\"Images with high values\",\n",
    "    xlabel=\"Image\",\n",
    "    ylabel=\"Banzhaf Value\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dvl = df.iloc[:30]\n",
    "low_dvl.index = low_dvl.index.map(str)\n",
    "ax = plot_shapley(\n",
    "    low_dvl,\n",
    "    title=\"Images with low values\",\n",
    "    xlabel=\"Image\",\n",
    "    ylabel=\"Banzhaf value\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare convergence speed of Banzhaf and MSR Banzhaf Values\n",
    "\n",
    "While the conventional Banzhaf values algorithm evaluates the utility twice to do one update of the Banzhaf values, the Maximum Samples Reuse (MSR) algorithm promises higher sample efficiency because it updates multiple samples per one evaluation of the utility. This part of the notebook takes a look at the convergence speed of the algorithms compared with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_CI:\n",
    "    max_checks = 1\n",
    "    moving_avg = 1\n",
    "else:\n",
    "    max_checks = 10000\n",
    "    moving_avg = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility = Utility(\n",
    "    model=TorchCNNModel(),\n",
    "    data=dataset,\n",
    "    scorer=Scorer(\"accuracy\", default=0.0, range=(0, 1)),\n",
    ")\n",
    "\n",
    "\n",
    "def get_semivalues_and_history(\n",
    "    sampler_t, num_checks=max_checks, num_jobs=n_jobs, progress=True\n",
    "):\n",
    "    _history = HistoryDeviation(n_steps=num_checks, rtol=1e-9)\n",
    "    if sampler_t == MSRSampler:\n",
    "        _values = compute_msr_banzhaf_semivalues(\n",
    "            utility,\n",
    "            sampler_t=sampler_t,\n",
    "            done=MaxChecks(num_checks + 2) | _history,\n",
    "            n_jobs=num_jobs,\n",
    "            progress=progress,\n",
    "        )\n",
    "    else:\n",
    "        _values = compute_banzhaf_semivalues(\n",
    "            utility,\n",
    "            done=MaxChecks(num_checks + 2) | _history,\n",
    "            n_jobs=num_jobs,\n",
    "            progress=progress,\n",
    "        )\n",
    "    return _history, _values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Permutation Sampling Banzhaf semivalues\n",
    "history_permutation, permutation_values = get_semivalues_and_history(PermutationSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSR Banzhaf values\n",
    "history_msr, msr_values = get_semivalues_and_history(MSRSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UniformSampler\n",
    "history_uniform, uniform_values = get_semivalues_and_history(UniformSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AntitheticSampler\n",
    "history_antithetic, antithetic_values = get_semivalues_and_history(AntitheticSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomHierarchicalSampler\n",
    "history_random, random_values = get_semivalues_and_history(RandomHierarchicalSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare convergence speed of both methods\n",
    "import numpy as np\n",
    "\n",
    "names = [\"Permutation\", \"MSR\", \"Uniform\", \"Antithetic\", \"RandomHierarchical\"]\n",
    "all_values = [\n",
    "    history_permutation._memory,\n",
    "    history_msr._memory,\n",
    "    history_uniform._memory,\n",
    "    history_antithetic._memory,\n",
    "    history_random._memory,\n",
    "]\n",
    "distances = [[] for _ in names]\n",
    "moving_avgs = []\n",
    "\n",
    "for sampler_id, name in enumerate(names):\n",
    "    for iteration in range(max_checks):\n",
    "        abs_dist = np.abs(\n",
    "            all_values[sampler_id][:, iteration]\n",
    "            - all_values[sampler_id][:, iteration + 1]\n",
    "        )\n",
    "        if abs_dist.max() == 0.0:\n",
    "            distances[sampler_id].append(0.0)\n",
    "        else:\n",
    "            distances[sampler_id].append(np.mean(abs_dist[abs_dist > 0]))\n",
    "    moving_avgs.append(\n",
    "        np.convolve(\n",
    "            distances[sampler_id], np.ones(moving_avg) / moving_avg, mode=\"same\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "for sampler_id, name in enumerate(names):\n",
    "    ax.plot(list(range(max_checks)), moving_avgs[sampler_id], label=name)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Mean semivalue change between iterations\")\n",
    "ax.set_title(\"Convergence speed of different samplers\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-5, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above visualizes the convergence speed of different samplers used for Banzhaf semivalue calculation.  \n",
    "It shows the average magnitude of how much the semivalues are updated in every step of the algorithm. \n",
    "\n",
    "As you can see, **MSR Banzhaf** converges much faster. \n",
    "After 1000 iterations (subsets tried), Monte Carlo Banzhaf has evaluated the marginal function about 5 times per data point (we are using 200 data points). \n",
    "For maximum sample reuse, the semivalue of each data point was updated 1000 times. Due to this, the values converge much quicker, which is one of the advantages of MSR sampling. \n",
    "\n",
    "MSR sampling does come at a cost, however, which we will visualize in the next plot. The semivalues that MSR converges to are less consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(\"Similarity of the ranking of Permutation Sampler and MSR Sampler semivalues:\")\n",
    "print(spearmanr(permutation_values.values, msr_values.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency of the semivalues\n",
    "\n",
    "In this part, we want to analyze how consistent the semivalues returned by the different samplers are.  \n",
    "To evaluate this, we will compute semivalues multiple times and check how many of the data points in the top and lowest 20% of valuation of the data overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_checks = [1e3, 2e3, 3e3] if not is_CI else [1, 2]\n",
    "num_retries = 3 if not is_CI else 2\n",
    "num_samplers = 5\n",
    "twenty_percent = training_data[0].shape[0] // 5\n",
    "if is_CI:\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "results = [[[] for i in range(num_samplers)] for _ in max_checks]\n",
    "pbar = tqdm(total=len(max_checks) * num_retries * 5)\n",
    "for check_index, max_check in enumerate(max_checks):\n",
    "    for retry in range(num_retries):\n",
    "        for sampler_index, sampler in enumerate(\n",
    "            [\n",
    "                PermutationSampler,\n",
    "                MSRSampler,\n",
    "                UniformSampler,\n",
    "                AntitheticSampler,\n",
    "                RandomHierarchicalSampler,\n",
    "            ]\n",
    "        ):\n",
    "            _, vals = get_semivalues_and_history(sampler, num_checks=max_check)\n",
    "            results[check_index][sampler_index].append(vals)\n",
    "            pbar.n = (\n",
    "                check_index * (num_samplers * num_retries)\n",
    "                + retry * num_samplers\n",
    "                + sampler_index\n",
    "                + 1\n",
    "            )\n",
    "            pbar.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results from experiments\n",
    "\n",
    "plot_results_top = [[] for _ in range(num_samplers)]\n",
    "plot_results_low = [[] for _ in range(num_samplers)]\n",
    "for check_index, _ in enumerate(max_checks):\n",
    "    for sampler_index, sampler in enumerate(\n",
    "        [\n",
    "            PermutationSampler,\n",
    "            MSRSampler,\n",
    "            UniformSampler,\n",
    "            AntitheticSampler,\n",
    "            RandomHierarchicalSampler,\n",
    "        ]\n",
    "    ):\n",
    "        value_list = results[check_index][sampler_index]\n",
    "        top_20 = None\n",
    "        lower_20 = None\n",
    "        for vals in value_list:\n",
    "            vals.sort(key=\"value\")\n",
    "            if top_20 is None:\n",
    "                top_20 = set(vals.indices[-twenty_percent:].tolist())\n",
    "                lower_20 = set(vals.indices[:twenty_percent].tolist())\n",
    "            else:\n",
    "                top_20 = top_20.intersection(\n",
    "                    set(vals.indices[-twenty_percent:].tolist())\n",
    "                )\n",
    "                lower_20 = lower_20.intersection(\n",
    "                    set(vals.indices[:twenty_percent].tolist())\n",
    "                )\n",
    "        plot_results_top[sampler_index].append(len(top_20))\n",
    "        plot_results_low[sampler_index].append(len(lower_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 8))\n",
    "for sampler_index, name in enumerate(names):\n",
    "    axes[0].plot(max_checks, plot_results_top[sampler_index], label=name)\n",
    "    axes[1].plot(max_checks, plot_results_low[sampler_index], label=name)\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[0].set_xlabel(\"Number of iterations\")\n",
    "axes[1].set_xlabel(\"Number of iterations\")\n",
    "axes[0].set_ylabel(f\"Number of common points \\nin top 20% in {num_retries} runs\")\n",
    "axes[1].set_ylabel(f\"Number of common points \\nin lower 20% in {num_retries} runs\")\n",
    "fig.suptitle(f\"Evaluation of consistency of samplers (Max value: {twenty_percent})\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "MSR"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e000971326892723e7f31ded70802f690c31c3620f59a0f99e594aaee3047ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

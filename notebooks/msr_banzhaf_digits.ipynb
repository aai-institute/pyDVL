{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banzhaf Semi-values for data valuation\n",
    "\n",
    "This notebook showcases [Data Banzhaf: A Robust Data Valuation Framework for Machine Learning](https://proceedings.mlr.press/v206/wang23e.html) by Wang, and Jia.\n",
    "\n",
    "Computing Banzhaf semi-values using pyDVL follows basically the same procedure as all other semi-value-based methods like Shapley values. However, Data-Banzhaf tends to be more robust to stochasticity in the training process than other semi-values. A property that we study here.\n",
    "\n",
    "Additionally, we compare two sampling techniques: the standard permutation-based Monte Carlo sampling, and the so-called MSR (Maximum Sample Reuse) principle.\n",
    "\n",
    "In order to highlight the strengths of Data-Banzhaf, we require a stochastic model. For this reason, we use a CNN to classify handwritten digits from the [scikit-learn toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html#optical-recognition-of-handwritten-digits-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import Type\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from support.common import filecache\n",
    "from support.datasets import load_digits_dataset\n",
    "\n",
    "from pydvl.reporting.plots import plot_result_errors\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ioff()  # Prevent jupyter from automatically plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 6)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"xtick.labelsize\"] = 10\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10\n",
    "plt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\n",
    "plt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\n",
    "\n",
    "is_CI = os.environ.get(\"CI\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "random_state = 42\n",
    "\n",
    "n_jobs = 16\n",
    "n_epochs = 24\n",
    "batch_size = 64\n",
    "random.seed(random_state)\n",
    "torch.manual_seed(random_state);"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The dataset\n",
    "\n",
    "The data consists of ~1800 grayscale images of 8x8 pixels with 16 shades of gray. These images contain handwritten digits from 0 to 9. The helper function `load_digits_dataset()` downloads and prepares it for usage returning two [Datasets][pydvl.valuation.dataset.Dataset]."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train, test = load_digits_dataset(\n",
    "    train_size=0.7, random_state=random_state, device=device\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(4):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(train.data().x[i].reshape((8, 8)).cpu(), cmap=\"gray\")\n",
    "    ax.set_xlabel(f\"Label: {train.data().y[i]}\")\n",
    "plt.suptitle(\"Example images from the dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating the utility and computing Banzhaf semi-values\n",
    "\n",
    "Now we can calculate the contribution of each training sample to the model performance. We use a simple CNN written in torch, and wrapped into an object to convert numpy arrays into tensors (as of v0.9.2 valuation methods in pyDVL work only with numpy arrays). Note that any model that implements the protocol [SupervisedModel][pydvl.utils.types.SupervisedModel], which is just the standard sklearn interface of `fit()`,`predict()` and `score()` can be used to construct the utility."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reduce computation time in CI\n",
    "if is_CI:\n",
    "    train = train[:10]\n",
    "    test = test[:10]\n",
    "    n_jobs = 1\n",
    "    n_epochs = 1\n",
    "    batch_size = 1\n",
    "    filecache = lambda x: lambda y: y  # passthrough"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from support.banzhaf import SimpleCNN, TorchClassifierModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TorchClassifierModel(\n",
    "    model_factory=SimpleCNN,\n",
    "    lr=0.01,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    ")\n",
    "model.fit(*train.data())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Training accuracy: {model.score(*train.data()):.3f}\")\n",
    "print(f\"Test accuracy: {model.score(*test.data()):.3f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As with all other model-based valuation methods, for Data Banzhaf we need a scoring function to measure performance of the model over the test set. We will use accuracy, but it can be anything, like e.g. $R^2$, using strings from the [standard sklearn scoring methods](https://scikit-learn.org/stable/modules/model_evaluation.html), passed to [SupervisedScorer][pydvl.valuation.scorers.supervised.SupervisedScorer].\n",
    "\n",
    "We group our torch model and the scoring function into an instance of [ModelUtility][pydvl.valuation.utility.ModelUtility]."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pydvl.valuation.samplers import PermutationSampler, RelativeTruncation\n",
    "from pydvl.valuation.scorers import SupervisedScorer\n",
    "from pydvl.valuation.stopping import MinUpdates\n",
    "from pydvl.valuation.utility import ModelUtility\n",
    "\n",
    "accuracy_over_test_set = SupervisedScorer(\n",
    "    model, test_data=test, default=0.0, range=(0, 1)\n",
    ")\n",
    "\n",
    "utility = ModelUtility(model=model, scorer=accuracy_over_test_set)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In order to compute the Banzhaf semi-values, we use [BanzhafValuation][pydvl.valuation.methods.banzhaf.BanzhafValuation], which also requires choosing a sampler and a stopping criterion.\n",
    "\n",
    "We use the standard [PermutationSampler][pydvl.valuation.samplers.permutation.PermutationSampler], and choose to stop computation using the [MinUpdates][pydvl.valuation.stopping.MinUpdates] stopping criterion, which terminates after a fixed number of value updates. This is a simple stopping criterion, but it is not very efficient. We will later compare it to [RankCorrelation][pydvl.valuation.stopping.RankCorrelation], which terminates after the change in Spearman correlation between two successive iterations is below a certain threshold.\n",
    "\n",
    "We also define a relative [TruncationPolicy][pydvl.valuation.samplers.truncation.TruncationPolicy], which is a policy used to early-stop computation of marginal values in permutations, once the utility is close to the total utility. This is a heuristic to speed up computation introduced in the Data-Shapley paper called Truncated Monte Carlo Shapley. Note how we tell it to wait until at least 30% of every permutation has been processed in order to start evaluation. This is to ensure that noise doesn't stop the computation too early."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "truncation = RelativeTruncation(rtol=0.05, burn_in_fraction=0.3)\n",
    "sampler = PermutationSampler(truncation=truncation)\n",
    "stopping = MinUpdates(100)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if is_CI:\n",
    "    from pydvl.valuation.stopping import MaxChecks\n",
    "\n",
    "    stopping = MaxChecks(1)  # Stop after 1 utility evaluation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now instantiate and fit the valuation. Note how parallelization is just a matter of using joblib's context manager `parallel_config` in order to set the number of jobs."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from joblib import parallel_config\n",
    "\n",
    "from pydvl.valuation.methods import BanzhafValuation\n",
    "\n",
    "valuation = BanzhafValuation(utility, sampler=sampler, is_done=stopping, progress=True)\n",
    "\n",
    "# filecache is a very simple wrapper not intended for production code\n",
    "cached_fit = filecache(\"digits_banzhaf_result.pkl\")(lambda d: valuation.fit(d).result)\n",
    "with parallel_config(n_jobs=n_jobs):\n",
    "    result = cached_fit(train)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results object is of type [ValuationResult][pydvl.valuation.result.ValuationResult], and contains values, variances and number of updates of the Monte Carlo estimates. It can be indexed, sliced and copied in natural ways, as we illustrate below."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us plot the results. In the next cell we will take the 10 images with the lowest score and plot their values with 95% Normal confidence intervals. Keep in mind that Permutation Monte Carlo Banzhaf is typically very noisy, and it can take many steps to arrive at a clean estimate."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bottom = result.sort()[:10]\n",
    "plot_result_errors(\n",
    "    bottom,\n",
    "    level=0.05,\n",
    "    title=\"Images with low values\",\n",
    "    xlabel=\"Image\",\n",
    "    ylabel=\"Banzhaf value\",\n",
    ")\n",
    "plt.show();"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation on anomalous data\n",
    "\n",
    "An interesting use-case for data valuation is finding anomalous data. Maybe some of the data is really noisy or has been mislabeled. To simulate this for our dataset, we will change some of the labels and add noise to some images. Intuitively, these anomalous data points should then have a lower value.\n",
    "\n",
    "To evaluate this, let us first check the average value of the 10 data points with the highest value, as these will be the ones that we modify:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top = result.sort()[-10:]\n",
    "print(f\"Average value of top 10 data points: {top.values.mean():.4f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For the first 5 images, we will falsify their label, for images 6-10, we will add some noise."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_train_anomalous = train.data().x.clone()\n",
    "y_train_anomalous = train.data().y.clone()\n",
    "anomalous_indices = top.indices\n",
    "\n",
    "# Change the label of the first 5 images\n",
    "y_train_anomalous[anomalous_indices[:5]] = torch.remainder(\n",
    "    y_train_anomalous[anomalous_indices[:5]] + 1, 10\n",
    ")\n",
    "# Add noise to images 6-10\n",
    "current_images = x_train_anomalous[anomalous_indices[5:10]]\n",
    "noisy_images = current_images + 0.5 * torch.randn_like(current_images)\n",
    "noisy_images[noisy_images < 0] = 0.0\n",
    "noisy_images[noisy_images > 1] = 1.0\n",
    "x_train_anomalous[anomalous_indices[5:10]] = noisy_images"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(9, 5))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(current_images[i].reshape((8, 8)).cpu(), cmap=\"gray\")\n",
    "    axes[1, i].imshow(noisy_images[i].reshape((8, 8)).cpu(), cmap=\"gray\")\n",
    "    axes[0, i].set_xlabel(f\"Original: {train.data().y[anomalous_indices[5 + i]]}\")\n",
    "    axes[1, i].set_xlabel(f\"Noisy: {y_train_anomalous[anomalous_indices[5 + i]]}\")\n",
    "plt.suptitle(\"Original and noisy versions of images 6-10\")\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pydvl.valuation.dataset import Dataset\n",
    "\n",
    "anomalous_dataset = Dataset(x=x_train_anomalous, y=y_train_anomalous)\n",
    "\n",
    "# Note that we reuse the same stopping criterion. fit() resets it, but\n",
    "# to be sure we can always call stopping.reset()\n",
    "anomalous_valuation = BanzhafValuation(\n",
    "    utility, sampler=sampler, is_done=stopping.reset(), progress=True\n",
    ")\n",
    "\n",
    "cached_fit = filecache(\"digits_banzhaf_anomalous_result.pkl\")(\n",
    "    lambda d: valuation.fit(d).result\n",
    ")\n",
    "with parallel_config(n_jobs=n_jobs):\n",
    "    new_result = cached_fit(anomalous_dataset)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us now look at how the value has changed for the images that we manipulated:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 6))\n",
    "plot_result_errors(result[anomalous_indices], ax=ax, legend_label=\"Original\")\n",
    "plot_result_errors(\n",
    "    new_result[anomalous_indices],\n",
    "    ax=ax,\n",
    "    xlabel=\"Image\",\n",
    "    ylabel=\"Banzhaf Value\",\n",
    "    legend_label=\"Modified\",\n",
    ")\n",
    "ax.set_title(\"Data values of anomalous images\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=60)\n",
    "plt.show();"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As can be seen in this figure, the valuation of the data points has decreased significantly by adding noise or falsifying their labels. This shows the potential of using Banzhaf values or other data valuation methods to detect mislabeled data points or noisy input data."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\n",
    "    f\"Average value of intervened indices before modification: \"\n",
    "    f\"{result[anomalous_indices].values.mean():.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Average value of intervened indices after modification: \"\n",
    "    f\"{new_result[anomalous_indices].values.mean():.4f}\"\n",
    ")\n",
    "print(\"For reference:\")\n",
    "print(f\"Average value of all original indices: {result.values.mean():.4f}\")\n",
    "print(\n",
    "    \"Average value of all indices in the modified dataset:\"\n",
    "    f\"{new_result.values.mean():.4f}\"\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Maximum Sample Reuse Banzhaf\n",
    "\n",
    "Despite the previous results already being useful, we had to retrain the model a number of times and yet the variance of the value estimates was high. This has consequences for the stability of the top-k ranking of points, which decreases the applicability of the method. We will now use a different sampling method called Maximum Sample Reuse (MSR) which reuses every sample for updating the Banzhaf values. The method was introduced by the authors of Data-Banzhaf and is much more sample-efficient, as we will show."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "All that is required to compute the values with MSR is using [MSRSampler][pydvl.valuation.samplers.msr.MSRSampler] as sampler.\n",
    "\n",
    "Because values converge much faster, we can use a better stopping criterion. Instead of fixing the number of value updates with [MinUpdates][pydvl.valuation.stopping.MinUpdates], we use [RankCorrelation][pydvl.valuation.stopping.RankCorrelation] to stop when the change in Spearman correlation between the ranking of two successive iterations is below a threshold. Despite the much stricter stopping criterion, fitting the Banzhaf values with the MSR sampler is much faster."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pydvl.valuation.methods import MSRBanzhafValuation\n",
    "from pydvl.valuation.samplers import MSRSampler\n",
    "from pydvl.valuation.stopping import RankCorrelation\n",
    "\n",
    "valuation = MSRBanzhafValuation(\n",
    "    utility,\n",
    "    is_done=RankCorrelation(rtol=1e-2, burn_in=32),\n",
    "    batch_size=32,\n",
    "    seed=random_state,\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "cached_fit = filecache(\"digits_msr_banzhaf_result.pkl\")(\n",
    "    lambda d: valuation.fit(d).result\n",
    ")\n",
    "with parallel_config(n_jobs=n_jobs):\n",
    "    msr_result = cached_fit(train)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inspection of the values reveals (generally) much lower variances. Notice the number of updates to each value as well."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "msr_result.sort().to_dataframe(column=\"msr\")"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top = msr_result.sort()[-10:]\n",
    "_ = plot_result_errors(\n",
    "    top,\n",
    "    title=\"Images with high values\",\n",
    "    xlabel=\"Image\",\n",
    "    ylabel=\"Banzhaf Value\",\n",
    ")\n",
    "plt.show();"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bottom = msr_result.sort()[:10]\n",
    "_ = plot_result_errors(\n",
    "    bottom,\n",
    "    title=\"Images with low values\",\n",
    "    xlabel=\"Image\",\n",
    "    ylabel=\"Banzhaf value\",\n",
    ")\n",
    "plt.show();"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "low_images = train[bottom.indices].data().x\n",
    "low_labels = train[bottom.indices].data().y\n",
    "high_images = train[top.indices].data().x\n",
    "high_labels = train[top.indices].data().y\n",
    "\n",
    "_, axes = plt.subplots(2, 5, figsize=(9, 5))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(low_images[i].reshape((8, 8)).cpu(), cmap=\"gray\")\n",
    "    axes[1, i].imshow(high_images[i].reshape((8, 8)).cpu(), cmap=\"gray\")\n",
    "    axes[0, i].set_xlabel(f\"Label: {low_labels[i]}\")\n",
    "    axes[1, i].set_xlabel(f\"Label: {high_labels[i]}\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Low values\")\n",
    "axes[1, 0].set_ylabel(\"High values\")\n",
    "plt.suptitle(\"Images with low and high values\")\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Compare convergence speed of Banzhaf and MSR Banzhaf Values\n",
    "\n",
    "Conventional margin-based samplers produce require evaluating the utility twice to do one update of the value, and permutation samplers do instead $n+1$ evaluations for $n$ updates. Maximum Sample Reuse (MSR) updates instead all indices in every sample that the utility evaluates. We compare the convergence rates of these methods.\n",
    "\n",
    "In order to do so, we will compute the semi-values using different samplers and use a high number of iterations to make sure that the values have converged."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_checks = 1000\n",
    "moving_avg = 200"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if is_CI:\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "    max_checks = 1\n",
    "    moving_avg = 1\n",
    "    utility = ModelUtility(\n",
    "        model=SGDClassifier(max_iter=2), scorer=accuracy_over_test_set\n",
    "    )"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pydvl.utils import maybe_add_argument\n",
    "from pydvl.valuation import IndexSampler, SemivalueValuation, ValuationResult\n",
    "from pydvl.valuation.stopping import History, MaxChecks\n",
    "\n",
    "\n",
    "def compute_semivalues_and_history(\n",
    "    method_t: Type[SemivalueValuation],\n",
    "    sampler_t: Type[IndexSampler],\n",
    "    sampler_args: dict,\n",
    "    max_checks: int,\n",
    "    progress: bool = True,\n",
    ") -> tuple[History, ValuationResult]:\n",
    "    history = History(n_steps=max_checks)\n",
    "    wrapper = maybe_add_argument(sampler_t, \"seed\")\n",
    "    utility.show_warnings = True\n",
    "    valuation = method_t(\n",
    "        utility,\n",
    "        sampler=wrapper(**sampler_args, seed=random_state),\n",
    "        is_done=MaxChecks(max_checks + 2) | history,\n",
    "        progress=progress,\n",
    "    )\n",
    "\n",
    "    @filecache(\n",
    "        f\"digits_banzhaf_comparison_{method_t.__name__}_{sampler_t.__name__}_result.pkl\"\n",
    "    )\n",
    "    def cached_fit(data) -> tuple[History, ValuationResult]:\n",
    "        valuation.fit(data)\n",
    "        return history, valuation.result\n",
    "\n",
    "    with parallel_config(n_jobs=n_jobs):\n",
    "        return cached_fit(train)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pydvl.valuation.samplers import (\n",
    "    AntitheticSampler,\n",
    "    HarmonicSampleSize,\n",
    "    RandomIndexIteration,\n",
    "    RandomSizeIteration,\n",
    "    StratifiedSampler,\n",
    "    UniformSampler,\n",
    ")\n",
    "\n",
    "experiments = OrderedDict(\n",
    "    [\n",
    "        (\n",
    "            PermutationSampler,\n",
    "            {\n",
    "                \"name\": \"Permutation\",\n",
    "                \"kwargs\": {\n",
    "                    \"truncation\": RelativeTruncation(rtol=0.05, burn_in_fraction=0.3),\n",
    "                },\n",
    "            },\n",
    "        ),\n",
    "        (MSRSampler, {\"name\": \"MSR\", \"kwargs\": {\"batch_size\": 16}}),\n",
    "        (UniformSampler, {\"name\": \"Uniform\", \"kwargs\": {}}),\n",
    "        (AntitheticSampler, {\"name\": \"Antithetic\", \"kwargs\": {}}),\n",
    "        (\n",
    "            StratifiedSampler,\n",
    "            {\n",
    "                \"name\": \"Stratified\",\n",
    "                \"kwargs\": {\n",
    "                    \"sample_sizes\": HarmonicSampleSize(1),\n",
    "                    \"sample_sizes_iteration\": RandomSizeIteration,\n",
    "                    \"index_iteration\": RandomIndexIteration,\n",
    "                },\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "results = {}\n",
    "history = {}\n",
    "\n",
    "for sampler_t, params in experiments.items():\n",
    "    history[sampler_t], results[sampler_t] = compute_semivalues_and_history(\n",
    "        BanzhafValuation, sampler_t, params.get(\"kwargs\", {}), max_checks\n",
    "    )"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compare convergence speed of all methods\n",
    "all_values = {\n",
    "    sampler_t: history[sampler_t].memory.data for sampler_t in experiments.keys()\n",
    "}\n",
    "distances = {sampler_t: [] for sampler_t in experiments.keys()}\n",
    "moving_avgs = {}\n",
    "\n",
    "for sampler_t, params in experiments.items():\n",
    "    for iteration in range(max_checks):\n",
    "        abs_dist = np.abs(\n",
    "            all_values[sampler_t][:, iteration]\n",
    "            - all_values[sampler_t][:, iteration + 1]\n",
    "        )\n",
    "        if abs_dist.max() == 0.0:\n",
    "            distances[sampler_t].append(0.0)\n",
    "        else:\n",
    "            distances[sampler_t].append(np.mean(abs_dist[abs_dist > 0]))\n",
    "    moving_avgs[sampler_t] = np.convolve(\n",
    "        distances[sampler_t], np.ones(moving_avg) / moving_avg, mode=\"same\"\n",
    "    )\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "for sampler_t, params in experiments.items():\n",
    "    ax.plot(list(range(max_checks)), moving_avgs[sampler_t], label=params[\"name\"])\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Mean semi-value change between iterations\")\n",
    "ax.set_title(\"Convergence speed of different samplers\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-5, 1)\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The plot above visualizes the convergence speed of different samplers used for Banzhaf semi-value calculation. It shows the average magnitude of how much the semi-values are updated in every step of the algorithm.\n",
    "\n",
    "As you can see, **MSR Banzhaf** stabilizes much faster. After 1000 iterations (subsets sampled and evaluated with the utility), Permutation Monte Carlo Banzhaf has evaluated the marginal function about 5 times per data point (we are using 200 data points). For MSR, the semi-value of each data point was updated 1000 times. Due to this, the values converge much faster wrt. the number of utility evaluations, which is the key advantage of MSR sampling.\n",
    "\n",
    "MSR sampling does come at a cost, however, which is that the updates to the semi-values are more noisy than in other methods.  We will analyze the impact of this tradeoff in the next sections. First, let us look at how similar all the computed semi-values are. They are all Banzhaf values, so in a perfect world, all samplers should result in the exact same semi-values. However, due to randomness in the utility (recall that we use a neural network) and randomness in the samplers, the resulting values are likely never exactly the same. Another quality measure is that a good sampler would lead to very consistent values, a bad one to less consistent values. Let us first examine how similar the results are, then we'll look at consistency."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Similarity of the semi-values computed using different samplers"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top_consistency = np.zeros((len(experiments), len(experiments)))\n",
    "low_consistency = np.zeros((len(experiments), len(experiments)))\n",
    "twenty_percent = train.data().x.shape[0] // 5\n",
    "\n",
    "for i, (sampler1_t, sampler1_values) in enumerate(results.items()):\n",
    "    for j, (sampler2_t, sampler2_values) in enumerate(results.items()):\n",
    "        sampler1_values.sort(key=\"value\", inplace=True)\n",
    "        sampler2_values.sort(key=\"value\", inplace=True)\n",
    "        top_20_1 = set(sampler1_values.indices[-twenty_percent:].tolist())\n",
    "        lower_20_1 = set(sampler1_values.indices[:twenty_percent].tolist())\n",
    "        top_20_2 = set(sampler2_values.indices[-twenty_percent:].tolist())\n",
    "        lower_20_2 = set(sampler2_values.indices[:twenty_percent].tolist())\n",
    "        top_consistency[i, j] = len(top_20_1.intersection(top_20_2))\n",
    "        low_consistency[i, j] = len(lower_20_1.intersection(lower_20_2))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "names = [v[\"name\"] for _, v in experiments.items()]\n",
    "fix, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].matshow(top_consistency, vmin=0, vmax=twenty_percent)\n",
    "mat2 = axes[1].matshow(low_consistency, vmin=0, vmax=twenty_percent)\n",
    "axes[0].set_xticks(np.arange(len(experiments)), names, rotation=90)\n",
    "axes[1].set_xticks(np.arange(len(experiments)), names, rotation=90)\n",
    "for (i, j), z in np.ndenumerate(top_consistency):\n",
    "    axes[0].text(j, i, f\"{int(z)}\", ha=\"center\", va=\"center\", c=\"white\")\n",
    "for (i, j), z in np.ndenumerate(low_consistency):\n",
    "    axes[1].text(j, i, f\"{int(z)}\", ha=\"center\", va=\"center\", c=\"white\")\n",
    "\n",
    "axes[0].set_yticks(np.arange(len(experiments)), names)\n",
    "axes[1].set_yticks(np.arange(len(experiments)), names)\n",
    "axes[0].set_title(\"Top 20% of points\")\n",
    "axes[1].set_title(\"Low 20% of points\")\n",
    "fig.colorbar(mat2)\n",
    "plt.suptitle(\"Overlapping high and low value points between samplers\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This plot shows that the samplers lead to quite different Banzhaf semi-values, however, all of them have some points in common. The MSR Sampler does not seem to be significantly worse than any others.\n",
    "\n",
    "In an ideal setting without randomness, the overlap of points would be higher, however, the stochastic nature of the CNN model that we use together with the fact that we use only 200 data points for training, might overshadow these results. As a matter of fact we have the rather discouraging following result:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_in_common = set(results[PermutationSampler].indices.tolist())\n",
    "for sampler_t, sampler_values in results.items():\n",
    "    sampler_values.sort(key=\"value\", inplace=True)\n",
    "    top_20 = set(sampler_values.indices[-twenty_percent:].tolist())\n",
    "    all_in_common = all_in_common.intersection(top_20)\n",
    "print(\n",
    "    f\"Total number of top 20 points that all samplers have in common: {len(all_in_common)}\"\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Consistency of the semi-values\n",
    "\n",
    "Finally, we want to analyze how consistent the semi-values are when computed using the different samplers. In order to do this, we calculate them multiple times and check how many of the data points in the top and lowest 20% of the valuation overlap."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_checks = 1000\n",
    "n_runs = 5"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if is_CI:\n",
    "    n_runs = 1\n",
    "    max_checks = 1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "all_results = defaultdict(lambda: defaultdict(list))\n",
    "snapshot_times = np.linspace(0, max_checks, 20, dtype=int)\n",
    "\n",
    "with tqdm(total=n_runs * len(experiments), position=0) as pbar:\n",
    "    for sampler_t, params in experiments.items():\n",
    "        for _ in range(n_runs):\n",
    "            history, vals = compute_semivalues_and_history(\n",
    "                BanzhafValuation,\n",
    "                sampler_t,\n",
    "                params.get(\"kwargs\", {}),\n",
    "                max_checks,\n",
    "                progress=False,\n",
    "            )\n",
    "            for step in snapshot_times:\n",
    "                raw_values = history[-(max_checks - step)]\n",
    "                # We will want to access the indices later, so we use a fake object\n",
    "                fake_result = ValuationResult(indices=vals.indices, values=raw_values)\n",
    "                all_results[sampler_t][step].append(fake_result)\n",
    "            pbar.n += 1\n",
    "            pbar.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Extract results from experiments\n",
    "twenty_percent = train.data().x.shape[0] // 5\n",
    "plot_results_top = defaultdict(list)\n",
    "plot_results_low = defaultdict(list)\n",
    "for sampler_t, params in experiments.items():\n",
    "    for step in snapshot_times:\n",
    "        values_for_all_runs = all_results[sampler_t][step]\n",
    "        top_20 = None\n",
    "        lower_20 = None\n",
    "        for vals in values_for_all_runs:\n",
    "            vals.sort(key=\"value\", inplace=True)\n",
    "            if top_20 is None:\n",
    "                top_20 = set(vals.indices[-twenty_percent:].tolist())\n",
    "                lower_20 = set(vals.indices[:twenty_percent].tolist())\n",
    "            else:\n",
    "                top_20 = top_20.intersection(\n",
    "                    set(vals.indices[-twenty_percent:].tolist())\n",
    "                )\n",
    "                lower_20 = lower_20.intersection(\n",
    "                    set(vals.indices[:twenty_percent].tolist())\n",
    "                )\n",
    "        plot_results_top[sampler_t].append(len(top_20) / 20)\n",
    "        plot_results_low[sampler_t].append(len(lower_20) / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "for sampler_t, params in experiments.items():\n",
    "    axes[0].plot(snapshot_times, plot_results_top[sampler_t], label=params[\"name\"])\n",
    "    axes[1].plot(snapshot_times, plot_results_low[sampler_t], label=params[\"name\"])\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[0].set_xlabel(\"Number of iterations\")\n",
    "axes[1].set_xlabel(\"Number of iterations\")\n",
    "axes[0].set_ylabel(f\"Fraction of common points \\nin top 20% across {n_runs} runs\")\n",
    "axes[1].set_ylabel(f\"Fraction of common points \\nin lower 20% across {n_runs} runs\")\n",
    "fig.suptitle(f\"Consistency of samplers (max. value: {twenty_percent})\")\n",
    "fig.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "MSR sampling updates the semi-value estimates for every index in the sample, much more frequently than any other sampler available, which leads to much **faster convergence**. Additionally, the sampler is more consistent with its value estimates than the other samplers, which might be caused by the higher number of value updates.\n",
    "\n",
    "There is alas no general recommendation. It is best to try different samplers when computing semi-values and test which one is best suited for your use case. Nevertheless, the MSR sampler seems like a more efficient sampler which may bring fast results and is well-suited for stochastic models."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e000971326892723e7f31ded70802f690c31c3620f59a0f99e594aaee3047ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

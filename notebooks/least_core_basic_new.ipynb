{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995d4271",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Least Core for Data Valuation\n",
    "\n",
    "This notebook introduces Least Core methods for the computation of data values using pyDVL.\n",
    "\n",
    "Shapley values define a fair way of distributing the worth of the whole training set when every data point is part of it. But they do not consider the question of stability of subsets: Could some data points obtain a higher payoff if they formed smaller subsets? It is argued that this might be relevant if data providers are paid based on data value, since Shapley values can incentivise them not to contribute their data to the \"grand coalition\", but instead try to form smaller ones. Whether this is of actual practical relevance is debatable, but in any case, the least core is an alternative tool available for any task of Data Valuation\n",
    "\n",
    "The Core is another approach to compute data values originating in cooperative game theory that attempts to answer those questions. It is the set of feasible payoffs that cannot be improved upon by a coalition of the participants.\n",
    "\n",
    "Its use for Data Valuation was first described in the paper [*If You Like Shapley Then Youâ€™ll Love the Core*](https://ojs.aaai.org/index.php/AAAI/article/view/16721) by Tom Yan and Ariel D. Procaccia.\n",
    "\n",
    "The Least Core value $v$ of the $i$-th sample in dataset $D$ wrt. utility $u$ is computed\n",
    "by solving the following Linear Program:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\text{minimize} & \\displaystyle{e} & \\\\\n",
    "\\text{subject to} & \\displaystyle\\sum_{x_i\\in D} v_u(x_i) = u(D) & \\\\\n",
    "& \\displaystyle\\sum_{x_i\\in S} v_u(x_i) + e \\geq u(S) &, \\forall S \\subset D, S \\neq \\emptyset \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To illustrate this method we will use a synthetic dataset. We will first use a subset of 10 data point to compute the exact values and use them to assess the Monte Carlo approximation. Afterwards, we will conduct the data removal experiments as described by Ghorbani and Zou in their paper [Data Shapley: Equitable Valuation of Data for Machine Learning](https://arxiv.org/abs/1904.02868v1): We compute the data valuation given different computation budgets and incrementally remove a percentage of the best, respectively worst, data points and observe how that affects the utility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21e5d1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing the main libraries and setting some defaults.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6656599",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c212a517774f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import annotations  # noqa: F404\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from typing import Iterable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "plt.ioff()  # Prevent jupyter from automatically plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 8)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10\n",
    "plt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\n",
    "plt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\n",
    "\n",
    "mean_colors = [\"dodgerblue\", \"indianred\", \"limegreen\", \"darkorange\", \"darkorchid\"]\n",
    "shade_colors = [\"lightskyblue\", \"firebrick\", \"seagreen\", \"gold\", \"plum\"]\n",
    "\n",
    "random_state = 16\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "is_CI = os.environ.get(\"CI\")\n",
    "\n",
    "dataset_size = 200\n",
    "n_iterations = 5000\n",
    "train_size = 10\n",
    "n_jobs = 4\n",
    "\n",
    "if is_CI:\n",
    "    dataset_size = 20\n",
    "    n_iterations = 500\n",
    "    train_size = 0.2\n",
    "    n_jobs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11d1ff",
   "metadata": {},
   "source": [
    "We will be using the following functions and classes from pyDVL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from pydvl.reporting.plots import shaded_mean_std\n",
    "from pydvl.valuation import (\n",
    "    Dataset,\n",
    "    ExactLeastCoreValuation,\n",
    "    ModelUtility,\n",
    "    MonteCarloLeastCoreValuation,\n",
    "    SupervisedScorer,\n",
    "    ValuationResult,\n",
    ")\n",
    "from pydvl.valuation.types import Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d81aa",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We generate a synthetic dataset using the [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function from scikit-learn.\n",
    "\n",
    "We sample 200 data points from a 50-dimensional Gaussian distribution with 25 informative features and 25 non-informative features (generated as random linear combinations of the informative features).\n",
    "\n",
    "The 200 samples are uniformly distributed across 3 classes with a small percentage of noise added to the labels to make the task a bit more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e916a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=dataset_size,\n",
    "    n_features=50,\n",
    "    n_informative=25,\n",
    "    n_classes=3,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b128695",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_data, full_test_data = Dataset.from_arrays(\n",
    "    X, y, stratify_by_target=True, random_state=random_state\n",
    ")\n",
    "small_train_data, small_test_data = Dataset.from_arrays(\n",
    "    X,\n",
    "    y,\n",
    "    stratify_by_target=True,\n",
    "    train_size=train_size,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffce8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=500, solver=\"liblinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(*full_train_data.data())\n",
    "print(f\"Training accuracy: {100 * model.score(*full_train_data.data()):0.2f}%\")\n",
    "print(f\"Testing accuracy: {100 * model.score(*full_test_data.data()):0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(*small_train_data.data())\n",
    "print(f\"Training accuracy: {100 * model.score(*small_train_data.data()):0.2f}%\")\n",
    "print(f\"Testing accuracy: {100 * model.score(*small_test_data.data()):0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4a2d1",
   "metadata": {},
   "source": [
    "## Estimating Least-Core Values\n",
    "\n",
    "In this first section we will use a smaller subset of the dataset containing 10 samples in order to be able to compute exact values in a reasonable amount of time. Afterwards, we will use the Monte Carlo method with a limited budget (maximum number of subsets) to approximate these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a22915",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = SupervisedScorer(\n",
    "    scoring=model,\n",
    "    test_data=small_test_data,\n",
    "    default=0,\n",
    ")\n",
    "utility = ModelUtility(model=model, scorer=scorer)\n",
    "\n",
    "valuation = ExactLeastCoreValuation(utility=utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91a124",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "valuation.fit(small_train_data)\n",
    "exact_values = valuation.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b84bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_values_df = exact_values.to_dataframe(column=\"exact_value\").T\n",
    "exact_values_df = exact_values_df[sorted(exact_values_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1091849a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "budget_array = np.linspace(200, 2 ** len(small_train_data), num=10, dtype=int)\n",
    "\n",
    "all_estimated_values_df = []\n",
    "all_errors = {budget: [] for budget in budget_array}\n",
    "\n",
    "for budget in tqdm(budget_array):\n",
    "    dfs = []\n",
    "    errors = []\n",
    "    column_name = f\"estimated_value_{budget}\"\n",
    "    for i in range(20):\n",
    "        valuation = MonteCarloLeastCoreValuation(\n",
    "            utility=utility,\n",
    "            n_samples=budget,\n",
    "            progress=False,\n",
    "        )\n",
    "        valuation.fit(small_train_data)\n",
    "        values = valuation.values()\n",
    "        df = values.to_dataframe(column=column_name)[[column_name]].T\n",
    "        df = df[sorted(df.columns)]\n",
    "        error = mean_squared_error(\n",
    "            exact_values_df.loc[\"exact_value\"].values, np.nan_to_num(df.values.ravel())\n",
    "        )\n",
    "        all_errors[budget].append(error)\n",
    "        df[\"budget\"] = budget\n",
    "        dfs.append(df)\n",
    "    estimated_values_df = pd.concat(dfs)\n",
    "    all_estimated_values_df.append(estimated_values_df)\n",
    "\n",
    "values_df = pd.concat(all_estimated_values_df)\n",
    "errors_df = pd.DataFrame(all_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e02c36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "_ = shaded_mean_std(\n",
    "    errors_df,\n",
    "    abscissa=errors_df.columns,\n",
    "    num_std=1,\n",
    "    xlabel=\"Budget\",\n",
    "    ylabel=\"$l_2$ Error\",\n",
    "    label=\"Estimated values\",\n",
    "    title=\"$l_2$ approximation error of values as a function of the budget\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8488311",
   "metadata": {},
   "source": [
    "We can see that the approximation error decreases, on average, as the we increase the budget. \n",
    "\n",
    "Still, the decrease may not always necessarily happen when we increase the number of iterations because of the fact that we sample the subsets with replacement in the Monte Carlo method i.e there may be repeated subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bccf93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "mean_std_values_df = values_df.drop(columns=\"budget\").agg([\"mean\", \"std\"])\n",
    "df = pd.concat([exact_values_df, mean_std_values_df])\n",
    "df = df.sort_values(\"exact_value\", ascending=False, axis=1).T\n",
    "df.plot(\n",
    "    kind=\"bar\",\n",
    "    title=\"Comparison of Exact and Monte Carlo Methods\",\n",
    "    xlabel=\"Index\",\n",
    "    ylabel=\"Value\",\n",
    "    color=[\"dodgerblue\", \"indianred\"],\n",
    "    y=[\"exact_value\", \"mean\"],\n",
    "    yerr=[exact_values_df.loc[\"exact_value_stderr\"], mean_std_values_df.loc[\"std\"]],\n",
    ")\n",
    "plt.legend([\"Exact\", \"Monte Carlo\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ea343",
   "metadata": {},
   "source": [
    "## Data Removal\n",
    "\n",
    "We now move on to the data removal experiments using the full dataset.\n",
    "\n",
    "In these experiments, we first rank the data points from most valuable \n",
    "to least valuable using the values estimated by the Monte Carlo Least Core method.\n",
    "Then, we gradually remove from 5 to 40 percent, by increments of 5 percentage points, of the most valuable/least valuable ones, train the model on this subset and compute its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a0a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = SupervisedScorer(\n",
    "    scoring=model,\n",
    "    test_data=full_test_data,\n",
    "    default=0,\n",
    ")\n",
    "utility = ModelUtility(model=model, scorer=scorer)\n",
    "\n",
    "\n",
    "method_names = [\"Random\", \"Least Core\"]\n",
    "removal_percentages = np.arange(0, 0.41, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e30a4",
   "metadata": {},
   "source": [
    "### Remove Best\n",
    "\n",
    "We start by removing the best data points and seeing how the model's accuracy evolves.\n",
    "\n",
    "To do so, we define a helper function to compute removal scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b7b6eb4043dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_removal_score(\n",
    "    u: ModelUtility,\n",
    "    values: ValuationResult,\n",
    "    training_data: Dataset,\n",
    "    percentages: NDArray[np.float64] | Iterable[float],\n",
    "    *,\n",
    "    remove_best: bool = False,\n",
    "    progress: bool = False,\n",
    ") -> dict[float, float]:\n",
    "    r\"\"\"Fits model and computes score on the test set after incrementally removing\n",
    "    a percentage of data points from the training set, based on their values.\n",
    "\n",
    "    Args:\n",
    "        u: Utility object with model, data, and scoring function.\n",
    "        values: Data values of data instances in the training set.\n",
    "        percentages: Sequence of removal percentages.\n",
    "        remove_best: If True, removes data points in order of decreasing valuation.\n",
    "        progress: If True, display a progress bar.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary that maps the percentages to their respective scores.\n",
    "    \"\"\"\n",
    "    u = u.with_dataset(training_data)\n",
    "\n",
    "    # Sanity checks\n",
    "    if np.any([x >= 1.0 or x < 0.0 for x in percentages]):\n",
    "        raise ValueError(\"All percentages should be in the range [0.0, 1.0)\")\n",
    "\n",
    "    if len(values) != len(training_data):\n",
    "        raise ValueError(\n",
    "            f\"The number of values, {len(values)}, should be equal to the number of data indices, {len(training_data)}\"\n",
    "        )\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    # We sort in descending order if we want to remove the best values\n",
    "    values.sort(reverse=remove_best)\n",
    "\n",
    "    for pct in tqdm(percentages, disable=not progress, desc=\"Removal Scores\"):\n",
    "        n_removal = int(pct * len(training_data))\n",
    "        indices = values.indices[n_removal:]\n",
    "        score = u(Sample(idx=None, subset=indices))\n",
    "        scores[pct] = score\n",
    "    return scores\n",
    "\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for i in trange(5):\n",
    "    for method_name in method_names:\n",
    "        if method_name == \"Random\":\n",
    "            values = ValuationResult.from_random(size=len(full_train_data))\n",
    "        else:\n",
    "            valuation = MonteCarloLeastCoreValuation(\n",
    "                utility=utility,\n",
    "                n_samples=n_iterations,\n",
    "                progress=False,\n",
    "            )\n",
    "            valuation.fit(full_train_data)\n",
    "            values = valuation.values()\n",
    "        scores = compute_removal_score(\n",
    "            u=utility,\n",
    "            values=values,\n",
    "            training_data=full_train_data,\n",
    "            percentages=removal_percentages,\n",
    "            remove_best=True,\n",
    "        )\n",
    "        scores[\"method_name\"] = method_name\n",
    "        all_scores.append(scores)\n",
    "\n",
    "scores_df = pd.DataFrame(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc70f6d7d4f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, method_name in enumerate(method_names):\n",
    "    shaded_mean_std(\n",
    "        scores_df[scores_df[\"method_name\"] == method_name].drop(\n",
    "            columns=[\"method_name\"]\n",
    "        ),\n",
    "        abscissa=removal_percentages,\n",
    "        mean_color=mean_colors[i],\n",
    "        shade_color=shade_colors[i],\n",
    "        xlabel=\"Percentage Removal\",\n",
    "        ylabel=\"Accuracy\",\n",
    "        label=method_name,\n",
    "        title=\"Accuracy as a function of percentage of removed best data points\",\n",
    "        ax=ax,\n",
    "    )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe590fa",
   "metadata": {},
   "source": [
    "We can clearly see that removing the most valuable data points, as given by the Least Core method, leads to, on average, a decrease in the model's performance and that the method outperforms random removal of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f0ba7",
   "metadata": {},
   "source": [
    "### Remove Worst\n",
    "\n",
    "We then proceed to removing the worst data points and seeing how the model's accuracy evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33b5bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "\n",
    "for i in trange(5):\n",
    "    for method_name in method_names:\n",
    "        if method_name == \"Random\":\n",
    "            values = ValuationResult.from_random(size=len(full_train_data))\n",
    "        else:\n",
    "            valuation = MonteCarloLeastCoreValuation(\n",
    "                utility=utility,\n",
    "                n_samples=n_iterations,\n",
    "                progress=False,\n",
    "            )\n",
    "            valuation.fit(full_train_data)\n",
    "            values = valuation.values()\n",
    "        scores = compute_removal_score(\n",
    "            u=utility,\n",
    "            values=values,\n",
    "            training_data=full_train_data,\n",
    "            percentages=removal_percentages,\n",
    "        )\n",
    "        scores[\"method_name\"] = method_name\n",
    "        all_scores.append(scores)\n",
    "\n",
    "scores_df = pd.DataFrame(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d69593",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, method_name in enumerate(method_names):\n",
    "    shaded_mean_std(\n",
    "        scores_df[scores_df[\"method_name\"] == method_name].drop(\n",
    "            columns=[\"method_name\"]\n",
    "        ),\n",
    "        abscissa=removal_percentages,\n",
    "        mean_color=mean_colors[i],\n",
    "        shade_color=shade_colors[i],\n",
    "        xlabel=\"Percentage Removal\",\n",
    "        ylabel=\"Accuracy\",\n",
    "        label=method_name,\n",
    "        title=\"Accuracy as a function of percentage of removed worst data points\",\n",
    "        ax=ax,\n",
    "    )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc86c3",
   "metadata": {},
   "source": [
    "We can clearly see that removing the least valuable data points, as given by the Least Core method, leads to, on average, an increase in the model's performance and that the method outperforms the random removal of data points."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

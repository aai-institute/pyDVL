{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# LAVA\n",
    "\n",
    "This notebook explores the use of LAVA for data valuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "If you are reading this in the documentation, some boilerplate has been omitted for convenience.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, resnet18, ResNet18_Weights\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from support.common import (\n",
    "    plot_sample_images,\n",
    "    plot_losses,\n",
    ")\n",
    "from support.torch import (\n",
    "    TrainingManager,\n",
    "    MODEL_PATH,\n",
    "    new_resnet_model,\n",
    ")\n",
    "from support.types import Losses\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 7)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10\n",
    "plt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\n",
    "plt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\n",
    "\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from pydvl.ot.lava import LAVA\n",
    "from pydvl.utils.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "trainset = CIFAR10(root=\"/tmp/cifar10\", train=True, download=True, transform=transform)\n",
    "valset = CIFAR10(root=\"/tmp/cifar10\", train=False, download=True, transform=transform)\n",
    "classes = trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Subset(trainset, np.random.randint(low=0, high=len(trainset), size=100))\n",
    "valset = Subset(valset, np.random.randint(low=0, high=len(valset), size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)\n",
    "valloader = DataLoader(valset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at a few image samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train a model on the validation data (This is the same as in the paper) in order to use it as a feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(BasicBlock, [1, 1, 1, 1], num_classes=10)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model has {num_params} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "mgr = TrainingManager(\n",
    "    \"model_lava_cifar10\",\n",
    "    model,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    valloader,\n",
    "    trainloader,\n",
    "    MODEL_PATH,\n",
    "    device=DEVICE,\n",
    ")\n",
    "# Set use_cache=False to retrain the model\n",
    "train_loss, val_loss = mgr.train(n_epochs=10, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "plot_losses(Losses(train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix and $F_1$ score look good, especially considering the low resolution of the images and their complexity (they contain different objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "for inputs, targets in tqdm(valloader, total=len(valloader)):\n",
    "    y_test.append(targets.cpu().numpy().ravel())\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    pred = np.argmax(model(inputs).cpu().detach().numpy(), axis=1).ravel()\n",
    "    y_pred.append(pred)\n",
    "\n",
    "\n",
    "y_test = np.concatenate(y_test)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove the last layer in order to use the model as a feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for inputs, targets in tqdm(trainloader, total=len(trainloader)):\n",
    "    y_train.append(targets.cpu().numpy().ravel())\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    pred = model(inputs).cpu().detach().numpy()\n",
    "    x_train.append(pred)\n",
    "\n",
    "x_train = np.concatenate(x_train)\n",
    "y_train = np.concatenate(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for inputs, targets in tqdm(valloader, total=len(valloader)):\n",
    "    y_test.append(targets.cpu().numpy().ravel())\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    pred = model(inputs).cpu().detach().numpy()\n",
    "    x_test.append(pred)\n",
    "\n",
    "x_test = np.concatenate(x_test)\n",
    "y_test = np.concatenate(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = []\n",
    "\n",
    "for regularization in [1, 0.5, 0.1, 0.01]:\n",
    "    for lambda_ in [3.0, 1.0, 0.1, 0.01, 0]:\n",
    "        print(f\"{regularization=}, {lambda_=}\")\n",
    "        lava = LAVA(\n",
    "            dataset,\n",
    "            inner_ot_method=\"exact\",\n",
    "            regularization=regularization,\n",
    "            lambda_=lambda_,\n",
    "        )\n",
    "        values = lava.compute_values()\n",
    "        all_values.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df = pd.DataFrame(np.stack(all_values).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.plot.boxplot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = 1.0\n",
    "lambda_ = 1.0\n",
    "lava = LAVA(\n",
    "    dataset, inner_ot_method=\"gaussian\", regularization=regularization, lambda_=lambda_\n",
    ")\n",
    "values = lava.compute_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cost = lava._compute_feature_cost()\n",
    "plt.boxplot(feature_cost.ravel());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lava._compute_gaussian_label_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lava._compute_exact_label_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Model\n",
    "\n",
    "What if we use a pre-trained model instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = torch.nn.Identity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b3369ace3ad477f5e763d9fa7767e0177027059e92a8b1ded9e92b707c0b1513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

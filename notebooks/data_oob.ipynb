{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# TODO:\\n\",\n",
    "    \"\\n\",\n",
    "    \"* Fix the problems with the removal job:\\n\",\n",
    "    \"   - different initial value for the curves\\n\",\n",
    "    \"   - increasing by high-value removal??\\n\",\n",
    "    \"* Remove old text fragments\\n\",\n",
    "    \"* Factor out any boilerplate\\n\",\n",
    "    \"* Remove leftovers\\n\",\n",
    "    \"* Actually perform the analysis using the values\\n\",\n",
    "    \"* Add support for pipeline objects to DataOOBValuation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Data-OOB for random forests and bagged classifiers\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"This notebook illustrates the use of [Data-OOB][../../value/data-oob] from Kwon and Zou \\\"[Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value](https://proceedings.mlr.press/v202/kwon23e.html)\\\" (ICML 2023), to compute values for bagged models.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will work with the [adult classification dataset](https://archive.ics.uci.edu/dataset/2/adult) from the UCI repository. It's an imbalanced dataset where the objective is to predict whether a person earns more than $50K a year (the \\\"positive\\\" class) based on a set of features such as age, education, occupation, etc. After training a random forest on this dataset, we will compute the Data-OOB values and analyze them.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Setup\\n\",\n",
    "    \"\\n\",\n",
    "    \"<div class=\\\"alert alert-info\\\">\\n\",\n",
    "    \"\\n\",\n",
    "    \"If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience.\\n\",\n",
    "    \"\\n\",\n",
    "    \"</div>\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We begin by importing the main libraries and setting some defaults.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"slideshow\": {\n",
    "     \"slide_type\": \"\"\n",
    "    },\n",
    "    \"tags\": [\n",
    "     \"hide\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"%load_ext autoreload\\n\",\n",
    "    \"\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import random\\n\",\n",
    "    \"\\n\",\n",
    "    \"import matplotlib\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"from pydvl.reporting.plots import plot_ci_array, plot_ci_values\\n\",\n",
    "    \"from sklearn.metrics import ConfusionMatrixDisplay, f1_score\\n\",\n",
    "    \"from support.common import load_adult_data, load_adult_data_raw\\n\",\n",
    "    \"\\n\",\n",
    "    \"matplotlib.rcParams[\\\"axes.facecolor\\\"] = (1, 1, 1, 0)\\n\",\n",
    "    \"plt.rcParams[\\\"axes.facecolor\\\"] = (1, 1, 1, 0)\\n\",\n",
    "    \"plt.rcParams[\\\"figure.facecolor\\\"] = (1, 1, 1, 0)\\n\",\n",
    "    \"MEAN_COLORS = [\\\"dodgerblue\\\", \\\"indianred\\\", \\\"limegreen\\\", \\\"darkorange\\\",\\n\",\n",
    "    \"               \\\"darkorchid\\\"]\\n\",\n",
    "    \"SHADE_COLORS = [\\\"lightskyblue\\\", \\\"firebrick\\\", \\\"seagreen\\\", \\\"gold\\\", \\\"plum\\\"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"is_CI = os.environ.get(\\\"CI\\\")\\n\",\n",
    "    \"random_state = 42\\n\",\n",
    "    \"random.seed(random_state);\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from sklearn.ensemble import RandomForestClassifier\\n\",\n",
    "    \"from pydvl.valuation import DataOOBValuation, Dataset\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"With a helper function we download the data, encode the categorical variables using [TargetEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html), and split it into training and testing sets. We must be careful to stratify the split by the target variable (income).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-output\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"train, test = load_adult_data(train_size=0.6, subsample=0.2,\\n\",\n",
    "    \"                              random_state=random_state)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"if is_CI:  # Subsample 1% of the data for faster testing\\n\",\n",
    "    \"    train, test = load_adult_data(train_size=0.6, subsample=0.01,\\n\",\n",
    "    \"                                  random_state=random_state)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Usually we would carefully look at the features, check for missing values, outliers, etc. But for the sake of this example, we will skip this step and jump straight into training a model. We will only look at the class distribution since it will matter later:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-input\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"print(f\\\"{len(train)} samples. Class distribution: {100 * np.mean(train.y == 1):.1f}% positive, \\\"\\n\",\n",
    "    \"      f\\\"{100 * np.mean(train.y == 0):.1f}% negative\\\")\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"As a quick baseline, we train a standard sklearn [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Since the dataset is imbalanced, besides the accuracy we look at the confusion matrix, and notice that despite weighting the class by their inverse frequency with `class_weight=\\\"balanced\\\"`, the model is not very good at predicting the minority (\\\"positive\\\", or \\\"1\\\") class: in the left hand side of the figure below we see a high rate of false negatives. This will play a role later in how we interpret the values that Data-OOB returns, and requires us to address the imbalance in the dataset. We do this with a simple random over sampling using `imblearn`'s [RandomOverSampler](...) class.\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from imblearn.over_sampling import RandomOverSampler\\n\",\n",
    "    \"\\n\",\n",
    "    \"n_est = 100\\n\",\n",
    "    \"max_samples = 0.2  # Use small bootstrap samples\\n\",\n",
    "    \"model = RandomForestClassifier(n_estimators=n_est,\\n\",\n",
    "    \"                               max_samples=max_samples,\\n\",\n",
    "    \"                               class_weight=\\\"balanced\\\",\\n\",\n",
    "    \"                               random_state=random_state)\\n\",\n",
    "    \"model.fit(train.x, train.y)\\n\",\n",
    "    \"base_predictions = model.predict(test.x)\\n\",\n",
    "    \"\\n\",\n",
    "    \"sampler = RandomOverSampler(random_state=random_state)\\n\",\n",
    "    \"train.x, train.y = sampler.fit_resample(train.x, train.y)\\n\",\n",
    "    \"\\n\",\n",
    "    \"model.fit(train.x, train.y)\\n\",\n",
    "    \"predictions_oversampled = model.predict(test.x) \"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-input\"\n",
    "    ]\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"print(f\\\"{len(train)} samples. Class distribution: {100 * train.y.mean():.1f}% positive, \\\"\\n\",\n",
    "    \"      f\\\"{100 * (1 - train.y.mean()):.1f}% negative\\\")\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-input\"\n",
    "    ]\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from sklearn.metrics import accuracy_score\\n\",\n",
    "    \"base_accuracy = accuracy_score(test.y, base_predictions)\\n\",\n",
    "    \"accuracy_oversampled = accuracy_score(test.y, predictions_oversampled)\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_predictions(test.y, base_predictions, ax=axs[0],\\n\",\n",
    "    \"                                      labels=[0, 1], normalize=\\\"true\\\")\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_predictions(test.y, predictions_oversampled, ax=axs[1],\\n\",\n",
    "    \"                                      labels=[0, 1], normalize=\\\"true\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"axs[0].set_title(f\\\"Original dataset, {n_est} trees.\\\\nAccuracy: {base_accuracy:.2f}\\\")\\n\",\n",
    "    \"axs[1].set_title(f\\\"Oversampled dataset, {n_est} trees.\\\\nAccuracy: {accuracy_oversampled:.2f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.show()\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"invertible-output\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Computing the OOB values\\n\",\n",
    "    \"\\n\",\n",
    "    \"The main idea of Data-OOB is to use the out-of-bag error estimates of a bagged model to compute data values. In pyDVL, we provide a class [DataOOBValuation][pydvl.valuation.DataOOBValuation] that takes an existing classification or regression bagging model and uses the per-sample out-of-bag performance estimate for the value of each point.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's compute and compare the Data-OOB values with three choices for the number of estimators of a random forest. After fitting the random forest, we use the [fit][pydvl.valuation.DataOOBValuation.fit] method to compute the values and store them in [ValuationResult][pydvl.value.result.ValuationResult] objects.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Even though it's not relevant to our discussion, notice how the accuracy barely changes with the number of estimators. Below, we will discuss using the values to identify \\\"easy\\\" or \\\"hard\\\" samples in the dataset, but first let's quickly look at the values themselves.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"n_estimators = [50, 100, 200]\\n\",\n",
    "    \"oob_values = []\\n\",\n",
    "    \"for i, n_est in enumerate(n_estimators, start=1):\\n\",\n",
    "    \"    model = RandomForestClassifier(n_estimators=n_est,\\n\",\n",
    "    \"                                   max_samples=max_samples,\\n\",\n",
    "    \"                                   class_weight=\\\"balanced\\\",\\n\",\n",
    "    \"                                   random_state=random_state)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Tuning messes everything up if we oversample the dataset\\n\",\n",
    "    \"    # model = ThresholdTunerCV(classifier, n_splits=6, metric=f1_score, n_jobs=6)\\n\",\n",
    "    \"    model.fit(train.x, train.y)\\n\",\n",
    "    \"    accuracy = model.score(test.x, test.y)\\n\",\n",
    "    \"    print(f\\\"Accuracy with {n_est} estimators: {accuracy:.2f}\\\")\\n\",\n",
    "    \"    valuation = DataOOBValuation(model)\\n\",\n",
    "    \"    valuation.fit(train)\\n\",\n",
    "    \"    oob_values.append(valuation.values())\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"### The distribution of values\\n\",\n",
    "    \"\\n\",\n",
    "    \"The left-hand side of the figure below depicts value as it increases with rank and a 95% t-confidence interval. The right-hand side shows the histogram of values.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We observe a long tail of high values. This is because the score $T$ used in Data-OOB (accuracy in this case) is a binary variable, and the value $\\\\psi_i$ is the fraction of times that all weak learners not trained on the $i$-th point classify it correctly. Given the imbalance in the dataset, many learners will always predict the majority (\\\"negative\\\", < $50K earnings / year) class and be correct on 75% of the dataset, leading to this tail.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Besides the actual value $\\\\psi_i$, [ValuationResult][pydvl.value.result.ValuationResult] objects store the number of times a sample is OOB —the quantity $\\\\sum_{b=1}^{B} \\\\mathbb{1} (w_{bi}=0)$— in the `counts` attribute, and the variance of the OOB score in the `variances` attribute. We use the latter in the plot below in order to display the confidence intervals, but it is important to note that the interpretation varies from one valuation method to another:\\n\",\n",
    "    \"\\n\",\n",
    "    \"For Shapley-based valuation methods, the variance is that of the marginal changes in the performance of the model when trained on subsets of the data with and without a sample, evaluated on a fixed valuation dataset, and could in principle be used to see whether values have (roughly) converged. But for Data-OOB, it is the variance of the performance of the ensemble of weak learners on the sample when it is OOB. Although similar in spirit, the construction is different and can be misleading.\\n\",\n",
    "    \"\\n\",\n",
    "    \"As a matter of fact, the interpretation of the vanishing variance at the tail has little to do with valuation and everything to do with our dataset, as introduced above: As the number of estimators increases, the chance of all of them failing on the same points decreases, up to a point. The same happens when we increase the maximum depth (try it!). This behaviour is then not a deep property of Data-OOB from which to gain new insights, but rather a consequence of the dataset and the model, as we further elaborate below.\\n\",\n",
    "    \"\\n\",\n",
    "    \"*Note that a symmetric CI is actually incorrect in this situation since all values are bounded between 0 and 1 (instead we could use a bootstrapped CI if we stored the scores of all estimators in the bagging model, but this is not implemented in pyDVL).*\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"slideshow\": {\n",
    "     \"slide_type\": \"\"\n",
    "    },\n",
    "    \"tags\": [\n",
    "     \"hide-input\",\n",
    "     \"invertible-output\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=[15, 5])\\n\",\n",
    "    \"\\n\",\n",
    "    \"for (n_est, values, mean_color, shade_color) in zip(\\n\",\n",
    "    \"        n_estimators, oob_values, MEAN_COLORS, SHADE_COLORS):\\n\",\n",
    "    \"    values.sort()\\n\",\n",
    "    \"    plot_ci_values(values,\\n\",\n",
    "    \"                   level=0.05,\\n\",\n",
    "    \"                   mean_color=mean_color,\\n\",\n",
    "    \"                   shade_color=shade_color,\\n\",\n",
    "    \"                   ax=ax1,\\n\",\n",
    "    \"                   label=f\\\"{n_est} estimators\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ax2.hist(values, bins=50, color=mean_color, alpha=0.5,\\n\",\n",
    "    \"             label=f\\\"{n_est} estimators\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax1.set_title(\\\"Point rank\\\")\\n\",\n",
    "    \"ax1.set_xlabel(\\\"Rank\\\")\\n\",\n",
    "    \"ax1.set_ylabel(\\\"Data-OOB values\\\")\\n\",\n",
    "    \"ax1.legend()\\n\",\n",
    "    \"ax2.set_title(\\\"Histogram of Data-OOB values\\\")\\n\",\n",
    "    \"ax2.set_xlabel(\\\"Value\\\")\\n\",\n",
    "    \"ax2.legend()\\n\",\n",
    "    \"plt.plot();\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"To see this, focus on the long tails with zero variance. These are samples for which the score $T(y_i, \\\\hat{f}_b(x_i)) = 1$ for *every* estimator $\\\\hat{f}_b$ not trained on them, that is: *every weak learner in the ensemble correctly classifies these samples*. As we said above, this can happen because it is likely for weak estimators to be fitted to always predict the majority (negative) class:\\n\",\n",
    "    \"\\n\",\n",
    "    \"## This analysis is now wrong: 73% of the points in the tail are in the minority class (probably have been oversampled)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-input\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"v = oob_values[0]\\n\",\n",
    "    \"from_index = \\\\\\n\",\n",
    "    \"    np.where(np.isclose(v.variances, 0.0) & np.isclose(v.values, 1))[0][0]\\n\",\n",
    "    \"tail = values.indices[from_index:]\\n\",\n",
    "    \"print(f\\\"There are {len(tail)} points with value 1 and zero variance\\\"\\n\",\n",
    "    \"      f\\\" ({100 * len(tail) / len(train):.2f}% of the data).\\\")\\n\",\n",
    "    \"print(f\\\"Of these, {100 * (train.y[tail] == 0).sum() / len(tail):.2f}% \\\"\\n\",\n",
    "    \"      f\\\"are in the majority class.\\\")\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"plt.hist(tail, bins=20)\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"slideshow\": {\n",
    "     \"slide_type\": \"\"\n",
    "    },\n",
    "    \"tags\": []\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Simply put, the ensemble is mostly good at classifying the majority class, and the variance of the OOB score for these samples is very low. This is a common issue in imbalanced datasets, and it is one of the reasons why the OOB score might not be a good metric for model performance in these cases. For us, it shows that the values reflect only poorly fit models.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We test this below by ensembling a number of constant classifiers, but how does this affect our ability to use the values for data inspection, cleaning, etc.?\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### A detour to further interpret the OOB values\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can verify that the OOB values in our case reflect the imbalance of the dataset by training a [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) with constant estimators. A fraction of `n_estimators` will always pick class 1, and the rest class 0. This leads to a clear jump in the value rank plot, either around 25% or 75% of them, since, as we saw above, 25% of the samples are in the positive (\\\"1\\\") class, and 75% in the negative (\\\"0\\\").\\n\",\n",
    "    \"\\n\",\n",
    "    \"We will use three different probabilities for the constant estimators to predict class 0: 0.01, 0.5, and 0.99. Again, the idea is that the OOB values will reflect the class distribution of the dataset, and we should see a clear jump in the values around 25% and 75% of the data.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The code is analogous to the above when we fitted the random forest, so it is ommitted from the documentation, but we use a custom class `ConstantBinaryClassifier` as base estimator.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-input\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"from sklearn.ensemble import BaggingClassifier\\n\",\n",
    "    \"from support.common import ConstantBinaryClassifier\\n\",\n",
    "    \"\\n\",\n",
    "    \"train, test = load_adult_data(train_size=0.6, subsample=0.2,\\n\",\n",
    "    \"                              random_state=random_state)\\n\",\n",
    "    \"\\n\",\n",
    "    \"probs = [0.01, 0.5, 0.99]\\n\",\n",
    "    \"all_values = []\\n\",\n",
    "    \"for p in probs:\\n\",\n",
    "    \"    model = BaggingClassifier(ConstantBinaryClassifier(p, random_state),\\n\",\n",
    "    \"                              n_estimators=100,\\n\",\n",
    "    \"                              max_samples=1.0,\\n\",\n",
    "    \"                              random_state=random_state)\\n\",\n",
    "    \"    model.fit(train.x, train.y)\\n\",\n",
    "    \"    score = model.score(test.x, test.y)\\n\",\n",
    "    \"    print(\\n\",\n",
    "    \"            f\\\"Accuracy when ~{100 * p:.1f}% of estimators always predict class 0: {score:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    valuation = DataOOBValuation(model)\\n\",\n",
    "    \"    valuation.fit(train)\\n\",\n",
    "    \"    all_values.append(valuation.values())\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-input\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"fig, ax = plt.subplots(figsize=[15, 5])\\n\",\n",
    "    \"\\n\",\n",
    "    \"for (p, values, mean_color, shade_color) in zip(probs, all_values, MEAN_COLORS,\\n\",\n",
    "    \"                                                SHADE_COLORS):\\n\",\n",
    "    \"    values.sort(key=\\\"value\\\")\\n\",\n",
    "    \"    plot_ci_values(\\n\",\n",
    "    \"            values,\\n\",\n",
    "    \"            ax=ax,\\n\",\n",
    "    \"            level=0.01,\\n\",\n",
    "    \"            mean_color=mean_color,\\n\",\n",
    "    \"            shade_color=shade_color,\\n\",\n",
    "    \"            label=f\\\"Data-OOB, p={p:.2f}\\\",\\n\",\n",
    "    \"            )\\n\",\n",
    "    \"\\n\",\n",
    "    \"ax.axvline(0.25 * len(all_values[0]), color=\\\"darkorange\\\", linestyle=\\\"--\\\",\\n\",\n",
    "    \"           label=\\\"25% data\\\")\\n\",\n",
    "    \"ax.axvline(0.75 * len(all_values[0]), color=\\\"darkred\\\", linestyle=\\\"--\\\",\\n\",\n",
    "    \"           label=\\\"75% data\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Data-OOB values\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"Rank\\\")\\n\",\n",
    "    \"plt.title(f\\\"Model with constant estimators\\\")\\n\",\n",
    "    \"plt.legend();\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## A bogus (?) conclusion\\n\",\n",
    "    \"\\n\",\n",
    "    \"The conclusion of this discussion seems to be that for this imbalanced dataset and poorly performing model, the usual intuition that extreme values characterize \\\"easy\\\" or \\\"hard\\\" points might be bogus.\\n\",\n",
    "    \" \\n\",\n",
    "    \"If we discard the datapoints with the highest values, i.e. those which the ensemble of weak learners classifies correctly every time, because we believe that those are trivial in some sense (e.g. repeated) and bias the ensemble towards stricter decisions boundaries, we obtain very mild changes. The same happens when removing low-valued points. Basically, the changes in test scores could be just noise.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Evaluating Data-OOB values with data removal\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can go beyond the simple analysis above and systematically evaluate the impact of removing data points with high or low values on the model's performance. This is a common practice in the literature. We can use the [compute_removal_score][pydvl.reporting.scores.compute_removal_score] function to do it. This function takes a [ModelUtility][pydvl.valuation.ModelUtility] object, a [ValuationResult][pydvl.value.result.ValuationResult] object, and a [Dataset][pydvl.data.dataset.Dataset] object, and computes the performance of the model after removing a fraction of the data with the highest or lowest values.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Recall from above that we can construct these objects as follows:\\n\",\n",
    "    \"\\n\",\n",
    "    \"```python\\n\",\n",
    "    \"model = RandomForestClassifier(n_estimators=n_est, max_samples=max_samples, random_state=seed)\\n\",\n",
    "    \"utility = ModelUtility(model, SupervisedScorer(\\\"accuracy\\\", test, 0.0), clone_before_fit=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"valuation = DataOOBValuation(model)\\n\",\n",
    "    \"valuation.fit(train)\\n\",\n",
    "    \"values = valuation.values()\\n\",\n",
    "    \"```\\n\",\n",
    "    \"\\n\",\n",
    "    \"The details are hidden in the rendered documentation, please refer to the notebook for the full code.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"In order to call `run_removal_experiment()` we need to define 3 types of factories:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. A factory that returns a train-test split of the data given a random state \\n\",\n",
    "    \"2. A factory that returns a utility that evaluates a model on a given test set.\\n\",\n",
    "    \"   This is used for the performance evaluation. The model need not be the same\\n\",\n",
    "    \"   as the one used for the valuation.\\n\",\n",
    "    \"4. A factory returning a valuation method. The training set is passed to the\\n\",\n",
    "    \"   factory, in case the valuation needs to train something. E.g. for Data-OOB\\n\",\n",
    "    \"   we need the bagging model to be fitted before the valuation is computed.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"%autoreload\\n\",\n",
    "    \"from support.common import ThresholdTunerCV\\n\",\n",
    "    \"from support.removal_experiment import run_removal_experiment\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"from pydvl.valuation import KNNClassifierUtility, ModelUtility, SupervisedScorer\\n\",\n",
    "    \"\\n\",\n",
    "    \"from support.removal_experiment import run_removal_experiment\\n\",\n",
    "    \"from pydvl.valuation.methods.random import RandomValuation\\n\",\n",
    "    \"from pydvl.valuation.methods.data_oob import point_wise_accuracy\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def make_data(random_state: int) -> tuple[Dataset, Dataset]:\\n\",\n",
    "    \"    return load_adult_data(train_size=0.6, random_state=random_state)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def make_utility(test: Dataset, random_state: int) -> ModelUtility:\\n\",\n",
    "    \"    classifier = RandomForestClassifier(n_estimators=n_est,\\n\",\n",
    "    \"                                        max_samples=max_samples,\\n\",\n",
    "    \"                                        class_weight=\\\"balanced\\\",\\n\",\n",
    "    \"                                        random_state=random_state + 1)\\n\",\n",
    "    \"    model = ThresholdTunerCV(classifier,\\n\",\n",
    "    \"                             n_splits=5,\\n\",\n",
    "    \"                             metric=f1_score,\\n\",\n",
    "    \"                             n_jobs=1,\\n\",\n",
    "    \"                             random_state=random_state)\\n\",\n",
    "    \"    return ModelUtility(model, SupervisedScorer(\\\"accuracy\\\", test, 0.0))\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def make_oob(train: Dataset, random_state: int) -> DataOOBValuation:\\n\",\n",
    "    \"    classifier = RandomForestClassifier(n_estimators=n_est,\\n\",\n",
    "    \"                                        max_samples=max_samples,\\n\",\n",
    "    \"                                        class_weight=\\\"balanced\\\",\\n\",\n",
    "    \"                                        random_state=random_state)\\n\",\n",
    "    \"    model = ThresholdTunerCV(classifier,\\n\",\n",
    "    \"                             n_splits=5,\\n\",\n",
    "    \"                             metric=f1_score,\\n\",\n",
    "    \"                             n_jobs=1,\\n\",\n",
    "    \"                             random_state=random_state)\\n\",\n",
    "    \"    model.fit(train.x, train.y)\\n\",\n",
    "    \"    return DataOOBValuation(model, point_wise_accuracy)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def make_random(train: Dataset, random_state: int) -> RandomValuation:\\n\",\n",
    "    \"    return RandomValuation(random_state=random_state)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"removal_percentages = np.arange(0, 0.51, 0.02)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"low_scores_df, high_scores_df = \\\\\\n\",\n",
    "    \"    run_removal_experiment(\\n\",\n",
    "    \"            data_factory=make_data,\\n\",\n",
    "    \"            valuation_factories=[make_random, make_oob],\\n\",\n",
    "    \"            utility_factory=make_utility,  # for evaluation\\n\",\n",
    "    \"            removal_percentages=removal_percentages,\\n\",\n",
    "    \"            n_runs=6,\\n\",\n",
    "    \"            n_jobs=30,\\n\",\n",
    "    \"            random_state=random_state)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"slideshow\": {\n",
    "     \"slide_type\": \"\"\n",
    "    },\n",
    "    \"tags\": [\n",
    "     \"hide-input\",\n",
    "     \"invertible-output\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=[15, 5])\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, scores_df in enumerate((low_scores_df, high_scores_df)):\\n\",\n",
    "    \"    for j, (method_name, df) in enumerate(scores_df.groupby(\\\"method_name\\\")):\\n\",\n",
    "    \"        plot_ci_array(\\n\",\n",
    "    \"                data=df.drop(columns=[\\\"method_name\\\"]).values,\\n\",\n",
    "    \"                level=0.05,\\n\",\n",
    "    \"                abscissa=np.round(removal_percentages, 2),\\n\",\n",
    "    \"                mean_color=MEAN_COLORS[j],\\n\",\n",
    "    \"                shade_color=SHADE_COLORS[j],\\n\",\n",
    "    \"                label=method_name,\\n\",\n",
    "    \"                ax=axs[i],\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"        axs[i].legend()\\n\",\n",
    "    \"        axs[i].set_ylabel(\\\"Accuracy\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"axs[0].set_title(\\\"Lowest value removal, 95% CI\\\")\\n\",\n",
    "    \"axs[0].set_xlabel(\\\"Fraction of data removed\\\")\\n\",\n",
    "    \"axs[1].set_title(\\\"Highest value removal, 95% CI\\\")\\n\",\n",
    "    \"axs[1].set_xlabel(\\\"Fraction of data removed\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.show();\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# FIXME: THERE'S A BUG ABOVE\\n\",\n",
    "    \"\\n\",\n",
    "    \"The accuracy curves do not start at the same value!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Using Data-OOB with arbitrary models\\n\",\n",
    "    \"\\n\",\n",
    "    \"Note that even though the method is designed for bagging models, in principle it can be used with any other estimator by fitting a bagging model on top of it. This can generally be quite expensive, but it might prove useful in some cases. Below is what happens when we do this with a k-nearest neighbors classifier.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from sklearn.neighbors import KNeighborsClassifier\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# We want to use the KNN classifier as model for the utility, not the bagged model \\n\",\n",
    "    \"def make_knn_utility(test: Dataset, random_state: int) -> ModelUtility:\\n\",\n",
    "    \"    return KNNClassifierUtility(KNeighborsClassifier(n_neighbors=10), test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def make_oob_knn(train: Dataset, random_state: int) -> DataOOBValuation:\\n\",\n",
    "    \"    classifier = BaggingClassifier(\\n\",\n",
    "    \"            estimator=KNeighborsClassifier(n_neighbors=10),\\n\",\n",
    "    \"            max_samples=0.4,\\n\",\n",
    "    \"            n_estimators=10,\\n\",\n",
    "    \"            random_state=random_state)\\n\",\n",
    "    \"    model = ThresholdTunerCV(classifier,\\n\",\n",
    "    \"                             n_splits=5,\\n\",\n",
    "    \"                             metric=f1_score,\\n\",\n",
    "    \"                             n_jobs=1,\\n\",\n",
    "    \"                             random_state=random_state)\\n\",\n",
    "    \"    model.fit(train.x, train.y)\\n\",\n",
    "    \"    return DataOOBValuation(model, point_wise_accuracy)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"low_scores_df, high_scores_df = \\\\\\n\",\n",
    "    \"    run_removal_experiment(\\n\",\n",
    "    \"            data_factory=make_data,\\n\",\n",
    "    \"            utility_factory=make_knn_utility,\\n\",\n",
    "    \"            valuation_factories=[make_random, make_oob_knn],\\n\",\n",
    "    \"            removal_percentages=removal_percentages,\\n\",\n",
    "    \"            n_runs=20,\\n\",\n",
    "    \"            n_jobs=30)\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"tags\": [\n",
    "     \"hide-input\",\n",
    "     \"invertible-output\"\n",
    "    ]\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=[15, 5])\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, scores_df in enumerate((low_scores_df, high_scores_df)):\\n\",\n",
    "    \"    for j, (method_name, df) in enumerate(scores_df.groupby(\\\"method_name\\\")):\\n\",\n",
    "    \"        plot_ci_array(\\n\",\n",
    "    \"                data=df.drop(columns=[\\\"method_name\\\"]).values,\\n\",\n",
    "    \"                level=0.05,\\n\",\n",
    "    \"                abscissa=np.round(removal_percentages, 2),\\n\",\n",
    "    \"                mean_color=MEAN_COLORS[j],\\n\",\n",
    "    \"                shade_color=SHADE_COLORS[j],\\n\",\n",
    "    \"                label=method_name,\\n\",\n",
    "    \"                ax=axs[i],\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"        axs[i].legend()\\n\",\n",
    "    \"        axs[i].set_ylabel(\\\"Accuracy\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"axs[0].set_title(\\\"Lowest value removal, 95% CI\\\")\\n\",\n",
    "    \"axs[0].set_xlabel(\\\"Fraction of data removed\\\")\\n\",\n",
    "    \"axs[1].set_title(\\\"Highest value removal, 95% CI\\\")\\n\",\n",
    "    \"axs[1].set_xlabel(\\\"Fraction of data removed\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.show();\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## The impact of removing data on the three random forest models\\n\",\n",
    "    \"\\n\",\n",
    "    \"Manually removing and retraining:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"df = load_adult_data_raw()\\n\",\n",
    "    \"corr = df.corr(numeric_only=True)\\n\",\n",
    "    \"corr.style.background_gradient(cmap='coolwarm')\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"train, test = load_adult_data(train_size=0.6, random_state=random_state)\\n\",\n",
    "    \"for i, values in enumerate(oob_values):\\n\",\n",
    "    \"    zero_variances = np.isclose(values.variances, 0.0)\\n\",\n",
    "    \"    to_index = \\\\\\n\",\n",
    "    \"        np.where(zero_variances & np.isclose(values.values, 1))[0][0]\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        from_index = \\\\\\n\",\n",
    "    \"            np.where(zero_variances & np.isclose(values.values, 0))[0][-1]\\n\",\n",
    "    \"    except IndexError:\\n\",\n",
    "    \"        from_index = 0\\n\",\n",
    "    \"    model = RandomForestClassifier(n_estimators=n_estimators[i],\\n\",\n",
    "    \"                                   max_samples=max_samples,\\n\",\n",
    "    \"                                   random_state=random_state)\\n\",\n",
    "    \"    model.fit(train.x, train.y)\\n\",\n",
    "    \"    full_training_score = model.score(test.x, test.y)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    model.fit(train.x[:to_index], train.y[:to_index])\\n\",\n",
    "    \"    reduced_training_score = model.score(test.x, test.y)\\n\",\n",
    "    \"    score_change = 100 * (\\n\",\n",
    "    \"            reduced_training_score - full_training_score) / full_training_score\\n\",\n",
    "    \"    fraction_of_data = 100 * to_index / len(train)\\n\",\n",
    "    \"    print(f\\\"Model with {n_estimators[i]} estimators:\\\")\\n\",\n",
    "    \"    print(f\\\"  Remove HIGH: Relative change in test score after training on \\\"\\n\",\n",
    "    \"          f\\\"{fraction_of_data:.2f}% of the data: {score_change:.2f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    model.fit(train.x[from_index:], train.y[from_index:])\\n\",\n",
    "    \"    reduced_training_score = model.score(test.x, test.y)\\n\",\n",
    "    \"    score_change = 100 * (\\n\",\n",
    "    \"            reduced_training_score - full_training_score) / full_training_score\\n\",\n",
    "    \"    fraction_of_data = 100 * (len(train) - from_index) / len(train)\\n\",\n",
    "    \"    print(f\\\"  Remove LOW: Relative change in test score after training on \\\"\\n\",\n",
    "    \"          f\\\"{fraction_of_data:.2f}% of the data: {score_change:.2f}%\\\")\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Removing high variance points, or training just on them\\n\",\n",
    "    \"\\n\",\n",
    "    \"=> same result\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"variance_q95 = np.quantile(values.variances, 0.95)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, values in enumerate(oob_values):\\n\",\n",
    "    \"    high_variance_indices = np.where(values.variances > variance_q95)[0]\\n\",\n",
    "    \"    model = RandomForestClassifier(n_estimators=n_estimators[i],\\n\",\n",
    "    \"                                   max_samples=1.0,\\n\",\n",
    "    \"                                   class_weight='balanced',\\n\",\n",
    "    \"                                   random_state=random_state)\\n\",\n",
    "    \"    model.fit(train.x, train.y)\\n\",\n",
    "    \"    full_score = model.score(test.x, test.y)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    selected_x, selected_y = train.x[high_variance_indices], train.y[\\n\",\n",
    "    \"        high_variance_indices]\\n\",\n",
    "    \"    model.fit(selected_x, selected_y)\\n\",\n",
    "    \"    reduced_score = model.score(test.x, test.y)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    score_change = 100 * (reduced_score - full_score) / full_score\\n\",\n",
    "    \"    fraction_of_data = 100 * len(selected_x) / len(train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(f\\\"Model with {n_estimators[i]} estimators:\\\")\\n\",\n",
    "    \"    print(f\\\"  Relative change in test score after training on \\\"\\n\",\n",
    "    \"          f\\\"{fraction_of_data:.2f}% of the data: {score_change:.2f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    random_indices = np.random.choice(len(train), len(high_variance_indices),\\n\",\n",
    "    \"                                      replace=False)\\n\",\n",
    "    \"    model.fit(train.x[random_indices], train.y[random_indices])\\n\",\n",
    "    \"    random_score = model.score(test.x, test.y)\\n\",\n",
    "    \"    score_change = 100 * (random_score - full_score) / full_score\\n\",\n",
    "    \"    print(f\\\"  Relative change in test score after training on a random \\\"\\n\",\n",
    "    \"          f\\\"{fraction_of_data:.2f}% of the data: {score_change:.2f}%\\\")\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from sklearn.metrics import confusion_matrix\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = RandomForestClassifier(n_estimators=100,\\n\",\n",
    "    \"                               max_samples=1.0,\\n\",\n",
    "    \"                               class_weight=\\\"balanced\\\")\\n\",\n",
    "    \"model.fit(train.x, train.y)\\n\",\n",
    "    \"full_score = model.score(test.x, test.y)\\n\",\n",
    "    \"confusion_matrix(test.y, model.predict(test.x), normalize='true')\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"model.fit(train.x[high_variance_indices], train.y[high_variance_indices])\\n\",\n",
    "    \"print(f\\\"Using {100 * len(high_variance_indices) / len(train):.2f}% of the data\\\")\\n\",\n",
    "    \"confusion_matrix(test.y, model.predict(test.x), normalize='true')\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Subsampling a random subset of the same size\\n\",\n",
    "    \"\\n\",\n",
    "    \"Yields similar accuracy (expected) but worse FNR and FPR\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"random_indices = np.random.choice(len(train), len(high_variance_indices),\\n\",\n",
    "    \"                                  replace=False)\\n\",\n",
    "    \"model.fit(train.x[random_indices], train.y[random_indices])\\n\",\n",
    "    \"random_score = model.score(test.x, test.y)\\n\",\n",
    "    \"score_change = 100 * (random_score - full_score) / full_score\\n\",\n",
    "    \"fraction_of_data = 100 * len(random_indices) / len(train)\\n\",\n",
    "    \"print(f\\\"Relative change in test score after training on a random \\\"\\n\",\n",
    "    \"      f\\\"{fraction_of_data:.2f}% of the data: {score_change:.2f}%\\\")\\n\",\n",
    "    \"confusion_matrix(test.y, model.predict(test.x), normalize='true')\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Calibrating the model\\n\",\n",
    "    \"\\n\",\n",
    "    \"Fails miserably:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from sklearn.calibration import CalibratedClassifierCV\\n\",\n",
    "    \"\\n\",\n",
    "    \"calibrated_model = CalibratedClassifierCV(estimator=model, method='isotonic')\\n\",\n",
    "    \"calibrated_model.fit(train.x[random_indices], train.y[random_indices])\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=[15, 5])\\n\",\n",
    "    \"\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_estimator(model, test.x, test.y,\\n\",\n",
    "    \"                                      labels=[0, 1], normalize=\\\"true\\\",\\n\",\n",
    "    \"                                      ax=axs[0])\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_estimator(calibrated_model, test.x, test.y,\\n\",\n",
    "    \"                                      labels=[0, 1], normalize=\\\"true\\\",\\n\",\n",
    "    \"                                      ax=axs[1])\\n\",\n",
    "    \"\\n\",\n",
    "    \"axs[0].set_title(\\\"Original model\\\")\\n\",\n",
    "    \"axs[1].set_title(\\\"Calibrated model\\\")\\n\",\n",
    "    \"plt.show();\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Oversampling with imblearn\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"from support.common import ThresholdTunerCV, load_adult_data_raw\\n\",\n",
    "    \"\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"from sklearn import clone\\n\",\n",
    "    \"from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\\n\",\n",
    "    \"from imblearn.pipeline import make_pipeline\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier\\n\",\n",
    "    \"from sklearn.metrics import ConfusionMatrixDisplay, f1_score\\n\",\n",
    "    \"from sklearn.preprocessing import TargetEncoder\\n\",\n",
    "    \"\\n\",\n",
    "    \"random_state = 42\\n\",\n",
    "    \"n_est = 50\\n\",\n",
    "    \"max_samples = 0.2\\n\",\n",
    "    \"df = load_adult_data_raw()\\n\",\n",
    "    \"column_names = df.columns.tolist()\\n\",\n",
    "    \"\\n\",\n",
    "    \"df[\\\"income\\\"] = df[\\\"income\\\"].cat.codes\\n\",\n",
    "    \"df.drop(columns=[\\\"education\\\"], inplace=True)  # education-num is enough\\n\",\n",
    "    \"df.dropna(inplace=True)\\n\",\n",
    "    \"column_names.remove(\\\"education\\\")\\n\",\n",
    "    \"column_names.remove(\\\"income\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"x_train, x_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"        df.drop(columns=[\\\"income\\\"]).values,\\n\",\n",
    "    \"        df[\\\"income\\\"].values,\\n\",\n",
    "    \"        train_size=0.6,\\n\",\n",
    "    \"        random_state=random_state,\\n\",\n",
    "    \"        stratify=df[\\\"income\\\"].values,\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"encoder = TargetEncoder(target_type=\\\"binary\\\", random_state=random_state)\\n\",\n",
    "    \"classifier = RandomForestClassifier(n_estimators=n_est,\\n\",\n",
    "    \"                                    max_samples=max_samples,\\n\",\n",
    "    \"                                    class_weight=\\\"balanced\\\",\\n\",\n",
    "    \"                                    random_state=random_state)\\n\",\n",
    "    \"tuned_classifier = ThresholdTunerCV(classifier, n_splits=6, metric=f1_score,\\n\",\n",
    "    \"                                    n_jobs=6)\\n\",\n",
    "    \"ros_sampler = RandomOverSampler(random_state=random_state)\\n\",\n",
    "    \"smote_sampler = SMOTE(random_state=random_state)\\n\",\n",
    "    \"adasyn_sampler = ADASYN(random_state=random_state)\\n\",\n",
    "    \"\\n\",\n",
    "    \"simple_pipeline = make_pipeline(encoder, clone(classifier))\\n\",\n",
    "    \"ros_pipeline = make_pipeline(encoder, ros_sampler, clone(classifier))\\n\",\n",
    "    \"tuned_pipeline = make_pipeline(encoder, clone(tuned_classifier))\\n\",\n",
    "    \"tuned_oversampling_pipeline = make_pipeline(encoder,\\n\",\n",
    "    \"                                            ros_sampler,\\n\",\n",
    "    \"                                            clone(tuned_classifier))\\n\",\n",
    "    \"smote_pipeline = make_pipeline(encoder, smote_sampler, clone(tuned_classifier))\\n\",\n",
    "    \"adasyn_pipeline = make_pipeline(encoder, adasyn_sampler, clone(tuned_classifier))\\n\",\n",
    "    \"\\n\",\n",
    "    \"simple_pipeline.fit(x_train, y_train)\\n\",\n",
    "    \"full_score = simple_pipeline.score(x_test, y_test)\\n\",\n",
    "    \"print(f\\\"Accuracy with no oversampling: {full_score:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"ros_pipeline.fit(x_train, y_train)\\n\",\n",
    "    \"full_score = ros_pipeline.score(x_test, y_test)\\n\",\n",
    "    \"print(f\\\"Accuracy with random oversampling: {full_score:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"tuned_pipeline.fit(x_train, y_train)\\n\",\n",
    "    \"full_score = tuned_pipeline.score(x_test, y_test)\\n\",\n",
    "    \"print(f\\\"Accuracy with tuned threshold: {full_score:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"tuned_oversampling_pipeline.fit(x_train, y_train)\\n\",\n",
    "    \"full_score = tuned_oversampling_pipeline.score(x_test, y_test)\\n\",\n",
    "    \"print(f\\\"Accuracy with tuned threshold and ROS: {full_score:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"smote_pipeline.fit(x_train, y_train)\\n\",\n",
    "    \"full_score = smote_pipeline.score(x_test, y_test)\\n\",\n",
    "    \"print(f\\\"Accuracy with SMOTE: {full_score:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"adasyn_pipeline.fit(x_train, y_train)\\n\",\n",
    "    \"full_score = adasyn_pipeline.score(x_test, y_test)\\n\",\n",
    "    \"print(f\\\"Accuracy with ADASYN: {full_score:.4f}\\\")\\n\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Sanity check: did we fit the training set well?\\n\",\n",
    "    \"(simple_pipeline.score(x_train, y_train),\\n\",\n",
    "    \" ros_pipeline.score(x_train, y_train),\\n\",\n",
    "    \" tuned_pipeline.score(x_train, y_train),\\n\",\n",
    "    \" tuned_oversampling_pipeline.score(x_train, y_train),\\n\",\n",
    "    \" smote_pipeline.score(x_train, y_train),\\n\",\n",
    "    \" adasyn_pipeline.score(x_train, y_train),\\n\",\n",
    "    \" )\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"fig, axs = plt.subplots(nrows=3, ncols=2, figsize=[12, 12])\\n\",\n",
    "    \"\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_estimator(simple_pipeline, x_test, y_test,\\n\",\n",
    "    \"                                      labels=[0, 1], normalize=\\\"true\\\",\\n\",\n",
    "    \"                                      ax=axs[0][0])\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_estimator(ros_pipeline, x_test, y_test,\\n\",\n",
    "    \"                                      labels=[0, 1], normalize=\\\"true\\\",\\n\",\n",
    "    \"                                      ax=axs[0][1])\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_estimator(tuned_pipeline, x_test, y_test,\\n\",\n",
    "    \"                                      labels=[0, 1], normalize=\\\"true\\\",\\n\",\n",
    "    \"                                      ax=axs[1][0])\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_estimator(tuned_oversampling_pipeline, x_test,\\n\",\n",
    "    \"                                      y_test,\\n\",\n",
    "    \"                                      labels=[0, 1], normalize=\\\"true\\\",\\n\",\n",
    "    \"                                      ax=axs[1][1])\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_estimator(smote_pipeline, x_test, y_test,\\n\",\n",
    "    \"                                      labels=[0, 1], normalize=\\\"true\\\",\\n\",\n",
    "    \"                                      ax=axs[2][0])\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_estimator(adasyn_pipeline, x_test,\\n\",\n",
    "    \"                                      y_test,\\n\",\n",
    "    \"                                      labels=[0, 1], normalize=\\\"true\\\",\\n\",\n",
    "    \"                                      ax=axs[2][1])\\n\",\n",
    "    \"\\n\",\n",
    "    \"axs[0][0].set_title(\\\"No oversampling\\\")\\n\",\n",
    "    \"axs[0][1].set_title(\\\"ROS\\\")\\n\",\n",
    "    \"axs[1][0].set_title(\\\"Tuned threshold\\\")\\n\",\n",
    "    \"axs[1][1].set_title(\\\"Tuned threshold with ROS\\\")\\n\",\n",
    "    \"axs[2][0].set_title(\\\"SMOTE\\\")\\n\",\n",
    "    \"axs[2][1].set_title(\\\"ADASYN\\\")\\n\",\n",
    "    \"plt.show();\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from sklearn.metrics import precision_recall_curve\\n\",\n",
    "    \"\\n\",\n",
    "    \"y_proba = ros_pipeline.predict_proba(x_test)[:, 1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\\n\",\n",
    "    \"f_score = 2 * precision * recall / (precision + recall)\\n\",\n",
    "    \"optimal_index = np.argmax(f_score)\\n\",\n",
    "    \"chosen_threshold = thresholds[optimal_index]\\n\",\n",
    "    \"y_pred = (y_proba >= chosen_threshold).astype(int)\\n\",\n",
    "    \"print(\\n\",\n",
    "    \"        f\\\"Accuracy at chosen threshold {chosen_threshold:.2f}: {np.mean(y_pred == y_test):.4f}\\\")\\n\",\n",
    "    \"ConfusionMatrixDisplay.from_predictions(y_test, y_pred, labels=[0, 1],\\n\",\n",
    "    \"                                        normalize=\\\"true\\\");\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"celltoolbar\": \"Edit Metadata\",\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.12\"\n",
    "  },\n",
    "  \"vscode\": {\n",
    "   \"interpreter\": {\n",
    "    \"hash\": \"4e000971326892723e7f31ded70802f690c31c3620f59a0f99e594aaee3047ef\"\n",
    "   }\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-OOB for random forests and bagged classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the use of [Data-OOB][../../value/data-oob] from Kwon and Zou \"[Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value](https://proceedings.mlr.press/v202/kwon23e.html)\" (ICML 2023), to compute values for bagged models.\n",
    "\n",
    "We will work with the [adult classification dataset](https://archive.ics.uci.edu/dataset/2/adult) from the UCI repository. It's an imbalanced dataset where the objective is to predict whether a person earns more than $50K a year (the \"positive\" class) based on a set of features such as age, education, occupation, etc. After training a random forest on this dataset, we will compute the Data-OOB values and analyze them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "We begin by importing the main libraries and setting some defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, f1_score\n",
    "from support.common import load_adult_data, load_adult_data_raw\n",
    "\n",
    "from pydvl.reporting.plots import plot_ci_array, plot_ci_values\n",
    "\n",
    "matplotlib.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\n",
    "plt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\n",
    "plt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\n",
    "MEAN_COLORS = [\"dodgerblue\", \"indianred\", \"limegreen\", \"darkorange\", \"darkorchid\"]\n",
    "SHADE_COLORS = [\"lightskyblue\", \"firebrick\", \"seagreen\", \"gold\", \"plum\"]\n",
    "\n",
    "is_CI = os.environ.get(\"CI\")\n",
    "random_state = 42\n",
    "random.seed(random_state);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pydvl.valuation import DataOOBValuation, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With a helper function we download the data, encode the categorical variables using [TargetEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html), and split it into training and testing sets. We must be careful to stratify the split by the target variable (income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "train, test = load_adult_data(train_size=0.6, subsample=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "if is_CI:  # Subsample 1% of the data for faster testing\n",
    "    train, test = load_adult_data(\n",
    "        train_size=0.6, subsample=0.01, random_state=random_state\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we would carefully look at the features, check for missing values, outliers, etc. But for the sake of this example, we will skip this step and jump straight into training a model. We will only look at the class distribution since it will matter later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{len(train)} samples. Class distribution: {100 * np.mean(train.y == 1):.1f}% positive, \"\n",
    "    f\"{100 * np.mean(train.y == 0):.1f}% negative\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick baseline, we train a standard sklearn [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Since the dataset is imbalanced, besides the accuracy we look at the confusion matrix, and notice that despite weighting the class by their inverse frequency with `class_weight=\"balanced\"`, the model is not very good at predicting the minority (\"positive\", or \"1\") class: in the left hand side of the figure below we see a high rate of false negatives. This will play a role later in how we interpret the values that Data-OOB returns, and requires us to address the imbalance in the dataset. We do this with a simple random over sampling using `imblearn`'s [RandomOverSampler](...) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "n_est = 100\n",
    "max_samples = 0.2  # Use small bootstrap samples\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=n_est,\n",
    "    max_samples=max_samples,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=random_state,\n",
    ")\n",
    "model.fit(train.x, train.y)\n",
    "base_predictions = model.predict(test.x)\n",
    "\n",
    "sampler = RandomOverSampler(random_state=random_state)\n",
    "train.x, train.y = sampler.fit_resample(train.x, train.y)\n",
    "\n",
    "model.fit(train.x, train.y)\n",
    "predictions_oversampled = model.predict(test.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{len(train)} samples. Class distribution: {100 * train.y.mean():.1f}% positive, \"\n",
    "    f\"{100 * (1 - train.y.mean()):.1f}% negative\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "base_accuracy = accuracy_score(test.y, base_predictions)\n",
    "accuracy_oversampled = accuracy_score(test.y, predictions_oversampled)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    test.y, base_predictions, ax=axs[0], labels=[0, 1], normalize=\"true\"\n",
    ")\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    test.y, predictions_oversampled, ax=axs[1], labels=[0, 1], normalize=\"true\"\n",
    ")\n",
    "\n",
    "axs[0].set_title(f\"Original dataset, {n_est} trees.\\nAccuracy: {base_accuracy:.2f}\")\n",
    "axs[1].set_title(\n",
    "    f\"Oversampled dataset, {n_est} trees.\\nAccuracy: {accuracy_oversampled:.2f}\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "invertible-output"
    ]
   },
   "source": [
    "## Computing the OOB values\n",
    "\n",
    "The main idea of Data-OOB is to use the out-of-bag error estimates of a bagged model to compute data values. In pyDVL, we provide a class [DataOOBValuation][pydvl.valuation.DataOOBValuation] that takes an existing classification or regression bagging model and uses the per-sample out-of-bag performance estimate for the value of each point.\n",
    "\n",
    "Let's compute and compare the Data-OOB values with three choices for the number of estimators of a random forest. After fitting the random forest, we use the [fit][pydvl.valuation.DataOOBValuation.fit] method to compute the values and store them in [ValuationResult][pydvl.value.result.ValuationResult] objects.\n",
    "\n",
    "Even though it's not relevant to our discussion, notice how the accuracy barely changes with the number of estimators. Below, we will discuss using the values to identify \"easy\" or \"hard\" samples in the dataset, but first let's quickly look at the values themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [50, 100, 200]\n",
    "oob_values = []\n",
    "for i, n_est in enumerate(n_estimators, start=1):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_est,\n",
    "        max_samples=max_samples,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # Tuning messes everything up if we oversample the dataset\n",
    "    # model = ThresholdTunerCV(classifier, n_splits=6, metric=f1_score, n_jobs=6)\n",
    "    model.fit(train.x, train.y)\n",
    "    accuracy = model.score(test.x, test.y)\n",
    "    print(f\"Accuracy with {n_est} estimators: {accuracy:.2f}\")\n",
    "    valuation = DataOOBValuation(model)\n",
    "    valuation.fit(train)\n",
    "    oob_values.append(valuation.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The distribution of values\n",
    "\n",
    "The left-hand side of the figure below depicts value as it increases with rank and a 95% t-confidence interval. The right-hand side shows the histogram of values.\n",
    "\n",
    "We observe a long tail of high values. This is because the score $T$ used in Data-OOB (accuracy in this case) is a binary variable, and the value $\\psi_i$ is the fraction of times that all weak learners not trained on the $i$-th point classify it correctly. Given the imbalance in the dataset, many learners will always predict the majority (\"negative\", < $50K earnings / year) class and be correct on 75% of the dataset, leading to this tail.\n",
    "\n",
    "Besides the actual value $\\psi_i$, [ValuationResult][pydvl.value.result.ValuationResult] objects store the number of times a sample is OOB —the quantity $\\sum_{b=1}^{B} \\mathbb{1} (w_{bi}=0)$— in the `counts` attribute, and the variance of the OOB score in the `variances` attribute. We use the latter in the plot below in order to display the confidence intervals, but it is important to note that the interpretation varies from one valuation method to another:\n",
    "\n",
    "For Shapley-based valuation methods, the variance is that of the marginal changes in the performance of the model when trained on subsets of the data with and without a sample, evaluated on a fixed valuation dataset, and could in principle be used to see whether values have (roughly) converged. But for Data-OOB, it is the variance of the performance of the ensemble of weak learners on the sample when it is OOB. Although similar in spirit, the construction is different and can be misleading.\n",
    "\n",
    "As a matter of fact, the interpretation of the vanishing variance at the tail has little to do with valuation and everything to do with our dataset, as introduced above: As the number of estimators increases, the chance of all of them failing on the same points decreases, up to a point. The same happens when we increase the maximum depth (try it!). This behaviour is then not a deep property of Data-OOB from which to gain new insights, but rather a consequence of the dataset and the model, as we further elaborate below.\n",
    "\n",
    "*Note that a symmetric CI is actually incorrect in this situation since all values are bounded between 0 and 1 (instead we could use a bootstrapped CI if we stored the scores of all estimators in the bagging model, but this is not implemented in pyDVL).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=[15, 5])\n",
    "\n",
    "for n_est, values, mean_color, shade_color in zip(\n",
    "    n_estimators, oob_values, MEAN_COLORS, SHADE_COLORS\n",
    "):\n",
    "    values.sort()\n",
    "    plot_ci_values(\n",
    "        values,\n",
    "        level=0.05,\n",
    "        mean_color=mean_color,\n",
    "        shade_color=shade_color,\n",
    "        ax=ax1,\n",
    "        label=f\"{n_est} estimators\",\n",
    "    )\n",
    "\n",
    "    ax2.hist(values, bins=50, color=mean_color, alpha=0.5, label=f\"{n_est} estimators\")\n",
    "\n",
    "ax1.set_title(\"Point rank\")\n",
    "ax1.set_xlabel(\"Rank\")\n",
    "ax1.set_ylabel(\"Data-OOB values\")\n",
    "ax1.legend()\n",
    "ax2.set_title(\"Histogram of Data-OOB values\")\n",
    "ax2.set_xlabel(\"Value\")\n",
    "ax2.legend()\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this, focus on the long tails with zero variance. These are samples for which the score $T(y_i, \\hat{f}_b(x_i)) = 1$ for *every* estimator $\\hat{f}_b$ not trained on them, that is: *every weak learner in the ensemble correctly classifies these samples*. As we said above, this can happen because it is likely for weak estimators to be fitted to always predict the majority (negative) class:\n",
    "\n",
    "## This analysis is now wrong: 73% of the points in the tail are in the minority class (probably have been oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "v = oob_values[0]\n",
    "from_index = np.where(np.isclose(v.variances, 0.0) & np.isclose(v.values, 1))[0][0]\n",
    "tail = values.indices[from_index:]\n",
    "print(\n",
    "    f\"There are {len(tail)} points with value 1 and zero variance\"\n",
    "    f\" ({100 * len(tail) / len(train):.2f}% of the data).\"\n",
    ")\n",
    "print(\n",
    "    f\"Of these, {100 * (train.y[tail] == 0).sum() / len(tail):.2f}% \"\n",
    "    f\"are in the majority class.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tail, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Simply put, the ensemble is mostly good at classifying the majority class, and the variance of the OOB score for these samples is very low. This is a common issue in imbalanced datasets, and it is one of the reasons why the OOB score might not be a good metric for model performance in these cases. For us, it shows that the values reflect only poorly fit models.\n",
    "\n",
    "We test this below by ensembling a number of constant classifiers, but how does this affect our ability to use the values for data inspection, cleaning, etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A detour to further interpret the OOB values\n",
    "\n",
    "We can verify that the OOB values in our case reflect the imbalance of the dataset by training a [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) with constant estimators. A fraction of `n_estimators` will always pick class 1, and the rest class 0. This leads to a clear jump in the value rank plot, either around 25% or 75% of them, since, as we saw above, 25% of the samples are in the positive (\"1\") class, and 75% in the negative (\"0\").\n",
    "\n",
    "We will use three different probabilities for the constant estimators to predict class 0: 0.01, 0.5, and 0.99. Again, the idea is that the OOB values will reflect the class distribution of the dataset, and we should see a clear jump in the values around 25% and 75% of the data.\n",
    "\n",
    "The code is analogous to the above when we fitted the random forest, so it is ommitted from the documentation, but we use a custom class `ConstantBinaryClassifier` as base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from support.common import ConstantBinaryClassifier\n",
    "\n",
    "train, test = load_adult_data(train_size=0.6, subsample=0.2, random_state=random_state)\n",
    "\n",
    "probs = [0.01, 0.5, 0.99]\n",
    "all_values = []\n",
    "for p in probs:\n",
    "    model = BaggingClassifier(\n",
    "        ConstantBinaryClassifier(p, random_state),\n",
    "        n_estimators=100,\n",
    "        max_samples=1.0,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    model.fit(train.x, train.y)\n",
    "    score = model.score(test.x, test.y)\n",
    "    print(\n",
    "        f\"Accuracy when ~{100 * p:.1f}% of estimators always predict class 0: {score:.4f}\"\n",
    "    )\n",
    "\n",
    "    valuation = DataOOBValuation(model)\n",
    "    valuation.fit(train)\n",
    "    all_values.append(valuation.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[15, 5])\n",
    "\n",
    "for p, values, mean_color, shade_color in zip(\n",
    "    probs, all_values, MEAN_COLORS, SHADE_COLORS\n",
    "):\n",
    "    values.sort(key=\"value\")\n",
    "    plot_ci_values(\n",
    "        values,\n",
    "        ax=ax,\n",
    "        level=0.01,\n",
    "        mean_color=mean_color,\n",
    "        shade_color=shade_color,\n",
    "        label=f\"Data-OOB, p={p:.2f}\",\n",
    "    )\n",
    "\n",
    "ax.axvline(\n",
    "    0.25 * len(all_values[0]), color=\"darkorange\", linestyle=\"--\", label=\"25% data\"\n",
    ")\n",
    "ax.axvline(0.75 * len(all_values[0]), color=\"darkred\", linestyle=\"--\", label=\"75% data\")\n",
    "plt.ylabel(\"Data-OOB values\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.title(\"Model with constant estimators\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bogus (?) conclusion\n",
    "\n",
    "The conclusion of this discussion seems to be that for this imbalanced dataset and poorly performing model, the usual intuition that extreme values characterize \"easy\" or \"hard\" points might be bogus.\n",
    " \n",
    "If we discard the datapoints with the highest values, i.e. those which the ensemble of weak learners classifies correctly every time, because we believe that those are trivial in some sense (e.g. repeated) and bias the ensemble towards stricter decisions boundaries, we obtain very mild changes. The same happens when removing low-valued points. Basically, the changes in test scores could be just noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Data-OOB values with data removal\n",
    "\n",
    "We can go beyond the simple analysis above and systematically evaluate the impact of removing data points with high or low values on the model's performance. This is a common practice in the literature. We can use the [compute_removal_score][pydvl.reporting.scores.compute_removal_score] function to do it. This function takes a [ModelUtility][pydvl.valuation.ModelUtility] object, a [ValuationResult][pydvl.value.result.ValuationResult] object, and a [Dataset][pydvl.data.dataset.Dataset] object, and computes the performance of the model after removing a fraction of the data with the highest or lowest values.\n",
    "\n",
    "Recall from above that we can construct these objects as follows:\n",
    "\n",
    "```python\n",
    "model = RandomForestClassifier(n_estimators=n_est, max_samples=max_samples, random_state=seed)\n",
    "utility = ModelUtility(model, SupervisedScorer(\"accuracy\", test, 0.0), clone_before_fit=False)\n",
    "\n",
    "valuation = DataOOBValuation(model)\n",
    "valuation.fit(train)\n",
    "values = valuation.values()\n",
    "```\n",
    "\n",
    "The details are hidden in the rendered documentation, please refer to the notebook for the full code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "In order to call `run_removal_experiment()` we need to define 3 types of factories:\n",
    "\n",
    "1. A factory that returns a train-test split of the data given a random state \n",
    "2. A factory that returns a utility that evaluates a model on a given test set.\n",
    "   This is used for the performance evaluation. The model need not be the same\n",
    "   as the one used for the valuation.\n",
    "4. A factory returning a valuation method. The training set is passed to the\n",
    "   factory, in case the valuation needs to train something. E.g. for Data-OOB\n",
    "   we need the bagging model to be fitted before the valuation is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from support.common import ThresholdTunerCV\n",
    "from support.removal_experiment import run_removal_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "from pydvl.valuation import KNNClassifierUtility, ModelUtility, SupervisedScorer\n",
    "from pydvl.valuation.methods.data_oob import point_wise_accuracy\n",
    "from pydvl.valuation.methods.random import RandomValuation\n",
    "\n",
    "\n",
    "def make_data(random_state: int) -> tuple[Dataset, Dataset]:\n",
    "    return load_adult_data(train_size=0.6, random_state=random_state)\n",
    "\n",
    "\n",
    "def make_utility(test: Dataset, random_state: int) -> ModelUtility:\n",
    "    classifier = RandomForestClassifier(\n",
    "        n_estimators=n_est,\n",
    "        max_samples=max_samples,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=random_state + 1,\n",
    "    )\n",
    "    model = ThresholdTunerCV(\n",
    "        classifier, n_splits=5, metric=f1_score, n_jobs=1, random_state=random_state\n",
    "    )\n",
    "    return ModelUtility(model, SupervisedScorer(\"accuracy\", test, 0.0))\n",
    "\n",
    "\n",
    "def make_oob(train: Dataset, random_state: int) -> DataOOBValuation:\n",
    "    classifier = RandomForestClassifier(\n",
    "        n_estimators=n_est,\n",
    "        max_samples=max_samples,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    model = ThresholdTunerCV(\n",
    "        classifier, n_splits=5, metric=f1_score, n_jobs=1, random_state=random_state\n",
    "    )\n",
    "    model.fit(train.x, train.y)\n",
    "    return DataOOBValuation(model, point_wise_accuracy)\n",
    "\n",
    "\n",
    "def make_random(train: Dataset, random_state: int) -> RandomValuation:\n",
    "    return RandomValuation(random_state=random_state)\n",
    "\n",
    "\n",
    "removal_percentages = np.arange(0, 0.51, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "low_scores_df, high_scores_df = run_removal_experiment(\n",
    "    data_factory=make_data,\n",
    "    valuation_factories=[make_random, make_oob],\n",
    "    utility_factory=make_utility,  # for evaluation\n",
    "    removal_percentages=removal_percentages,\n",
    "    n_runs=6,\n",
    "    n_jobs=30,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=[15, 5])\n",
    "\n",
    "for i, scores_df in enumerate((low_scores_df, high_scores_df)):\n",
    "    for j, (method_name, df) in enumerate(scores_df.groupby(\"method_name\")):\n",
    "        plot_ci_array(\n",
    "            data=df.drop(columns=[\"method_name\"]).values,\n",
    "            level=0.05,\n",
    "            abscissa=np.round(removal_percentages, 2),\n",
    "            mean_color=MEAN_COLORS[j],\n",
    "            shade_color=SHADE_COLORS[j],\n",
    "            label=method_name,\n",
    "            ax=axs[i],\n",
    "        )\n",
    "        axs[i].legend()\n",
    "        axs[i].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axs[0].set_title(\"Lowest value removal, 95% CI\")\n",
    "axs[0].set_xlabel(\"Fraction of data removed\")\n",
    "axs[1].set_title(\"Highest value removal, 95% CI\")\n",
    "axs[1].set_xlabel(\"Fraction of data removed\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIXME: THERE'S A BUG ABOVE\n",
    "\n",
    "The accuracy curves do not start at the same value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Data-OOB with arbitrary models\n",
    "\n",
    "Note that even though the method is designed for bagging models, in principle it can be used with any other estimator by fitting a bagging model on top of it. This can generally be quite expensive, but it might prove useful in some cases. Below is what happens when we do this with a k-nearest neighbors classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# We want to use the KNN classifier as model for the utility, not the bagged model\n",
    "def make_knn_utility(test: Dataset, random_state: int) -> ModelUtility:\n",
    "    return KNNClassifierUtility(KNeighborsClassifier(n_neighbors=10), test)\n",
    "\n",
    "\n",
    "def make_oob_knn(train: Dataset, random_state: int) -> DataOOBValuation:\n",
    "    classifier = BaggingClassifier(\n",
    "        estimator=KNeighborsClassifier(n_neighbors=10),\n",
    "        max_samples=0.4,\n",
    "        n_estimators=10,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    model = ThresholdTunerCV(\n",
    "        classifier, n_splits=5, metric=f1_score, n_jobs=1, random_state=random_state\n",
    "    )\n",
    "    model.fit(train.x, train.y)\n",
    "    return DataOOBValuation(model, point_wise_accuracy)\n",
    "\n",
    "\n",
    "low_scores_df, high_scores_df = run_removal_experiment(\n",
    "    data_factory=make_data,\n",
    "    utility_factory=make_knn_utility,\n",
    "    valuation_factories=[make_random, make_oob_knn],\n",
    "    removal_percentages=removal_percentages,\n",
    "    n_runs=20,\n",
    "    n_jobs=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "invertible-output"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=[15, 5])\n",
    "\n",
    "for i, scores_df in enumerate((low_scores_df, high_scores_df)):\n",
    "    for j, (method_name, df) in enumerate(scores_df.groupby(\"method_name\")):\n",
    "        plot_ci_array(\n",
    "            data=df.drop(columns=[\"method_name\"]).values,\n",
    "            level=0.05,\n",
    "            abscissa=np.round(removal_percentages, 2),\n",
    "            mean_color=MEAN_COLORS[j],\n",
    "            shade_color=SHADE_COLORS[j],\n",
    "            label=method_name,\n",
    "            ax=axs[i],\n",
    "        )\n",
    "        axs[i].legend()\n",
    "        axs[i].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axs[0].set_title(\"Lowest value removal, 95% CI\")\n",
    "axs[0].set_xlabel(\"Fraction of data removed\")\n",
    "axs[1].set_title(\"Highest value removal, 95% CI\")\n",
    "axs[1].set_xlabel(\"Fraction of data removed\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The impact of removing data on the three random forest models\n",
    "\n",
    "Manually removing and retraining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_adult_data_raw()\n",
    "corr = df.corr(numeric_only=True)\n",
    "corr.style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_adult_data(train_size=0.6, random_state=random_state)\n",
    "for i, values in enumerate(oob_values):\n",
    "    zero_variances = np.isclose(values.variances, 0.0)\n",
    "    to_index = np.where(zero_variances & np.isclose(values.values, 1))[0][0]\n",
    "    try:\n",
    "        from_index = np.where(zero_variances & np.isclose(values.values, 0))[0][-1]\n",
    "    except IndexError:\n",
    "        from_index = 0\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators[i], max_samples=max_samples, random_state=random_state\n",
    "    )\n",
    "    model.fit(train.x, train.y)\n",
    "    full_training_score = model.score(test.x, test.y)\n",
    "\n",
    "    model.fit(train.x[:to_index], train.y[:to_index])\n",
    "    reduced_training_score = model.score(test.x, test.y)\n",
    "    score_change = (\n",
    "        100 * (reduced_training_score - full_training_score) / full_training_score\n",
    "    )\n",
    "    fraction_of_data = 100 * to_index / len(train)\n",
    "    print(f\"Model with {n_estimators[i]} estimators:\")\n",
    "    print(\n",
    "        f\"  Remove HIGH: Relative change in test score after training on \"\n",
    "        f\"{fraction_of_data:.2f}% of the data: {score_change:.2f}%\"\n",
    "    )\n",
    "\n",
    "    model.fit(train.x[from_index:], train.y[from_index:])\n",
    "    reduced_training_score = model.score(test.x, test.y)\n",
    "    score_change = (\n",
    "        100 * (reduced_training_score - full_training_score) / full_training_score\n",
    "    )\n",
    "    fraction_of_data = 100 * (len(train) - from_index) / len(train)\n",
    "    print(\n",
    "        f\"  Remove LOW: Relative change in test score after training on \"\n",
    "        f\"{fraction_of_data:.2f}% of the data: {score_change:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing high variance points, or training just on them\n",
    "\n",
    "=> same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_q95 = np.quantile(values.variances, 0.95)\n",
    "\n",
    "for i, values in enumerate(oob_values):\n",
    "    high_variance_indices = np.where(values.variances > variance_q95)[0]\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators[i],\n",
    "        max_samples=1.0,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    model.fit(train.x, train.y)\n",
    "    full_score = model.score(test.x, test.y)\n",
    "\n",
    "    selected_x, selected_y = (\n",
    "        train.x[high_variance_indices],\n",
    "        train.y[high_variance_indices],\n",
    "    )\n",
    "    model.fit(selected_x, selected_y)\n",
    "    reduced_score = model.score(test.x, test.y)\n",
    "\n",
    "    score_change = 100 * (reduced_score - full_score) / full_score\n",
    "    fraction_of_data = 100 * len(selected_x) / len(train)\n",
    "\n",
    "    print(f\"Model with {n_estimators[i]} estimators:\")\n",
    "    print(\n",
    "        f\"  Relative change in test score after training on \"\n",
    "        f\"{fraction_of_data:.2f}% of the data: {score_change:.2f}%\"\n",
    "    )\n",
    "\n",
    "    random_indices = np.random.choice(\n",
    "        len(train), len(high_variance_indices), replace=False\n",
    "    )\n",
    "    model.fit(train.x[random_indices], train.y[random_indices])\n",
    "    random_score = model.score(test.x, test.y)\n",
    "    score_change = 100 * (random_score - full_score) / full_score\n",
    "    print(\n",
    "        f\"  Relative change in test score after training on a random \"\n",
    "        f\"{fraction_of_data:.2f}% of the data: {score_change:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100, max_samples=1.0, class_weight=\"balanced\"\n",
    ")\n",
    "model.fit(train.x, train.y)\n",
    "full_score = model.score(test.x, test.y)\n",
    "confusion_matrix(test.y, model.predict(test.x), normalize=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train.x[high_variance_indices], train.y[high_variance_indices])\n",
    "print(f\"Using {100 * len(high_variance_indices) / len(train):.2f}% of the data\")\n",
    "confusion_matrix(test.y, model.predict(test.x), normalize=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling a random subset of the same size\n",
    "\n",
    "Yields similar accuracy (expected) but worse FNR and FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(len(train), len(high_variance_indices), replace=False)\n",
    "model.fit(train.x[random_indices], train.y[random_indices])\n",
    "random_score = model.score(test.x, test.y)\n",
    "score_change = 100 * (random_score - full_score) / full_score\n",
    "fraction_of_data = 100 * len(random_indices) / len(train)\n",
    "print(\n",
    "    f\"Relative change in test score after training on a random \"\n",
    "    f\"{fraction_of_data:.2f}% of the data: {score_change:.2f}%\"\n",
    ")\n",
    "confusion_matrix(test.y, model.predict(test.x), normalize=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrating the model\n",
    "\n",
    "Fails miserably:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "calibrated_model = CalibratedClassifierCV(estimator=model, method=\"isotonic\")\n",
    "calibrated_model.fit(train.x[random_indices], train.y[random_indices])\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=[15, 5])\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    model, test.x, test.y, labels=[0, 1], normalize=\"true\", ax=axs[0]\n",
    ")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    calibrated_model, test.x, test.y, labels=[0, 1], normalize=\"true\", ax=axs[1]\n",
    ")\n",
    "\n",
    "axs[0].set_title(\"Original model\")\n",
    "axs[1].set_title(\"Calibrated model\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling with imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from support.common import load_adult_data_raw\n",
    "\n",
    "random_state = 42\n",
    "n_est = 50\n",
    "max_samples = 0.2\n",
    "df = load_adult_data_raw()\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "df[\"income\"] = df[\"income\"].cat.codes\n",
    "df.drop(columns=[\"education\"], inplace=True)  # education-num is enough\n",
    "df.dropna(inplace=True)\n",
    "column_names.remove(\"education\")\n",
    "column_names.remove(\"income\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=[\"income\"]).values,\n",
    "    df[\"income\"].values,\n",
    "    train_size=0.6,\n",
    "    random_state=random_state,\n",
    "    stratify=df[\"income\"].values,\n",
    ")\n",
    "\n",
    "encoder = TargetEncoder(target_type=\"binary\", random_state=random_state)\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=n_est,\n",
    "    max_samples=max_samples,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=random_state,\n",
    ")\n",
    "tuned_classifier = ThresholdTunerCV(classifier, n_splits=6, metric=f1_score, n_jobs=6)\n",
    "ros_sampler = RandomOverSampler(random_state=random_state)\n",
    "smote_sampler = SMOTE(random_state=random_state)\n",
    "adasyn_sampler = ADASYN(random_state=random_state)\n",
    "\n",
    "simple_pipeline = make_pipeline(encoder, clone(classifier))\n",
    "ros_pipeline = make_pipeline(encoder, ros_sampler, clone(classifier))\n",
    "tuned_pipeline = make_pipeline(encoder, clone(tuned_classifier))\n",
    "tuned_oversampling_pipeline = make_pipeline(\n",
    "    encoder, ros_sampler, clone(tuned_classifier)\n",
    ")\n",
    "smote_pipeline = make_pipeline(encoder, smote_sampler, clone(tuned_classifier))\n",
    "adasyn_pipeline = make_pipeline(encoder, adasyn_sampler, clone(tuned_classifier))\n",
    "\n",
    "simple_pipeline.fit(x_train, y_train)\n",
    "full_score = simple_pipeline.score(x_test, y_test)\n",
    "print(f\"Accuracy with no oversampling: {full_score:.4f}\")\n",
    "\n",
    "ros_pipeline.fit(x_train, y_train)\n",
    "full_score = ros_pipeline.score(x_test, y_test)\n",
    "print(f\"Accuracy with random oversampling: {full_score:.4f}\")\n",
    "\n",
    "tuned_pipeline.fit(x_train, y_train)\n",
    "full_score = tuned_pipeline.score(x_test, y_test)\n",
    "print(f\"Accuracy with tuned threshold: {full_score:.4f}\")\n",
    "\n",
    "tuned_oversampling_pipeline.fit(x_train, y_train)\n",
    "full_score = tuned_oversampling_pipeline.score(x_test, y_test)\n",
    "print(f\"Accuracy with tuned threshold and ROS: {full_score:.4f}\")\n",
    "\n",
    "smote_pipeline.fit(x_train, y_train)\n",
    "full_score = smote_pipeline.score(x_test, y_test)\n",
    "print(f\"Accuracy with SMOTE: {full_score:.4f}\")\n",
    "\n",
    "adasyn_pipeline.fit(x_train, y_train)\n",
    "full_score = adasyn_pipeline.score(x_test, y_test)\n",
    "print(f\"Accuracy with ADASYN: {full_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: did we fit the training set well?\n",
    "(\n",
    "    simple_pipeline.score(x_train, y_train),\n",
    "    ros_pipeline.score(x_train, y_train),\n",
    "    tuned_pipeline.score(x_train, y_train),\n",
    "    tuned_oversampling_pipeline.score(x_train, y_train),\n",
    "    smote_pipeline.score(x_train, y_train),\n",
    "    adasyn_pipeline.score(x_train, y_train),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=[12, 12])\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    simple_pipeline, x_test, y_test, labels=[0, 1], normalize=\"true\", ax=axs[0][0]\n",
    ")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    ros_pipeline, x_test, y_test, labels=[0, 1], normalize=\"true\", ax=axs[0][1]\n",
    ")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    tuned_pipeline, x_test, y_test, labels=[0, 1], normalize=\"true\", ax=axs[1][0]\n",
    ")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    tuned_oversampling_pipeline,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    labels=[0, 1],\n",
    "    normalize=\"true\",\n",
    "    ax=axs[1][1],\n",
    ")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    smote_pipeline, x_test, y_test, labels=[0, 1], normalize=\"true\", ax=axs[2][0]\n",
    ")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    adasyn_pipeline, x_test, y_test, labels=[0, 1], normalize=\"true\", ax=axs[2][1]\n",
    ")\n",
    "\n",
    "axs[0][0].set_title(\"No oversampling\")\n",
    "axs[0][1].set_title(\"ROS\")\n",
    "axs[1][0].set_title(\"Tuned threshold\")\n",
    "axs[1][1].set_title(\"Tuned threshold with ROS\")\n",
    "axs[2][0].set_title(\"SMOTE\")\n",
    "axs[2][1].set_title(\"ADASYN\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_proba = ros_pipeline.predict_proba(x_test)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "f_score = 2 * precision * recall / (precision + recall)\n",
    "optimal_index = np.argmax(f_score)\n",
    "chosen_threshold = thresholds[optimal_index]\n",
    "y_pred = (y_proba >= chosen_threshold).astype(int)\n",
    "print(\n",
    "    f\"Accuracy at chosen threshold {chosen_threshold:.2f}: {np.mean(y_pred == y_test):.4f}\"\n",
    ")\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred, labels=[0, 1], normalize=\"true\"\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e000971326892723e7f31ded70802f690c31c3620f59a0f99e594aaee3047ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

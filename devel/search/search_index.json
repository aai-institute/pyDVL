{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The python library for data valuation","text":"<p>pyDVL collects algorithms for data valuation and influence function computation. For the full list see Methods. It supports out-of-core and distributed computation, as well as local or distributed caching of results.</p> <p>If you're a first time user of pyDVL, we recommend you to go through Getting started.</p> <ul> <li> <p> Getting started</p> <p>Steps to install pyDVL and its requirements</p> </li> <li> <p> Example gallery</p> <p>Notebooks with worked-out examples of data valuation and influence functions</p> </li> <li> <p> Data valuation</p> <p>Basics of data valuation and description of the main algorithms</p> </li> <li> <p> Influence Function</p> <p>An introduction to the influence function and its computation with pyDVL</p> </li> <li> <p> Supported methods</p> <p>List of all methods implemented with references.</p> </li> <li> <p> Glossary</p> <p>Glossary of terms used in pyDVL</p> </li> <li> <p> API Reference</p> <p>Full documentation of the API</p> </li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Simple memory monitor / reporting   PR #663</li> <li>New stopping criterion <code>MaxSamples</code> PR #661</li> <li>Introduced <code>UtilityModel</code> and two implementations <code>IndicatorUtilityModel</code>   and <code>DeepSetsUtilityModel</code> for data utility learning   PR #650</li> <li>Introduced the concept of <code>ResultUpdater</code> in order to allow samplers to   declare the proper strategy to use by valuations    PR #641</li> <li>Added Banzhaf precomputed values to some games.   PR #641</li> <li>Introduced new <code>IndexIterations</code>, for consistent usage across all   <code>PowersetSamplers</code> PR #641</li> <li>Added <code>run_removal_experiment</code> for easy removal experiments   PR #636</li> <li>Refactor Classwise Shapley valuation with the interfaces and sampler   architecture PR #616</li> <li>Refactor KNN Shapley values with the new interface   PR #610 PR #645</li> <li>Refactor MSR Banzhaf semivalues with the new sampler architecture.   PR #605 PR #641</li> <li>Refactor group-testing shapley values with new sampler architecture   PR #602</li> <li>Refactor least-core data valuation methods with more supported sampling   methods and consistent interface.   PR #580</li> <li>Refactor Owen-Shapley valuation with new sampler architecture. Enable use of   <code>OwenSamplers</code> with all semi-values   PR #597 PR #641</li> <li>New method <code>InverseHarmonicMeanInfluence</code>, implementation for the paper   <code>DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and     Diffusion Models</code> PR #582</li> <li>Add new backend implementations for influence computation to account for   block-diagonal approximations   PR #582</li> <li>Extend <code>DirectInfluence</code> with block-diagonal and Gauss-Newton approximation   PR #591</li> <li>Extend <code>LissaInfluence</code> with block-diagonal and Gauss-Newton approximation   PR #593</li> <li>Extend <code>NystroemSketchInfluence</code> with block-diagonal and Gauss-Newton   approximation   PR #596</li> <li>Extend <code>ArnoldiInfluence</code> with block-diagonal and Gauss-Newton   approximation   PR #598</li> <li>Extend <code>CgInfluence</code> with block-diagonal and Gauss-Newton approximation   PR #601</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>Fixed <code>show_warnings=False</code> not being respected in subprocesses. Introduced   <code>suppress_warninigs</code> decorator for more flexibility   PR #647 PR #662</li> <li>Fixed several bugs in diverse stopping criteria, including: iteration counts,   computing completion, resetting, nested composition   PR #641 PR #650</li> <li>Fixed all weights of all samplers to ensure that mix-and-matching samplers and   semi-value methods always works, for all possible combinations   PR #641</li> <li>Fixed a bug whereby progress bars would not report the last step and remain   incomplete PR #641</li> <li>Fixed the analysis of the adult dataset in the Data-OOB notebook   PR #636</li> <li>Replace <code>np.float_</code> with <code>np.float64</code> and <code>np.alltrue</code> with <code>np.all</code>,   as the old aliases are removed in NumPy 2.0   PR #604</li> <li>Fix a bug in pydvl.utils.numeric.random_subset where 1 - q was used instead of q   as the probability of an element being sampled   PR #597</li> <li>Fix a bug in the calculation of variance estimates for MSR Banzhaf   PR #605</li> <li>Fix a bug in KNN Shapley values. See Issue 613 for details.</li> <li>Backport the KNN Shapley fix to the <code>value</code> module   PR #633 </li> </ul>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Slicing, comparing and setting of <code>ValuationResult</code> behave in a more    natural way   PR #660</li> <li>Switched all semi-value coefficients and sampler weights to log-space in   order to avoid overflows   PR #643</li> <li>Updated and rewrote some of the MSR banzhaf notebook   PR #641</li> <li>Updated Least-Core notebook   PR #641</li> <li>Updated Shapley spotify notebook   PR #628</li> <li>Updated Data Utility notebook   PR #650</li> <li>Restructured and generalized <code>StratifiedSampler</code> to allow using heuristics,   thus subsuming Variance-Reduced stratified sampling into a unified framework.   Implemented the heuristics proposed in that paper   PR #641</li> <li>Uniformly distribute test points across processes for KNNShapley. Fail for   <code>GroupedDataset</code> PR #632</li> <li>Introduced the concept of logical vs data indices for <code>Dataset</code>, and   <code>GroupedDataset</code>, fixing inconsistencies in how the latter operates on indices.   Also, both now return objects of the same type when slicing.   PR #631 PR #648</li> <li>Use tighter bounds for the calculation of the minimal sample size that guarantees   an epsilon-delta approximation in group testing (Jia et al. 2023)   PR #602</li> <li>Dropped black, isort and pylint from the CI pipeline, in favour of ruff   PR #633</li> <li>Breaking Changes</li> <li>Changed <code>DataOOBValuation</code> to only accept bagged models     PR #636</li> <li>Dropped support for python 3.8 after EOL     PR #633</li> <li>Rename parameter <code>hessian_regularization</code> of <code>DirectInfluence</code>     to <code>regularization</code> and change the type annotation to allow     for block-wise regularization parameters     PR #591</li> <li>Rename parameter <code>hessian_regularization</code> of <code>LissaInfluence</code>     to <code>regularization</code> and change the type annotation to allow     for block-wise regularization parameters     PR #593</li> <li>Remove parameter <code>h0</code> from init of <code>LissaInfluence</code> PR #593</li> <li>Rename parameter <code>hessian_regularization</code> of <code>NystroemSketchInfluence</code>     to <code>regularization</code> and change the type annotation to allow     for block-wise regularization parameters     PR #596</li> <li>Renaming of parameters of <code>ArnoldiInfluence</code>,     <code>hessian_regularization</code> -&gt; <code>regularization</code> (modify type annotation),     <code>rank_estimate</code> -&gt; <code>rank</code> PR #598</li> <li>Remove functions remove obsolete functions      <code>lanczos_low_rank_hessian_approximation</code>, <code>model_hessian_low_rank</code>     from <code>influence.torch.functional</code> PR #598</li> <li>Renaming of parameters of <code>CgInfluence</code>,     <code>hessian_regularization</code> -&gt; <code>regularization</code> (modify type annotation),     <code>pre_conditioner</code> -&gt; <code>preconditioner</code>,     <code>use_block_cg</code> -&gt; <code>solve_simultaneously</code> PR #601</li> <li>Remove parameter <code>x0</code> from <code>CgInfluence</code> PR #601</li> <li>Rename module      <code>influence.torch.pre_conditioner</code> -&gt; <code>influence.torch.preconditioner</code> PR #601</li> <li>Refactor preconditioner:<ul> <li>renaming <code>PreConditioner</code> -&gt; <code>Preconditioner</code></li> <li>fit to <code>TensorOperator</code> PR #601</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#092-bug-fixes-logging-improvement","title":"0.9.2 - \ud83c\udfd7  Bug fixes, logging improvement","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Add progress bars to the computation of <code>LazyChunkSequence</code> and   <code>NestedLazyChunkSequence</code> PR #567</li> <li>Add a device fixture for <code>pytest</code>, which depending on the availability and   user input (<code>pytest --with-cuda</code>) resolves to cuda device   PR #574</li> </ul>"},{"location":"CHANGELOG/#fixed_1","title":"Fixed","text":"<ul> <li>Fixed logging issue in decorator <code>log_duration</code> PR #567</li> <li>Fixed missing move of tensors to model device in <code>EkfacInfluence</code>   implementation PR #570</li> <li>Missing move to device of <code>preconditioner</code> in <code>CgInfluence</code> implementation   PR #572</li> <li>Raise a more specific error message, when a <code>RunTimeError</code> occurs in   <code>torch.linalg.eigh</code>, so the user can check if it is related to a known   issue   PR #578</li> <li>Fix an edge case (empty train data) in the test   <code>test_classwise_scorer_accuracies_manual_derivation</code>, which resulted   in undefined behavior (<code>np.nan</code> to <code>int</code> conversion with different results   depending on OS)   PR #579</li> </ul>"},{"location":"CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li>Changed logging behavior of iterative methods <code>LissaInfluence</code> and   <code>CgInfluence</code> to warn on not achieving desired tolerance within <code>maxiter</code>,   add parameter <code>warn_on_max_iteration</code> to set the level for this information   to <code>logging.DEBUG</code> PR #567</li> </ul>"},{"location":"CHANGELOG/#091-bug-fixes-logging-improvement","title":"0.9.1 - Bug fixes, logging improvement","text":""},{"location":"CHANGELOG/#fixed_2","title":"Fixed","text":"<ul> <li><code>FutureWarning</code> for <code>ParallelConfig</code> constantly raised without actually   instantiating the object   PR #562</li> </ul>"},{"location":"CHANGELOG/#090-new-methods-better-docs-and-bugfixes","title":"0.9.0 - \ud83c\udd95 New methods, better docs and bugfixes \ud83d\udcda\ud83d\udc1e","text":""},{"location":"CHANGELOG/#added_2","title":"Added","text":"<ul> <li>New method <code>MSR Banzhaf</code> with accompanying notebook, and new stopping   criterion <code>RankCorrelation</code> PR #520</li> <li>New method: <code>NystroemSketchInfluence</code> PR #504</li> <li>New preconditioned block variant of conjugate gradient   PR #507</li> <li>Improvements to documentation: fixes, links, text, example gallery, LFS and   more PR #532,   PR #543</li> <li>Glossary of data valuation and influence terms in the documentation   [PR #537](https://github.com/aai-institute/pyDVL/pull/537</li> <li>Documentation about writing notes for new features, changes or deprecations   PR #557</li> </ul>"},{"location":"CHANGELOG/#fixed_3","title":"Fixed","text":"<ul> <li>Bug in <code>LissaInfluence</code>, when not using CPU device   PR #495</li> <li>Memory issue with <code>CgInfluence</code> and <code>ArnoldiInfluence</code> PR #498</li> <li>Raising specific error message with install instruction, when trying to load   <code>pydvl.utils.cache.memcached</code> without <code>pymemcache</code> installed.   If <code>pymemcache</code> is available, all symbols from <code>pydvl.utils.cache.memcached</code>   are available through <code>pydvl.utils.cache</code> PR #509</li> </ul>"},{"location":"CHANGELOG/#changed_2","title":"Changed","text":"<ul> <li>Add property <code>model_dtype</code> to instances of type <code>TorchInfluenceFunctionModel</code></li> <li>Bump versions of CI actions to avoid warnings   PR #502</li> <li>Add Python Version 3.11 to supported versions   PR #510</li> <li>Documentation improvements and cleanup   PR #521,   PR #522</li> <li>Simplified parallel backend configuration   PR #549</li> </ul>"},{"location":"CHANGELOG/#081-new-method-and-notebook-games-with-exact-shapley-values-bug-fixes-and-cleanup","title":"0.8.1 - \ud83c\udd95 \ud83c\udfd7  New method and notebook, Games with exact shapley values, bug fixes and cleanup","text":""},{"location":"CHANGELOG/#added_3","title":"Added","text":"<ul> <li>Implement new method: <code>EkfacInfluence</code> PR #451</li> <li>New notebook to showcase ekfac for LLMs   PR #483</li> <li>Implemented exact games in Castro et al. 2009 and 2017   PR #341</li> </ul>"},{"location":"CHANGELOG/#fixed_4","title":"Fixed","text":"<ul> <li>Bug in using <code>DaskInfluenceCalcualator</code> with <code>TorchnumpyConverter</code>   for single dimensional arrays   PR #485</li> <li>Fix implementations of <code>to</code> methods of <code>TorchInfluenceFunctionModel</code>   implementations PR #487</li> <li>Fixed bug with checking for converged values in semivalues   PR #341</li> </ul>"},{"location":"CHANGELOG/#changed_3","title":"Changed","text":"<ul> <li>Add applications of data valuation section, display examples more prominently,   make all sections visible in table of contents, use mkdocs material cards   in the home page PR #492</li> </ul>"},{"location":"CHANGELOG/#080-new-interfaces-scaling-computation-bug-fixes-and-improvements","title":"0.8.0 - \ud83c\udd95 New interfaces, scaling computation, bug fixes and improvements \ud83c\udf81","text":""},{"location":"CHANGELOG/#added_4","title":"Added","text":"<ul> <li>New cache backends: InMemoryCacheBackend and DiskCacheBackend   PR #458</li> <li>New influence function interface <code>InfluenceFunctionModel</code></li> <li>Data parallel computation with <code>DaskInfluenceCalculator</code> PR #26</li> <li>Sequential batch-wise computation and write to disk with   <code>SequentialInfluenceCalculator</code> PR #377</li> <li>Adapt notebooks to new influence abstractions   PR #430</li> </ul>"},{"location":"CHANGELOG/#changed_4","title":"Changed","text":"<ul> <li>Refactor and simplify caching implementation   PR #458</li> <li>Simplify display of computation progress   PR #466</li> <li>Improve readme and explain better the examples   PR #465</li> <li>Simplify and improve tests, add CodeCov code coverage   PR #429</li> <li>Breaking Changes</li> <li>Removed <code>compute_influences</code> and all related code.     Replaced by new <code>InfluenceFunctionModel</code> interface. Removed modules:<ul> <li>influence.general</li> <li>influence.inversion</li> <li>influence.twice_differentiable</li> <li>influence.torch.torch_differentiable</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#fixed_5","title":"Fixed","text":"<ul> <li>Import bug in README PR #457</li> </ul>"},{"location":"CHANGELOG/#071-new-methods-bug-fixes-and-improvements-for-local-tests","title":"0.7.1 - \ud83c\udd95 New methods, bug fixes and improvements for local tests \ud83d\udc1e\ud83e\uddea","text":""},{"location":"CHANGELOG/#added_5","title":"Added","text":"<ul> <li>New method: Class-wise Shapley values   PR #338</li> <li>New method: Data-OOB by @BastienZim   PR #426,   PR $431</li> <li>Added <code>AntitheticPermutationSampler</code> PR #439</li> <li>Faster semi-value computation with per-index check of stopping criteria (optional)   PR #437</li> </ul>"},{"location":"CHANGELOG/#fixed_6","title":"Fixed","text":"<ul> <li>Fix initialization of <code>data_names</code> in <code>ValuationResult.zeros()</code> PR #443</li> </ul>"},{"location":"CHANGELOG/#changed_5","title":"Changed","text":"<ul> <li>No longer using docker within tests to start a memcached server   PR #444</li> <li>Using pytest-xdist for faster local tests   PR #440</li> <li>Improvements and fixes to notebooks   PR #436</li> <li>Refactoring of parallel module. Old imports will stop working in v0.9.0   PR #421</li> </ul>"},{"location":"CHANGELOG/#070-documentation-and-if-overhaul-new-methods-and-bug-fixes","title":"0.7.0 - \ud83d\udcda\ud83c\udd95 Documentation and IF overhaul, new methods and bug fixes \ud83d\udca5\ud83d\udc1e","text":"<p>This is our first \u03b2 release! We have worked hard to deliver improvements across the board, with a focus on documentation and usability. We have also reworked the internals of the <code>influence</code> module, improved parallelism and handling of randomness.</p>"},{"location":"CHANGELOG/#added_6","title":"Added","text":"<ul> <li>Implemented solving the Hessian equation via spectral low-rank approximation   PR #365</li> <li>Enabled parallel computation for Leave-One-Out values   PR #406</li> <li>Added more abbreviations to documentation   PR #415</li> <li>Added seed to functions from <code>pydvl.utils.numeric</code>, <code>pydvl.value.shapley</code> and   <code>pydvl.value.semivalues</code>. Introduced new type <code>Seed</code> and conversion function   <code>ensure_seed_sequence</code>.   PR #396</li> <li>Added <code>batch_size</code> parameter to <code>compute_banzhaf_semivalues</code>,   <code>compute_beta_shapley_semivalues</code>, <code>compute_shapley_semivalues</code> and   <code>compute_generic_semivalues</code>.   PR #428</li> <li>Added classwise Shapley as proposed by (Schoch et al. 2021)   [https://arxiv.org/abs/2211.06800]   PR #338</li> </ul>"},{"location":"CHANGELOG/#changed_6","title":"Changed","text":"<ul> <li>Replaced sphinx with mkdocs for documentation. Major overhaul of documentation   PR #352</li> <li>Made ray an optional dependency, relying on joblib as default parallel backend   PR #408</li> <li>Decoupled <code>ray.init</code> from <code>ParallelConfig</code> PR #373</li> <li>Breaking Changes</li> <li>Signature change: return information about Hessian inversion from     <code>compute_influence_factors</code> PR #375</li> <li>Major changes to IF interface and functionality. Foundation for a framework     abstraction for IF computation.     PR #278 PR #394</li> <li>Renamed <code>semivalues</code> to <code>compute_generic_semivalues</code> PR #413</li> <li>New <code>joblib</code> backend as default instead of ray. Simplify MapReduceJob.     PR #355</li> <li>Bump torch dependency for influence package to 2.0     PR #365</li> </ul>"},{"location":"CHANGELOG/#fixed_7","title":"Fixed","text":"<ul> <li>Fixes to parallel computation of generic semi-values: properly handle all   samplers and stopping criteria, irrespective of parallel backend.   PR #372</li> <li>Optimises memory usage in IF calculation   PR #375</li> <li>Fix adding valuation results with overlapping indices and different lengths   PR #370</li> <li>Fixed bugs in conjugate gradient and <code>linear_solve</code> PR #358</li> <li>Fix installation of dev requirements for Python3.10   PR #382</li> <li>Improvements to IF documentation   PR #371</li> </ul>"},{"location":"CHANGELOG/#061-bug-fixes-and-small-improvements","title":"0.6.1 - \ud83c\udfd7 Bug fixes and small improvements","text":"<ul> <li>Fix parsing keyword arguments of <code>compute_semivalues</code> dispatch function   PR #333</li> <li>Create new <code>RayExecutor</code> class based on the concurrent.futures API,   use the new class to fix an issue with Truncated Monte Carlo Shapley   (TMCS) starting too many processes and dying, plus other small changes   PR #329</li> <li>Fix creation of GroupedDataset objects using the <code>from_arrays</code>   and <code>from_sklearn</code> class methods   PR #324</li> <li>Fix release job not triggering on CI when a new tag is pushed   PR #331</li> <li>Added alias <code>ApproShapley</code> from Castro et al. 2009 for permutation Shapley   PR #332</li> </ul>"},{"location":"CHANGELOG/#060-new-algorithms-cleanup-and-bug-fixes","title":"0.6.0 - \ud83c\udd95 New algorithms, cleanup and bug fixes \ud83c\udfd7","text":"<ul> <li>Fixes in <code>ValuationResult</code>: bugs around data names, semantics of   <code>empty()</code>, new method <code>zeros()</code> and normalised random values   PR #327</li> <li>New method: Implements generalised semi-values for data valuation,   including Data Banzhaf and Beta Shapley, with configurable sampling strategies   PR #319</li> <li>Adds kwargs parameter to <code>from_array</code> and <code>from_sklearn</code> Dataset and   GroupedDataset class methods   PR #316</li> <li>PEP-561 conformance: added <code>py.typed</code> PR #307</li> <li>Removed default non-negativity constraint on least core subsidy   and added instead a <code>non_negative_subsidy</code> boolean flag.   Renamed <code>options</code> to <code>solver_options</code> and pass it as dict.   Change default least-core solver to SCS with 10000 max_iters.   PR #304</li> <li>Cleanup: removed unnecessary decorator <code>@unpackable</code> PR #233</li> <li>Stopping criteria: fixed problem with <code>StandardError</code> and enable proper   composition of index convergence statuses. Fixed a bug with <code>n_jobs</code> in   <code>truncated_montecarlo_shapley</code>.   PR #300 and   PR #305</li> <li>Shuffling code around to allow for simpler user imports, some cleanup and   documentation fixes.   PR #284</li> <li>Bug fix: Warn instead of raising an error when <code>n_iterations</code>   is less than the size of the dataset in Monte Carlo Least Core   PR #281</li> </ul>"},{"location":"CHANGELOG/#050-fixes-nicer-interfaces-and-more-breaking-changes","title":"0.5.0 - \ud83d\udca5 Fixes, nicer interfaces and... more breaking changes \ud83d\ude12","text":"<ul> <li>Fixed parallel and antithetic Owen sampling for Shapley values. Simplified   and extended tests.   PR #267</li> <li>Added <code>Scorer</code> class for a cleaner interface. Fixed minor bugs around   Group-Testing Shapley, added more tests and switched to cvxpy for the solver.   PR #264</li> <li>Generalised stopping criteria for valuation algorithms. Improved classes   <code>ValuationResult</code> and <code>Status</code> with more operations. Some minor issues fixed.   PR #252</li> <li>Fixed a bug whereby <code>compute_shapley_values</code> would only spawn one process when   using <code>n_jobs=-1</code> and Monte Carlo methods.   PR #270</li> <li>Bugfix in <code>RayParallelBackend</code>: wrong semantics for <code>kwargs</code>.   PR #268</li> <li>Splitting of problem preparation and solution in Least-Core computation.   Umbrella function for LC methods.   PR #257</li> <li>Operations on <code>ValuationResult</code> and <code>Status</code> and some cleanup   PR #248</li> <li>Bug fix and minor improvements: Fixes bug in TMCS with remote Ray cluster,   raises an error for dummy sequential parallel backend with TMCS, clones model   inside <code>Utility</code> before fitting by default, with flag <code>clone_before_fit</code>   to disable it, catches all warnings in <code>Utility</code> when <code>show_warnings</code> is   <code>False</code>. Adds Miner and Gloves toy games utilities   PR #247</li> </ul>"},{"location":"CHANGELOG/#040-new-algorithms-and-more-breaking-changes","title":"0.4.0 - \ud83c\udfed\ud83d\udca5 New algorithms and more breaking changes","text":"<ul> <li>GH action to mark issues as stale   PR #201</li> <li>Disabled caching of Utility values as well as repeated evaluations by default   PR #211</li> <li>Test and officially support Python version 3.9 and 3.10   PR #208</li> <li>Breaking change: Introduces a class ValuationResult to gather and inspect   results from all valuation algorithms   PR #214</li> <li>Fixes bug in Influence calculation with multidimensional input and adds new   example notebook   PR #195</li> <li>Breaking change: Passes the input to <code>MapReduceJob</code> at initialization,   removes <code>chunkify_inputs</code> argument from <code>MapReduceJob</code>, removes <code>n_runs</code>   argument from <code>MapReduceJob</code>, calls the parallel backend's <code>put()</code> method for   each generated chunk in <code>_chunkify()</code>, renames ParallelConfig's <code>num_workers</code>   attribute to <code>n_local_workers</code>, fixes a bug in <code>MapReduceJob</code>'s chunkification   when <code>n_runs</code> &gt;= <code>n_jobs</code>, and defines a sequential parallel backend to run   all jobs in the current thread   PR #232</li> <li>New method: Implements exact and monte carlo Least Core for data valuation,   adds <code>from_arrays()</code> class method to the <code>Dataset</code> and <code>GroupedDataset</code>   classes, adds <code>extra_values</code> argument to <code>ValuationResult</code>, adds   <code>compute_removal_score()</code> and <code>compute_random_removal_score()</code> helper functions   PR #237</li> <li>New method: Group Testing Shapley for valuation, from Jia et al. 2019 PR #240</li> <li>Fixes bug in ray initialization in <code>RayParallelBackend</code> class   PR #239</li> <li>Implements \"Egalitarian Least Core\", adds cvxpy as a   dependency and uses it instead of scipy as optimizer   PR #243</li> </ul>"},{"location":"CHANGELOG/#030-breaking-changes","title":"0.3.0 - \ud83d\udca5 Breaking changes","text":"<ul> <li>Simplified and fixed powerset sampling and testing   PR #181</li> <li>Simplified and fixed publishing to PyPI from CI   PR #183</li> <li>Fixed bug in release script and updated contributing docs.   PR #184</li> <li>Added Pull Request template   PR #185</li> <li>Modified Pull Request template to automatically link PR to issue   PR ##186</li> <li>First implementation of Owen Sampling, squashed scores, better testing   PR #194</li> <li>Improved documentation on caching, Shapley, caveats of values, bibtex   PR #194</li> <li>Breaking change: Rearranging of modules to accommodate for new methods   PR #194</li> </ul>"},{"location":"CHANGELOG/#020-better-docs","title":"0.2.0 - \ud83d\udcda Better docs","text":"<p>Mostly API documentation and notebooks, plus some bugfixes.</p>"},{"location":"CHANGELOG/#added_7","title":"Added","text":"<p>In PR #161: - Support for $$ math in sphinx docs. - Usage of sphinx extension for external links (introducing new directives like   <code>:gh:</code>, <code>:issue:</code> and <code>:tfl:</code> to construct standardised links to external   resources). - Only update auto-generated documentation files if there are changes. Some   minor additions to <code>update_docs.py</code>. - Parallelization of exact combinatorial Shapley. - Integrated KNN shapley into the main interface <code>compute_shapley_values</code>.</p>"},{"location":"CHANGELOG/#changed_7","title":"Changed","text":"<p>In PR #161: - Improved main docs and Shapley notebooks. Added or fixed many docstrings,   readme and documentation for contributors. Typos, grammar and style in code,   documentation and notebooks. - Internal renaming and rearranging in the parallelization and caching modules.</p>"},{"location":"CHANGELOG/#fixed_8","title":"Fixed","text":"<ul> <li>Bug in random matrix generation   PR #161.</li> <li>Bugs in MapReduceJob's <code>_chunkify</code> and <code>_backpressure</code> methods   PR #176.</li> </ul>"},{"location":"CHANGELOG/#010-first-release","title":"0.1.0 - \ud83c\udf89 first release","text":"<p>This is very first release of pyDVL.</p> <p>It contains:</p> <ul> <li> <p>Data Valuation Methods:</p> </li> <li> <p>Leave-One-Out</p> </li> <li>Influence Functions</li> <li>Shapley:<ul> <li>Exact Permutation and Combinatorial</li> <li>Montecarlo Permutation and Combinatorial</li> <li>Truncated Montecarlo Permutation</li> </ul> </li> <li>Caching of results with Memcached</li> <li>Parallelization of computations with Ray</li> <li>Documentation</li> <li>Notebooks containing examples of different use cases</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing to pyDVL","text":"<p>The goal of pyDVL is to be a repository of successful algorithms for the valuation of data, in a broader sense. Contributions are welcome from anyone in the form of pull requests, bug reports and feature requests.</p> <p>We will consider for inclusion any (tested) implementation of an algorithm appearing in a peer-reviewed journal (even if the method does not improve the state of the art, for benchmarking and comparison purposes). We are also open to improvements to the currently implemented methods and other ideas. Please open a ticket with yours.</p> <p>If you are interested in setting up a similar project, consider the template  pymetrius.</p>"},{"location":"CONTRIBUTING/#local-development","title":"Local development","text":"<p>This project uses ruff to lint and format code and pre-commit to invoke it as a git pre-commit hook. Consider installing any of ruff's IDE integrations to make your life easier.</p> <p>Run the following to set up the pre-commit git hook to run before pushes:</p> <pre><code>pre-commit install --hook-type pre-push\n</code></pre> <p>Additionally, we use Git LFS for some files like images. Install with</p> <pre><code>git lfs install\n</code></pre>"},{"location":"CONTRIBUTING/#setting-up-your-environment","title":"Setting up your environment","text":"<p>We strongly suggest using some form of virtual environment for working with the library. E.g. with venv:</p> <pre><code>python -m venv ./venv\n. venv/bin/activate  # `venv\\Scripts\\activate` in windows\npip install -r requirements-dev.txt -r requirements-docs.txt\n</code></pre> <p>With conda:</p> <pre><code>conda create -n pydvl python=3.9\nconda activate pydvl\npip install -r requirements-dev.txt -r requirements-docs.txt\n</code></pre> <p>A very convenient way of working with your library during development is to install it in editable mode into your environment by running</p> <pre><code>pip install -e .\n</code></pre> <p>In order to build the documentation locally (which is done as part of the tox suite) you need to install additional non-python dependencies as described in the documentation of mkdocs-material.</p> <p>In addition, pandoc is required. Except for OSX,  it should be installed automatically as a dependency with  <code>requirements-docs.txt</code>. Under OSX you can install pandoc  (you'll need at least version 2.11) with:</p> <pre><code>brew install pandoc\n</code></pre> <p>Remember to mark all autogenerated directories as excluded in your IDE. In particular <code>docs_build</code> and <code>.tox</code> should be marked as excluded to avoid slowdowns when searching or refactoring code.</p> <p>If you use remote execution, don't forget to exclude data paths from deployment (unless you really want to sync them).</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>Automated builds, tests, generation of documentation and publishing are handled by CI pipelines. Before pushing your changes to the remote we recommend to execute <code>tox</code> locally in order to detect mistakes early on and to avoid failing pipelines. tox will: * run the test suite * build the documentation * build and test installation of the package. * generate coverage reports in html, as well as badges.</p> <p>You can configure pytest, coverage and ruff by adjusting pyproject.toml.</p> <p>Besides the usual unit tests, most algorithms are tested using pytest. This requires ray for the parallelization and Memcached for caching. Please install both before running the tests. We run tests in CI as well.</p> <p>It is possible to pass optional command line arguments to pytest, for example to run only certain tests using patterns (<code>-k</code>) or marker (<code>-m</code>).</p> <pre><code>tox -e tests -- &lt;optional arguments&gt;\n</code></pre> <p>There are a few important arguments:</p> <ul> <li><code>--memcached-service</code> allows to change the default of <code>localhost:11211</code> (memcached's default)   to a different address.</li> </ul> <p>Memcached is needed for testing   caching as well as speeding certain methods (e.g. Permutation Shapley).</p> <p>To start memcached locally in the background with Docker use:</p> <pre><code>docker run --name pydvl-memcache -p 11211:11211 -d memcached\n</code></pre> <ul> <li><code>-n</code> sets the number of parallel workers for    pytest-xdist.</li> </ul> <p>There are two layers of parallelization in the tests.   An inner one within the tests themselves, i.e. the parallelism in the algorithms,   and an outer one by pytest-xdist. The latter is controlled by the <code>-n</code> argument.   If you experience segmentation faults with the tests,   try running them with <code>-n 0</code> to disable parallelization.</p> <ul> <li> <p><code>--slow-tests</code> enables running slow tests. See below for a description   of slow tests.</p> </li> <li> <p><code>--with-cuda</code> sets the device fixture in tests/influence/torch/conftest.py   to <code>cuda</code> if it is available. Using this fixture within tests, you can run parts   of your tests on a <code>cuda</code> device. Be aware, that you still have to take care of   the usage of the device manually in a specific test. Setting this flag does not   result in running all tests on a GPU.</p> </li> </ul>"},{"location":"CONTRIBUTING/#markers","title":"Markers","text":"<p>We use a few different markers to differentiate between tests and runs groups of them of separately. Use <code>pytest --markers</code> to get a list and description of all available markers.</p> <p>Two important markers are:</p> <ul> <li><code>pytest.mark.slow</code> which is used to mark slow tests and skip them by default.</li> </ul> <p>A slow test is any test that takes 45 seconds or more to run and that can be   skipped most of the time. In some cases a test is slow, but it is required   in order to ensure that a feature works as expected and that are no bugs.   In those cases, we should not use this marker.</p> <p>Slow tests are always run on CI. Locally, they are skipped   by default but can be additionally run using: <code>pytest --slow-tests</code>.</p> <ul> <li><code>pytest.mark.torch</code> which is used to mark tests that require PyTorch.</li> </ul> <p>To test modules that rely on PyTorch, use:</p> <pre><code>tox -e tests -- -m \"torch\"\n</code></pre>"},{"location":"CONTRIBUTING/#other-things","title":"Other Things","text":"<p>To test the notebooks separately, run (see below for details):</p> <pre><code>tox -e notebook-tests\n</code></pre> <p>To create a package locally, run: <pre><code>python setup.py sdist bdist_wheel\n</code></pre></p>"},{"location":"CONTRIBUTING/#notebooks","title":"Notebooks","text":"<p>We use notebooks both as documentation (copied over to <code>docs/examples</code>) and as integration tests. All notebooks in the <code>notebooks</code> directory are executed during the test run. Because run times are typically too long for large datasets, you must check for the <code>CI</code> environment variable to work with smaller ones. For example, you can select a subset of the data:</p> <pre><code># In CI we only use a subset of the training set\nif os.environ.get('CI'):\n    training_data = training_data[:10]\n</code></pre> <p>This switching should happen in a separate notebook cell tagged with <code>hide</code> to hide the cell's input and output when rendering it as part of the documents. We want to avoid as much clutter and boilerplate as possible in the notebooks themselves.</p> <p>Because we want documentation to include the full dataset, we commit notebooks with their outputs running with full datasets to the repo. The notebooks are then added by CI to the section Examples of the documentation.</p>"},{"location":"CONTRIBUTING/#hiding-cells-in-notebooks","title":"Hiding cells in notebooks","text":"<p>Switching between CI or not, importing generic modules and plotting results are all examples of boilerplate code irrelevant to a reader interested in pyDVL's functionality. For this reason we choose to isolate this code into separate cells which are then hidden in the documentation.</p> <p>In order to do this, cells are marked with tags understood by the mkdocs plugin <code>mkdocs-jupyter</code>, namely adding the following to the metadata of the relevant cells:</p> <pre><code>\"tags\": [\n  \"hide\"\n]\n</code></pre> <p>To hide the cell's input and output.</p> <p>Or:</p> <pre><code>\"tags\": [\n  \"hide-input\"\n]\n</code></pre> <p>To only hide the input and</p> <p><pre><code>\"tags\": [\n  \"hide-output\"\n]\n</code></pre> for hiding the output only.</p> <p>It is important to leave a warning at the top of the document to avoid confusion. Examples for hidden imports and plots are available in the notebooks, e.g. in notebooks/shapley_basic_spotify.ipynb.</p>"},{"location":"CONTRIBUTING/#plots-in-notebooks","title":"Plots in Notebooks","text":"<p>If you add a plot to a notebook, which should also render nicely in browser dark mode, add the tag invertible-output, i.e.</p> <p><pre><code>\"tags\": [\n  \"invertible-output\"\n]\n</code></pre> This applies a simple CSS-filter to the output image of the cell.</p>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>API documentation and examples from notebooks are built with mkdocs, using a number of plugins, including mkdoctrings, with versioning handled by mike.</p> <p>Notebooks are an integral part of the documentation as well, please read the section on notebooks above.</p> <p>If you want to build the documentation locally, please make sure you followed the instructions in the section  Setting up your environment.</p> <p>Use the following command to build the documentation the same way it is done in CI:</p> <pre><code>mkdocs build\n</code></pre> <p>Locally, you can use this command instead to continuously rebuild documentation on changes to the <code>docs</code> and <code>src</code> folder:</p> <pre><code>mkdocs serve\n</code></pre> <p>This will rebuild the documentation on changes to <code>.md</code> files inside <code>docs</code>, notebooks and python files.</p> <p>On OSX, it is possible that the cairo lib file is not properly linked when installed via homebrew. In this case you might encounter an error like this <pre><code>OSError: no library called \"cairo-2\" was found\nno library called \"cairo\" was found\nno library called \"libcairo-2\" was found\n</code></pre> when calling <code>mkdocs build</code> or <code>mkdocs serve</code>. This can be resolved via setting the environment variable <code>DYLD_FALLBACK_LIBRARY_PATH</code>: <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=$DYLD_FALLBACK_LIBRARY_PATH:/opt/homebrew/lib\n</code></pre></p>"},{"location":"CONTRIBUTING/#automatic-api-documentation","title":"Automatic API documentation","text":"<p>We use mkdocstrings to automatically generate API documentation from docstrings, following almost verbatim this recipe: Stubs are generated for all modules on the fly using generate_api_docs.py thanks to the pluging mkdocstrings-gen-files and navigation is generated for mkdocs-literate-nav.</p> <p>With some renaming and using section-index <code>__init__.py</code> files are used as entry points for the documentation of a module.</p> <p>Since very often we re-export symbols in the <code>__init__.py</code> files, the automatic generation of the documentation skips all symbols in those files. If you want to document any in particular you can do so by overriding mkdocs_genfiles: Create a file under <code>docs/api/pydvl/module/index.md</code> and add your documentation there. For example, to document the whole module and every (re-)exported symbol just add this to the file:</p> <pre><code>::: pydvl.module\n</code></pre>"},{"location":"CONTRIBUTING/#adding-new-pages","title":"Adding new pages","text":"<p>Navigation is configured in <code>mkdocs.yaml</code> using the nav section. We use the plugin mkdoc-literate-nav which allows fine-grained control of the navigation structure. However, most pages are explicitly listed and manually arranged in the <code>nav</code> section of the configuration.</p>"},{"location":"CONTRIBUTING/#creating-stable-references-for-autorefs","title":"Creating stable references for autorefs","text":"<p>mkdocstrings includes the plugin autorefs to enable automatic linking across pages with e.g. <code>[a link][to-something]</code>. Anchors are autogenerated from section titles, and are not guaranteed to be unique. In order to ensure that a link will remain valid, add a custom anchor to the section title:</p> <pre><code>## Some section { #permanent-anchor-to-some-section }\n</code></pre> <p>(note the space after the opening brace). You can then refer to it within another markdown file with <code>[Some section][permanent-anchor-to-some-section]</code>.</p>"},{"location":"CONTRIBUTING/#adding-notes-about-new-features-changes-or-deprecations","title":"Adding notes about new features, changes or deprecations","text":"<p>We use the admonition extension of Mkdocs Material to create admonitions, also known as call-outs, that hold information about when a certain feature was added, changed or deprecated and optionally a description with more details. We put the admonition directly in a module's, a function's or class' docstring.</p> <p>We use the following syntax:</p> <pre><code>!!! tip \"&lt;Event Type&gt; in version &lt;Version Number&gt;\"\n\n    &lt;Optional Description&gt;\n</code></pre> <p>The description is useful when the note is about a smaller change such as a parameter.</p> <ul> <li>For a new feature, we use:</li> </ul> <pre><code>!!! tip \"New in version &lt;Version Number&gt;\"\n\n    &lt;Optional Description&gt;\n</code></pre> <ul> <li>For a change to an existing feature we use:</li> </ul> <pre><code>!!! tip \"Changed in version &lt;Version Number&gt;\"\n\n    &lt;Optional Description&gt;\n</code></pre> <p>For example, for a change in version <code>1.2.3</code> that adds kwargs   to a class' constructor we would write:</p> <pre><code>!!! tip \"Changed in version 1.2.3\"\n\n    Added kwargs to the constructor.\n</code></pre> <ul> <li>For a deprecation we use:</li> </ul> <pre><code>!!! tip \"Deprecated in version &lt;Version Number&gt;\"\n\n    &lt;Optional Description&gt;\n</code></pre>"},{"location":"CONTRIBUTING/#using-bibliography","title":"Using bibliography","text":"<p>Bibliographic citations are managed with the plugin mkdocs-bibtex. To enter a citation first add the entry to <code>docs/pydvl.bib</code>. For team contributor this should be an export of the Zotero folder <code>software/pydvl</code> in the TransferLab Zotero library. All other contributors just add the bibtex data, and a maintainer will add it to the group library upon merging.</p> <p>To add a citation inside a markdown file, use the notation <code>[@ citekey]</code> (with no space). Alas, because of when mkdocs-bibtex enters the pipeline, it won't process docstrings. For module documentation, we manually inject html into the markdown files. For example, in <code>pydvl.value.shapley.montecarlo</code> we have:</p> <pre><code>\"\"\"\nModule docstring...\n\n## References\n\n[^1]: &lt;a name=\"ghorbani_data_2019\"&gt;&lt;/a&gt;Ghorbani, A., Zou, J., 2019.\n    [Data Shapley: Equitable Valuation of Data for Machine\n    Learning](https://proceedings.mlr.press/v97/ghorbani19c.html).\n    In: Proceedings of the 36th International Conference on Machine Learning,\n    PMLR, pp. 2242\u20132251.\n\"\"\"\n</code></pre> <p>and then later in the file, inside a function's docstring:</p> <pre><code>    This function implements (Ghorbani and Zou, 2019)&lt;sup&gt;&lt;a \n    href=\"#ghorbani_data_2019\"&gt;1&lt;/a&gt;&lt;/sup&gt;\n</code></pre>"},{"location":"CONTRIBUTING/#writing-mathematics","title":"Writing mathematics","text":"<p>Use LaTeX delimiters <code>$</code> and <code>$$</code> for inline and displayed mathematics respectively.</p> <p>Warning: backslashes must be escaped in docstrings! (although there are exceptions). For simplicity, declare the string as \"raw\" with the prefix <code>r</code>:</p> <pre><code># This will work\ndef f(x: float) -&gt; float:\n    r\"\"\" Computes \n    $${ f(x) = \\frac{1}{x^2} }$$\n    \"\"\"\n    return 1/(x*x)\n\n# This throws an obscure error\ndef f(x: float) -&gt; float:\n    \"\"\" Computes \n    $$\\frac{1}{x^2}$$\n    \"\"\"\n    return 1/(x*x)\n</code></pre> <p>Note how there is no space after the dollar signs. This is important! You can use braces for legibility like in the first example.</p>"},{"location":"CONTRIBUTING/#abbreviations","title":"Abbreviations","text":"<p>We keep the abbreviations used in the documentation inside the docs_include/abbreviations.md file.</p> <p>The syntax for abbreviations is:</p> <pre><code>*[ABBR]: Abbreviation\n</code></pre>"},{"location":"CONTRIBUTING/#ci","title":"CI","text":"<p>We use workflows to:</p> <ul> <li>Run the tests.</li> <li>Publish documentation.</li> <li>Publish packages to testpypi / pypi.</li> <li>Mark issues as stale after 30 days. We do this only for issues with the label   <code>awaiting-reply</code>   which indicates that we have answered a question / feature request / PR and   are waiting for the OP to reply / update his work.</li> </ul>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>We test all algorithms with simple datasets in CI jobs. This can amount to a sizeable amount of time, so care must be taken not to overdo it: 1. All algorithm tests must be on very simple datasets and as quick as possible 2. We try not to trigger CI pipelines when unnecessary (see Skipping CI runs). 3. We split the tests based on their duration into groups and run them in parallel.</p> <p>For that we use pytest-split    to first store the duration of all tests with    <code>tox -e tests -- --store-durations --slow-tests</code>    in a <code>.test_durations</code> file.</p> <p>Alternatively, we case use pytest directly    <code>pytest --store-durations --slow-tests</code>.</p> <p>Note This does not have to be done each time a new test or test case is added. For new tests and test cases pytes-split assumes average test execution time(calculated based on the stored information) for every test which does not have duration information stored. Thus, there's no need to store durations after changing the test suite. However, when there are major changes in the suite compared to what's stored in .test_durations, it's recommended to update the duration information with <code>--store-durations</code> to ensure that the splitting is in balance.</p> <p>Then we can have as many splits as we want:</p> <pre><code>tox -e tests -- --splits 3 --group 1\ntox -e tests -- --splits 3 --group 2\ntox -e tests -- --splits 3 --group 3\n</code></pre> <p>Alternatively, we case use pytest directly    <code>pytest --splits 3 ---group 1</code>.</p> <p>Each one of these commands should be run in a separate shell/job    to run the test groups in parallel and decrease the total runtime.</p>"},{"location":"CONTRIBUTING/#running-github-actions-locally","title":"Running Github Actions locally","text":"<p>To run Github Actions locally we use act. It uses the workflows defined in <code>.github/workflows</code> and determines the set of actions that need to be run. It uses the Docker API to either pull or build the necessary images, as defined in our workflow files and finally determines the execution path based on the dependencies that were defined.</p> <p>Once it has the execution path, it then uses the Docker API to run containers for each action based on the images prepared earlier. The environment variables  and filesystem are all configured to match what GitHub provides.</p> <p>You can install it manually using:</p> <pre><code>curl -s https://raw.githubusercontent.com/nektos/act/master/install.sh | sudo bash -s -- -d -b ~/bin \n</code></pre> <p>And then simply add it to your PATH variable: <code>PATH=~/bin:$PATH</code></p> <p>Refer to its official readme for more installation options.</p>"},{"location":"CONTRIBUTING/#act-cheatsheet","title":"act cheatsheet","text":"<p>By default, <code>act</code> will run all  workflows in <code>.github/workflows</code>. You can use the <code>-W</code> flag to specify a specific workflow file to run, or you can rely on the job id to be unique (but then you'll see warnings for the workflows without that job id).</p> <pre><code># Run only the main tests for python 3.9 after a push event (implicit) \nact -W .github/workflows/run-tests-workflow.yaml \\\n    -j run-tests \\\n    --input tests_to_run=base\\\n    --input python_version=3.9\n</code></pre> <p>Other common flags are: </p> <pre><code># List all actions for all events:\nact -l\n\n# List the actions for a specific event:\nact workflow_dispatch -l\n\n# List the actions for a specific job:\nact -j lint -l\n\n# Run the default (`push`) event:\nact\n\n# Run a specific event:\nact pull_request\n\n# Run a specific job:\nact -j lint\n\n# Collect artifacts to the /tmp/artifacts folder:\nact --artifact-server-path /tmp/artifacts\n\n# Run a job in a specific workflow (useful if you have duplicate job names)\nact -j lint -W .github/workflows/publish.yml\n\n# Run in dry-run mode:\nact -n\n\n# Enable verbose-logging (can be used with any of the above commands)\nact -v\n</code></pre>"},{"location":"CONTRIBUTING/#example","title":"Example","text":"<p>To run the <code>publish</code> job (the most difficult one to test) you would simply use:</p> <ul> <li>When triggered by a release:</li> </ul> <pre><code>act release -j publish --eventpath events.json\n</code></pre> <p>With <code>events.json</code> containing:</p> <pre><code>{\n  \"act\": true\n}\n</code></pre> <p>This will use your current branch. If you want to test a specific branch   you have to use the <code>workflow_dispatch</code> event (see below).</p> <ul> <li>To instead run it as if it had been manually triggered (i.e. <code>workflow_dispatch</code>)   you would instead use:</li> </ul> <pre><code>act workflow_dispatch -j publish --eventpath events.json\n</code></pre> <p>With <code>events.json</code> containing:</p> <pre><code>{\n  \"act\": true,\n  \"inputs\": {\n    \"tag_name\": \"v0.6.0\"\n  }\n}\n</code></pre>"},{"location":"CONTRIBUTING/#skipping-ci-runs","title":"Skipping CI runs","text":"<p>One sometimes would like to skip CI for certain commits (e.g. updating the readme). In order to do this, simply prefix the commit message with <code>[skip ci]</code>. The string can be anywhere, but adding it to the beginning of the commit message makes it more evident when looking at commits in a PR.</p> <p>Refer to the official GitHub documentation  for more information.</p>"},{"location":"CONTRIBUTING/#release-processes","title":"Release processes","text":""},{"location":"CONTRIBUTING/#automatic-release-process","title":"Automatic release process","text":"<p>In order to create an automatic release, a few prerequisites need to be satisfied:</p> <ul> <li>The project's virtualenv needs to be active</li> <li>The repository needs to be on the <code>develop</code> branch</li> <li>The repository must be clean (including no untracked files)</li> </ul> <p>Then, a new release can be created using the script <code>build_scripts/release-version.sh</code> (leave out the version parameter to have <code>bumpversion</code> automatically derive the next release version by bumping the patch part):</p> <pre><code>build_scripts/release-version.sh 0.1.6\n</code></pre> <p>To find out how to use the script, pass the <code>-h</code> or <code>--help</code> flags:</p> <pre><code>build_scripts/release-version.sh --help\n</code></pre> <p>If running in interactive mode (without <code>-y|--yes</code>), the script will output a summary of pending changes and ask for confirmation before executing the actions.</p> <p>Once this is done, a tag will be created on the repository. You should then create a GitHub release for that tag. That will a trigger a CI pipeline that will automatically create a package and publish it from CI to PyPI.</p>"},{"location":"CONTRIBUTING/#manual-release-process","title":"Manual release process","text":"<p>If the automatic release process doesn't cover your use case, you can also create a new release manually by following these steps:</p> <ol> <li>(Repeat as needed) implement features on feature branches merged into   <code>develop</code>. Each merge into develop will publish a new pre-release version     to TestPyPI. These versions can be installed using <code>pip install --pre     --index-url https://test.pypi.org/simple/</code>.</li> <li>When ready to release: From the develop branch create the release branch and    perform release activities (update changelog, news, ...). For your own    convenience, define an env variable for the release version     <pre><code>export RELEASE_VERSION=\"vX.Y.Z\"\ngit checkout develop\ngit branch release/${RELEASE_VERSION} &amp;&amp; git checkout release/${RELEASE_VERSION}\n</code></pre></li> <li>Run <code>bumpversion --commit release</code> if the release is only a patch release,    otherwise the full version can be specified using     <code>bumpversion --commit --new-version X.Y.Z release</code>    (the <code>release</code> part is ignored but required by bumpversion ).</li> <li>Merge the release branch into <code>master</code>, tag the merge commit, and push back to the repo.     The CI pipeline publishes the package based on the tagged commit.     <pre><code>git checkout master\ngit merge --no-ff release/${RELEASE_VERSION}\ngit tag -a ${RELEASE_VERSION} -m\"Release ${RELEASE_VERSION}\"\ngit push --follow-tags origin master\n</code></pre></li> <li>Switch back to the release branch <code>release/vX.Y.Z</code> and pre-bump the version:    <code>bumpversion --commit patch</code>. This ensures that <code>develop</code> pre-releases are    always strictly more recent than the last published release version from     <code>master</code>.</li> <li>Merge the release branch into <code>develop</code>:     <pre><code>git checkout develop\ngit merge --no-ff release/${RELEASE_VERSION}\ngit push origin develop\n</code></pre></li> <li>Delete the release branch if necessary:     <code>git branch -d release/${RELEASE_VERSION}</code></li> <li>Create a GitHub    release    for the created tag.</li> <li>Pour yourself a cup of coffee, you earned it!  </li> <li>A package will be automatically created and published from CI to PyPI.</li> </ol>"},{"location":"CONTRIBUTING/#ci-and-requirements-for-publishing","title":"CI and requirements for publishing","text":"<p>In order to publish new versions of the package from the development branch, the CI pipeline requires the following secret variables set up:</p> <pre><code>TEST_PYPI_USERNAME\nTEST_PYPI_PASSWORD\nPYPI_USERNAME\nPYPI_PASSWORD\n</code></pre> <p>The first 2 are used after tests run on the develop branch's CI workflow  to automatically publish packages to TestPyPI.</p> <p>The last 2 are used in the publish.yaml CI workflow to publish packages to PyPI from <code>develop</code> after a GitHub release.</p>"},{"location":"CONTRIBUTING/#publish-to-testpypi","title":"Publish to TestPyPI","text":"<p>We use bump2version to bump the build part of the version number without commiting or tagging the change and then publish a package to TestPyPI from CI using Twine. The version has the GitHub run number appended. </p> <p>For more details refer to the file .github/workflows/publish.yaml.</p>"},{"location":"api/pydvl/","title":"Intro","text":""},{"location":"api/pydvl/#pydvl","title":"pydvl","text":""},{"location":"api/pydvl/#pydvl--the-python-data-valuation-library-api","title":"The Python Data Valuation Library API","text":"<p>This is the API documentation for the Python Data Valuation Library (PyDVL). Use the table of contents to access the documentation for each module.</p> <p>The two main modules you will want to look at are valuation and influence.</p>"},{"location":"api/pydvl/influence/","title":"Influence Function","text":""},{"location":"api/pydvl/influence/#pydvl.influence","title":"pydvl.influence","text":"<p>This package contains algorithms for the computation of the influence function.</p> <p>See The Influence function for an introduction to the concepts and methods implemented here.</p> <p>Warning</p> <p>Much of the code in this package is experimental or untested and is subject to modification. In particular, the package structure and basic API will probably change.</p>"},{"location":"api/pydvl/influence/array/","title":"Array","text":""},{"location":"api/pydvl/influence/array/#pydvl.influence.array","title":"pydvl.influence.array","text":"<p>This module provides classes and utilities for handling large arrays that are chunked and lazily evaluated. It includes abstract base classes for converting between tensor types and NumPy arrays, aggregating blocks of data, and abstract representations of lazy arrays. Concrete implementations are provided for handling chunked lazy arrays (chunked in one resp. two dimensions), with support for efficient storage and retrieval using the Zarr library.</p>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.LazyChunkSequence","title":"LazyChunkSequence","text":"<pre><code>LazyChunkSequence(\n    generator_factory: Callable[[], Generator[TensorType, None, None]],\n    len_generator: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>Generic[TensorType]</code></p> <p>A class representing a chunked, and lazily evaluated array, where the chunking is restricted to the first dimension</p> <p>This class is designed to handle large arrays that don't fit in memory. It works by generating chunks of the array on demand and can also convert these chunks to a Zarr array for efficient storage and retrieval.</p> ATTRIBUTE DESCRIPTION <code>generator_factory</code> <p>A factory function that returns a generator. This generator yields chunks of the large array when called.</p> <p> </p> <code>len_generator</code> <p>if the number of elements from the generator is known from the context, this optional parameter can be used to improve logging by adding a progressbar.</p> <p> </p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>def __init__(\n    self,\n    generator_factory: Callable[[], Generator[TensorType, None, None]],\n    len_generator: Optional[int] = None,\n):\n    self.generator_factory = generator_factory\n    self.len_generator = len_generator\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.LazyChunkSequence.compute","title":"compute","text":"<pre><code>compute(aggregator: Optional[SequenceAggregator] = None) -&gt; Any\n</code></pre> <p>Computes and optionally aggregates the chunks of the array using the provided aggregator. This method initiates the generation of chunks and then combines them according to the aggregator's logic.</p> PARAMETER DESCRIPTION <code>aggregator</code> <p>An optional aggregator for combining the chunks of the array. If None, a default ListAggregator is used to simply collect the chunks into a list.</p> <p> TYPE: <code>Optional[SequenceAggregator]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The aggregated result of all chunks of the array, the format of which depends on the aggregator used.</p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef compute(self, aggregator: Optional[SequenceAggregator] = None) -&gt; Any:\n    \"\"\"\n    Computes and optionally aggregates the chunks of the array using the provided\n    aggregator. This method initiates the generation of chunks and then\n    combines them according to the aggregator's logic.\n\n    Args:\n        aggregator: An optional aggregator for combining the chunks of\n            the array. If None, a default ListAggregator is used to simply collect\n            the chunks into a list.\n\n    Returns:\n        The aggregated result of all chunks of the array, the format of which\n            depends on the aggregator used.\n\n    \"\"\"\n    if aggregator is None:\n        aggregator = ListAggregator()\n    return aggregator(self)\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.LazyChunkSequence.to_zarr","title":"to_zarr","text":"<pre><code>to_zarr(\n    path_or_url: Union[str, StoreLike],\n    converter: NumpyConverter,\n    return_stored: bool = False,\n    overwrite: bool = False,\n) -&gt; Optional[Array]\n</code></pre> <p>Converts the array into Zarr format, a storage format optimized for large arrays, and stores it at the specified path or URL. This method is suitable for scenarios where the data needs to be saved for later use or for large datasets requiring efficient storage.</p> PARAMETER DESCRIPTION <code>path_or_url</code> <p>The file path or URL where the Zarr array will be stored. Also excepts instances of zarr stores.</p> <p> TYPE: <code>Union[str, StoreLike]</code> </p> <code>converter</code> <p>A converter for transforming blocks into NumPy arrays compatible with Zarr.</p> <p> TYPE: <code>NumpyConverter</code> </p> <code>return_stored</code> <p>If True, the method returns the stored Zarr array; otherwise, it returns None.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>overwrite</code> <p>If True, overwrites existing data at the given path_or_url. If False, an error is raised in case of existing data.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Optional[Array]</code> <p>The Zarr array if return_stored is True; otherwise, None.</p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef to_zarr(\n    self,\n    path_or_url: Union[str, StoreLike],\n    converter: NumpyConverter,\n    return_stored: bool = False,\n    overwrite: bool = False,\n) -&gt; Optional[zarr.Array]:\n    \"\"\"\n    Converts the array into Zarr format, a storage format optimized for large\n    arrays, and stores it at the specified path or URL. This method is suitable for\n    scenarios where the data needs to be saved for later use or for large datasets\n    requiring efficient storage.\n\n    Args:\n        path_or_url: The file path or URL where the Zarr array will be stored.\n            Also excepts instances of zarr stores.\n        converter: A converter for transforming blocks into NumPy arrays\n            compatible with Zarr.\n        return_stored: If True, the method returns the stored Zarr array; otherwise,\n            it returns None.\n        overwrite: If True, overwrites existing data at the given path_or_url.\n            If False, an error is raised in case of existing data.\n\n    Returns:\n        The Zarr array if return_stored is True; otherwise, None.\n    \"\"\"\n    row_idx = 0\n    z = None\n\n    gen = cast(Iterator[TensorType], self.generator_factory())\n\n    if self.len_generator is not None:\n        gen = cast(\n            Iterator[TensorType], tqdm(gen, total=self.len_generator, desc=\"Blocks\")\n        )\n\n    for block in gen:\n        numpy_block = converter.to_numpy(block)\n\n        if z is None:\n            z = self._initialize_zarr_array(numpy_block, path_or_url, overwrite)\n\n        new_shape = self._new_shape_according_to_block(numpy_block, row_idx)\n        z.resize(new_shape)\n\n        z[row_idx : row_idx + numpy_block.shape[0]] = numpy_block\n        row_idx += numpy_block.shape[0]\n\n    return z if return_stored else None\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.ListAggregator","title":"ListAggregator","text":"<p>               Bases: <code>SequenceAggregator</code></p>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.ListAggregator.__call__","title":"__call__","text":"<pre><code>__call__(tensor_sequence: LazyChunkSequence) -&gt; List[TensorType]\n</code></pre> <p>Aggregates tensors from a single-level generator into a list. This method simply collects each tensor emitted by the generator into a single list.</p> PARAMETER DESCRIPTION <code>tensor_sequence</code> <p>Object wrapping a generator that yields <code>TensorType</code> objects.</p> <p> TYPE: <code>LazyChunkSequence</code> </p> RETURNS DESCRIPTION <code>List[TensorType]</code> <p>A list containing all the tensors provided by the tensor_generator.</p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>def __call__(\n    self,\n    tensor_sequence: LazyChunkSequence,\n) -&gt; List[TensorType]:\n    \"\"\"\n    Aggregates tensors from a single-level generator into a list. This method simply\n    collects each tensor emitted by the generator into a single list.\n\n    Args:\n        tensor_sequence: Object wrapping a generator that yields `TensorType`\n            objects.\n\n    Returns:\n        A list containing all the tensors provided by the tensor_generator.\n    \"\"\"\n\n    gen = cast(Iterator[TensorType], tensor_sequence.generator_factory())\n\n    if tensor_sequence.len_generator is not None:\n        gen = cast(\n            Iterator[TensorType],\n            tqdm(gen, total=tensor_sequence.len_generator, desc=\"Blocks\"),\n        )\n\n    return [t for t in gen]\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.NestedLazyChunkSequence","title":"NestedLazyChunkSequence","text":"<pre><code>NestedLazyChunkSequence(\n    generator_factory: Callable[\n        [], Generator[Generator[TensorType, None, None], None, None]\n    ],\n    len_outer_generator: Optional[int] = None,\n)\n</code></pre> <p>               Bases: <code>Generic[TensorType]</code></p> <p>A class representing chunked, and lazily evaluated array, where the chunking is restricted to the first two dimensions.</p> <p>This class is designed for handling large arrays where individual chunks are loaded and processed lazily. It supports converting these chunks into a Zarr array for efficient storage and retrieval, with chunking applied along the first two dimensions.</p> ATTRIBUTE DESCRIPTION <code>generator_factory</code> <p>A factory function that returns a generator of generators. Each inner generator yields chunks</p> <p> </p> <code>len_outer_generator</code> <p>if the number of elements from the outer generator is known from the context, this optional parameter can be used to improve logging by adding a progressbar.</p> <p> </p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>def __init__(\n    self,\n    generator_factory: Callable[\n        [], Generator[Generator[TensorType, None, None], None, None]\n    ],\n    len_outer_generator: Optional[int] = None,\n):\n    self.generator_factory = generator_factory\n    self.len_outer_generator = len_outer_generator\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.NestedLazyChunkSequence.compute","title":"compute","text":"<pre><code>compute(aggregator: Optional[NestedSequenceAggregator] = None) -&gt; Any\n</code></pre> <p>Computes and optionally aggregates the chunks of the array using the provided aggregator. This method initiates the generation of chunks and then combines them according to the aggregator's logic.</p> PARAMETER DESCRIPTION <code>aggregator</code> <p>An optional aggregator for combining the chunks of the array. If None, a default NestedListAggregator is used to simply collect the chunks into a list of lists.</p> <p> TYPE: <code>Optional[NestedSequenceAggregator]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The aggregated result of all chunks of the array, the format of which</p> <code>Any</code> <p>depends on the aggregator used.</p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef compute(self, aggregator: Optional[NestedSequenceAggregator] = None) -&gt; Any:\n    \"\"\"\n    Computes and optionally aggregates the chunks of the array using the provided\n    aggregator. This method initiates the generation of chunks and then\n    combines them according to the aggregator's logic.\n\n    Args:\n        aggregator: An optional aggregator for combining the chunks of\n            the array. If None, a default\n            [NestedListAggregator][pydvl.influence.array.NestedListAggregator]\n            is used to simply collect the chunks into a list of lists.\n\n    Returns:\n        The aggregated result of all chunks of the array, the format of which\n        depends on the aggregator used.\n\n    \"\"\"\n    if aggregator is None:\n        aggregator = NestedListAggregator()\n    return aggregator(self)\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.NestedLazyChunkSequence.to_zarr","title":"to_zarr","text":"<pre><code>to_zarr(\n    path_or_url: Union[str, StoreLike],\n    converter: NumpyConverter,\n    return_stored: bool = False,\n    overwrite: bool = False,\n) -&gt; Optional[Array]\n</code></pre> <p>Converts the array into Zarr format, a storage format optimized for large arrays, and stores it at the specified path or URL. This method is suitable for scenarios where the data needs to be saved for later use or for large datasets requiring efficient storage.</p> PARAMETER DESCRIPTION <code>path_or_url</code> <p>The file path or URL where the Zarr array will be stored. Also excepts instances of zarr stores.</p> <p> TYPE: <code>Union[str, StoreLike]</code> </p> <code>converter</code> <p>A converter for transforming blocks into NumPy arrays compatible with Zarr.</p> <p> TYPE: <code>NumpyConverter</code> </p> <code>return_stored</code> <p>If True, the method returns the stored Zarr array; otherwise, it returns None.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>overwrite</code> <p>If True, overwrites existing data at the given path_or_url. If False, an error is raised in case of existing data.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Optional[Array]</code> <p>The Zarr array if return_stored is True; otherwise, None.</p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef to_zarr(\n    self,\n    path_or_url: Union[str, StoreLike],\n    converter: NumpyConverter,\n    return_stored: bool = False,\n    overwrite: bool = False,\n) -&gt; Optional[zarr.Array]:\n    \"\"\"\n    Converts the array into Zarr format, a storage format optimized for large\n    arrays, and stores it at the specified path or URL. This method is suitable for\n    scenarios where the data needs to be saved for later use or for large datasets\n    requiring efficient storage.\n\n    Args:\n        path_or_url: The file path or URL where the Zarr array will be stored.\n            Also excepts instances of zarr stores.\n        converter: A converter for transforming blocks into NumPy arrays\n            compatible with Zarr.\n        return_stored: If True, the method returns the stored Zarr array;\n            otherwise, it returns None.\n        overwrite: If True, overwrites existing data at the given path_or_url.\n            If False, an error is raised in case of existing data.\n\n    Returns:\n        The Zarr array if return_stored is True; otherwise, None.\n    \"\"\"\n\n    row_idx = 0\n    z = None\n    numpy_block = None\n    block_generator = cast(Iterator[Iterator[TensorType]], self.generator_factory())\n\n    if self.len_outer_generator is not None:\n        block_generator = cast(\n            Iterator[Iterator[TensorType]],\n            tqdm(\n                block_generator, total=self.len_outer_generator, desc=\"Row blocks\"\n            ),\n        )\n\n    for row_blocks in block_generator:\n        col_idx = 0\n        for block in row_blocks:\n            numpy_block = converter.to_numpy(block)\n            if z is None:\n                z = self._initialize_zarr_array(numpy_block, path_or_url, overwrite)\n            new_shape = self._new_shape_according_to_block(\n                z, numpy_block, row_idx, col_idx\n            )\n            z.resize(new_shape)\n            idx_slice_to_update = self._idx_slice_for_update(\n                numpy_block, row_idx, col_idx\n            )\n            z[idx_slice_to_update] = numpy_block\n\n            col_idx += numpy_block.shape[1]\n\n        if numpy_block is None:\n            raise ValueError(\"Generator is empty\")\n\n        row_idx += numpy_block.shape[0]\n\n    return z if return_stored else None\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.NestedListAggregator","title":"NestedListAggregator","text":"<p>               Bases: <code>NestedSequenceAggregator</code></p>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.NestedListAggregator.__call__","title":"__call__","text":"<pre><code>__call__(\n    nested_sequence_of_tensors: NestedLazyChunkSequence,\n) -&gt; List[List[TensorType]]\n</code></pre> <p>Aggregates tensors from a nested generator structure into a list of lists.  Each inner generator is converted into a list of tensors, resulting in a nested  list structure.</p> <p>Args:      nested_sequence_of_tensors: Object wrapping a generator of generators,         where each inner generator yields TensorType objects.</p> RETURNS DESCRIPTION <code>List[List[TensorType]]</code> <p>A list of lists, where each inner list contains tensors returned from one of the inner generators.</p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>def __call__(\n    self,\n    nested_sequence_of_tensors: NestedLazyChunkSequence,\n) -&gt; List[List[TensorType]]:\n    \"\"\"\n     Aggregates tensors from a nested generator structure into a list of lists.\n     Each inner generator is converted into a list of tensors, resulting in a nested\n     list structure.\n\n     Args:\n         nested_sequence_of_tensors: Object wrapping a generator of generators,\n            where each inner generator yields TensorType objects.\n\n    Returns:\n        A list of lists, where each inner list contains tensors returned from one\n            of the inner generators.\n    \"\"\"\n    outer_gen = cast(\n        Iterator[Iterator[TensorType]],\n        nested_sequence_of_tensors.generator_factory(),\n    )\n    len_outer_gen = nested_sequence_of_tensors.len_outer_generator\n    if len_outer_gen is not None:\n        outer_gen = cast(\n            Iterator[Iterator[TensorType]],\n            tqdm(outer_gen, total=len_outer_gen, desc=\"Row blocks\"),\n        )\n\n    return [list(tensor_gen) for tensor_gen in outer_gen]\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.NestedSequenceAggregator","title":"NestedSequenceAggregator","text":"<p>               Bases: <code>Generic[TensorType]</code>, <code>ABC</code></p>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.NestedSequenceAggregator.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(nested_sequence_of_tensors: NestedLazyChunkSequence)\n</code></pre> <p>Aggregates tensors from a nested sequence of tensors.</p> <p>Implement this method to specify how tensors, nested in two layers of generators, should be combined. Useful for complex data structures where tensors are not directly accessible in a flat list.</p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>@abstractmethod\ndef __call__(self, nested_sequence_of_tensors: NestedLazyChunkSequence):\n    \"\"\"\n    Aggregates tensors from a nested sequence of tensors.\n\n    Implement this method to specify how tensors, nested in two layers of\n    generators, should be combined. Useful for complex data structures where tensors\n    are not directly accessible in a flat list.\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.NumpyConverter","title":"NumpyConverter","text":"<p>               Bases: <code>Generic[TensorType]</code>, <code>ABC</code></p> <p>Base class for converting TensorType objects into numpy arrays and vice versa.</p>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.NumpyConverter.from_numpy","title":"from_numpy  <code>abstractmethod</code>","text":"<pre><code>from_numpy(x: NDArray) -&gt; TensorType\n</code></pre> <p>Override this method for converting a numpy array into a TensorType object</p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>@abstractmethod\ndef from_numpy(self, x: NDArray) -&gt; TensorType:\n    \"\"\"Override this method for converting a numpy array into a TensorType object\"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.NumpyConverter.to_numpy","title":"to_numpy  <code>abstractmethod</code>","text":"<pre><code>to_numpy(x: TensorType) -&gt; NDArray\n</code></pre> <p>Override this method for converting a TensorType object into a numpy array</p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>@abstractmethod\ndef to_numpy(self, x: TensorType) -&gt; NDArray:\n    \"\"\"Override this method for converting a TensorType object into a numpy array\"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.SequenceAggregator","title":"SequenceAggregator","text":"<p>               Bases: <code>Generic[TensorType]</code>, <code>ABC</code></p>"},{"location":"api/pydvl/influence/array/#pydvl.influence.array.SequenceAggregator.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(tensor_sequence: LazyChunkSequence)\n</code></pre> <p>Aggregates tensors from a sequence.</p> <p>Implement this method to define how a sequence of tensors, provided by a generator, should be combined.</p> Source code in <code>src/pydvl/influence/array.py</code> <pre><code>@abstractmethod\ndef __call__(\n    self,\n    tensor_sequence: LazyChunkSequence,\n):\n    \"\"\"\n    Aggregates tensors from a sequence.\n\n    Implement this method to define how a sequence of tensors, provided by a\n    generator, should be combined.\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/","title":"Base influence function model","text":""},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model","title":"pydvl.influence.base_influence_function_model","text":""},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence","title":"ComposableInfluence","text":"<p>               Bases: <code>InfluenceFunctionModel</code>, <code>Generic[TensorType, BatchType, DataLoaderType, BlockMapperType]</code>, <code>ABC</code></p> <p>Generic abstract base class, that allow for block-wise computation of influence quantities. Inherit from this base class for specific influence algorithms and tensor frameworks.</p>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence.is_thread_safe","title":"is_thread_safe  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>is_thread_safe: bool\n</code></pre> <p>Whether the influence computation is thread safe</p>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence.n_parameters","title":"n_parameters  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>n_parameters\n</code></pre> <p>Number of trainable parameters of the underlying model</p>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence._concat","title":"_concat  <code>abstractmethod</code>","text":"<pre><code>_concat(tensors: Iterable[TensorType], dim: int)\n</code></pre> <p>Implement this to concat tensors at a specified dimension</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@abstractmethod\ndef _concat(self, tensors: Iterable[TensorType], dim: int):\n    \"\"\"Implement this to concat tensors at a specified dimension\"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence._create_batch","title":"_create_batch  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>_create_batch(x: TensorType, y: TensorType) -&gt; BatchType\n</code></pre> <p>Implement this method to provide the creation of a subtype of Batch for a specific framework</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef _create_batch(x: TensorType, y: TensorType) -&gt; BatchType:\n    \"\"\"Implement this method to provide the creation of a subtype of\n    [Batch][pydvl.influence.types.Batch] for a specific framework\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence._create_block_mapper","title":"_create_block_mapper  <code>abstractmethod</code>","text":"<pre><code>_create_block_mapper(data: DataLoaderType) -&gt; BlockMapperType\n</code></pre> <p>Override this method to create a block mapper instance, that can be used to compute block-wise influence quantities.</p> PARAMETER DESCRIPTION <code>data</code> <p>iterable of tensors</p> <p> TYPE: <code>DataLoaderType</code> </p> RETURNS DESCRIPTION <code>BlockMapperType</code> <p>BlockMapper instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@abstractmethod\ndef _create_block_mapper(self, data: DataLoaderType) -&gt; BlockMapperType:\n    \"\"\"\n    Override this method to create a block mapper instance, that can be used\n    to compute block-wise influence quantities.\n\n    Args:\n        data: iterable of tensors\n\n    Returns:\n        BlockMapper instance\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence._flatten_trailing_dim","title":"_flatten_trailing_dim  <code>abstractmethod</code>","text":"<pre><code>_flatten_trailing_dim(tensor: TensorType)\n</code></pre> <p>Implement this to flatten all but the first dimension</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@abstractmethod\ndef _flatten_trailing_dim(self, tensor: TensorType):\n    \"\"\"Implement this to flatten all but the first dimension\"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence.fit","title":"fit","text":"<pre><code>fit(data: DataLoaderType) -&gt; InfluenceFunctionModel\n</code></pre> <p>Fitting to provided data, by internally creating a block mapper instance from it. Args:     data: iterable of tensors</p> RETURNS DESCRIPTION <code>InfluenceFunctionModel</code> <p>Fitted instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef fit(self, data: DataLoaderType) -&gt; InfluenceFunctionModel:\n    \"\"\"\n    Fitting to provided data, by internally creating a block mapper instance from\n    it.\n    Args:\n        data: iterable of tensors\n\n    Returns:\n        Fitted instance\n    \"\"\"\n    self.block_mapper = self._create_block_mapper(data)\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: TensorType, y: TensorType) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influence_factors(self, x: TensorType, y: TensorType) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    return self._influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence.influence_factors_by_block","title":"influence_factors_by_block","text":"<pre><code>influence_factors_by_block(\n    x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise</p> <code>OrderedDict[str, TensorType]</code> <p>approximate inverse Hessian matrix vector products per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influence_factors_by_block(\n    self, x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise\n        approximate inverse Hessian matrix vector products per block.\n\n    \"\"\"\n    return self.block_mapper.transformed_grads(self._create_batch(x, y))\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence.influences","title":"influences","text":"<pre><code>influences(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    if x is None and y is not None:\n        raise ValueError(\n            \"Providing labels y, without providing model input x is not supported\"\n        )\n\n    if x is not None and y is None:\n        raise ValueError(\n            \"Providing model input x, without providing labels y is not supported\"\n        )\n\n    return self._influences(x_test, y_test, x, y, mode)\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence.influences_by_block","title":"influences_by_block","text":"<pre><code>influences_by_block(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise influence values for the provided data, i.e. an approximation of</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of the approximation of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_by_block(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise influence values for the provided data, i.e. an\n    approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of the approximation of\n            $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block.\n\n    \"\"\"\n    left_batch = self._create_batch(x_test, y_test)\n\n    if x is None:\n        if y is not None:\n            raise ValueError(\n                \"Providing labels y, without providing model input x \"\n                \"is not supported\"\n            )\n        right_batch = left_batch\n    else:\n        if y is None:\n            raise ValueError(\n                \"Providing model input x, without providing labels y \"\n                \"is not supported\"\n            )\n        right_batch = self._create_batch(x, y)\n\n    return self.block_mapper.interactions(left_batch, right_batch, mode)\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors(\n    self,\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    tensors = self.block_mapper.generate_interactions_from_transformed_grads(\n        z_test_factors,\n        self._create_batch(x, y),\n        mode,\n    )\n    result: TensorType = next(tensors)\n    for tensor in tensors:\n        result = result + tensor\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.ComposableInfluence.influences_from_factors_by_block","title":"influences_from_factors_by_block","text":"<pre><code>influences_from_factors_by_block(\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Block-wise computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>OrderedDict[str, TensorType]</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors_by_block(\n    self,\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Block-wise computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block\n\n    \"\"\"\n    return self.block_mapper.interactions_from_transformed_grads(\n        z_test_factors, self._create_batch(x, y), mode\n    )\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel","title":"InfluenceFunctionModel","text":"<p>               Bases: <code>Generic[TensorType, DataLoaderType]</code>, <code>ABC</code></p> <p>Generic abstract base class for computing influence related quantities. For a specific influence algorithm and tensor framework, inherit from this base class</p>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel.is_fitted","title":"is_fitted  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>is_fitted\n</code></pre> <p>Override this, to expose the fitting status of the instance.</p>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel.is_thread_safe","title":"is_thread_safe  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>is_thread_safe: bool\n</code></pre> <p>Whether the influence computation is thread safe</p>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel.n_parameters","title":"n_parameters  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>n_parameters\n</code></pre> <p>Number of trainable parameters of the underlying model</p>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel._influence_factors","title":"_influence_factors  <code>abstractmethod</code>","text":"<pre><code>_influence_factors(x: TensorType, y: TensorType) -&gt; TensorType\n</code></pre> <p>Override this method to implement the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@abstractmethod\ndef _influence_factors(self, x: TensorType, y: TensorType) -&gt; TensorType:\n    r\"\"\"\n    Override this method to implement the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel._influences","title":"_influences  <code>abstractmethod</code>","text":"<pre><code>_influences(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Override this method to implement the approximation of</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@abstractmethod\ndef _influences(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Override this method to implement the approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(data: DataLoaderType) -&gt; InfluenceFunctionModel\n</code></pre> <p>Override this method to fit the influence function model to training data, e.g. pre-compute hessian matrix or matrix decompositions</p> PARAMETER DESCRIPTION <code>data</code> <p> TYPE: <code>DataLoaderType</code> </p> RETURNS DESCRIPTION <code>InfluenceFunctionModel</code> <p>The fitted instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@abstractmethod\ndef fit(self, data: DataLoaderType) -&gt; InfluenceFunctionModel:\n    \"\"\"\n    Override this method to fit the influence function model to training data,\n    e.g. pre-compute hessian matrix or matrix decompositions\n\n    Args:\n        data:\n\n    Returns:\n        The fitted instance\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: TensorType, y: TensorType) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influence_factors(self, x: TensorType, y: TensorType) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    return self._influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel.influences","title":"influences","text":"<pre><code>influences(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    if x is None and y is not None:\n        raise ValueError(\n            \"Providing labels y, without providing model input x is not supported\"\n        )\n\n    if x is not None and y is None:\n        raise ValueError(\n            \"Providing model input x, without providing labels y is not supported\"\n        )\n\n    return self._influences(x_test, y_test, x, y, mode)\n</code></pre>"},{"location":"api/pydvl/influence/base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel.influences_from_factors","title":"influences_from_factors  <code>abstractmethod</code>","text":"<pre><code>influences_from_factors(\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Override this method to implement the computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@abstractmethod\ndef influences_from_factors(\n    self,\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Override this method to implement the computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/","title":"Influence calculator","text":""},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator","title":"pydvl.influence.influence_calculator","text":"<p>This module provides functionality for calculating influences for large amount of data. The computation is based on a chunk computation model in the form of an instance of InfluenceFunctionModel, which is mapped over collection of chunks.</p>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.DaskInfluenceCalculator","title":"DaskInfluenceCalculator","text":"<pre><code>DaskInfluenceCalculator(\n    influence_function_model: InfluenceFunctionModel,\n    converter: NumpyConverter,\n    client: Union[Client, Type[DisableClientSingleThreadCheck]],\n)\n</code></pre> <p>This class is designed to compute influences over dask.array.Array collections, leveraging the capabilities of Dask for distributed computing and parallel processing. It requires an influence computation model of type InfluenceFunctionModel, which defines how influences are computed on a chunk of data. Essentially, this class functions by mapping the influence function model across the various chunks of a dask.array.Array collection.</p> PARAMETER DESCRIPTION <code>influence_function_model</code> <p>instance of type InfluenceFunctionModel, that specifies the computation logic for influence on data chunks. It's a pivotal part of the calculator, determining how influence is computed and applied across the data array.</p> <p> TYPE: <code>InfluenceFunctionModel</code> </p> <code>converter</code> <p>A utility for converting numpy arrays to TensorType objects, facilitating the interaction between numpy arrays and the influence function model.</p> <p> TYPE: <code>NumpyConverter</code> </p> <code>client</code> <p>This parameter accepts either of two types:</p> <ol> <li> <p>A distributed Client object</p> </li> <li> <p>The special type DisableClientSingleThreadCheck, which serves as a flag to bypass certain checks.</p> </li> </ol> <p>During initialization, the system verifies if all workers are operating in single-threaded mode when the provided influence_function_model is designated as not thread-safe (indicated by the <code>is_thread_safe</code> property). If this condition is not met, the initialization will raise a specific error, signaling a potential thread-safety conflict.</p> <p>To intentionally skip this safety check (e.g., for debugging purposes using the single machine synchronous scheduler), you can supply the DisableClientSingleThreadCheck type.</p> <p> TYPE: <code>Union[Client, Type[DisableClientSingleThreadCheck]]</code> </p> <p>Warning</p> <p>Make sure to set <code>threads_per_worker=1</code>, when using the distributed scheduler for computing, if your implementation of InfluenceFunctionModel is not thread-safe. <pre><code>client = Client(threads_per_worker=1)\n</code></pre> For details on dask schedulers see the official documentation.</p> Example <pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom pydvl.influence import DaskInfluenceCalculator\nfrom pydvl.influence.torch import CgInfluence\nfrom pydvl.influence.torch.util import (\n    torch_dataset_to_dask_array,\n    TorchNumpyConverter,\n)\nfrom distributed import Client\n\n# Possible some out of memory large Dataset\ntrain_data_set: Dataset = LargeDataSet(...)\ntest_data_set: Dataset = LargeDataSet(...)\n\ntrain_dataloader = DataLoader(train_data_set)\ninfl_model = CgInfluence(model, loss, hessian_regularization=0.01)\ninfl_model = if_model.fit(train_dataloader)\n\n# wrap your input data into dask arrays\nchunk_size = 10\nda_x, da_y = torch_dataset_to_dask_array(train_data_set, chunk_size=chunk_size)\nda_x_test, da_y_test = torch_dataset_to_dask_array(test_data_set,\n                                                   chunk_size=chunk_size)\n\n# use only one thread for scheduling, due to non-thread safety of some torch\n# operations\nclient = Client(n_workers=4, threads_per_worker=1)\n\ninfl_calc = DaskInfluenceCalculator(infl_model,\n                                    TorchNumpyConverter(device=torch.device(\"cpu\")),\n                                    client)\nda_influences = infl_calc.influences(da_x_test, da_y_test, da_x, da_y)\n# da_influences is a dask.array.Array\n\n# trigger computation and write chunks to disk in parallel\nda_influences.to_zarr(\"path/or/url\")\n</code></pre> Source code in <code>src/pydvl/influence/influence_calculator.py</code> <pre><code>def __init__(\n    self,\n    influence_function_model: InfluenceFunctionModel,\n    converter: NumpyConverter,\n    client: Union[Client, Type[DisableClientSingleThreadCheck]],\n):\n    self._n_parameters = influence_function_model.n_parameters\n    self.influence_function_model = influence_function_model\n    self.numpy_converter = converter\n\n    if isinstance(client, type(DisableClientSingleThreadCheck)):\n        logger.warning(DisableClientSingleThreadCheck.warning_msg())\n        self.influence_function_model = delayed(influence_function_model)\n    elif isinstance(client, Client):\n        self._validate_client(client, influence_function_model)\n        self.influence_function_model = client.scatter(\n            influence_function_model, broadcast=True\n        )\n    else:\n        raise ValueError(\n            \"The 'client' parameter \"\n            \"must either be a distributed.Client object or the\"\n            \"type 'DisableClientSingleThreadCheck'.\"\n        )\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.DaskInfluenceCalculator.n_parameters","title":"n_parameters  <code>property</code>","text":"<pre><code>n_parameters\n</code></pre> <p>Number of trainable parameters of the underlying model used in the batch computation</p>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.DaskInfluenceCalculator._validate_aligned_chunking","title":"_validate_aligned_chunking  <code>staticmethod</code>","text":"<pre><code>_validate_aligned_chunking(x: Array, y: Array)\n</code></pre> <p>Check that the chunking in the first dimensions of the two input arrays are aligned</p> Source code in <code>src/pydvl/influence/influence_calculator.py</code> <pre><code>@staticmethod\ndef _validate_aligned_chunking(x: da.Array, y: da.Array):\n    \"\"\"\n    Check that the chunking in the first dimensions of the two input arrays\n    are aligned\n    \"\"\"\n    if x.chunks[0] != y.chunks[0]:\n        raise UnalignedChunksError(x.chunks[0], y.chunks[0])\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.DaskInfluenceCalculator._validate_dimensions_not_chunked","title":"_validate_dimensions_not_chunked  <code>staticmethod</code>","text":"<pre><code>_validate_dimensions_not_chunked(x: Array)\n</code></pre> <p>Check if all but the first dimension are not chunked</p> Source code in <code>src/pydvl/influence/influence_calculator.py</code> <pre><code>@staticmethod\ndef _validate_dimensions_not_chunked(x: da.Array):\n    \"\"\"\n    Check if all but the first dimension are not chunked\n    \"\"\"\n    if any([len(c) &gt; 1 for c in x.chunks[1:]]):\n        raise InvalidDimensionChunksError(x.chunks)\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.DaskInfluenceCalculator.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: Array, y: Array) -&gt; Array\n</code></pre> <p>Computes the expression</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradients are computed for the chunks of \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>Array</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>dask.array.Array representing the element-wise inverse Hessian matrix vector products for the provided batch.</p> Source code in <code>src/pydvl/influence/influence_calculator.py</code> <pre><code>def influence_factors(self, x: da.Array, y: da.Array) -&gt; da.Array:\n    r\"\"\"\n    Computes the expression\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradients are computed for the chunks of $(x, y)$.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        [dask.array.Array][dask.array.Array] representing the element-wise inverse\n            Hessian matrix vector products for the provided batch.\n\n    \"\"\"\n\n    self._validate_aligned_chunking(x, y)\n    self._validate_dimensions_not_chunked(x)\n    self._validate_dimensions_not_chunked(y)\n\n    def func(x_numpy: NDArray, y_numpy: NDArray, model: InfluenceFunctionModel):\n        factors = model.influence_factors(\n            self.numpy_converter.from_numpy(x_numpy),\n            self.numpy_converter.from_numpy(y_numpy),\n        )\n        return self.numpy_converter.to_numpy(factors)\n\n    chunks = []\n    for x_chunk, y_chunk, chunk_size in zip(\n        x.to_delayed(), y.to_delayed(), x.chunks[0]\n    ):\n        chunk_shape = (chunk_size, self.n_parameters)\n        chunk_array = da.from_delayed(\n            delayed(func)(\n                x_chunk.squeeze()[()],\n                y_chunk.squeeze()[()],\n                self.influence_function_model,\n            ),\n            dtype=x.dtype,\n            shape=chunk_shape,\n        )\n        chunks.append(chunk_array)\n\n    return da.concatenate(chunks)\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.DaskInfluenceCalculator.influences","title":"influences","text":"<pre><code>influences(\n    x_test: Array,\n    y_test: Array,\n    x: Optional[Array] = None,\n    y: Optional[Array] = None,\n    mode: InfluenceMode = Up,\n) -&gt; Array\n</code></pre> <p>Compute approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}})), \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}})), \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The computation is done block-wise for the chunks of the provided dask arrays.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>Array</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Array</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>Optional[Array]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[Array]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>dask.array.Array representing the element-wise scalar products for the provided batch.</p> Source code in <code>src/pydvl/influence/influence_calculator.py</code> <pre><code>def influences(\n    self,\n    x_test: da.Array,\n    y_test: da.Array,\n    x: Optional[da.Array] = None,\n    y: Optional[da.Array] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; da.Array:\n    r\"\"\"\n    Compute approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n    f_{\\theta}(x_{\\text{test}})), \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n    f_{\\theta}(x_{\\text{test}})),\n    \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The computation is done block-wise\n    for the chunks of the provided dask arrays.\n\n    Args:\n        x_test: model input to use in the gradient computations of\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n                f_{\\theta}(x_{\\text{test}}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        [dask.array.Array][dask.array.Array] representing the element-wise scalar\n            products for the provided batch.\n\n    \"\"\"\n\n    self._validate_aligned_chunking(x_test, y_test)\n    self._validate_dimensions_not_chunked(x_test)\n    self._validate_dimensions_not_chunked(y_test)\n\n    if (x is None) != (y is None):\n        if x is None:\n            raise ValueError(\n                \"Providing labels y without providing model input x \"\n                \"is not supported\"\n            )\n        if y is None:\n            raise ValueError(\n                \"Providing model input x without labels y is not supported\"\n            )\n    elif x is not None:\n        self._validate_aligned_chunking(x, y)\n        self._validate_dimensions_not_chunked(x)\n        self._validate_dimensions_not_chunked(y)\n    else:\n        x, y = x_test, y_test\n    assert x is not None and y is not None  # For the type checker's benefit\n\n    def func(\n        x_test_numpy: NDArray,\n        y_test_numpy: NDArray,\n        x_numpy: NDArray,\n        y_numpy: NDArray,\n        model: InfluenceFunctionModel,\n    ):\n        values = model.influences(\n            self.numpy_converter.from_numpy(x_test_numpy),\n            self.numpy_converter.from_numpy(y_test_numpy),\n            self.numpy_converter.from_numpy(x_numpy),\n            self.numpy_converter.from_numpy(y_numpy),\n            mode,\n        )\n        return self.numpy_converter.to_numpy(values)\n\n    un_chunked_x_shapes = [s[0] for s in x_test.chunks[1:]]\n    x_test_chunk_sizes = x_test.chunks[0]\n    x_chunk_sizes = x.chunks[0]\n    blocks = []\n    block_shape: Tuple[int, ...]\n\n    for x_test_chunk, y_test_chunk, test_chunk_size in zip(\n        x_test.to_delayed(), y_test.to_delayed(), x_test_chunk_sizes\n    ):\n        row = []\n        for x_chunk, y_chunk, chunk_size in zip(\n            x.to_delayed(),\n            y.to_delayed(),\n            x_chunk_sizes,  # type:ignore\n        ):\n            if mode == InfluenceMode.Up:\n                block_shape = (test_chunk_size, chunk_size)\n            elif mode == InfluenceMode.Perturbation:\n                block_shape = (test_chunk_size, chunk_size, *un_chunked_x_shapes)\n            else:\n                raise UnsupportedInfluenceModeException(mode)\n\n            block_array = da.from_delayed(\n                delayed(func)(\n                    x_test_chunk.squeeze()[()],\n                    y_test_chunk.squeeze()[()],\n                    x_chunk.squeeze()[()],\n                    y_chunk.squeeze()[()],\n                    self.influence_function_model,\n                ),\n                shape=block_shape,\n                dtype=x_test.dtype,\n            )\n\n            if mode == InfluenceMode.Perturbation:\n                n_dims = block_array.ndim\n                new_order = tuple(range(2, n_dims)) + (0, 1)\n                block_array = block_array.transpose(new_order)\n\n            row.append(block_array)\n        blocks.append(row)\n\n    values_array = da.block(blocks)\n\n    if mode == InfluenceMode.Perturbation:\n        n_dims = values_array.ndim\n        new_order = (n_dims - 2, n_dims - 1) + tuple(range(n_dims - 2))\n        values_array = values_array.transpose(new_order)\n\n    return values_array\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.DaskInfluenceCalculator.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: Array, x: Array, y: Array, mode: InfluenceMode = Up\n) -&gt; Array\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>Array</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>Array</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Array</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>dask.array.Array representing the element-wise scalar product of the provided batch</p> Source code in <code>src/pydvl/influence/influence_calculator.py</code> <pre><code>def influences_from_factors(\n    self,\n    z_test_factors: da.Array,\n    x: da.Array,\n    y: da.Array,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; da.Array:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant\n    to be per sample of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n                f_{\\theta}(x_{\\text{test}}))$\n        x: optional model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n      [dask.array.Array][dask.array.Array] representing the element-wise scalar\n        product of the provided batch\n\n    \"\"\"\n    self._validate_aligned_chunking(x, y)\n    self._validate_dimensions_not_chunked(x)\n    self._validate_dimensions_not_chunked(y)\n    self._validate_dimensions_not_chunked(z_test_factors)\n\n    def func(\n        z_test_numpy: NDArray,\n        x_numpy: NDArray,\n        y_numpy: NDArray,\n        model: InfluenceFunctionModel,\n    ):\n        ups = model.influences_from_factors(\n            self.numpy_converter.from_numpy(z_test_numpy),\n            self.numpy_converter.from_numpy(x_numpy),\n            self.numpy_converter.from_numpy(y_numpy),\n            mode=mode,\n        )\n        return self.numpy_converter.to_numpy(ups)\n\n    un_chunked_x_shape = [s[0] for s in x.chunks[1:]]\n    x_chunk_sizes = x.chunks[0]\n    z_test_chunk_sizes = z_test_factors.chunks[0]\n    blocks = []\n    block_shape: Tuple[int, ...]\n\n    for z_test_chunk, z_test_chunk_size in zip(\n        z_test_factors.to_delayed(), z_test_chunk_sizes\n    ):\n        row = []\n        for x_chunk, y_chunk, chunk_size in zip(\n            x.to_delayed(), y.to_delayed(), x_chunk_sizes\n        ):\n            if mode == InfluenceMode.Perturbation:\n                block_shape = (z_test_chunk_size, chunk_size, *un_chunked_x_shape)\n            elif mode == InfluenceMode.Up:\n                block_shape = (z_test_chunk_size, chunk_size)\n            else:\n                raise UnsupportedInfluenceModeException(mode)\n\n            block_array = da.from_delayed(\n                delayed(func)(\n                    z_test_chunk.squeeze()[()],\n                    x_chunk.squeeze()[()],\n                    y_chunk.squeeze()[()],\n                    self.influence_function_model,\n                ),\n                shape=block_shape,\n                dtype=z_test_factors.dtype,\n            )\n\n            if mode == InfluenceMode.Perturbation:\n                n_dims = block_array.ndim\n                new_order = tuple(range(2, n_dims)) + (0, 1)\n                block_array = block_array.transpose(*new_order)\n\n            row.append(block_array)\n        blocks.append(row)\n\n    values_array = da.block(blocks)\n\n    if mode == InfluenceMode.Perturbation:\n        n_dims = values_array.ndim\n        new_order = (n_dims - 2, n_dims - 1) + tuple(range(n_dims - 2))\n        values_array = values_array.transpose(*new_order)\n\n    return values_array\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.DisableClientSingleThreadCheck","title":"DisableClientSingleThreadCheck","text":"<p>This type can be provided to the initialization of a DaskInfluenceCalculator instead of a distributed client object. It is useful in those scenarios, where the user want to disable the checking for thread-safety in the initialization phase, e.g. when using the single machine synchronous scheduler for debugging purposes.</p> Example <pre><code>from pydvl.influence import DisableClientThreadingCheck\n\nda_calc = DaskInfluenceCalculator(if_model,\n                                  TorchNumpyConverter(),\n                                  DisableClientThreadingCheck)\nda_influences = da_calc.influences(da_x_test, da_y_test, da_x, da_y)\nda_influences.compute(scheduler='synchronous')\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.SequentialInfluenceCalculator","title":"SequentialInfluenceCalculator","text":"<pre><code>SequentialInfluenceCalculator(influence_function_model: InfluenceFunctionModel)\n</code></pre> <p>This class serves as a simple wrapper for processing batches of data in a sequential manner. It is particularly useful in scenarios where parallel or distributed processing is not required or not feasible. The core functionality of this class is to apply a specified influence computation model, of type InfluenceFunctionModel, to batches of data one at a time.</p> PARAMETER DESCRIPTION <code>influence_function_model</code> <p>An instance of type     [InfluenceFunctionModel]     [pydvl.influence.base_influence_function_model.InfluenceFunctionModel], that     specifies the computation logic for influence on data chunks.</p> <p> TYPE: <code>InfluenceFunctionModel</code> </p> Example <pre><code>from pydvl.influence import SequentialInfluenceCalculator\nfrom pydvl.influence.torch.util import (\nNestedTorchCatAggregator,\nTorchNumpyConverter,\n)\nfrom pydvl.influence.torch import CgInfluence\n\nbatch_size = 10\ntrain_dataloader = DataLoader(..., batch_size=batch_size)\ntest_dataloader = DataLoader(..., batch_size=batch_size)\n\ninfl_model = CgInfluence(model, loss, hessian_regularization=0.01)\ninfl_model = infl_model.fit(train_dataloader)\n\ninfl_calc = SequentialInfluenceCalculator(if_model)\n\n# this does not trigger the computation\nlazy_influences = infl_calc.influences(test_dataloader, train_dataloader)\n\n# trigger computation and pull the result into main memory, result is the full\n# tensor for all combinations of the two loaders\ninfluences = lazy_influences.compute(aggregator=NestedTorchCatAggregator())\n# or\n# trigger computation and write results chunk-wise to disk using zarr in a\n# sequential manner\nlazy_influences.to_zarr(\"local_path/or/url\", TorchNumpyConverter())\n</code></pre> Source code in <code>src/pydvl/influence/influence_calculator.py</code> <pre><code>def __init__(\n    self,\n    influence_function_model: InfluenceFunctionModel,\n):\n    self.influence_function_model = influence_function_model\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.SequentialInfluenceCalculator.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(\n    data_iterable: Iterable[Tuple[TensorType, TensorType]]\n) -&gt; LazyChunkSequence\n</code></pre> <p>Compute the expression</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient are computed for the chunks \\((x, y)\\) of the data_iterable in a sequential manner.</p> PARAMETER DESCRIPTION <code>data_iterable</code> <p>An iterable that returns tuples of tensors. Each tuple consists of a pair of tensors (x, y), representing input data and corresponding targets.</p> <p> TYPE: <code>Iterable[Tuple[TensorType, TensorType]]</code> </p> RETURNS DESCRIPTION <code>LazyChunkSequence</code> <p>A lazy data structure representing the chunks of the resulting tensor</p> Source code in <code>src/pydvl/influence/influence_calculator.py</code> <pre><code>def influence_factors(\n    self,\n    data_iterable: Iterable[Tuple[TensorType, TensorType]],\n) -&gt; LazyChunkSequence:\n    r\"\"\"\n    Compute the expression\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient are computed for the chunks $(x, y)$ of the data_iterable in\n    a sequential manner.\n\n    Args:\n        data_iterable: An iterable that returns tuples of tensors.\n            Each tuple consists of a pair of tensors (x, y), representing input data\n            and corresponding targets.\n\n    Returns:\n        A lazy data structure representing the chunks of the resulting tensor\n    \"\"\"\n    try:\n        len_iterable = len(cast(Sized, data_iterable))\n    except Exception as e:\n        logger.debug(f\"Failed to retrieve len of data iterable: {e}\")\n        len_iterable = None\n\n    tensors_gen_factory = partial(self._influence_factors_gen, data_iterable)\n    return LazyChunkSequence(tensors_gen_factory, len_generator=len_iterable)\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.SequentialInfluenceCalculator.influences","title":"influences","text":"<pre><code>influences(\n    test_data_iterable: Iterable[Tuple[TensorType, TensorType]],\n    train_data_iterable: Iterable[Tuple[TensorType, TensorType]],\n    mode: InfluenceMode = Up,\n) -&gt; NestedLazyChunkSequence\n</code></pre> <p>Compute approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}})), \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}})), \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The computation is done block-wise for the chunks of the provided data iterables and aggregated into a single tensor in memory.</p> PARAMETER DESCRIPTION <code>test_data_iterable</code> <p>An iterable that returns tuples of tensors. Each tuple consists of a pair of tensors (x, y), representing input data and corresponding targets.</p> <p> TYPE: <code>Iterable[Tuple[TensorType, TensorType]]</code> </p> <code>train_data_iterable</code> <p>An iterable that returns tuples of tensors. Each tuple consists of a pair of tensors (x, y), representing input data and corresponding targets.</p> <p> TYPE: <code>Iterable[Tuple[TensorType, TensorType]]</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>NestedLazyChunkSequence</code> <p>A lazy data structure representing the chunks of the resulting tensor</p> Source code in <code>src/pydvl/influence/influence_calculator.py</code> <pre><code>def influences(\n    self,\n    test_data_iterable: Iterable[Tuple[TensorType, TensorType]],\n    train_data_iterable: Iterable[Tuple[TensorType, TensorType]],\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; NestedLazyChunkSequence:\n    r\"\"\"\n    Compute approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n    f_{\\theta}(x_{\\text{test}})), \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n    f_{\\theta}(x_{\\text{test}})),\n    \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The computation is done block-wise for\n    the chunks of the provided\n    data iterables and aggregated into a single tensor in memory.\n\n    Args:\n        test_data_iterable: An iterable that returns tuples of tensors.\n            Each tuple consists of a pair of tensors (x, y), representing input data\n            and corresponding targets.\n        train_data_iterable: An iterable that returns tuples of tensors.\n            Each tuple consists of a pair of tensors (x, y), representing input data\n            and corresponding targets.\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        A lazy data structure representing the chunks of the resulting tensor\n\n    \"\"\"\n    nested_tensor_gen_factory = partial(\n        self._influences_gen,\n        test_data_iterable,\n        train_data_iterable,\n        mode,\n    )\n\n    try:\n        len_iterable = len(cast(Sized, test_data_iterable))\n    except Exception as e:\n        logger.debug(f\"Failed to retrieve len of test data iterable: {e}\")\n        len_iterable = None\n\n    return NestedLazyChunkSequence(\n        nested_tensor_gen_factory, len_outer_generator=len_iterable\n    )\n</code></pre>"},{"location":"api/pydvl/influence/influence_calculator/#pydvl.influence.influence_calculator.SequentialInfluenceCalculator.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: Iterable[TensorType],\n    train_data_iterable: Iterable[Tuple[TensorType, TensorType]],\n    mode: InfluenceMode = Up,\n) -&gt; NestedLazyChunkSequence\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}}, \\nabla_{\\theta} \\ell(y, f_{\\theta}(x))     \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}}, \\nabla_{x} \\nabla_{\\theta}     \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>Pre-computed iterable of tensors, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>Iterable[TensorType]</code> </p> <code>train_data_iterable</code> <p>An iterable that returns tuples of tensors. Each tuple consists of a pair of tensors (x, y), representing input data and corresponding targets.</p> <p> TYPE: <code>Iterable[Tuple[TensorType, TensorType]]</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>NestedLazyChunkSequence</code> <p>A lazy data structure representing the chunks of the resulting tensor</p> Source code in <code>src/pydvl/influence/influence_calculator.py</code> <pre><code>def influences_from_factors(\n    self,\n    z_test_factors: Iterable[TensorType],\n    train_data_iterable: Iterable[Tuple[TensorType, TensorType]],\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; NestedLazyChunkSequence:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}}, \\nabla_{\\theta} \\ell(y, f_{\\theta}(x))\n        \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}}, \\nabla_{x} \\nabla_{\\theta}\n        \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: Pre-computed iterable of tensors, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n                f_{\\theta}(x_{\\text{test}}))$\n        train_data_iterable: An iterable that returns tuples of tensors.\n            Each tuple consists of a pair of tensors (x, y), representing input data\n            and corresponding targets.\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n      A lazy data structure representing the chunks of the resulting tensor\n\n    \"\"\"\n    nested_tensor_gen = partial(\n        self._influences_from_factors_gen,\n        z_test_factors,\n        train_data_iterable,\n        mode,\n    )\n\n    try:\n        len_iterable = len(cast(Sized, z_test_factors))\n    except Exception as e:\n        logger.debug(f\"Failed to retrieve len of factors iterable: {e}\")\n        len_iterable = None\n\n    return NestedLazyChunkSequence(\n        nested_tensor_gen, len_outer_generator=len_iterable\n    )\n</code></pre>"},{"location":"api/pydvl/influence/types/","title":"Types","text":""},{"location":"api/pydvl/influence/types/#pydvl.influence.types","title":"pydvl.influence.types","text":"<p>This module offers a set of generic types, which can be used to build modular and flexible components for influence computation for different tensor frameworks.</p> <p>Key components include:</p> <ol> <li> <p>GradientProvider: A generic     abstract base class designed to provide methods for computing per-sample     gradients and other related computations for given data batches.</p> </li> <li> <p>BilinearForm: A generic abstract base class     for representing bilinear forms for computing inner products involving gradients.</p> </li> <li> <p>Operator: A generic abstract base class for     operators that can apply transformations to vectors and matrices and can be     represented as bilinear forms.</p> </li> <li> <p>OperatorGradientComposition: A     generic abstract composition class that integrates an operator with a gradient     provider to compute interactions between batches of data.</p> </li> <li> <p>BlockMapper: A generic abstract base class     for mapping operations across multiple compositional blocks, given by objects     of type     OperatorGradientComposition,     and aggregating the results.</p> </li> </ol> <p>To see the usage of these types, see the implementation ComposableInfluence . Using these components allows the straightforward implementation of various combinations of approximations of inverse Hessian applications (or Gauss-Newton approximations), different blocking strategies (e.g. layer-wise or block-wise) and different ways to compute gradients.</p> <p>For the usage with a specific tensor framework, these types must be subclassed. An example for torch is provided in the module pydvl.influence.torch.base and the base class TorchComposableInfluence.</p>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.Batch","title":"Batch  <code>dataclass</code>","text":"<pre><code>Batch(x: TensorType, y: TensorType)\n</code></pre> <p>               Bases: <code>Generic[TensorType]</code></p> <p>Represents a batch of data containing features and labels.</p> ATTRIBUTE DESCRIPTION <code>x</code> <p>Represents the input features of the batch.</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>Represents the labels or targets associated with the input features.</p> <p> TYPE: <code>TensorType</code> </p>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BilinearForm","title":"BilinearForm","text":"<p>               Bases: <code>Generic[TensorType, BatchType, GradientProviderType]</code>, <code>ABC</code></p> <p>Abstract base class for bilinear forms, which facilitates the computation of inner products involving gradients of batches of data.</p>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BilinearForm.grads_inner_prod","title":"grads_inner_prod","text":"<pre><code>grads_inner_prod(\n    left: BatchType,\n    right: Optional[BatchType],\n    gradient_provider: GradientProviderType,\n) -&gt; TensorType\n</code></pre> <p>Computes the gradient inner product of two batches of data, i.e.</p> \\[ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}), \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}\\] <p>where \\(\\nabla_{\\omega}\\ell(\\omega, \\cdot, \\cdot)\\) is represented by the <code>gradient_provider</code> and the expression must be understood sample-wise.</p> PARAMETER DESCRIPTION <code>left</code> <p>The first batch for gradient and inner product computation</p> <p> TYPE: <code>BatchType</code> </p> <code>right</code> <p>The second batch for gradient and inner product computation, optional; if not provided, the inner product will use the gradient computed for <code>left</code> for both arguments.</p> <p> TYPE: <code>Optional[BatchType]</code> </p> <code>gradient_provider</code> <p>The gradient provider to compute the gradients.</p> <p> TYPE: <code>GradientProviderType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the inner products of the per-sample gradients</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def grads_inner_prod(\n    self,\n    left: BatchType,\n    right: Optional[BatchType],\n    gradient_provider: GradientProviderType,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the gradient inner product of two batches of data, i.e.\n\n    $$ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}),\n    \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}$$\n\n    where $\\nabla_{\\omega}\\ell(\\omega, \\cdot, \\cdot)$ is represented by the\n    `gradient_provider` and the expression must be understood sample-wise.\n\n    Args:\n        left: The first batch for gradient and inner product computation\n        right: The second batch for gradient and inner product computation,\n            optional; if not provided, the inner product will use the gradient\n            computed for `left` for both arguments.\n        gradient_provider: The gradient provider to compute the gradients.\n\n    Returns:\n        A tensor representing the inner products of the per-sample gradients\n    \"\"\"\n    left_grad = gradient_provider.flat_grads(left)\n    if right is None:\n        right_grad = left_grad\n    else:\n        right_grad = gradient_provider.flat_grads(right)\n    return self.inner_prod(left_grad, right_grad)\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BilinearForm.inner_prod","title":"inner_prod  <code>abstractmethod</code>","text":"<pre><code>inner_prod(left: TensorType, right: Optional[TensorType]) -&gt; TensorType\n</code></pre> <p>Computes the inner product of two vectors, i.e.</p> \\[ \\langle x, y \\rangle_{B}\\] <p>if we denote the bilinear-form by \\(\\langle \\cdot, \\cdot \\rangle_{B}\\). The implementations must take care of according vectorization to make it applicable to the case, where <code>left</code> and <code>right</code> are not one-dimensional. In this case, the trailing dimension of the <code>left</code> and <code>right</code> tensors are considered for the computation of the inner product. For example, if <code>left</code> is a tensor of shape \\((N, D)\\) and, <code>right</code> is of shape \\((M,..., D)\\), then the result is of shape \\((N, M, ...)\\).</p> PARAMETER DESCRIPTION <code>left</code> <p>The first tensor in the inner product computation.</p> <p> TYPE: <code>TensorType</code> </p> <code>right</code> <p>The second tensor, optional; if not provided, the inner product will use <code>left</code> tensor for both arguments.</p> <p> TYPE: <code>Optional[TensorType]</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the inner product.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>@abstractmethod\ndef inner_prod(self, left: TensorType, right: Optional[TensorType]) -&gt; TensorType:\n    r\"\"\"\n    Computes the inner product of two vectors, i.e.\n\n    $$ \\langle x, y \\rangle_{B}$$\n\n    if we denote the bilinear-form by $\\langle \\cdot, \\cdot \\rangle_{B}$.\n    The implementations must take care of according vectorization to make\n    it applicable to the case, where `left` and `right` are not one-dimensional.\n    In this case, the trailing dimension of the `left` and `right` tensors are\n    considered for the computation of the inner product. For example,\n    if `left` is a tensor of shape $(N, D)$ and, `right` is of shape $(M,..., D)$,\n    then the result is of shape $(N, M, ...)$.\n\n    Args:\n        left: The first tensor in the inner product computation.\n        right: The second tensor, optional; if not provided, the inner product will\n            use `left` tensor for both arguments.\n\n    Returns:\n        A tensor representing the inner product.\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BilinearForm.mixed_grads_inner_prod","title":"mixed_grads_inner_prod","text":"<pre><code>mixed_grads_inner_prod(\n    left: BatchType,\n    right: Optional[BatchType],\n    gradient_provider: GradientProviderType,\n) -&gt; TensorType\n</code></pre> <p>Computes the mixed gradient inner product of two batches of data, i.e.</p> \\[ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}), \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}\\] <p>where \\(\\nabla_{\\omega}\\ell(\\omega, \\cdot)\\) and \\(\\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\cdot)\\) are represented by the <code>gradient_provider</code>. The expression must be understood sample-wise.</p> PARAMETER DESCRIPTION <code>left</code> <p>The first batch for gradient and inner product computation</p> <p> TYPE: <code>BatchType</code> </p> <code>right</code> <p>The second batch for gradient and inner product computation</p> <p> TYPE: <code>Optional[BatchType]</code> </p> <code>gradient_provider</code> <p>The gradient provider to compute the gradients.</p> <p> TYPE: <code>GradientProviderType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the inner products of the mixed per-sample gradients</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def mixed_grads_inner_prod(\n    self,\n    left: BatchType,\n    right: Optional[BatchType],\n    gradient_provider: GradientProviderType,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the mixed gradient inner product of two batches of data, i.e.\n\n    $$ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}),\n    \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y})\n    \\rangle_{B}$$\n\n    where $\\nabla_{\\omega}\\ell(\\omega, \\cdot)$ and\n    $\\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\cdot)$ are represented by the\n    `gradient_provider`. The expression must be understood sample-wise.\n\n    Args:\n        left: The first batch for gradient and inner product computation\n        right: The second batch for gradient and inner product computation\n        gradient_provider: The gradient provider to compute the gradients.\n\n    Returns:\n        A tensor representing the inner products of the mixed per-sample gradients\n    \"\"\"\n    left_grad = gradient_provider.flat_grads(left)\n    if right is None:\n        right = left\n    right_mixed_grad = gradient_provider.flat_mixed_grads(right)\n    return self.inner_prod(left_grad, right_mixed_grad)\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BlockMapper","title":"BlockMapper","text":"<pre><code>BlockMapper(\n    composable_block_dict: OrderedDict[str, OperatorGradientCompositionType]\n)\n</code></pre> <p>               Bases: <code>Generic[TensorType, BatchType, OperatorGradientCompositionType]</code>, <code>ABC</code></p> <p>Abstract base class for mapping operations across multiple compositional blocks.</p> <p>This class takes a dictionary of compositional blocks and applies their methods to batches or tensors, and aggregates the results.</p> ATTRIBUTE DESCRIPTION <code>composable_block_dict</code> <p>A dictionary mapping string identifiers to composable blocks which define operations like transformations and interactions.</p> <p> </p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def __init__(\n    self, composable_block_dict: OrderedDict[str, OperatorGradientCompositionType]\n):\n    self.composable_block_dict = composable_block_dict\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BlockMapper._split_to_blocks","title":"_split_to_blocks  <code>abstractmethod</code>","text":"<pre><code>_split_to_blocks(z: TensorType, dim: int = -1) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Must be implemented in a way to preserve the ordering defined by the <code>composable_block_dict</code> attribute</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>@abstractmethod\ndef _split_to_blocks(\n    self, z: TensorType, dim: int = -1\n) -&gt; OrderedDict[str, TensorType]:\n    \"\"\"Must be implemented in a way to preserve the ordering defined by the\n    `composable_block_dict` attribute\"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BlockMapper.generate_interactions","title":"generate_interactions","text":"<pre><code>generate_interactions(\n    left_batch: BatchType, right_batch: Optional[BatchType], mode: InfluenceMode\n) -&gt; Generator[TensorType, None, None]\n</code></pre> <p>Generator that yields gradient interactions between two batches, processed by each block based on a mode.</p> PARAMETER DESCRIPTION <code>left_batch</code> <p>The left batch for interaction computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>right_batch</code> <p>The right batch for interaction computation.</p> <p> TYPE: <code>Optional[BatchType]</code> </p> <code>mode</code> <p>The mode determining the type of interactions.</p> <p> TYPE: <code>InfluenceMode</code> </p> YIELDS DESCRIPTION <code>TensorType</code> <p>Gradient interactions for each block.</p> <p> TYPE:: <code>TensorType</code> </p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def generate_interactions(\n    self,\n    left_batch: BatchType,\n    right_batch: Optional[BatchType],\n    mode: InfluenceMode,\n) -&gt; Generator[TensorType, None, None]:\n    \"\"\"\n    Generator that yields gradient interactions between two batches, processed by\n    each block based on a mode.\n\n    Args:\n        left_batch: The left batch for interaction computation.\n        right_batch: The right batch for interaction computation.\n        mode: The mode determining the type of interactions.\n\n    Yields:\n        TensorType: Gradient interactions for each block.\n    \"\"\"\n    for comp_block in self.composable_block_dict.values():\n        yield comp_block.interactions(left_batch, right_batch, mode)\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BlockMapper.generate_interactions_from_transformed_grads","title":"generate_interactions_from_transformed_grads","text":"<pre><code>generate_interactions_from_transformed_grads(\n    left_factors: Union[TensorType, OrderedDict[str, TensorType]],\n    right_batch: BatchType,\n    mode: InfluenceMode,\n) -&gt; Generator[TensorType, None, None]\n</code></pre> <p>Generator that yields interactions computed from pre-computed factors and a right batch, processed by each block based on a mode.</p> PARAMETER DESCRIPTION <code>left_factors</code> <p>Pre-computed factors as a tensor or an ordered dictionary of tensors by block.</p> <p> TYPE: <code>Union[TensorType, OrderedDict[str, TensorType]]</code> </p> <code>right_batch</code> <p>The right batch for interaction computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>mode</code> <p>The mode determining the type of interactions.</p> <p> TYPE: <code>InfluenceMode</code> </p> YIELDS DESCRIPTION <code>TensorType</code> <p>Interactions for each block.</p> <p> TYPE:: <code>TensorType</code> </p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def generate_interactions_from_transformed_grads(\n    self,\n    left_factors: Union[TensorType, OrderedDict[str, TensorType]],\n    right_batch: BatchType,\n    mode: InfluenceMode,\n) -&gt; Generator[TensorType, None, None]:\n    \"\"\"\n    Generator that yields interactions computed from pre-computed factors and a\n    right batch, processed by each block based on a mode.\n\n    Args:\n        left_factors: Pre-computed factors as a tensor or an ordered dictionary of\n            tensors by block.\n        right_batch: The right batch for interaction computation.\n        mode: The mode determining the type of interactions.\n\n    Yields:\n        TensorType: Interactions for each block.\n    \"\"\"\n    if not isinstance(left_factors, dict):\n        left_factors_dict = self._split_to_blocks(left_factors)\n    else:\n        left_factors_dict = cast(OrderedDict[str, TensorType], left_factors)\n    for k, comp_block in self.composable_block_dict.items():\n        yield comp_block.interactions_from_transformed_grads(\n            left_factors_dict[k], right_batch, mode\n        )\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BlockMapper.generate_transformed_grads","title":"generate_transformed_grads","text":"<pre><code>generate_transformed_grads(\n    batch: BatchType,\n) -&gt; Generator[TensorType, None, None]\n</code></pre> <p>Generator that yields transformed gradients for a given batch, processed by each block.</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data for which to generate transformed gradients.</p> <p> TYPE: <code>BatchType</code> </p> YIELDS DESCRIPTION <code>TensorType</code> <p>Transformed gradients for each block.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def generate_transformed_grads(\n    self, batch: BatchType\n) -&gt; Generator[TensorType, None, None]:\n    \"\"\"\n    Generator that yields transformed gradients for a given batch,\n    processed by each block.\n\n    Args:\n        batch: The batch of data for which to generate transformed gradients.\n\n    Yields:\n        Transformed gradients for each block.\n    \"\"\"\n    for comp_block in self.composable_block_dict.values():\n        yield comp_block.transformed_grads(batch)\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BlockMapper.interactions","title":"interactions","text":"<pre><code>interactions(\n    left_batch: BatchType, right_batch: BatchType, mode: InfluenceMode\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Computes interactions between two batches, aggregated by block, based on a specified mode.</p> PARAMETER DESCRIPTION <code>left_batch</code> <p>The left batch for interaction computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>right_batch</code> <p>The right batch for interaction computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>mode</code> <p>The mode determining the type of interactions.</p> <p> TYPE: <code>InfluenceMode</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>An ordered dictionary of gradient interactions by block.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def interactions(\n    self, left_batch: BatchType, right_batch: BatchType, mode: InfluenceMode\n) -&gt; OrderedDict[str, TensorType]:\n    \"\"\"\n    Computes interactions between two batches, aggregated by block,\n    based on a specified mode.\n\n    Args:\n        left_batch: The left batch for interaction computation.\n        right_batch: The right batch for interaction computation.\n        mode: The mode determining the type of interactions.\n\n    Returns:\n        An ordered dictionary of gradient interactions by block.\n    \"\"\"\n    tensor_gen = self.generate_interactions(left_batch, right_batch, mode)\n    return self._to_ordered_dict(tensor_gen)\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BlockMapper.interactions_from_transformed_grads","title":"interactions_from_transformed_grads","text":"<pre><code>interactions_from_transformed_grads(\n    left_factors: OrderedDict[str, TensorType],\n    right_batch: BatchType,\n    mode: InfluenceMode,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Computes interactions from transformed gradients and a right batch, aggregated by block and based on a mode.</p> PARAMETER DESCRIPTION <code>left_factors</code> <p>Pre-computed factors as a tensor or an ordered dictionary of tensors by block. If the input is a tensor, it is split into blocks according to the ordering in the <code>composable_block_dict</code> attribute.</p> <p> TYPE: <code>OrderedDict[str, TensorType]</code> </p> <code>right_batch</code> <p>The right batch for interaction computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>mode</code> <p>The mode determining the type of interactions.</p> <p> TYPE: <code>InfluenceMode</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>An ordered dictionary of interactions from transformed gradients by block.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def interactions_from_transformed_grads(\n    self,\n    left_factors: OrderedDict[str, TensorType],\n    right_batch: BatchType,\n    mode: InfluenceMode,\n) -&gt; OrderedDict[str, TensorType]:\n    \"\"\"\n    Computes interactions from transformed gradients and a right batch,\n    aggregated by block and based on a mode.\n\n    Args:\n        left_factors: Pre-computed factors as a tensor or an ordered dictionary of\n            tensors by block. If the input is a tensor, it is split into blocks\n            according to the ordering in the `composable_block_dict` attribute.\n        right_batch: The right batch for interaction computation.\n        mode: The mode determining the type of interactions.\n\n    Returns:\n        An ordered dictionary of interactions from transformed gradients by block.\n    \"\"\"\n    tensor_gen = self.generate_interactions_from_transformed_grads(\n        left_factors, right_batch, mode\n    )\n    return self._to_ordered_dict(tensor_gen)\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.BlockMapper.transformed_grads","title":"transformed_grads","text":"<pre><code>transformed_grads(batch: BatchType) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Computes and returns the transformed gradients for a batch in dictionary with the keys defined by the block names.</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data for which to compute transformed gradients.</p> <p> TYPE: <code>BatchType</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>An ordered dictionary of transformed gradients by block.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def transformed_grads(\n    self,\n    batch: BatchType,\n) -&gt; OrderedDict[str, TensorType]:\n    \"\"\"\n    Computes and returns the transformed gradients for a batch in dictionary\n    with the keys defined by the block names.\n\n    Args:\n        batch: The batch of data for which to compute transformed gradients.\n\n    Returns:\n        An ordered dictionary of transformed gradients by block.\n    \"\"\"\n    tensor_gen = self.generate_transformed_grads(batch)\n    return self._to_ordered_dict(tensor_gen)\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.GradientProvider","title":"GradientProvider","text":"<p>               Bases: <code>Generic[BatchType, TensorType]</code>, <code>ABC</code></p> <p>Provides an interface for calculating per-sample gradients and other related computations for a given batch of data.</p> <p>This class must be subclassed with implementations for its abstract methods tailored to specific gradient computation needs, e.g. using an autograd engine for a model loss function. Consider a function</p> \\[ \\ell: \\mathbb{R}^{d_1} \\times \\mathbb{R}^{d_2} \\times \\mathbb{R}^{n} \\times     \\mathbb{R}^{n}, \\quad \\ell(\\omega_1, \\omega_2, x, y) =     \\operatorname{loss}(f(\\omega_1, \\omega_2; x), y) \\] <p>e.g. a two layer neural network \\(f\\) with a loss function, then this object should compute the expressions:</p> \\[ \\nabla_{\\omega_{i}}\\ell(\\omega_1, \\omega_2, x, y), \\nabla_{\\omega_{i}}\\nabla_{x}\\ell(\\omega_1, \\omega_2, x, y), \\nabla_{\\omega}\\ell(\\omega_1, \\omega_2, x, y) \\cdot v\\]"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.GradientProvider.flat_grads","title":"flat_grads  <code>abstractmethod</code>","text":"<pre><code>flat_grads(batch: BatchType) -&gt; TensorType\n</code></pre> <p>Computes and returns the flat per-sample gradients for the provided batch. Given the example in the class docstring, this means</p> \\[ (\\nabla_{\\omega_{1}}\\ell(\\omega_1, \\omega_2,     \\text{batch.x}, \\text{batch.y}),     \\nabla_{\\omega_{2}}\\ell(\\omega_1, \\omega_2,     \\text{batch.x}, \\text{batch.y}))\\] <p>where the first dimension of the resulting tensor is always considered to be the batch dimension, so the shape of the resulting tensor is \\((N, d_1+d_2)\\), where \\(N\\) is the number of samples in the batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data for which to compute the gradients.</p> <p> TYPE: <code>BatchType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor containing the flat gradients computed per sample.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>@abstractmethod\ndef flat_grads(self, batch: BatchType) -&gt; TensorType:\n    r\"\"\"\n    Computes and returns the flat per-sample gradients for the provided batch.\n    Given the example in the class docstring, this means\n\n    $$ (\\nabla_{\\omega_{1}}\\ell(\\omega_1, \\omega_2,\n        \\text{batch.x}, \\text{batch.y}),\n        \\nabla_{\\omega_{2}}\\ell(\\omega_1, \\omega_2,\n        \\text{batch.x}, \\text{batch.y}))$$\n\n    where the first dimension of the resulting tensor is always considered to be\n    the batch dimension, so the shape of the resulting tensor is $(N, d_1+d_2)$,\n    where $N$ is the number of samples in the batch.\n\n    Args:\n        batch: The batch of data for which to compute the gradients.\n\n    Returns:\n        A tensor containing the flat gradients computed per sample.\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.GradientProvider.flat_mixed_grads","title":"flat_mixed_grads  <code>abstractmethod</code>","text":"<pre><code>flat_mixed_grads(batch: BatchType) -&gt; TensorType\n</code></pre> <p>Computes and returns the flat per-sample mixed gradients for the provided batch. Given the example in the class docstring, this means</p> \\[ (\\nabla_{\\omega_1}\\nabla_{x}\\ell(\\omega_1,     \\omega_2, \\text{batch.x}, \\text{batch.y}),     \\nabla_{\\omega_1}\\nabla_{x}\\ell(\\omega_1,     \\omega_2, \\text{batch.x}, \\text{batch.y} ))\\] <p>where the first dimension of the resulting tensor is always considered to be the batch dimension and the last to be the non-batch input related derivatives. So the shape of the resulting tensor is \\((N, n, d_1 + d_2)\\), where \\(N\\) is the number of samples in the batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data for which to compute the flat mixed gradients.</p> <p> TYPE: <code>BatchType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor containing the flat mixed gradients computed per sample.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>@abstractmethod\ndef flat_mixed_grads(self, batch: BatchType) -&gt; TensorType:\n    r\"\"\"\n    Computes and returns the flat per-sample mixed gradients for the provided batch.\n    Given the example in the class docstring, this means\n\n    $$ (\\nabla_{\\omega_1}\\nabla_{x}\\ell(\\omega_1,\n        \\omega_2, \\text{batch.x}, \\text{batch.y}),\n        \\nabla_{\\omega_1}\\nabla_{x}\\ell(\\omega_1,\n        \\omega_2, \\text{batch.x}, \\text{batch.y} ))$$\n\n    where the first dimension of the resulting tensor is always considered to be\n    the batch dimension and the last to be the non-batch input related derivatives.\n    So the shape of the resulting tensor is $(N, n, d_1 + d_2)$,\n    where $N$ is the number of samples in the batch.\n\n    Args:\n        batch: The batch of data for which to compute the flat mixed gradients.\n\n    Returns:\n        A tensor containing the flat mixed gradients computed per sample.\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.GradientProvider.jacobian_prod","title":"jacobian_prod  <code>abstractmethod</code>","text":"<pre><code>jacobian_prod(batch: BatchType, g: TensorType) -&gt; TensorType\n</code></pre> <p>Computes the matrix-Jacobian product for the provided batch and input tensor. Given the example in the class docstring, this means</p> \\[ (\\nabla_{\\omega_{1}}\\ell(\\omega_1, \\omega_2,     \\text{batch.x}, \\text{batch.y}),     \\nabla_{\\omega_{2}}\\ell(\\omega_1, \\omega_2,     \\text{batch.x}, \\text{batch.y})) \\cdot g^T\\] <p>where g must be a tensor of shape \\((K, d_1+d_2)\\), so the resulting tensor is of shape \\((N, K)\\).</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data for which to compute the Jacobian.</p> <p> TYPE: <code>BatchType</code> </p> <code>g</code> <p>The tensor to be used in the matrix-Jacobian product calculation.</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>The resulting tensor from the matrix-Jacobian product computation.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>@abstractmethod\ndef jacobian_prod(\n    self,\n    batch: BatchType,\n    g: TensorType,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the matrix-Jacobian product for the provided batch and input tensor.\n    Given the example in the class docstring, this means\n\n    $$ (\\nabla_{\\omega_{1}}\\ell(\\omega_1, \\omega_2,\n        \\text{batch.x}, \\text{batch.y}),\n        \\nabla_{\\omega_{2}}\\ell(\\omega_1, \\omega_2,\n        \\text{batch.x}, \\text{batch.y})) \\cdot g^T$$\n\n    where g must be a tensor of shape $(K, d_1+d_2)$, so the resulting tensor\n    is of shape $(N, K)$.\n\n    Args:\n        batch: The batch of data for which to compute the Jacobian.\n        g: The tensor to be used in the matrix-Jacobian product\n            calculation.\n\n    Returns:\n        The resulting tensor from the matrix-Jacobian product computation.\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.InfluenceMode","title":"InfluenceMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum representation for the types of influence.</p> ATTRIBUTE DESCRIPTION <code>Up</code> <p>Approximating the influence of a point</p> <p> </p> <code>Perturbation</code> <p>Perturbation definition of the influence score</p> <p> </p>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.Operator","title":"Operator","text":"<p>               Bases: <code>Generic[TensorType, BilinearFormType]</code>, <code>ABC</code></p> <p>Abstract base class for operators, capable of applying transformations to vectors and matrices, and can be represented as a bilinear form.</p>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.Operator.input_size","title":"input_size  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>input_size: int\n</code></pre> <p>Abstract property to get the needed size for inputs to the operator instance</p> RETURNS DESCRIPTION <code>int</code> <p>An integer representing the input size.</p>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.Operator._apply","title":"_apply  <code>abstractmethod</code>","text":"<pre><code>_apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor. Implement this to handle batched input.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>@abstractmethod\ndef _apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor. Implement this to handle\n    batched input.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.Operator._validate_tensor_input","title":"_validate_tensor_input  <code>abstractmethod</code>","text":"<pre><code>_validate_tensor_input(tensor: TensorType) -&gt; None\n</code></pre> <p>Validates the input tensor for the operator.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor to validate.</p> <p> TYPE: <code>TensorType</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the tensor is invalid for the operator.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>@abstractmethod\ndef _validate_tensor_input(self, tensor: TensorType) -&gt; None:\n    \"\"\"\n    Validates the input tensor for the operator.\n\n    Args:\n        tensor: A tensor to validate.\n\n    Raises:\n        ValueError: If the tensor is invalid for the operator.\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.Operator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.Operator.as_bilinear_form","title":"as_bilinear_form  <code>abstractmethod</code>","text":"<pre><code>as_bilinear_form() -&gt; BilinearFormType\n</code></pre> <p>Represents the operator as a bilinear form, i.e. the weighted inner product</p> \\[ \\langle \\operatorname{Op}(x), y \\rangle\\] RETURNS DESCRIPTION <code>BilinearFormType</code> <p>An instance of type BilinearForm representing this operator.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>@abstractmethod\ndef as_bilinear_form(self) -&gt; BilinearFormType:\n    r\"\"\"\n    Represents the operator as a bilinear form, i.e. the weighted inner product\n\n    $$ \\langle \\operatorname{Op}(x), y \\rangle$$\n\n    Returns:\n        An instance of type [BilinearForm][pydvl.influence.types.BilinearForm]\n            representing this operator.\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.OperatorGradientComposition","title":"OperatorGradientComposition","text":"<pre><code>OperatorGradientComposition(op: OperatorType, gp: GradientProviderType)\n</code></pre> <p>               Bases: <code>Generic[TensorType, BatchType, OperatorType, GradientProviderType]</code></p> <p>Generic base class representing a composable block that integrates an operator and a gradient provider to compute interactions between batches of data.</p> <p>This block is designed to be flexible, handling different computational modes via an abstract operator and gradient provider.</p> ATTRIBUTE DESCRIPTION <code>op</code> <p>The operator used for transformations and influence computations.</p> <p> </p> <code>gp</code> <p>The gradient provider used for obtaining necessary gradients.</p> <p> </p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def __init__(self, op: OperatorType, gp: GradientProviderType):\n    self.gp = gp\n    self.op = op\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.OperatorGradientComposition._tensor_inner_product","title":"_tensor_inner_product  <code>abstractmethod</code>","text":"<pre><code>_tensor_inner_product(left: TensorType, right: TensorType) -&gt; TensorType\n</code></pre> <p>Implement this method in a way such that the aggregation of the tensors is represented by the Einstein summation convention ia,j...a -&gt; ij...</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>@abstractmethod\ndef _tensor_inner_product(self, left: TensorType, right: TensorType) -&gt; TensorType:\n    \"\"\"Implement this method in a way such that the aggregation of the tensors\n    is represented by the Einstein summation convention ia,j...a -&gt; ij...\"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.OperatorGradientComposition.interactions","title":"interactions","text":"<pre><code>interactions(\n    left_batch: BatchType, right_batch: Optional[BatchType], mode: InfluenceMode\n) -&gt; TensorType\n</code></pre> <p>Computes the interaction between the gradients on two batches of data based on the specified mode weighted by the operator action, i.e.</p> \\[ \\langle \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y})), \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle\\] <p>for the case <code>InfluenceMode.Up</code> and</p> \\[ \\langle \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y})), \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle \\] <p>for the case <code>InfluenceMode.Perturbation</code>.</p> PARAMETER DESCRIPTION <code>left_batch</code> <p>The left data batch for gradient computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>right_batch</code> <p>The right data batch for gradient computation.</p> <p> TYPE: <code>Optional[BatchType]</code> </p> <code>mode</code> <p>An instance of InfluenceMode determining the type of influence computation.</p> <p> TYPE: <code>InfluenceMode</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>The result of the influence computation as dictated by the mode.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def interactions(\n    self,\n    left_batch: BatchType,\n    right_batch: Optional[BatchType],\n    mode: InfluenceMode,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the interaction between the gradients on two batches of data based on\n    the specified mode weighted by the operator action,\n    i.e.\n\n    $$ \\langle \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{left.x},\n    \\text{left.y})),\n    \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle$$\n\n    for the case `InfluenceMode.Up` and\n\n    $$ \\langle \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{left.x},\n    \\text{left.y})),\n    \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle $$\n\n    for the case `InfluenceMode.Perturbation`.\n\n    Args:\n        left_batch: The left data batch for gradient computation.\n        right_batch: The right data batch for gradient computation.\n        mode: An instance of InfluenceMode determining the type of influence\n            computation.\n\n    Returns:\n        The result of the influence computation as dictated by the mode.\n    \"\"\"\n    bilinear_form = self.op.as_bilinear_form()\n    if mode == InfluenceMode.Up:\n        return cast(\n            TensorType,\n            bilinear_form.grads_inner_prod(left_batch, right_batch, self.gp),\n        )\n    elif mode == InfluenceMode.Perturbation:\n        return cast(\n            TensorType,\n            bilinear_form.mixed_grads_inner_prod(left_batch, right_batch, self.gp),\n        )\n    else:\n        raise UnsupportedInfluenceModeException(mode)\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.OperatorGradientComposition.interactions_from_transformed_grads","title":"interactions_from_transformed_grads","text":"<pre><code>interactions_from_transformed_grads(\n    left_factors: TensorType, right_batch: BatchType, mode: InfluenceMode\n) -&gt; TensorType\n</code></pre> <p>Computes the interaction between the transformed gradients on two batches of data using pre-computed factors and a batch of data, based on the specified mode. This means</p> \\[ \\langle \\text{left_factors}, \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle\\] <p>for the case <code>InfluenceMode.Up</code> and</p> \\[ \\langle \\text{left_factors}, \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle \\] <p>for the case <code>InfluenceMode.Perturbation</code>.</p> PARAMETER DESCRIPTION <code>left_factors</code> <p>Pre-computed tensor factors from a left batch.</p> <p> TYPE: <code>TensorType</code> </p> <code>right_batch</code> <p>The right data batch for influence computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>mode</code> <p>An instance of InfluenceMode determining the type of influence computation.</p> <p> TYPE: <code>InfluenceMode</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>The result of the interaction computation using the provided factors and batch gradients.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def interactions_from_transformed_grads(\n    self, left_factors: TensorType, right_batch: BatchType, mode: InfluenceMode\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the interaction between the transformed gradients on two batches of\n    data using pre-computed factors and a batch of data,\n    based on the specified mode. This means\n\n    $$ \\langle \\text{left_factors},\n    \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle$$\n\n    for the case `InfluenceMode.Up` and\n\n    $$ \\langle \\text{left_factors},\n    \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle $$\n\n    for the case `InfluenceMode.Perturbation`.\n\n    Args:\n        left_factors: Pre-computed tensor factors from a left batch.\n        right_batch: The right data batch for influence computation.\n        mode: An instance of InfluenceMode determining the type of influence\n            computation.\n\n    Returns:\n        The result of the interaction computation using the provided factors and\n            batch gradients.\n    \"\"\"\n    if mode is InfluenceMode.Up:\n        right_grads = self.gp.flat_grads(right_batch)\n    else:\n        right_grads = self.gp.flat_mixed_grads(right_batch)\n    return self._tensor_inner_product(left_factors, right_grads)\n</code></pre>"},{"location":"api/pydvl/influence/types/#pydvl.influence.types.OperatorGradientComposition.transformed_grads","title":"transformed_grads","text":"<pre><code>transformed_grads(batch: BatchType) -&gt; TensorType\n</code></pre> <p>Computes the gradients of a data batch, transformed by the operator application , i.e. the expressions</p> \\[ \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{batch.x},     \\text{batch.y})) \\] PARAMETER DESCRIPTION <code>batch</code> <p>The data batch for gradient computation.</p> <p> TYPE: <code>BatchType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the application of the operator to the gradients.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def transformed_grads(self, batch: BatchType) -&gt; TensorType:\n    r\"\"\"\n    Computes the gradients of a data batch, transformed by the operator application\n    , i.e. the expressions\n\n    $$ \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{batch.x},\n        \\text{batch.y})) $$\n\n    Args:\n        batch: The data batch for gradient computation.\n\n    Returns:\n        A tensor representing the application of the operator to the gradients.\n\n    \"\"\"\n    grads = self.gp.flat_grads(batch)\n    return cast(TensorType, self.op.apply(grads))\n</code></pre>"},{"location":"api/pydvl/influence/torch/","title":"Torch","text":""},{"location":"api/pydvl/influence/torch/#pydvl.influence.torch","title":"pydvl.influence.torch","text":""},{"location":"api/pydvl/influence/torch/base/","title":"Base","text":""},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base","title":"pydvl.influence.torch.base","text":""},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchOperatorType","title":"TorchOperatorType  <code>module-attribute</code>","text":"<pre><code>TorchOperatorType = TypeVar('TorchOperatorType', bound=TensorOperator)\n</code></pre> <p>Type variable bound to TensorOperator.</p>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.DictBilinearForm","title":"DictBilinearForm","text":"<pre><code>DictBilinearForm(operator: TensorDictOperator)\n</code></pre> <p>               Bases: <code>OperatorBilinearForm</code></p> <p>Base class for bi-linear forms based on an instance of TorchOperator. This means it computes weighted inner products of the form:</p> \\[ \\langle \\operatorname{Op}(x), y \\rangle \\] Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def __init__(self, operator: TensorDictOperator):\n    super().__init__(operator)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.DictBilinearForm.grads_inner_prod","title":"grads_inner_prod","text":"<pre><code>grads_inner_prod(\n    left: TorchBatch,\n    right: Optional[TorchBatch],\n    gradient_provider: TorchGradientProvider,\n) -&gt; Tensor\n</code></pre> <p>Computes the gradient inner product of two batches of data, i.e.</p> \\[ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}), \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}\\] <p>where \\(\\nabla_{\\omega}\\ell(\\omega, \\cdot, \\cdot)\\) is represented by the <code>gradient_provider</code> and the expression must be understood sample-wise.</p> PARAMETER DESCRIPTION <code>left</code> <p>The first batch for gradient and inner product computation</p> <p> TYPE: <code>TorchBatch</code> </p> <code>right</code> <p>The second batch for gradient and inner product computation, optional; if not provided, the inner product will use the gradient computed for <code>left</code> for both arguments.</p> <p> TYPE: <code>Optional[TorchBatch]</code> </p> <code>gradient_provider</code> <p>The gradient provider to compute the gradients.</p> <p> TYPE: <code>TorchGradientProvider</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor representing the inner products of the per-sample gradients</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def grads_inner_prod(\n    self,\n    left: TorchBatch,\n    right: Optional[TorchBatch],\n    gradient_provider: TorchGradientProvider,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the gradient inner product of two batches of data, i.e.\n\n    $$ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}),\n    \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}$$\n\n    where $\\nabla_{\\omega}\\ell(\\omega, \\cdot, \\cdot)$ is represented by the\n    `gradient_provider` and the expression must be understood sample-wise.\n\n    Args:\n        left: The first batch for gradient and inner product computation\n        right: The second batch for gradient and inner product computation,\n            optional; if not provided, the inner product will use the gradient\n            computed for `left` for both arguments.\n        gradient_provider: The gradient provider to compute the gradients.\n\n    Returns:\n        A tensor representing the inner products of the per-sample gradients\n    \"\"\"\n    operator = cast(TensorDictOperator, self.operator)\n    left_grads = gradient_provider.grads(left)\n    if right is None:\n        right_grads = left_grads\n    else:\n        right_grads = gradient_provider.grads(right)\n\n    left_batch_size, right_batch_size = next(\n        (\n            (l.shape[0], r.shape[0])\n            for r, l in zip(left_grads.values(), right_grads.values())\n        )\n    )\n\n    if left_batch_size &lt;= right_batch_size:\n        left_grads = operator.apply_to_dict(left_grads)\n        tensor_pairs = zip(left_grads.values(), right_grads.values())\n    else:\n        right_grads = operator.apply_to_dict(right_grads)\n        tensor_pairs = zip(left_grads.values(), right_grads.values())\n\n    tensors_to_reduce = (\n        self._aggregate_grads(left, right) for left, right in tensor_pairs\n    )\n\n    return cast(torch.Tensor, sum(tensors_to_reduce))\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.DictBilinearForm.inner_prod","title":"inner_prod","text":"<pre><code>inner_prod(left: Tensor, right: Optional[Tensor]) -&gt; Tensor\n</code></pre> <p>Computes the weighted inner product of two vectors, i.e.</p> \\[ \\langle \\operatorname{Op}(\\text{left}), \\text{right} \\rangle \\] PARAMETER DESCRIPTION <code>left</code> <p>The first tensor in the inner product computation.</p> <p> TYPE: <code>Tensor</code> </p> <code>right</code> <p>The second tensor, optional; if not provided, the inner product will use <code>left</code> tensor for both arguments.</p> <p> TYPE: <code>Optional[Tensor]</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor representing the inner product.</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def inner_prod(\n    self, left: torch.Tensor, right: Optional[torch.Tensor]\n) -&gt; torch.Tensor:\n    r\"\"\"Computes the weighted inner product of two vectors, i.e.\n\n    $$ \\langle \\operatorname{Op}(\\text{left}), \\text{right} \\rangle $$\n\n    Args:\n        left: The first tensor in the inner product computation.\n        right: The second tensor, optional; if not provided, the inner product will\n            use `left` tensor for both arguments.\n\n    Returns:\n        A tensor representing the inner product.\n    \"\"\"\n    if right is None:\n        right = left\n    if left.shape[0] &lt;= right.shape[0]:\n        return self._inner_product(left, right)\n    return self._inner_product(right, left).T\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.DictBilinearForm.mixed_grads_inner_prod","title":"mixed_grads_inner_prod","text":"<pre><code>mixed_grads_inner_prod(\n    left: TorchBatch,\n    right: Optional[TorchBatch],\n    gradient_provider: TorchGradientProvider,\n) -&gt; Tensor\n</code></pre> <p>Computes the mixed gradient inner product of two batches of data, i.e.</p> \\[ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}), \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}\\] <p>where \\(\\nabla_{\\omega}\\ell(\\omega, \\cdot)\\) and \\(\\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\cdot)\\) are represented by the <code>gradient_provider</code>. The expression must be understood sample-wise.</p> PARAMETER DESCRIPTION <code>left</code> <p>The first batch for gradient and inner product computation</p> <p> TYPE: <code>TorchBatch</code> </p> <code>right</code> <p>The second batch for gradient and inner product computation</p> <p> TYPE: <code>Optional[TorchBatch]</code> </p> <code>gradient_provider</code> <p>The gradient provider to compute the gradients.</p> <p> TYPE: <code>TorchGradientProvider</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor representing the inner products of the mixed per-sample gradients</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def mixed_grads_inner_prod(\n    self,\n    left: TorchBatch,\n    right: Optional[TorchBatch],\n    gradient_provider: TorchGradientProvider,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the mixed gradient inner product of two batches of data, i.e.\n\n    $$ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}),\n    \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y})\n    \\rangle_{B}$$\n\n    where $\\nabla_{\\omega}\\ell(\\omega, \\cdot)$ and\n    $\\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\cdot)$ are represented by the\n    `gradient_provider`. The expression must be understood sample-wise.\n\n    Args:\n        left: The first batch for gradient and inner product computation\n        right: The second batch for gradient and inner product computation\n        gradient_provider: The gradient provider to compute the gradients.\n\n    Returns:\n        A tensor representing the inner products of the mixed per-sample gradients\n    \"\"\"\n    operator = cast(TensorDictOperator, self.operator)\n    if right is None:\n        right = left\n    right_grads = gradient_provider.mixed_grads(right)\n    left_grads = gradient_provider.grads(left)\n    left_grads = operator.apply_to_dict(left_grads)\n    left_grads_views = (t.reshape(t.shape[0], -1) for t in left_grads.values())\n    right_grads_views = (\n        t.reshape(*right.x.shape, -1) for t in right_grads.values()\n    )\n    tensor_pairs = zip(left_grads_views, right_grads_views)\n    tensors_to_reduce = (\n        self._aggregate_mixed_grads(left, right) for left, right in tensor_pairs\n    )\n    return cast(torch.Tensor, sum(tensors_to_reduce))\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.LowRankBilinearForm","title":"LowRankBilinearForm","text":"<pre><code>LowRankBilinearForm(operator: LowRankOperator)\n</code></pre> <p>               Bases: <code>OperatorBilinearForm</code></p> <p>Specialized bilinear form for operators of the type</p> \\[ \\operatorname{Op}(b) = V D^{-1}V^Tb.\\] <p>It computes the expressions</p> \\[ \\langle \\operatorname{Op}(\\nabla_{\\theta} \\ell(z, \\theta)),     \\nabla_{\\theta} \\ell(z^{\\prime}, \\theta) \\rangle =     \\langle V\\nabla_{\\theta} \\ell(z, \\theta),     D^{-1}V\\nabla_{\\theta} \\ell(z^{\\prime}, \\theta) \\rangle\\] <p>in an efficient way using torch.autograd functionality.</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def __init__(self, operator: LowRankOperator):\n    super().__init__(operator)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.LowRankBilinearForm.grads_inner_prod","title":"grads_inner_prod","text":"<pre><code>grads_inner_prod(\n    left: TorchBatch,\n    right: Optional[TorchBatch],\n    gradient_provider: TorchGradientProvider,\n) -&gt; Tensor\n</code></pre> <p>Computes the gradient inner product of two batches of data, i.e.</p> \\[ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}), \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}\\] <p>where \\(\\nabla_{\\omega}\\ell(\\omega, \\cdot, \\cdot)\\) is represented by the <code>gradient_provider</code> and the expression must be understood sample-wise.</p> PARAMETER DESCRIPTION <code>left</code> <p>The first batch for gradient and inner product computation</p> <p> TYPE: <code>TorchBatch</code> </p> <code>right</code> <p>The second batch for gradient and inner product computation, optional; if not provided, the inner product will use the gradient computed for <code>left</code> for both arguments.</p> <p> TYPE: <code>Optional[TorchBatch]</code> </p> <code>gradient_provider</code> <p>The gradient provider to compute the gradients.</p> <p> TYPE: <code>TorchGradientProvider</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor representing the inner products of the per-sample gradients</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def grads_inner_prod(\n    self,\n    left: TorchBatch,\n    right: Optional[TorchBatch],\n    gradient_provider: TorchGradientProvider,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the gradient inner product of two batches of data, i.e.\n\n    $$ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}),\n    \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}$$\n\n    where $\\nabla_{\\omega}\\ell(\\omega, \\cdot, \\cdot)$ is represented by the\n    `gradient_provider` and the expression must be understood sample-wise.\n\n    Args:\n        left: The first batch for gradient and inner product computation\n        right: The second batch for gradient and inner product computation,\n            optional; if not provided, the inner product will use the gradient\n            computed for `left` for both arguments.\n        gradient_provider: The gradient provider to compute the gradients.\n\n    Returns:\n        A tensor representing the inner products of the per-sample gradients\n    \"\"\"\n    op = cast(\"LowRankOperator\", self.operator)\n\n    if op.exact:\n        return super().grads_inner_prod(left, right, gradient_provider)\n\n    V = op.low_rank_representation.projections\n    D = op.low_rank_representation.eigen_vals.clone()\n    regularization = op.regularization\n\n    if regularization is not None:\n        D += regularization\n\n    V_left = gradient_provider.jacobian_prod(left, V.t())\n    D_inv = 1.0 / D\n\n    if right is None:\n        V_right = V_left\n    else:\n        V_right = gradient_provider.jacobian_prod(right, V.t())\n\n    V_right = V_right * D_inv.unsqueeze(-1)\n\n    return torch.einsum(\"ij, ik -&gt; jk\", V_left, V_right)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.LowRankBilinearForm.inner_prod","title":"inner_prod","text":"<pre><code>inner_prod(left: Tensor, right: Optional[Tensor]) -&gt; Tensor\n</code></pre> <p>Computes the weighted inner product of two vectors, i.e.</p> \\[ \\langle \\operatorname{Op}(\\text{left}), \\text{right} \\rangle \\] PARAMETER DESCRIPTION <code>left</code> <p>The first tensor in the inner product computation.</p> <p> TYPE: <code>Tensor</code> </p> <code>right</code> <p>The second tensor, optional; if not provided, the inner product will use <code>left</code> tensor for both arguments.</p> <p> TYPE: <code>Optional[Tensor]</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor representing the inner product.</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def inner_prod(\n    self, left: torch.Tensor, right: Optional[torch.Tensor]\n) -&gt; torch.Tensor:\n    r\"\"\"Computes the weighted inner product of two vectors, i.e.\n\n    $$ \\langle \\operatorname{Op}(\\text{left}), \\text{right} \\rangle $$\n\n    Args:\n        left: The first tensor in the inner product computation.\n        right: The second tensor, optional; if not provided, the inner product will\n            use `left` tensor for both arguments.\n\n    Returns:\n        A tensor representing the inner product.\n    \"\"\"\n    if right is None:\n        right = left\n    if left.shape[0] &lt;= right.shape[0]:\n        return self._inner_product(left, right)\n    return self._inner_product(right, left).T\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.LowRankBilinearForm.mixed_grads_inner_prod","title":"mixed_grads_inner_prod","text":"<pre><code>mixed_grads_inner_prod(\n    left: BatchType,\n    right: Optional[BatchType],\n    gradient_provider: GradientProviderType,\n) -&gt; TensorType\n</code></pre> <p>Computes the mixed gradient inner product of two batches of data, i.e.</p> \\[ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}), \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}\\] <p>where \\(\\nabla_{\\omega}\\ell(\\omega, \\cdot)\\) and \\(\\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\cdot)\\) are represented by the <code>gradient_provider</code>. The expression must be understood sample-wise.</p> PARAMETER DESCRIPTION <code>left</code> <p>The first batch for gradient and inner product computation</p> <p> TYPE: <code>BatchType</code> </p> <code>right</code> <p>The second batch for gradient and inner product computation</p> <p> TYPE: <code>Optional[BatchType]</code> </p> <code>gradient_provider</code> <p>The gradient provider to compute the gradients.</p> <p> TYPE: <code>GradientProviderType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the inner products of the mixed per-sample gradients</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def mixed_grads_inner_prod(\n    self,\n    left: BatchType,\n    right: Optional[BatchType],\n    gradient_provider: GradientProviderType,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the mixed gradient inner product of two batches of data, i.e.\n\n    $$ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}),\n    \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y})\n    \\rangle_{B}$$\n\n    where $\\nabla_{\\omega}\\ell(\\omega, \\cdot)$ and\n    $\\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\cdot)$ are represented by the\n    `gradient_provider`. The expression must be understood sample-wise.\n\n    Args:\n        left: The first batch for gradient and inner product computation\n        right: The second batch for gradient and inner product computation\n        gradient_provider: The gradient provider to compute the gradients.\n\n    Returns:\n        A tensor representing the inner products of the mixed per-sample gradients\n    \"\"\"\n    left_grad = gradient_provider.flat_grads(left)\n    if right is None:\n        right = left\n    right_mixed_grad = gradient_provider.flat_mixed_grads(right)\n    return self.inner_prod(left_grad, right_mixed_grad)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.OperatorBilinearForm","title":"OperatorBilinearForm","text":"<pre><code>OperatorBilinearForm(operator: TensorOperator)\n</code></pre> <p>               Bases: <code>BilinearForm[Tensor, TorchBatch, TorchGradientProvider]</code></p> <p>Base class for bi-linear forms based on an instance of TensorOperator. This means it computes weighted inner products of the form:</p> \\[ \\langle \\operatorname{Op}(x), y \\rangle \\] PARAMETER DESCRIPTION <code>operator</code> <p>The operator to compute the inner product with.</p> <p> TYPE: <code>TensorOperator</code> </p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def __init__(self, operator: TensorOperator):\n    self.operator = operator\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.OperatorBilinearForm.grads_inner_prod","title":"grads_inner_prod","text":"<pre><code>grads_inner_prod(\n    left: BatchType,\n    right: Optional[BatchType],\n    gradient_provider: GradientProviderType,\n) -&gt; TensorType\n</code></pre> <p>Computes the gradient inner product of two batches of data, i.e.</p> \\[ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}), \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}\\] <p>where \\(\\nabla_{\\omega}\\ell(\\omega, \\cdot, \\cdot)\\) is represented by the <code>gradient_provider</code> and the expression must be understood sample-wise.</p> PARAMETER DESCRIPTION <code>left</code> <p>The first batch for gradient and inner product computation</p> <p> TYPE: <code>BatchType</code> </p> <code>right</code> <p>The second batch for gradient and inner product computation, optional; if not provided, the inner product will use the gradient computed for <code>left</code> for both arguments.</p> <p> TYPE: <code>Optional[BatchType]</code> </p> <code>gradient_provider</code> <p>The gradient provider to compute the gradients.</p> <p> TYPE: <code>GradientProviderType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the inner products of the per-sample gradients</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def grads_inner_prod(\n    self,\n    left: BatchType,\n    right: Optional[BatchType],\n    gradient_provider: GradientProviderType,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the gradient inner product of two batches of data, i.e.\n\n    $$ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}),\n    \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}$$\n\n    where $\\nabla_{\\omega}\\ell(\\omega, \\cdot, \\cdot)$ is represented by the\n    `gradient_provider` and the expression must be understood sample-wise.\n\n    Args:\n        left: The first batch for gradient and inner product computation\n        right: The second batch for gradient and inner product computation,\n            optional; if not provided, the inner product will use the gradient\n            computed for `left` for both arguments.\n        gradient_provider: The gradient provider to compute the gradients.\n\n    Returns:\n        A tensor representing the inner products of the per-sample gradients\n    \"\"\"\n    left_grad = gradient_provider.flat_grads(left)\n    if right is None:\n        right_grad = left_grad\n    else:\n        right_grad = gradient_provider.flat_grads(right)\n    return self.inner_prod(left_grad, right_grad)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.OperatorBilinearForm.inner_prod","title":"inner_prod","text":"<pre><code>inner_prod(left: Tensor, right: Optional[Tensor]) -&gt; Tensor\n</code></pre> <p>Computes the weighted inner product of two vectors, i.e.</p> \\[ \\langle \\operatorname{Op}(\\text{left}), \\text{right} \\rangle \\] PARAMETER DESCRIPTION <code>left</code> <p>The first tensor in the inner product computation.</p> <p> TYPE: <code>Tensor</code> </p> <code>right</code> <p>The second tensor, optional; if not provided, the inner product will use <code>left</code> tensor for both arguments.</p> <p> TYPE: <code>Optional[Tensor]</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor representing the inner product.</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def inner_prod(\n    self, left: torch.Tensor, right: Optional[torch.Tensor]\n) -&gt; torch.Tensor:\n    r\"\"\"Computes the weighted inner product of two vectors, i.e.\n\n    $$ \\langle \\operatorname{Op}(\\text{left}), \\text{right} \\rangle $$\n\n    Args:\n        left: The first tensor in the inner product computation.\n        right: The second tensor, optional; if not provided, the inner product will\n            use `left` tensor for both arguments.\n\n    Returns:\n        A tensor representing the inner product.\n    \"\"\"\n    if right is None:\n        right = left\n    if left.shape[0] &lt;= right.shape[0]:\n        return self._inner_product(left, right)\n    return self._inner_product(right, left).T\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.OperatorBilinearForm.mixed_grads_inner_prod","title":"mixed_grads_inner_prod","text":"<pre><code>mixed_grads_inner_prod(\n    left: BatchType,\n    right: Optional[BatchType],\n    gradient_provider: GradientProviderType,\n) -&gt; TensorType\n</code></pre> <p>Computes the mixed gradient inner product of two batches of data, i.e.</p> \\[ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}), \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle_{B}\\] <p>where \\(\\nabla_{\\omega}\\ell(\\omega, \\cdot)\\) and \\(\\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\cdot)\\) are represented by the <code>gradient_provider</code>. The expression must be understood sample-wise.</p> PARAMETER DESCRIPTION <code>left</code> <p>The first batch for gradient and inner product computation</p> <p> TYPE: <code>BatchType</code> </p> <code>right</code> <p>The second batch for gradient and inner product computation</p> <p> TYPE: <code>Optional[BatchType]</code> </p> <code>gradient_provider</code> <p>The gradient provider to compute the gradients.</p> <p> TYPE: <code>GradientProviderType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the inner products of the mixed per-sample gradients</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def mixed_grads_inner_prod(\n    self,\n    left: BatchType,\n    right: Optional[BatchType],\n    gradient_provider: GradientProviderType,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the mixed gradient inner product of two batches of data, i.e.\n\n    $$ \\langle \\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y}),\n    \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y})\n    \\rangle_{B}$$\n\n    where $\\nabla_{\\omega}\\ell(\\omega, \\cdot)$ and\n    $\\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\cdot)$ are represented by the\n    `gradient_provider`. The expression must be understood sample-wise.\n\n    Args:\n        left: The first batch for gradient and inner product computation\n        right: The second batch for gradient and inner product computation\n        gradient_provider: The gradient provider to compute the gradients.\n\n    Returns:\n        A tensor representing the inner products of the mixed per-sample gradients\n    \"\"\"\n    left_grad = gradient_provider.flat_grads(left)\n    if right is None:\n        right = left\n    right_mixed_grad = gradient_provider.flat_mixed_grads(right)\n    return self.inner_prod(left_grad, right_mixed_grad)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorDictOperator","title":"TensorDictOperator","text":"<p>               Bases: <code>TensorOperator</code>, <code>ABC</code></p> <p>Abstract base class for operators that can be applied to instances of torch.Tensor and compatible dictionaries mapping strings to tensors. Input dictionaries must conform to the structure defined by the property <code>input_dict_structure</code>. Useful for operators involving autograd functionality to avoid intermediate flattening and concatenating of gradient inputs.</p>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorDictOperator.input_dict_structure","title":"input_dict_structure  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>input_dict_structure: Dict[str, Tuple[int, ...]]\n</code></pre> <p>Implement this to expose the expected structure of the input tensor dict, i.e. a dictionary of shapes (excluding the first batch dimension), in order to validate the input tensor dicts.</p>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorDictOperator.input_size","title":"input_size  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>input_size: int\n</code></pre> <p>Abstract property to get the needed size for inputs to the operator instance</p> RETURNS DESCRIPTION <code>int</code> <p>An integer representing the input size.</p>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorDictOperator._apply_to_mat","title":"_apply_to_mat","text":"<pre><code>_apply_to_mat(mat: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the operator to a matrix. Args:     mat: A matrix to apply the operator to. The last dimension is         assumed to be consistent to the operation, i.e. it must equal         to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A matrix of shape \\((N,      ext{input_size})\\), given the shape of mat is \\((N,    ext{input_size})\\)</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def _apply_to_mat(self, mat: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the operator to a matrix.\n    Args:\n        mat: A matrix to apply the operator to. The last dimension is\n            assumed to be consistent to the operation, i.e. it must equal\n            to the property `input_size`.\n\n    Returns:\n        A matrix of shape $(N, \\text{input_size})$, given the shape of mat is\n            $(N, \\text{input_size})$\n\n    \"\"\"\n    return torch.func.vmap(self._apply_to_vec, in_dims=0, randomness=\"same\")(mat)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorDictOperator._apply_to_vec","title":"_apply_to_vec  <code>abstractmethod</code>","text":"<pre><code>_apply_to_vec(vec: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the operator to a single vector. Args:     vec: A single vector consistent to the operator, i.e. it's length         must be equal to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A single vector after applying the batch operation</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>@abstractmethod\ndef _apply_to_vec(self, vec: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the operator to a single vector.\n    Args:\n        vec: A single vector consistent to the operator, i.e. it's length\n            must be equal to the property `input_size`.\n\n    Returns:\n        A single vector after applying the batch operation\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorDictOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorDictOperator.apply_to_dict","title":"apply_to_dict","text":"<pre><code>apply_to_dict(mat: Dict[str, Tensor]) -&gt; Dict[str, Tensor]\n</code></pre> <p>Applies the operator to a dictionary of tensors, compatible to the structure defined by the property <code>input_dict_structure</code>.</p> PARAMETER DESCRIPTION <code>mat</code> <p>dictionary of tensors, whose keys and shapes match the property <code>input_dict_structure</code>.</p> <p> TYPE: <code>Dict[str, Tensor]</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dictionary of tensors after applying the operator</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def apply_to_dict(self, mat: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Applies the operator to a dictionary of tensors, compatible to the structure\n    defined by the property `input_dict_structure`.\n\n    Args:\n        mat: dictionary of tensors, whose keys and shapes match the property\n            `input_dict_structure`.\n\n    Returns:\n        A dictionary of tensors after applying the operator\n    \"\"\"\n\n    if not self._validate_mat_dict(mat):\n        raise ValueError(\n            f\"Incompatible input structure, expected (excluding batch\"\n            f\"dimension): \\n {self.input_dict_structure}\"\n        )\n\n    return self._apply_to_dict(self._dict_to_device(mat))\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorOperator","title":"TensorOperator","text":"<p>               Bases: <code>Operator[Tensor, OperatorBilinearForm]</code>, <code>ABC</code></p> <p>Abstract base class for operators that can be applied to instances of torch.Tensor.</p>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorOperator.input_size","title":"input_size  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>input_size: int\n</code></pre> <p>Abstract property to get the needed size for inputs to the operator instance</p> RETURNS DESCRIPTION <code>int</code> <p>An integer representing the input size.</p>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorOperator._apply_to_mat","title":"_apply_to_mat","text":"<pre><code>_apply_to_mat(mat: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the operator to a matrix. Args:     mat: A matrix to apply the operator to. The last dimension is         assumed to be consistent to the operation, i.e. it must equal         to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A matrix of shape \\((N,      ext{input_size})\\), given the shape of mat is \\((N,    ext{input_size})\\)</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def _apply_to_mat(self, mat: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the operator to a matrix.\n    Args:\n        mat: A matrix to apply the operator to. The last dimension is\n            assumed to be consistent to the operation, i.e. it must equal\n            to the property `input_size`.\n\n    Returns:\n        A matrix of shape $(N, \\text{input_size})$, given the shape of mat is\n            $(N, \\text{input_size})$\n\n    \"\"\"\n    return torch.func.vmap(self._apply_to_vec, in_dims=0, randomness=\"same\")(mat)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorOperator._apply_to_vec","title":"_apply_to_vec  <code>abstractmethod</code>","text":"<pre><code>_apply_to_vec(vec: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the operator to a single vector. Args:     vec: A single vector consistent to the operator, i.e. it's length         must be equal to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A single vector after applying the batch operation</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>@abstractmethod\ndef _apply_to_vec(self, vec: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the operator to a single vector.\n    Args:\n        vec: A single vector consistent to the operator, i.e. it's length\n            must be equal to the property `input_size`.\n\n    Returns:\n        A single vector after applying the batch operation\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TensorOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchBatch","title":"TorchBatch  <code>dataclass</code>","text":"<pre><code>TorchBatch(x: Tensor, y: Tensor)\n</code></pre> <p>               Bases: <code>Batch</code></p> <p>A convenience class for handling batches of data. Validates the alignment of the first dimension (batch dimension) of the input and target tensor</p> ATTRIBUTE DESCRIPTION <code>x</code> <p>The input tensor that contains features or data points.</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>The target tensor that contains labels corresponding to the inputs.</p> <p> TYPE: <code>Tensor</code> </p>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchBlockMapper","title":"TorchBlockMapper","text":"<pre><code>TorchBlockMapper(\n    composable_block_dict: OrderedDict[str, TorchOperatorGradientComposition]\n)\n</code></pre> <p>               Bases: <code>BlockMapper[Tensor, TorchBatch, TorchOperatorGradientComposition[TorchOperatorType]]</code></p> <p>Class for mapping operations across multiple compositional blocks represented by instances of TorchOperatorGradientComposition.</p> <p>This class takes a dictionary of compositional blocks and applies their methods to batches or tensors, and aggregates the results.</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def __init__(\n    self, composable_block_dict: OrderedDict[str, TorchOperatorGradientComposition]\n):\n    super().__init__(composable_block_dict)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchBlockMapper.generate_interactions","title":"generate_interactions","text":"<pre><code>generate_interactions(\n    left_batch: BatchType, right_batch: Optional[BatchType], mode: InfluenceMode\n) -&gt; Generator[TensorType, None, None]\n</code></pre> <p>Generator that yields gradient interactions between two batches, processed by each block based on a mode.</p> PARAMETER DESCRIPTION <code>left_batch</code> <p>The left batch for interaction computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>right_batch</code> <p>The right batch for interaction computation.</p> <p> TYPE: <code>Optional[BatchType]</code> </p> <code>mode</code> <p>The mode determining the type of interactions.</p> <p> TYPE: <code>InfluenceMode</code> </p> YIELDS DESCRIPTION <code>TensorType</code> <p>Gradient interactions for each block.</p> <p> TYPE:: <code>TensorType</code> </p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def generate_interactions(\n    self,\n    left_batch: BatchType,\n    right_batch: Optional[BatchType],\n    mode: InfluenceMode,\n) -&gt; Generator[TensorType, None, None]:\n    \"\"\"\n    Generator that yields gradient interactions between two batches, processed by\n    each block based on a mode.\n\n    Args:\n        left_batch: The left batch for interaction computation.\n        right_batch: The right batch for interaction computation.\n        mode: The mode determining the type of interactions.\n\n    Yields:\n        TensorType: Gradient interactions for each block.\n    \"\"\"\n    for comp_block in self.composable_block_dict.values():\n        yield comp_block.interactions(left_batch, right_batch, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchBlockMapper.generate_interactions_from_transformed_grads","title":"generate_interactions_from_transformed_grads","text":"<pre><code>generate_interactions_from_transformed_grads(\n    left_factors: Union[TensorType, OrderedDict[str, TensorType]],\n    right_batch: BatchType,\n    mode: InfluenceMode,\n) -&gt; Generator[TensorType, None, None]\n</code></pre> <p>Generator that yields interactions computed from pre-computed factors and a right batch, processed by each block based on a mode.</p> PARAMETER DESCRIPTION <code>left_factors</code> <p>Pre-computed factors as a tensor or an ordered dictionary of tensors by block.</p> <p> TYPE: <code>Union[TensorType, OrderedDict[str, TensorType]]</code> </p> <code>right_batch</code> <p>The right batch for interaction computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>mode</code> <p>The mode determining the type of interactions.</p> <p> TYPE: <code>InfluenceMode</code> </p> YIELDS DESCRIPTION <code>TensorType</code> <p>Interactions for each block.</p> <p> TYPE:: <code>TensorType</code> </p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def generate_interactions_from_transformed_grads(\n    self,\n    left_factors: Union[TensorType, OrderedDict[str, TensorType]],\n    right_batch: BatchType,\n    mode: InfluenceMode,\n) -&gt; Generator[TensorType, None, None]:\n    \"\"\"\n    Generator that yields interactions computed from pre-computed factors and a\n    right batch, processed by each block based on a mode.\n\n    Args:\n        left_factors: Pre-computed factors as a tensor or an ordered dictionary of\n            tensors by block.\n        right_batch: The right batch for interaction computation.\n        mode: The mode determining the type of interactions.\n\n    Yields:\n        TensorType: Interactions for each block.\n    \"\"\"\n    if not isinstance(left_factors, dict):\n        left_factors_dict = self._split_to_blocks(left_factors)\n    else:\n        left_factors_dict = cast(OrderedDict[str, TensorType], left_factors)\n    for k, comp_block in self.composable_block_dict.items():\n        yield comp_block.interactions_from_transformed_grads(\n            left_factors_dict[k], right_batch, mode\n        )\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchBlockMapper.generate_transformed_grads","title":"generate_transformed_grads","text":"<pre><code>generate_transformed_grads(\n    batch: BatchType,\n) -&gt; Generator[TensorType, None, None]\n</code></pre> <p>Generator that yields transformed gradients for a given batch, processed by each block.</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data for which to generate transformed gradients.</p> <p> TYPE: <code>BatchType</code> </p> YIELDS DESCRIPTION <code>TensorType</code> <p>Transformed gradients for each block.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def generate_transformed_grads(\n    self, batch: BatchType\n) -&gt; Generator[TensorType, None, None]:\n    \"\"\"\n    Generator that yields transformed gradients for a given batch,\n    processed by each block.\n\n    Args:\n        batch: The batch of data for which to generate transformed gradients.\n\n    Yields:\n        Transformed gradients for each block.\n    \"\"\"\n    for comp_block in self.composable_block_dict.values():\n        yield comp_block.transformed_grads(batch)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchBlockMapper.interactions","title":"interactions","text":"<pre><code>interactions(\n    left_batch: BatchType, right_batch: BatchType, mode: InfluenceMode\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Computes interactions between two batches, aggregated by block, based on a specified mode.</p> PARAMETER DESCRIPTION <code>left_batch</code> <p>The left batch for interaction computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>right_batch</code> <p>The right batch for interaction computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>mode</code> <p>The mode determining the type of interactions.</p> <p> TYPE: <code>InfluenceMode</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>An ordered dictionary of gradient interactions by block.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def interactions(\n    self, left_batch: BatchType, right_batch: BatchType, mode: InfluenceMode\n) -&gt; OrderedDict[str, TensorType]:\n    \"\"\"\n    Computes interactions between two batches, aggregated by block,\n    based on a specified mode.\n\n    Args:\n        left_batch: The left batch for interaction computation.\n        right_batch: The right batch for interaction computation.\n        mode: The mode determining the type of interactions.\n\n    Returns:\n        An ordered dictionary of gradient interactions by block.\n    \"\"\"\n    tensor_gen = self.generate_interactions(left_batch, right_batch, mode)\n    return self._to_ordered_dict(tensor_gen)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchBlockMapper.interactions_from_transformed_grads","title":"interactions_from_transformed_grads","text":"<pre><code>interactions_from_transformed_grads(\n    left_factors: OrderedDict[str, TensorType],\n    right_batch: BatchType,\n    mode: InfluenceMode,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Computes interactions from transformed gradients and a right batch, aggregated by block and based on a mode.</p> PARAMETER DESCRIPTION <code>left_factors</code> <p>Pre-computed factors as a tensor or an ordered dictionary of tensors by block. If the input is a tensor, it is split into blocks according to the ordering in the <code>composable_block_dict</code> attribute.</p> <p> TYPE: <code>OrderedDict[str, TensorType]</code> </p> <code>right_batch</code> <p>The right batch for interaction computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>mode</code> <p>The mode determining the type of interactions.</p> <p> TYPE: <code>InfluenceMode</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>An ordered dictionary of interactions from transformed gradients by block.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def interactions_from_transformed_grads(\n    self,\n    left_factors: OrderedDict[str, TensorType],\n    right_batch: BatchType,\n    mode: InfluenceMode,\n) -&gt; OrderedDict[str, TensorType]:\n    \"\"\"\n    Computes interactions from transformed gradients and a right batch,\n    aggregated by block and based on a mode.\n\n    Args:\n        left_factors: Pre-computed factors as a tensor or an ordered dictionary of\n            tensors by block. If the input is a tensor, it is split into blocks\n            according to the ordering in the `composable_block_dict` attribute.\n        right_batch: The right batch for interaction computation.\n        mode: The mode determining the type of interactions.\n\n    Returns:\n        An ordered dictionary of interactions from transformed gradients by block.\n    \"\"\"\n    tensor_gen = self.generate_interactions_from_transformed_grads(\n        left_factors, right_batch, mode\n    )\n    return self._to_ordered_dict(tensor_gen)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchBlockMapper.transformed_grads","title":"transformed_grads","text":"<pre><code>transformed_grads(batch: BatchType) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Computes and returns the transformed gradients for a batch in dictionary with the keys defined by the block names.</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data for which to compute transformed gradients.</p> <p> TYPE: <code>BatchType</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>An ordered dictionary of transformed gradients by block.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def transformed_grads(\n    self,\n    batch: BatchType,\n) -&gt; OrderedDict[str, TensorType]:\n    \"\"\"\n    Computes and returns the transformed gradients for a batch in dictionary\n    with the keys defined by the block names.\n\n    Args:\n        batch: The batch of data for which to compute transformed gradients.\n\n    Returns:\n        An ordered dictionary of transformed gradients by block.\n    \"\"\"\n    tensor_gen = self.generate_transformed_grads(batch)\n    return self._to_ordered_dict(tensor_gen)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchComposableInfluence","title":"TorchComposableInfluence","text":"<pre><code>TorchComposableInfluence(\n    model: Module,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = FULL,\n    regularization: Optional[Union[float, Dict[str, Optional[float]]]] = None,\n)\n</code></pre> <p>               Bases: <code>ComposableInfluence[Tensor, TorchBatch, DataLoader, TorchBlockMapper[TorchOperatorType]]</code>, <code>ModelInfoMixin</code>, <code>ABC</code></p> <p>Abstract base class, that allow for block-wise computation of influence quantities with the torch framework. Inherit from this base class for specific influence algorithms.</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = BlockMode.FULL,\n    regularization: Optional[Union[float, Dict[str, Optional[float]]]] = None,\n):\n    parameter_dict_builder = ModelParameterDictBuilder(model)\n    if isinstance(block_structure, BlockMode):\n        self.parameter_dict = parameter_dict_builder.build_from_block_mode(\n            block_structure\n        )\n    else:\n        self.parameter_dict = parameter_dict_builder.build(block_structure)\n\n    self._regularization_dict = self._build_regularization_dict(regularization)\n\n    super().__init__(model)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchComposableInfluence.is_thread_safe","title":"is_thread_safe  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>is_thread_safe: bool\n</code></pre> <p>Whether the influence computation is thread safe</p>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchComposableInfluence.fit","title":"fit","text":"<pre><code>fit(data: DataLoaderType) -&gt; InfluenceFunctionModel\n</code></pre> <p>Fitting to provided data, by internally creating a block mapper instance from it. Args:     data: iterable of tensors</p> RETURNS DESCRIPTION <code>InfluenceFunctionModel</code> <p>Fitted instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef fit(self, data: DataLoaderType) -&gt; InfluenceFunctionModel:\n    \"\"\"\n    Fitting to provided data, by internally creating a block mapper instance from\n    it.\n    Args:\n        data: iterable of tensors\n\n    Returns:\n        Fitted instance\n    \"\"\"\n    self.block_mapper = self._create_block_mapper(data)\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchComposableInfluence.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchComposableInfluence.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: TensorType, y: TensorType) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influence_factors(self, x: TensorType, y: TensorType) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    return self._influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchComposableInfluence.influence_factors_by_block","title":"influence_factors_by_block","text":"<pre><code>influence_factors_by_block(\n    x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise</p> <code>OrderedDict[str, TensorType]</code> <p>approximate inverse Hessian matrix vector products per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influence_factors_by_block(\n    self, x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise\n        approximate inverse Hessian matrix vector products per block.\n\n    \"\"\"\n    return self.block_mapper.transformed_grads(self._create_batch(x, y))\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchComposableInfluence.influences","title":"influences","text":"<pre><code>influences(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    if x is None and y is not None:\n        raise ValueError(\n            \"Providing labels y, without providing model input x is not supported\"\n        )\n\n    if x is not None and y is None:\n        raise ValueError(\n            \"Providing model input x, without providing labels y is not supported\"\n        )\n\n    return self._influences(x_test, y_test, x, y, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchComposableInfluence.influences_by_block","title":"influences_by_block","text":"<pre><code>influences_by_block(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise influence values for the provided data, i.e. an approximation of</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of the approximation of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_by_block(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise influence values for the provided data, i.e. an\n    approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of the approximation of\n            $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block.\n\n    \"\"\"\n    left_batch = self._create_batch(x_test, y_test)\n\n    if x is None:\n        if y is not None:\n            raise ValueError(\n                \"Providing labels y, without providing model input x \"\n                \"is not supported\"\n            )\n        right_batch = left_batch\n    else:\n        if y is None:\n            raise ValueError(\n                \"Providing model input x, without providing labels y \"\n                \"is not supported\"\n            )\n        right_batch = self._create_batch(x, y)\n\n    return self.block_mapper.interactions(left_batch, right_batch, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchComposableInfluence.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors(\n    self,\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    tensors = self.block_mapper.generate_interactions_from_transformed_grads(\n        z_test_factors,\n        self._create_batch(x, y),\n        mode,\n    )\n    result: TensorType = next(tensors)\n    for tensor in tensors:\n        result = result + tensor\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchComposableInfluence.influences_from_factors_by_block","title":"influences_from_factors_by_block","text":"<pre><code>influences_from_factors_by_block(\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Block-wise computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>OrderedDict[str, TensorType]</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors_by_block(\n    self,\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Block-wise computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block\n\n    \"\"\"\n    return self.block_mapper.interactions_from_transformed_grads(\n        z_test_factors, self._create_batch(x, y), mode\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchGradientProvider","title":"TorchGradientProvider","text":"<pre><code>TorchGradientProvider(\n    model: Module, loss: LossType, restrict_to: Optional[Dict[str, Parameter]]\n)\n</code></pre> <p>               Bases: <code>GradientProvider[TorchBatch, Tensor]</code></p> <p>Computes per-sample gradients of a function defined by a torch.nn.Module and a loss function using torch.func.</p> <p>Consider a function</p> \\[ \\ell: \\mathbb{R}^{d_1} \\times \\mathbb{R}^{d_2} \\times \\mathbb{R}^{n}     \\times \\mathbb{R}^{n}, \\quad \\ell(\\omega_1, \\omega_2, x, y) =     \\operatorname{loss}(f(\\omega_1, \\omega_2; x), y), \\] <p>e.g. a two layer neural network \\(f\\) with a loss function. This object computes the expressions:</p> \\[ \\nabla_{\\omega_{i}}\\ell(\\omega_1, \\omega_2, x, y),    \\nabla_{\\omega_{i}}\\nabla_{x}\\ell(\\omega_1, \\omega_2, x, y),    \\nabla_{\\omega}\\ell(\\omega_1, \\omega_2, x, y) \\cdot v. \\] Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    loss: LossType,\n    restrict_to: Optional[Dict[str, torch.nn.Parameter]],\n):\n    self.model = model\n    self.loss = loss\n\n    if restrict_to is None:\n        restrict_to = ModelParameterDictBuilder(model).build_from_block_mode(\n            BlockMode.FULL\n        )\n\n    self.params_to_restrict_to = restrict_to\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchGradientProvider.grads","title":"grads","text":"<pre><code>grads(batch: TorchBatch) -&gt; Dict[str, Tensor]\n</code></pre> <p>Computes and returns a dictionary mapping parameter names to their respective per-sample gradients. Given the example in the class docstring, this means</p> \\[ \\text{result}[\\omega_i] = \\nabla_{\\omega_{i}}\\ell(\\omega_1, \\omega_2,     \\text{batch.x}, \\text{batch.y}), \\] <p>where the first dimension of the resulting tensors is always considered to be the batch dimension, so the shape of the resulting tensors are \\((N, d_i)\\), where \\(N\\) is the number of samples in the batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data for which to compute gradients.</p> <p> TYPE: <code>TorchBatch</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dictionary where keys are gradient identifiers and values are the gradients computed per sample.</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def grads(self, batch: TorchBatch) -&gt; Dict[str, torch.Tensor]:\n    r\"\"\"\n    Computes and returns a dictionary mapping parameter names to their respective\n    per-sample gradients. Given the example in the class docstring, this means\n\n    $$ \\text{result}[\\omega_i] = \\nabla_{\\omega_{i}}\\ell(\\omega_1, \\omega_2,\n        \\text{batch.x}, \\text{batch.y}), $$\n\n    where the first dimension of the resulting tensors is always considered to be\n    the batch dimension, so the shape of the resulting tensors are $(N, d_i)$,\n    where $N$ is the number of samples in the batch.\n\n    Args:\n        batch: The batch of data for which to compute gradients.\n\n    Returns:\n        A dictionary where keys are gradient identifiers and values are the\n            gradients computed per sample.\n    \"\"\"\n    gradient_dict = self._grads(batch.to(self.device))\n    return self._detach_dict(gradient_dict)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchGradientProvider.jacobian_prod","title":"jacobian_prod","text":"<pre><code>jacobian_prod(batch: TorchBatch, g: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the matrix-Jacobian product for the provided batch and input tensor. Given the example in the class docstring, this means</p> \\[ (\\nabla_{\\omega_{1}}\\ell(\\omega_1, \\omega_2,     \\text{batch.x}, \\text{batch.y}),     \\nabla_{\\omega_{2}}\\ell(\\omega_1, \\omega_2,     \\text{batch.x}, \\text{batch.y})) \\cdot g^T\\] <p>where g must be a tensor of shape \\((K, d_1+d_2)\\), so the resulting tensor is of shape \\((N, K)\\).</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data for which to compute the Jacobian.</p> <p> TYPE: <code>TorchBatch</code> </p> <code>g</code> <p>The tensor to be used in the matrix-Jacobian product calculation.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The resulting tensor from the matrix-Jacobian product computation.</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def jacobian_prod(\n    self,\n    batch: TorchBatch,\n    g: torch.Tensor,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the matrix-Jacobian product for the provided batch and input tensor.\n    Given the example in the class docstring, this means\n\n    $$ (\\nabla_{\\omega_{1}}\\ell(\\omega_1, \\omega_2,\n        \\text{batch.x}, \\text{batch.y}),\n        \\nabla_{\\omega_{2}}\\ell(\\omega_1, \\omega_2,\n        \\text{batch.x}, \\text{batch.y})) \\cdot g^T$$\n\n    where g must be a tensor of shape $(K, d_1+d_2)$, so the resulting tensor\n    is of shape $(N, K)$.\n\n    Args:\n        batch: The batch of data for which to compute the Jacobian.\n        g: The tensor to be used in the matrix-Jacobian product\n            calculation.\n\n    Returns:\n        The resulting tensor from the matrix-Jacobian product computation.\n    \"\"\"\n    result = self._jacobian_prod(batch.to(self.device), g.to(self.device))\n    if result.requires_grad:\n        result = result.detach()\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchGradientProvider.mixed_grads","title":"mixed_grads","text":"<pre><code>mixed_grads(batch: TorchBatch) -&gt; Dict[str, Tensor]\n</code></pre> <p>Computes and returns a dictionary mapping gradient names to their respective per-sample mixed gradients. In this context, mixed gradients refer to computing gradients with respect to the instance definition in addition to compute derivatives with respect to the input batch. Given the example in the class docstring, this means</p> \\[ \\text{result}[\\omega_i] = \\nabla_{\\omega_{i}}\\nabla_{x}\\ell(\\omega_1,     \\omega_2, \\text{batch.x}, \\text{batch.y}), \\] <p>where the first dimension of the resulting tensors is always considered to be the batch dimension and the last to be the non-batch input related derivatives. So the shape of the resulting tensors are \\((N, n, d_i)\\), where \\(N\\) is the number of samples in the batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>The batch of data for which to compute mixed gradients.</p> <p> TYPE: <code>TorchBatch</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dictionary where keys are gradient identifiers and values are the mixed gradients computed per sample.</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def mixed_grads(self, batch: TorchBatch) -&gt; Dict[str, torch.Tensor]:\n    r\"\"\"\n    Computes and returns a dictionary mapping gradient names to their respective\n    per-sample mixed gradients. In this context, mixed gradients refer to computing\n    gradients with respect to the instance definition in addition to\n    compute derivatives with respect to the input batch.\n    Given the example in the class docstring, this means\n\n    $$ \\text{result}[\\omega_i] = \\nabla_{\\omega_{i}}\\nabla_{x}\\ell(\\omega_1,\n        \\omega_2, \\text{batch.x}, \\text{batch.y}), $$\n\n    where the first dimension of the resulting tensors is always considered to be\n    the batch dimension and the last to be the non-batch input related derivatives.\n    So the shape of the resulting tensors are $(N, n, d_i)$,\n    where $N$ is the number of samples in the batch.\n\n    Args:\n        batch: The batch of data for which to compute mixed gradients.\n\n    Returns:\n        A dictionary where keys are gradient identifiers and values are the\n            mixed gradients computed per sample.\n    \"\"\"\n    gradient_dict = self._mixed_grads(batch.to(self.device))\n    return self._detach_dict(gradient_dict)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchOperatorGradientComposition","title":"TorchOperatorGradientComposition","text":"<pre><code>TorchOperatorGradientComposition(\n    op: TorchOperatorType, gp: TorchGradientProvider\n)\n</code></pre> <p>               Bases: <code>OperatorGradientComposition[Tensor, TorchBatch, TorchOperatorType, TorchGradientProvider]</code></p> <p>Represents a composable block that integrates a TorchOperator and a TorchGradientProvider</p> <p>This block is designed to be flexible, handling different computational modes via an abstract operator and gradient provider.</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def __init__(self, op: TorchOperatorType, gp: TorchGradientProvider):\n    super().__init__(op, gp)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchOperatorGradientComposition.interactions","title":"interactions","text":"<pre><code>interactions(\n    left_batch: BatchType, right_batch: Optional[BatchType], mode: InfluenceMode\n) -&gt; TensorType\n</code></pre> <p>Computes the interaction between the gradients on two batches of data based on the specified mode weighted by the operator action, i.e.</p> \\[ \\langle \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y})), \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle\\] <p>for the case <code>InfluenceMode.Up</code> and</p> \\[ \\langle \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{left.x}, \\text{left.y})), \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle \\] <p>for the case <code>InfluenceMode.Perturbation</code>.</p> PARAMETER DESCRIPTION <code>left_batch</code> <p>The left data batch for gradient computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>right_batch</code> <p>The right data batch for gradient computation.</p> <p> TYPE: <code>Optional[BatchType]</code> </p> <code>mode</code> <p>An instance of InfluenceMode determining the type of influence computation.</p> <p> TYPE: <code>InfluenceMode</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>The result of the influence computation as dictated by the mode.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def interactions(\n    self,\n    left_batch: BatchType,\n    right_batch: Optional[BatchType],\n    mode: InfluenceMode,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the interaction between the gradients on two batches of data based on\n    the specified mode weighted by the operator action,\n    i.e.\n\n    $$ \\langle \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{left.x},\n    \\text{left.y})),\n    \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle$$\n\n    for the case `InfluenceMode.Up` and\n\n    $$ \\langle \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{left.x},\n    \\text{left.y})),\n    \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle $$\n\n    for the case `InfluenceMode.Perturbation`.\n\n    Args:\n        left_batch: The left data batch for gradient computation.\n        right_batch: The right data batch for gradient computation.\n        mode: An instance of InfluenceMode determining the type of influence\n            computation.\n\n    Returns:\n        The result of the influence computation as dictated by the mode.\n    \"\"\"\n    bilinear_form = self.op.as_bilinear_form()\n    if mode == InfluenceMode.Up:\n        return cast(\n            TensorType,\n            bilinear_form.grads_inner_prod(left_batch, right_batch, self.gp),\n        )\n    elif mode == InfluenceMode.Perturbation:\n        return cast(\n            TensorType,\n            bilinear_form.mixed_grads_inner_prod(left_batch, right_batch, self.gp),\n        )\n    else:\n        raise UnsupportedInfluenceModeException(mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchOperatorGradientComposition.interactions_from_transformed_grads","title":"interactions_from_transformed_grads","text":"<pre><code>interactions_from_transformed_grads(\n    left_factors: TensorType, right_batch: BatchType, mode: InfluenceMode\n) -&gt; TensorType\n</code></pre> <p>Computes the interaction between the transformed gradients on two batches of data using pre-computed factors and a batch of data, based on the specified mode. This means</p> \\[ \\langle \\text{left_factors}, \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle\\] <p>for the case <code>InfluenceMode.Up</code> and</p> \\[ \\langle \\text{left_factors}, \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle \\] <p>for the case <code>InfluenceMode.Perturbation</code>.</p> PARAMETER DESCRIPTION <code>left_factors</code> <p>Pre-computed tensor factors from a left batch.</p> <p> TYPE: <code>TensorType</code> </p> <code>right_batch</code> <p>The right data batch for influence computation.</p> <p> TYPE: <code>BatchType</code> </p> <code>mode</code> <p>An instance of InfluenceMode determining the type of influence computation.</p> <p> TYPE: <code>InfluenceMode</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>The result of the interaction computation using the provided factors and batch gradients.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def interactions_from_transformed_grads(\n    self, left_factors: TensorType, right_batch: BatchType, mode: InfluenceMode\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the interaction between the transformed gradients on two batches of\n    data using pre-computed factors and a batch of data,\n    based on the specified mode. This means\n\n    $$ \\langle \\text{left_factors},\n    \\nabla_{\\omega}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle$$\n\n    for the case `InfluenceMode.Up` and\n\n    $$ \\langle \\text{left_factors},\n    \\nabla_{\\omega}\\nabla_{x}\\ell(\\omega, \\text{right.x}, \\text{right.y}) \\rangle $$\n\n    for the case `InfluenceMode.Perturbation`.\n\n    Args:\n        left_factors: Pre-computed tensor factors from a left batch.\n        right_batch: The right data batch for influence computation.\n        mode: An instance of InfluenceMode determining the type of influence\n            computation.\n\n    Returns:\n        The result of the interaction computation using the provided factors and\n            batch gradients.\n    \"\"\"\n    if mode is InfluenceMode.Up:\n        right_grads = self.gp.flat_grads(right_batch)\n    else:\n        right_grads = self.gp.flat_mixed_grads(right_batch)\n    return self._tensor_inner_product(left_factors, right_grads)\n</code></pre>"},{"location":"api/pydvl/influence/torch/base/#pydvl.influence.torch.base.TorchOperatorGradientComposition.transformed_grads","title":"transformed_grads","text":"<pre><code>transformed_grads(batch: BatchType) -&gt; TensorType\n</code></pre> <p>Computes the gradients of a data batch, transformed by the operator application , i.e. the expressions</p> \\[ \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{batch.x},     \\text{batch.y})) \\] PARAMETER DESCRIPTION <code>batch</code> <p>The data batch for gradient computation.</p> <p> TYPE: <code>BatchType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the application of the operator to the gradients.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def transformed_grads(self, batch: BatchType) -&gt; TensorType:\n    r\"\"\"\n    Computes the gradients of a data batch, transformed by the operator application\n    , i.e. the expressions\n\n    $$ \\operatorname{Op}(\\nabla_{\\omega}\\ell(\\omega, \\text{batch.x},\n        \\text{batch.y})) $$\n\n    Args:\n        batch: The data batch for gradient computation.\n\n    Returns:\n        A tensor representing the application of the operator to the gradients.\n\n    \"\"\"\n    grads = self.gp.flat_grads(batch)\n    return cast(TensorType, self.op.apply(grads))\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/","title":"Batch operation","text":""},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation","title":"pydvl.influence.torch.batch_operation","text":"<p>This module contains abstractions and implementations for operations carried out on a batch \\(b\\). These operations are of the form</p> <p>$$ m(b) \\cdot v$$,</p> <p>where \\(m(b)\\) is a matrix defined by the data in the batch and \\(v\\) is a vector or matrix. These batch operations can be used to conveniently build aggregations or recursions over sequence of batches, e.g. an average of the form</p> <p>$$ \\frac{1}{|B|} \\sum_{b in B}m(b)\\cdot v$$,</p> <p>which is useful in the case that keeping \\(B\\) in memory is not feasible.</p>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.ChunkAveraging","title":"ChunkAveraging","text":"<p>               Bases: <code>_TensorAveraging[_TensorDictChunkAveraging]</code></p> <p>Averages tensors, provided by a generator, and normalizes by the number of tensors.</p>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.GaussNewtonBatchOperation","title":"GaussNewtonBatchOperation","text":"<pre><code>GaussNewtonBatchOperation(\n    model: Module,\n    loss: LossType,\n    restrict_to: Optional[Dict[str, Parameter]] = None,\n)\n</code></pre> <p>               Bases: <code>_ModelBasedBatchOperation</code></p> <p>Given a model and loss function computes the Gauss-Newton vector or matrix product with respect to the model parameters, i.e.</p> \\[\\begin{align*}     G(\\text{model}, \\text{loss}, b, \\theta) &amp;\\cdot v, \\\\\\     G(\\text{model}, \\text{loss}, b, \\theta) &amp;=     \\frac{1}{|b|}\\sum_{(x, y) \\in b}\\nabla_{\\theta}\\ell (x,y; \\theta)         \\nabla_{\\theta}\\ell (x,y; \\theta)^t, \\\\\\     \\ell(x,y; \\theta) &amp;= \\text{loss}(\\text{model}(x; \\theta), y) \\end{align*}\\] <p>where model is a torch.nn.Module and \\(v\\) is a vector or matrix.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>The loss function.</p> <p> TYPE: <code>LossType</code> </p> <code>restrict_to</code> <p>The parameters to restrict the differentiation to, i.e. the corresponding sub-matrix of the Jacobian. If None, the full Jacobian is used. Make sure the input matches the corrct dimension, i.e. the last dimension must be equal to the property <code>input_size</code>.</p> <p> TYPE: <code>Optional[Dict[str, Parameter]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    loss: LossType,\n    restrict_to: Optional[Dict[str, torch.nn.Parameter]] = None,\n):\n    super().__init__(model, restrict_to=restrict_to)\n    self.gradient_provider = TorchGradientProvider(\n        model, loss, self.params_to_restrict_to\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.GaussNewtonBatchOperation._apply_to_mat","title":"_apply_to_mat","text":"<pre><code>_apply_to_mat(batch: TorchBatch, mat: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the batch operation to a matrix. Args:     batch: Batch of data for computation     mat: A matrix to apply the batch operation to. The last dimension is         assumed to be consistent to the operation, i.e. it must equal         to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A matrix of shape \\((N,      ext{input_size})\\), given the shape of mat is \\((N,    ext{input_size})\\)</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def _apply_to_mat(self, batch: TorchBatch, mat: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the batch operation to a matrix.\n    Args:\n        batch: Batch of data for computation\n        mat: A matrix to apply the batch operation to. The last dimension is\n            assumed to be consistent to the operation, i.e. it must equal\n            to the property `input_size`.\n\n    Returns:\n        A matrix of shape $(N, \\text{input_size})$, given the shape of mat is\n            $(N, \\text{input_size})$\n\n    \"\"\"\n    return self._apply_to_vec(batch, mat)\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.GaussNewtonBatchOperation._rank_one_mvp","title":"_rank_one_mvp  <code>staticmethod</code>","text":"<pre><code>_rank_one_mvp(x: Tensor, v: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the matrix-vector product of xx^T and v for each row in X and V without forming xx^T and sums the result. Here, X and V are matrices where each row represents an individual vector. Effectively it is computing</p> \\[ V@( \\frac{1}{N}\\sum_i^N x[i]x[i]^T) \\] PARAMETER DESCRIPTION <code>x</code> <p>Matrix of vectors of size <code>(N, M)</code>.</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Matrix of vectors of size <code>(B, M)</code> to be multiplied by the corresponding \\(xx^T\\).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A matrix of size <code>(B, N)</code> where each column is the result of xx^T v for corresponding rows in x and v.</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>@staticmethod\ndef _rank_one_mvp(x: torch.Tensor, v: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the matrix-vector product of xx^T and v for each row in X and V without\n    forming xx^T and sums the result. Here, X and V are matrices where each row\n    represents an individual vector. Effectively it is computing\n\n    $$ V@( \\frac{1}{N}\\sum_i^N x[i]x[i]^T) $$\n\n    Args:\n        x: Matrix of vectors of size `(N, M)`.\n        v: Matrix of vectors of size `(B, M)` to be multiplied by the corresponding\n            $xx^T$.\n\n    Returns:\n        A matrix of size `(B, N)` where each column is the result of xx^T v for\n            corresponding rows in x and v.\n    \"\"\"\n    if v.ndim == 1:\n        result = torch.einsum(\"ij,kj-&gt;ki\", x, v.unsqueeze(0)) @ x\n        return result.squeeze() / x.shape[0]\n    return (torch.einsum(\"ij,kj-&gt;ki\", x, v) @ x) / x.shape[0]\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.GaussNewtonBatchOperation.apply","title":"apply","text":"<pre><code>apply(batch: TorchBatch, tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the batch operation to a tensor. Args:     batch: Batch of data for computation     tensor: A tensor consistent to the operation, i.e. it must be         at most 2-dim, and it's tailing dimension must         be equal to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor after applying the batch operation</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def apply(self, batch: TorchBatch, tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the batch operation to a tensor.\n    Args:\n        batch: Batch of data for computation\n        tensor: A tensor consistent to the operation, i.e. it must be\n            at most 2-dim, and it's tailing dimension must\n            be equal to the property `input_size`.\n\n    Returns:\n        A tensor after applying the batch operation\n    \"\"\"\n\n    if not tensor.ndim &lt;= 2:\n        raise ValueError(\n            f\"The input tensor must be at most 2-dimensional, got {tensor.ndim}\"\n        )\n\n    if tensor.shape[-1] != self.input_size:\n        raise ValueError(\n            \"The last dimension of the input tensor must be equal to the \"\n            \"property `input_size`.\"\n        )\n\n    if tensor.ndim == 2:\n        return self._apply_to_mat(batch.to(self.device), tensor.to(self.device))\n    return self._apply_to_vec(batch.to(self.device), tensor.to(self.device))\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.HessianBatchOperation","title":"HessianBatchOperation","text":"<pre><code>HessianBatchOperation(\n    model: Module,\n    loss: LossType,\n    restrict_to: Optional[Dict[str, Parameter]] = None,\n)\n</code></pre> <p>               Bases: <code>_ModelBasedBatchOperation</code></p> <p>Given a model and loss function computes the Hessian vector or matrix product with respect to the model parameters, i.e.</p> \\[\\begin{align*}     &amp;\\nabla^2_{\\theta} L(b;\\theta) \\cdot v \\\\\\     &amp;L(b;\\theta) = \\left( \\frac{1}{|b|} \\sum_{(x,y) \\in b}     \\text{loss}(\\text{model}(x; \\theta), y)\\right), \\end{align*}\\] <p>where model is a torch.nn.Module and \\(v\\) is a vector or matrix.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>The loss function.</p> <p> TYPE: <code>LossType</code> </p> <code>restrict_to</code> <p>The parameters to restrict the second order differentiation to, i.e. the corresponding sub-matrix of the Hessian. If None, the full Hessian is used. Make sure the input matches the corrct dimension, i.e. the last dimension must be equal to the property <code>input_size</code>.</p> <p> TYPE: <code>Optional[Dict[str, Parameter]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    loss: LossType,\n    restrict_to: Optional[Dict[str, torch.nn.Parameter]] = None,\n):\n    super().__init__(model, restrict_to=restrict_to)\n    self._batch_hvp = create_batch_hvp_function(model, loss, reverse_only=True)\n    self.loss = loss\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.HessianBatchOperation._apply_to_mat","title":"_apply_to_mat","text":"<pre><code>_apply_to_mat(batch: TorchBatch, mat: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the batch operation to a matrix. Args:     batch: Batch of data for computation     mat: A matrix to apply the batch operation to. The last dimension is         assumed to be consistent to the operation, i.e. it must equal         to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A matrix of shape \\((N,      ext{input_size})\\), given the shape of mat is \\((N,    ext{input_size})\\)</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def _apply_to_mat(self, batch: TorchBatch, mat: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the batch operation to a matrix.\n    Args:\n        batch: Batch of data for computation\n        mat: A matrix to apply the batch operation to. The last dimension is\n            assumed to be consistent to the operation, i.e. it must equal\n            to the property `input_size`.\n\n    Returns:\n        A matrix of shape $(N, \\text{input_size})$, given the shape of mat is\n            $(N, \\text{input_size})$\n\n    \"\"\"\n    result = torch.func.vmap(\n        lambda _x, _y, m: self._apply_to_vec(TorchBatch(_x, _y), m),\n        in_dims=(None, None, 0),\n        randomness=\"same\",\n    )(batch.x, batch.y, mat)\n    if result.requires_grad:\n        result = result.detach()\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.HessianBatchOperation.apply","title":"apply","text":"<pre><code>apply(batch: TorchBatch, tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the batch operation to a tensor. Args:     batch: Batch of data for computation     tensor: A tensor consistent to the operation, i.e. it must be         at most 2-dim, and it's tailing dimension must         be equal to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor after applying the batch operation</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def apply(self, batch: TorchBatch, tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the batch operation to a tensor.\n    Args:\n        batch: Batch of data for computation\n        tensor: A tensor consistent to the operation, i.e. it must be\n            at most 2-dim, and it's tailing dimension must\n            be equal to the property `input_size`.\n\n    Returns:\n        A tensor after applying the batch operation\n    \"\"\"\n\n    if not tensor.ndim &lt;= 2:\n        raise ValueError(\n            f\"The input tensor must be at most 2-dimensional, got {tensor.ndim}\"\n        )\n\n    if tensor.shape[-1] != self.input_size:\n        raise ValueError(\n            \"The last dimension of the input tensor must be equal to the \"\n            \"property `input_size`.\"\n        )\n\n    if tensor.ndim == 2:\n        return self._apply_to_mat(batch.to(self.device), tensor.to(self.device))\n    return self._apply_to_vec(batch.to(self.device), tensor.to(self.device))\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.InverseHarmonicMeanBatchOperation","title":"InverseHarmonicMeanBatchOperation","text":"<pre><code>InverseHarmonicMeanBatchOperation(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    regularization: float,\n    restrict_to: Optional[Dict[str, Parameter]] = None,\n)\n</code></pre> <p>               Bases: <code>_ModelBasedBatchOperation</code></p> <p>Given a model and loss function computes an approximation of the inverse Gauss-Newton vector or matrix product. Viewing the damped Gauss-newton matrix</p> \\[\\begin{align*}     G_{\\lambda}(\\text{model}, \\text{loss}, b, \\theta) &amp;=     \\frac{1}{|b|}\\sum_{(x, y) \\in b}\\nabla_{\\theta}\\ell (x,y; \\theta)         \\nabla_{\\theta}\\ell (x,y; \\theta)^t + \\lambda \\operatorname{I}, \\\\\\     \\ell(x,y; \\theta) &amp;= \\text{loss}(\\text{model}(x; \\theta), y) \\end{align*}\\] <p>as an arithmetic mean of the rank-\\(1\\) updates, this operation replaces it with the harmonic mean of the rank-\\(1\\) updates, i.e.</p> \\[ \\tilde{G}_{\\lambda}(\\text{model}, \\text{loss}, b, \\theta) =     \\left(n \\sum_{(x, y) \\in b}  \\left( \\nabla_{\\theta}\\ell (x,y; \\theta)         \\nabla_{\\theta}\\ell (x,y; \\theta)^t + \\lambda \\operatorname{I}\\right)^{-1}         \\right)^{-1}\\] <p>and computes</p> \\[ \\tilde{G}_{\\lambda}^{-1}(\\text{model}, \\text{loss}, b, \\theta) \\cdot v.\\] <p>where model is a torch.nn.Module and \\(v\\) is a vector or matrix. In other words, it switches the order of summation and inversion, which resolves to the <code>inverse harmonic mean</code> of the rank-\\(1\\) updates.</p> <p>The inverses of the rank-\\(1\\) updates are not calculated explicitly, but instead a vectorized version of the Sherman\u2013Morrison formula is applied.</p> <p>For more information, see Inverse Harmonic Mean.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>The loss function.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>restrict_to</code> <p>The parameters to restrict the differentiation to, i.e. the corresponding sub-matrix of the Jacobian. If None, the full Jacobian is used. Make sure the input matches the corrct dimension, i.e. the last dimension must be equal to the property <code>input_size</code>.</p> <p> TYPE: <code>Optional[Dict[str, Parameter]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    regularization: float,\n    restrict_to: Optional[Dict[str, torch.nn.Parameter]] = None,\n):\n    if regularization &lt;= 0:\n        raise ValueError(\"regularization must be positive\")\n    self.regularization = regularization\n\n    super().__init__(model, restrict_to=restrict_to)\n    self.gradient_provider = TorchGradientProvider(\n        model, loss, self.params_to_restrict_to\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.InverseHarmonicMeanBatchOperation._apply_to_mat","title":"_apply_to_mat","text":"<pre><code>_apply_to_mat(batch: TorchBatch, mat: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the batch operation to a matrix. Args:     batch: Batch of data for computation     mat: A matrix to apply the batch operation to. The last dimension is         assumed to be consistent to the operation, i.e. it must equal         to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A matrix of shape \\((N,      ext{input_size})\\), given the shape of mat is \\((N,    ext{input_size})\\)</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def _apply_to_mat(self, batch: TorchBatch, mat: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the batch operation to a matrix.\n    Args:\n        batch: Batch of data for computation\n        mat: A matrix to apply the batch operation to. The last dimension is\n            assumed to be consistent to the operation, i.e. it must equal\n            to the property `input_size`.\n\n    Returns:\n        A matrix of shape $(N, \\text{input_size})$, given the shape of mat is\n            $(N, \\text{input_size})$\n\n    \"\"\"\n    return self._apply_to_vec(batch, mat)\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.InverseHarmonicMeanBatchOperation._inverse_rank_one_update","title":"_inverse_rank_one_update  <code>staticmethod</code>","text":"<pre><code>_inverse_rank_one_update(x: Tensor, v: Tensor, regularization: float) -&gt; Tensor\n</code></pre> <p>Performs an inverse-rank one update on x and v. More precisely, it computes</p> \\[ \\sum_{i=1}^n \\left(x[i]x[i]^t+\\lambda \\operatorname{I}\\right)^{-1}v \\] <p>where \\(\\operatorname{I}\\) is the identity matrix and \\(\\lambda\\) is positive regularization parameter. The inverse matrices are not calculated explicitly, but instead a vectorized version of the Sherman\u2013Morrison formula is applied.</p> PARAMETER DESCRIPTION <code>x</code> <p>Input matrix used for the rank one expressions. First dimension is assumed to be the batch dimension.</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Matrix to multiply with. First dimension is assumed to be the batch dimension.</p> <p> TYPE: <code>Tensor</code> </p> <code>regularization</code> <p>Regularization parameter to make the rank-one expressions invertible, must be positive.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Matrix of size \\((D, M)\\) for x having shape \\((N, D)\\) and v having shape \\((M, D)\\).</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>@staticmethod\ndef _inverse_rank_one_update(\n    x: torch.Tensor, v: torch.Tensor, regularization: float\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Performs an inverse-rank one update on x and v. More precisely, it computes\n\n    $$ \\sum_{i=1}^n \\left(x[i]x[i]^t+\\lambda \\operatorname{I}\\right)^{-1}v $$\n\n    where $\\operatorname{I}$ is the identity matrix and $\\lambda$ is positive\n    regularization parameter. The inverse matrices are not calculated explicitly,\n    but instead a vectorized version of the\n    [Sherman\u2013Morrison formula](\n    https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula)\n    is applied.\n\n    Args:\n        x: Input matrix used for the rank one expressions. First dimension is\n            assumed to be the batch dimension.\n        v: Matrix to multiply with. First dimension is\n            assumed to be the batch dimension.\n        regularization: Regularization parameter to make the rank-one expressions\n            invertible, must be positive.\n\n    Returns:\n        Matrix of size $(D, M)$ for x having shape $(N, D)$ and v having shape\n            $(M, D)$.\n    \"\"\"\n    nominator = torch.einsum(\"ij,kj-&gt;ki\", x, v)\n    denominator = x.shape[0] * (regularization + torch.sum(x**2, dim=1))\n    return (v - (nominator / denominator) @ x) / regularization\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.InverseHarmonicMeanBatchOperation.apply","title":"apply","text":"<pre><code>apply(batch: TorchBatch, tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the batch operation to a tensor. Args:     batch: Batch of data for computation     tensor: A tensor consistent to the operation, i.e. it must be         at most 2-dim, and it's tailing dimension must         be equal to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor after applying the batch operation</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def apply(self, batch: TorchBatch, tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the batch operation to a tensor.\n    Args:\n        batch: Batch of data for computation\n        tensor: A tensor consistent to the operation, i.e. it must be\n            at most 2-dim, and it's tailing dimension must\n            be equal to the property `input_size`.\n\n    Returns:\n        A tensor after applying the batch operation\n    \"\"\"\n\n    if not tensor.ndim &lt;= 2:\n        raise ValueError(\n            f\"The input tensor must be at most 2-dimensional, got {tensor.ndim}\"\n        )\n\n    if tensor.shape[-1] != self.input_size:\n        raise ValueError(\n            \"The last dimension of the input tensor must be equal to the \"\n            \"property `input_size`.\"\n        )\n\n    if tensor.ndim == 2:\n        return self._apply_to_mat(batch.to(self.device), tensor.to(self.device))\n    return self._apply_to_vec(batch.to(self.device), tensor.to(self.device))\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation.PointAveraging","title":"PointAveraging","text":"<pre><code>PointAveraging(batch_dim: int = 0)\n</code></pre> <p>               Bases: <code>_TensorAveraging[_TensorDictPointAveraging]</code></p> <p>Averages tensors provided by a generator. The averaging is weighted by the number of points in each tensor and the final result is normalized by the number of total points.</p> PARAMETER DESCRIPTION <code>batch_dim</code> <p>Dimension to extract the number of points for the weighting.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def __init__(self, batch_dim: int = 0):\n    self.batch_dim = batch_dim\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation._ModelBasedBatchOperation","title":"_ModelBasedBatchOperation","text":"<pre><code>_ModelBasedBatchOperation(\n    model: Module, restrict_to: Optional[Dict[str, Parameter]] = None\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class to implement operations of the form</p> \\[ m(\\text{model}, b) \\cdot v \\] <p>where model is a torch.nn.Module.</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    restrict_to: Optional[Dict[str, torch.nn.Parameter]] = None,\n):\n    if restrict_to is None:\n        restrict_to = get_model_parameters(model)\n    self.params_to_restrict_to = restrict_to\n    self.model = model\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation._ModelBasedBatchOperation._apply_to_mat","title":"_apply_to_mat","text":"<pre><code>_apply_to_mat(batch: TorchBatch, mat: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the batch operation to a matrix. Args:     batch: Batch of data for computation     mat: A matrix to apply the batch operation to. The last dimension is         assumed to be consistent to the operation, i.e. it must equal         to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A matrix of shape \\((N,      ext{input_size})\\), given the shape of mat is \\((N,    ext{input_size})\\)</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def _apply_to_mat(self, batch: TorchBatch, mat: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the batch operation to a matrix.\n    Args:\n        batch: Batch of data for computation\n        mat: A matrix to apply the batch operation to. The last dimension is\n            assumed to be consistent to the operation, i.e. it must equal\n            to the property `input_size`.\n\n    Returns:\n        A matrix of shape $(N, \\text{input_size})$, given the shape of mat is\n            $(N, \\text{input_size})$\n\n    \"\"\"\n    result = torch.func.vmap(\n        lambda _x, _y, m: self._apply_to_vec(TorchBatch(_x, _y), m),\n        in_dims=(None, None, 0),\n        randomness=\"same\",\n    )(batch.x, batch.y, mat)\n    if result.requires_grad:\n        result = result.detach()\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/torch/batch_operation/#pydvl.influence.torch.batch_operation._ModelBasedBatchOperation.apply","title":"apply","text":"<pre><code>apply(batch: TorchBatch, tensor: Tensor) -&gt; Tensor\n</code></pre> <p>Applies the batch operation to a tensor. Args:     batch: Batch of data for computation     tensor: A tensor consistent to the operation, i.e. it must be         at most 2-dim, and it's tailing dimension must         be equal to the property <code>input_size</code>.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor after applying the batch operation</p> Source code in <code>src/pydvl/influence/torch/batch_operation.py</code> <pre><code>def apply(self, batch: TorchBatch, tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the batch operation to a tensor.\n    Args:\n        batch: Batch of data for computation\n        tensor: A tensor consistent to the operation, i.e. it must be\n            at most 2-dim, and it's tailing dimension must\n            be equal to the property `input_size`.\n\n    Returns:\n        A tensor after applying the batch operation\n    \"\"\"\n\n    if not tensor.ndim &lt;= 2:\n        raise ValueError(\n            f\"The input tensor must be at most 2-dimensional, got {tensor.ndim}\"\n        )\n\n    if tensor.shape[-1] != self.input_size:\n        raise ValueError(\n            \"The last dimension of the input tensor must be equal to the \"\n            \"property `input_size`.\"\n        )\n\n    if tensor.ndim == 2:\n        return self._apply_to_mat(batch.to(self.device), tensor.to(self.device))\n    return self._apply_to_vec(batch.to(self.device), tensor.to(self.device))\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/","title":"Functional","text":""},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional","title":"pydvl.influence.torch.functional","text":"<p>This module provides methods for efficiently computing tensors related to first and second order derivatives of torch models, using functionality from torch.func. To indicate higher-order functions, i.e. functions which return functions, we use the naming convention <code>create_**_function</code>.</p> <p>Among others, the module contains functionality for</p> <ul> <li>Sample, batch-wise and empirical loss functions:<ul> <li>create_per_sample_loss_function</li> <li>create_batch_loss_function</li> <li>create_empirical_loss_function</li> </ul> </li> <li>Per sample gradient and Jacobian product functions:<ul> <li>create_per_sample_gradient_function</li> <li>create_per_sample_mixed_derivative_function</li> <li>create_matrix_jacobian_product_function</li> </ul> </li> <li>Hessian, low rank approximation of Hessian and Hessian vector products:<ul> <li>hvp</li> <li>create_hvp_function</li> <li>create_batch_hvp_function</li> <li>hessian</li> <li>model_hessian_nystroem_approximation</li> </ul> </li> </ul>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.LowRankProductRepresentation","title":"LowRankProductRepresentation  <code>dataclass</code>","text":"<pre><code>LowRankProductRepresentation(eigen_vals: Tensor, projections: Tensor)\n</code></pre> <p>Representation of a low rank product of the form \\(H = V D V^T\\), where D is a diagonal matrix and V is orthogonal.</p> PARAMETER DESCRIPTION <code>eigen_vals</code> <p>Diagonal of D.</p> <p> TYPE: <code>Tensor</code> </p> <code>projections</code> <p>The matrix V.</p> <p> TYPE: <code>Tensor</code> </p>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.LowRankProductRepresentation.to","title":"to","text":"<pre><code>to(device: device)\n</code></pre> <p>Move the representing tensors to a device</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def to(self, device: torch.device):\n    \"\"\"\n    Move the representing tensors to a device\n    \"\"\"\n    return LowRankProductRepresentation(\n        self.eigen_vals.to(device), self.projections.to(device)\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.create_batch_hvp_function","title":"create_batch_hvp_function","text":"<pre><code>create_batch_hvp_function(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    reverse_only: bool = True,\n) -&gt; Callable[[Dict[str, Tensor], Tensor, Tensor, Tensor], Tensor]\n</code></pre> <p>Creates a function to compute Hessian-vector product (HVP) for a given model and loss function, where the Hessian information is computed for a provided batch.</p> <p>This function takes a PyTorch model, a loss function, and an optional boolean parameter. It returns a callable that computes the Hessian-vector product for batches of input data and a given vector. The computation can be performed in reverse mode only, based on the <code>reverse_only</code> parameter.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model for which the Hessian-vector product is to be computed.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>The loss function. It should take two torch.Tensor objects as input and return a torch.Tensor.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>reverse_only</code> <p>If True, the Hessian-vector product is computed in reverse mode only.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Callable[[Dict[str, Tensor], Tensor, Tensor, Tensor], Tensor]</code> <p>A function that takes three <code>torch.Tensor</code> objects - input data (<code>x</code>), target data (<code>y</code>), and a vector (<code>vec</code>), and returns the Hessian-vector product of the loss evaluated on <code>x</code>, <code>y</code> times <code>vec</code>.</p> Example <pre><code># Assume `model` is a PyTorch model and `loss_fn` is a loss function.\nb_hvp_function = batch_hvp(model, loss_fn)\n\n# `x_batch`, `y_batch` are batches of input and target data,\n# and `vec` is a vector.\nhvp_result = b_hvp_function(x_batch, y_batch, vec)\n</code></pre> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def create_batch_hvp_function(\n    model: torch.nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    reverse_only: bool = True,\n) -&gt; Callable[\n    [Dict[str, torch.Tensor], torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor\n]:\n    r\"\"\"\n    Creates a function to compute Hessian-vector product (HVP) for a given model and\n    loss function, where the Hessian information is computed for a provided batch.\n\n    This function takes a PyTorch model, a loss function,\n    and an optional boolean parameter. It returns a callable\n    that computes the Hessian-vector product for batches of input data\n    and a given vector. The computation can be performed in reverse mode only,\n    based on the `reverse_only` parameter.\n\n    Args:\n        model: The PyTorch model for which the Hessian-vector product is to be computed.\n        loss: The loss function. It should take two\n            torch.Tensor objects as input and return a torch.Tensor.\n        reverse_only (bool, optional): If True, the Hessian-vector product is computed\n            in reverse mode only.\n\n    Returns:\n        A function that takes three `torch.Tensor` objects - input data (`x`),\n            target data (`y`), and a vector (`vec`),\n            and returns the Hessian-vector product of the loss\n            evaluated on `x`, `y` times `vec`.\n\n    ??? Example\n        ```python\n        # Assume `model` is a PyTorch model and `loss_fn` is a loss function.\n        b_hvp_function = batch_hvp(model, loss_fn)\n\n        # `x_batch`, `y_batch` are batches of input and target data,\n        # and `vec` is a vector.\n        hvp_result = b_hvp_function(x_batch, y_batch, vec)\n        ```\n    \"\"\"\n\n    def b_hvp(\n        params: Dict[str, torch.Tensor],\n        x: torch.Tensor,\n        y: torch.Tensor,\n        vec: torch.Tensor,\n    ):\n        return flatten_dimensions(\n            hvp(\n                lambda p: create_batch_loss_function(model, loss)(p, x, y),\n                params,\n                align_structure(params, vec),\n                reverse_only=reverse_only,\n            ).values()\n        )\n\n    return b_hvp\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.create_batch_loss_function","title":"create_batch_loss_function","text":"<pre><code>create_batch_loss_function(\n    model: Module, loss: Callable[[Tensor, Tensor], Tensor]\n) -&gt; Callable[[Dict[str, Tensor], Tensor, Tensor], Tensor]\n</code></pre> <p>Creates a function to compute the loss of a given model on a given batch of data, i.e. the function</p> \\[f(\\theta, x, y) = \\frac{1}{N} \\sum_{i=1}^N     \\operatorname{loss}(\\operatorname{model}(\\theta, x_i), y_i)\\] <p>for a loss function \\(\\operatorname{loss}\\) and a model \\(\\operatorname{model}\\) with model parameters \\(\\theta\\), where \\(N\\) is the number of elements in the batch. Args:     model: The model for which the loss should be computed.     loss: The loss function to be used, which should be able to handle         a batch dimension</p> RETURNS DESCRIPTION <code>Callable[[Dict[str, Tensor], Tensor, Tensor], Tensor]</code> <p>A function that computes the loss of the model on a batch for given model parameters. The model parameter input to the function must take the form of a dict conform to model.named_parameters(), i.e. the keys must be a subset of the parameters and the corresponding tensor shapes must align. For the data input, the first dimension has to be the batch dimension.</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def create_batch_loss_function(\n    model: torch.nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n) -&gt; Callable[[Dict[str, torch.Tensor], torch.Tensor, torch.Tensor], torch.Tensor]:\n    r\"\"\"\n    Creates a function to compute the loss of a given model on a given batch of data,\n    i.e. the function\n\n    \\[f(\\theta, x, y) = \\frac{1}{N} \\sum_{i=1}^N\n        \\operatorname{loss}(\\operatorname{model}(\\theta, x_i), y_i)\\]\n\n    for a loss function $\\operatorname{loss}$ and a model $\\operatorname{model}$\n    with model parameters $\\theta$, where $N$ is the number of elements in the batch.\n    Args:\n        model: The model for which the loss should be computed.\n        loss: The loss function to be used, which should be able to handle\n            a batch dimension\n\n    Returns:\n        A function that computes the loss of the model on a batch for given\n            model parameters. The model parameter input to the function must take\n            the form of a dict conform to model.named_parameters(), i.e. the keys\n            must be a subset of the parameters and the corresponding tensor shapes\n            must align. For the data input, the first dimension has to be the batch\n            dimension.\n    \"\"\"\n\n    def batch_loss(params: Dict[str, torch.Tensor], x: torch.Tensor, y: torch.Tensor):\n        outputs = functional_call(model, params, (to_model_device(x, model),))\n        return loss(outputs, y)\n\n    return batch_loss\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.create_empirical_loss_function","title":"create_empirical_loss_function","text":"<pre><code>create_empirical_loss_function(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    data_loader: DataLoader,\n) -&gt; Callable[[Dict[str, Tensor]], Tensor]\n</code></pre> <p>Creates a function to compute the empirical loss of a given model on a given dataset. If we denote the model parameters with \\( \\theta \\), the resulting function approximates:</p> \\[     f(\\theta) = \\frac{1}{N}\\sum_{i=1}^N     \\operatorname{loss}(y_i, \\operatorname{model}(\\theta, x_i)) \\] <p>for a loss function \\(\\operatorname{loss}\\) and a model \\(\\operatorname{model}\\) with model parameters \\(\\theta\\), where \\(N\\) is the number of all elements provided by the data_loader.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model for which the loss should be computed.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>The loss function to be used.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>data_loader</code> <p>The data loader for iterating over the dataset.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>Callable[[Dict[str, Tensor]], Tensor]</code> <p>A function that computes the empirical loss of the model on the dataset for given model parameters.</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def create_empirical_loss_function(\n    model: torch.nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    data_loader: DataLoader,\n) -&gt; Callable[[Dict[str, torch.Tensor]], torch.Tensor]:\n    r\"\"\"\n    Creates a function to compute the empirical loss of a given model\n    on a given dataset. If we denote the model parameters with \\( \\theta \\),\n    the resulting function approximates:\n\n    \\[\n        f(\\theta) = \\frac{1}{N}\\sum_{i=1}^N\n        \\operatorname{loss}(y_i, \\operatorname{model}(\\theta, x_i))\n    \\]\n\n    for a loss function $\\operatorname{loss}$ and a model $\\operatorname{model}$\n    with model parameters $\\theta$, where $N$ is the number of all elements provided\n    by the data_loader.\n\n    Args:\n        model: The model for which the loss should be computed.\n        loss: The loss function to be used.\n        data_loader: The data loader for iterating over the dataset.\n\n    Returns:\n        A function that computes the empirical loss of the model on the dataset for\n            given model parameters.\n\n    \"\"\"\n\n    def empirical_loss(params: Dict[str, torch.Tensor]):\n        total_loss = to_model_device(torch.zeros((), requires_grad=True), model)\n        total_samples = to_model_device(torch.zeros(()), model)\n\n        for x, y in iter(data_loader):\n            output = functional_call(\n                model,\n                params,\n                (to_model_device(x, model),),\n            )\n            loss_value = loss(output, to_model_device(y, model))\n            total_loss = total_loss + loss_value * x.size(0)\n            total_samples += x.size(0)\n\n        return total_loss / total_samples\n\n    return empirical_loss\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.create_hvp_function","title":"create_hvp_function","text":"<pre><code>create_hvp_function(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    data_loader: DataLoader,\n    precompute_grad: bool = True,\n    use_average: bool = True,\n    reverse_only: bool = True,\n    track_gradients: bool = False,\n) -&gt; Callable[[Tensor], Tensor]\n</code></pre> <p>Returns a function that calculates the approximate Hessian-vector product for a given vector. If you want to compute the exact hessian, i.e., pulling all data into memory and compute a full gradient computation, use the function hvp.</p> PARAMETER DESCRIPTION <code>model</code> <p>A PyTorch module representing the model whose loss function's Hessian is to be computed.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that takes the model's output and target as input and returns the scalar loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>data_loader</code> <p>A DataLoader instance that provides batches of data for calculating the Hessian-vector product. Each batch from the DataLoader is assumed to return a tuple where the first element is the model's input and the second element is the target output.</p> <p> TYPE: <code>DataLoader</code> </p> <code>precompute_grad</code> <p>If <code>True</code>, the full data gradient is precomputed and kept in memory, which can speed up the hessian vector product computation. Set this to <code>False</code>, if you can't afford to keep the full computation graph in memory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_average</code> <p>If <code>True</code>, the returned function uses batch-wise computation via a batch loss function and averages the results. If <code>False</code>, the function uses backpropagation on the full empirical loss function, which is more accurate than averaging the batch hessians, but probably has a way higher memory usage.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>reverse_only</code> <p>Whether to use only reverse-mode autodiff or both forward- and reverse-mode autodiff. Ignored if <code>precompute_grad</code> is <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>track_gradients</code> <p>Whether to track gradients for the resulting tensor of the Hessian-vector products.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Callable[[Tensor], Tensor]</code> <p>A function that takes a single argument, a vector, and returns the</p> <code>Callable[[Tensor], Tensor]</code> <p>product of the Hessian of the <code>loss</code> function with respect to the</p> <code>Callable[[Tensor], Tensor]</code> <p><code>model</code>'s parameters and the input vector.</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def create_hvp_function(\n    model: torch.nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    data_loader: DataLoader,\n    precompute_grad: bool = True,\n    use_average: bool = True,\n    reverse_only: bool = True,\n    track_gradients: bool = False,\n) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n    \"\"\"\n    Returns a function that calculates the approximate Hessian-vector product\n    for a given vector. If you want to compute the exact hessian,\n    i.e., pulling all data into memory and compute a full gradient computation, use\n    the function [hvp][pydvl.influence.torch.functional.hvp].\n\n    Args:\n        model: A PyTorch module representing the model whose loss function's\n            Hessian is to be computed.\n        loss: A callable that takes the model's output and target as input and\n            returns the scalar loss.\n        data_loader: A DataLoader instance that provides batches of data for\n            calculating the Hessian-vector product. Each batch from the\n            DataLoader is assumed to return a tuple where the first element is\n            the model's input and the second element is the target output.\n        precompute_grad: If `True`, the full data gradient is precomputed and\n            kept in memory, which can speed up the hessian vector product\n            computation. Set this to `False`, if you can't afford to keep the\n            full computation graph in memory.\n        use_average: If `True`, the returned function uses batch-wise\n            computation via\n            [a batch loss function][pydvl.influence.torch.functional.create_batch_loss_function]\n            and averages the results.\n            If `False`, the function uses backpropagation on the full\n            [empirical loss function]\n            [pydvl.influence.torch.functional.create_empirical_loss_function],\n            which is more accurate than averaging the batch hessians, but\n            probably has a way higher memory usage.\n        reverse_only: Whether to use only reverse-mode autodiff or\n            both forward- and reverse-mode autodiff. Ignored if\n            `precompute_grad` is `True`.\n        track_gradients: Whether to track gradients for the resulting tensor of\n            the Hessian-vector products.\n\n    Returns:\n        A function that takes a single argument, a vector, and returns the\n        product of the Hessian of the `loss` function with respect to the\n        `model`'s parameters and the input vector.\n    \"\"\"\n\n    if precompute_grad:\n        model_params = {k: p for k, p in model.named_parameters() if p.requires_grad}\n\n        if use_average:\n            model_dtype = next(p.dtype for p in model.parameters() if p.requires_grad)\n            total_grad_xy = torch.empty(0, dtype=model_dtype)\n            total_points = 0\n            grad_func = torch.func.grad(create_batch_loss_function(model, loss))\n            for x, y in iter(data_loader):\n                grad_xy = grad_func(\n                    model_params, to_model_device(x, model), to_model_device(y, model)\n                )\n                grad_xy = flatten_dimensions(grad_xy.values())\n                if total_grad_xy.nelement() == 0:\n                    total_grad_xy = torch.zeros_like(grad_xy)\n                total_grad_xy += grad_xy * len(x)\n                total_points += len(x)\n            total_grad_xy /= total_points\n        else:\n            total_grad_xy = torch.func.grad(\n                create_empirical_loss_function(model, loss, data_loader)\n            )(model_params)\n            total_grad_xy = flatten_dimensions(total_grad_xy.values())\n\n        def precomputed_grads_hvp_function(\n            precomputed_grads: torch.Tensor, vec: torch.Tensor\n        ) -&gt; torch.Tensor:\n            vec = to_model_device(vec, model)\n            if vec.ndim == 1:\n                vec = vec.unsqueeze(0)\n\n            z = (precomputed_grads * torch.autograd.Variable(vec)).sum(dim=1)\n\n            mvp = []\n            for i in range(len(z)):\n                mvp.append(\n                    flatten_dimensions(\n                        torch.autograd.grad(\n                            z[i], list(model_params.values()), retain_graph=True\n                        )\n                    )\n                )\n            result = torch.stack([arr.contiguous().view(-1) for arr in mvp])\n\n            if not track_gradients:\n                result = result.detach()\n\n            return result\n\n        return partial(precomputed_grads_hvp_function, total_grad_xy)\n\n    def hvp_function(vec: torch.Tensor) -&gt; torch.Tensor:\n        params = get_model_parameters(model, detach=not track_gradients)\n        v = align_structure(params, vec)\n        empirical_loss = create_empirical_loss_function(model, loss, data_loader)\n        return flatten_dimensions(\n            hvp(empirical_loss, params, v, reverse_only=reverse_only).values()\n        )\n\n    def avg_hvp_function(vec: torch.Tensor) -&gt; torch.Tensor:\n        n_batches = len(data_loader)\n        avg_hessian = to_model_device(torch.zeros_like(vec), model)\n        b_hvp = create_batch_hvp_function(model, loss, reverse_only)\n        params = get_model_parameters(model, detach=not track_gradients)\n        for t_x, t_y in iter(data_loader):\n            t_x, t_y = to_model_device(t_x, model), to_model_device(t_y, model)\n            avg_hessian += b_hvp(params, t_x, t_y, to_model_device(vec, model))\n\n        return avg_hessian / float(n_batches)\n\n    return avg_hvp_function if use_average else hvp_function\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.create_matrix_jacobian_product_function","title":"create_matrix_jacobian_product_function","text":"<pre><code>create_matrix_jacobian_product_function(\n    model: Module, loss: Callable[[Tensor, Tensor], Tensor], g: Tensor\n) -&gt; Callable[[Dict[str, Tensor], Tensor, Tensor], Tensor]\n</code></pre> <p>Generates a function to computes the matrix-Jacobian product (MJP) of the per-sample loss with respect to the model's parameters, i.e. the function</p> \\[ f(\\theta, x, y) = g \\, @ \\, (\\nabla_{\\theta}\\operatorname{loss}     (\\operatorname{model}(\\theta, x_i), y_i))_i^T \\] <p>for a loss function \\(\\operatorname{loss}\\) and a model \\(\\operatorname{model}\\) with model parameters \\(\\theta\\).</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model for which the MJP will be computed.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that computes the loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>g</code> <p>Matrix for which the product with the Jacobian will be computed. The shape of this matrix should be consistent with the shape of the jacobian.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Callable[[Dict[str, Tensor], Tensor, Tensor], Tensor]</code> <p>A callable that takes a dictionary of model inputs, the model's input, and the labels. The callable returns the matrix-Jacobian product of the per-sample loss with respect to the model's parameters for the given matrix <code>g</code>.</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def create_matrix_jacobian_product_function(\n    model: torch.nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    g: torch.Tensor,\n) -&gt; Callable[[Dict[str, torch.Tensor], torch.Tensor, torch.Tensor], torch.Tensor]:\n    r\"\"\"\n    Generates a function to computes the matrix-Jacobian product (MJP) of the\n    per-sample loss with respect to the model's parameters, i.e. the function\n\n    \\[ f(\\theta, x, y) = g \\, @ \\, (\\nabla_{\\theta}\\operatorname{loss}\n        (\\operatorname{model}(\\theta, x_i), y_i))_i^T \\]\n\n    for a loss function $\\operatorname{loss}$ and a model $\\operatorname{model}$ with\n    model parameters $\\theta$.\n\n    Args:\n        model: The PyTorch model for which the MJP will be computed.\n        loss: A callable that computes the loss.\n        g: Matrix for which the product with the Jacobian will be computed.\n            The shape of this matrix should be consistent with the shape of\n            the jacobian.\n\n    Returns:\n        A callable that takes a dictionary of model inputs, the model's input,\n            and the labels. The callable returns the matrix-Jacobian product of the\n            per-sample loss with respect to the model's parameters for the given\n            matrix `g`.\n\n    \"\"\"\n\n    def single_jvp(\n        params: Dict[str, torch.Tensor],\n        x: torch.Tensor,\n        y: torch.Tensor,\n        _g: torch.Tensor,\n    ):\n        return torch.func.jvp(\n            lambda p: create_per_sample_loss_function(model, loss)(p, x, y),\n            (params,),\n            (align_with_model(_g, model),),\n        )[1]\n\n    def full_jvp(params: Dict[str, torch.Tensor], x: torch.Tensor, y: torch.Tensor):\n        return torch.func.vmap(single_jvp, in_dims=(None, None, None, 0))(\n            params, x, y, g\n        )\n\n    return full_jvp\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.create_per_sample_gradient_function","title":"create_per_sample_gradient_function","text":"<pre><code>create_per_sample_gradient_function(\n    model: Module, loss: Callable[[Tensor, Tensor], Tensor]\n) -&gt; Callable[[Dict[str, Tensor], Tensor, Tensor], Dict[str, Tensor]]\n</code></pre> <p>Generates a function to computes the per-sample gradient of the loss with respect to the model's parameters, i.e. the tensor-valued function</p> \\[ f(\\theta, x, y) = (\\nabla_{\\theta}\\operatorname{loss}     (\\operatorname{model}(\\theta, x_1), y_1), \\dots,     \\nabla_{\\theta}\\operatorname{loss}(\\operatorname{model}(\\theta, x_N), y_N) \\] <p>for a loss function \\(\\operatorname{loss}\\) and a model \\(\\operatorname{model}\\) with model parameters \\(\\theta\\), where \\(N\\) is the number of elements in the batch.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model for which per-sample gradients will be computed.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that computes the loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> RETURNS DESCRIPTION <code>Callable[[Dict[str, Tensor], Tensor, Tensor], Dict[str, Tensor]]</code> <p>A callable that takes a dictionary of model parameters, the model's input, and the labels. It returns a dictionary with the same keys as the model's named parameters. Each entry in the returned dictionary corresponds to the gradient of the corresponding model parameter for each sample in the batch.</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def create_per_sample_gradient_function(\n    model: torch.nn.Module, loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n) -&gt; Callable[\n    [Dict[str, torch.Tensor], torch.Tensor, torch.Tensor], Dict[str, torch.Tensor]\n]:\n    r\"\"\"\n    Generates a function to computes the per-sample gradient of the loss with respect to\n    the model's parameters, i.e. the tensor-valued function\n\n    \\[ f(\\theta, x, y) = (\\nabla_{\\theta}\\operatorname{loss}\n        (\\operatorname{model}(\\theta, x_1), y_1), \\dots,\n        \\nabla_{\\theta}\\operatorname{loss}(\\operatorname{model}(\\theta, x_N), y_N) \\]\n\n    for a loss function $\\operatorname{loss}$ and a model $\\operatorname{model}$ with\n    model parameters $\\theta$, where $N$ is the number of elements in the batch.\n\n    Args:\n        model: The PyTorch model for which per-sample gradients will be computed.\n        loss: A callable that computes the loss.\n\n    Returns:\n        A callable that takes a dictionary of model parameters, the model's input,\n            and the labels. It returns a dictionary with the same keys as the model's\n            named parameters. Each entry in the returned dictionary corresponds to\n            the gradient of the corresponding model parameter for each sample\n            in the batch.\n\n    \"\"\"\n\n    per_sample_grad: Callable[\n        [Dict[str, torch.Tensor], torch.Tensor, torch.Tensor], Dict[str, torch.Tensor]\n    ] = torch.func.jacrev(create_per_sample_loss_function(model, loss))\n    return per_sample_grad\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.create_per_sample_loss_function","title":"create_per_sample_loss_function","text":"<pre><code>create_per_sample_loss_function(\n    model: Module, loss: Callable[[Tensor, Tensor], Tensor]\n) -&gt; Callable[[Dict[str, Tensor], Tensor, Tensor], Tensor]\n</code></pre> <p>Generates a function to compute per-sample losses using PyTorch's vmap, i.e. the vector-valued function</p> \\[ f(\\theta, x, y)  = (\\operatorname{loss}(\\operatorname{model}(\\theta, x_1), y_1),     \\dots,     \\operatorname{loss}(\\operatorname{model}(\\theta, x_N), y_N)), \\] <p>for a loss function \\(\\operatorname{loss}\\) and a model \\(\\operatorname{model}\\) with model parameters \\(\\theta\\), where \\(N\\) is the number of elements in the batch.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model for which per-sample losses will be computed.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that computes the loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> RETURNS DESCRIPTION <code>Callable[[Dict[str, Tensor], Tensor, Tensor], Tensor]</code> <p>A callable that computes the loss for each sample in the batch, given a dictionary of model inputs, the model's predictions, and the true values. The callable will return a tensor where each entry corresponds to the loss of the corresponding sample.</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def create_per_sample_loss_function(\n    model: torch.nn.Module, loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n) -&gt; Callable[[Dict[str, torch.Tensor], torch.Tensor, torch.Tensor], torch.Tensor]:\n    r\"\"\"\n    Generates a function to compute per-sample losses using PyTorch's vmap,\n    i.e. the vector-valued function\n\n    \\[ f(\\theta, x, y)  = (\\operatorname{loss}(\\operatorname{model}(\\theta, x_1), y_1),\n        \\dots,\n        \\operatorname{loss}(\\operatorname{model}(\\theta, x_N), y_N)), \\]\n\n    for a loss function $\\operatorname{loss}$ and a model $\\operatorname{model}$ with\n    model parameters $\\theta$, where $N$ is the number of elements in the batch.\n\n    Args:\n        model: The PyTorch model for which per-sample losses will be computed.\n        loss: A callable that computes the loss.\n\n    Returns:\n        A callable that computes the loss for each sample in the batch,\n            given a dictionary of model inputs, the model's predictions,\n            and the true values. The callable will return a tensor where\n            each entry corresponds to the loss of the corresponding sample.\n    \"\"\"\n\n    def compute_loss(\n        params: Dict[str, torch.Tensor], x: torch.Tensor, y: torch.Tensor\n    ) -&gt; torch.Tensor:\n        outputs = functional_call(\n            model, params, (to_model_device(x.unsqueeze(0), model),)\n        )\n        return loss(outputs, y.unsqueeze(0))\n\n    vmap_loss: Callable[\n        [Dict[str, torch.Tensor], torch.Tensor, torch.Tensor], torch.Tensor\n    ] = torch.vmap(compute_loss, in_dims=(None, 0, 0))\n    return vmap_loss\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.create_per_sample_mixed_derivative_function","title":"create_per_sample_mixed_derivative_function","text":"<pre><code>create_per_sample_mixed_derivative_function(\n    model: Module, loss: Callable[[Tensor, Tensor], Tensor]\n) -&gt; Callable[[Dict[str, Tensor], Tensor, Tensor], Dict[str, Tensor]]\n</code></pre> <p>Generates a function to computes the mixed derivatives, of the per-sample loss with respect to the model parameters and the input, i.e. the function</p> \\[ f(\\theta, x, y) = \\nabla_{\\theta}\\nabla_{x}\\operatorname{loss}     (\\operatorname{model}(\\theta, x), y) \\] <p>for a loss function \\(\\operatorname{loss}\\) and a model \\(\\operatorname{model}\\) with model parameters \\(\\theta\\).</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model for which the mixed derivatives are computed.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that computes the loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> RETURNS DESCRIPTION <code>Callable[[Dict[str, Tensor], Tensor, Tensor], Dict[str, Tensor]]</code> <p>A callable that takes a dictionary of model inputs, the model's input, and the labels. The callable returns the mixed derivatives of the per-sample loss with respect to the model's parameters and input.</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def create_per_sample_mixed_derivative_function(\n    model: torch.nn.Module, loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n) -&gt; Callable[\n    [Dict[str, torch.Tensor], torch.Tensor, torch.Tensor], Dict[str, torch.Tensor]\n]:\n    r\"\"\"\n    Generates a function to computes the mixed derivatives, of the per-sample loss with\n    respect to the model parameters and the input, i.e. the function\n\n    \\[ f(\\theta, x, y) = \\nabla_{\\theta}\\nabla_{x}\\operatorname{loss}\n        (\\operatorname{model}(\\theta, x), y) \\]\n\n    for a loss function $\\operatorname{loss}$ and a model $\\operatorname{model}$ with\n    model parameters $\\theta$.\n\n    Args:\n        model: The PyTorch model for which the mixed derivatives are computed.\n        loss: A callable that computes the loss.\n\n    Returns:\n        A callable that takes a dictionary of model inputs, the model's input,\n            and the labels. The callable returns the mixed derivatives of the\n            per-sample loss with respect to the model's parameters and input.\n\n    \"\"\"\n\n    def compute_loss(params: Dict[str, torch.Tensor], x: torch.Tensor, y: torch.Tensor):\n        outputs = functional_call(\n            model, params, (to_model_device(x.unsqueeze(0), model),)\n        )\n        return loss(outputs, y.unsqueeze(0))\n\n    per_samp_mix_derivative: Callable[\n        [Dict[str, torch.Tensor], torch.Tensor, torch.Tensor], Dict[str, torch.Tensor]\n    ] = torch.vmap(\n        torch.func.jacrev(torch.func.grad(compute_loss, argnums=1)),\n        in_dims=(None, 0, 0),\n    )\n    return per_samp_mix_derivative\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.gauss_newton","title":"gauss_newton","text":"<pre><code>gauss_newton(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    data_loader: DataLoader,\n    restrict_to: Optional[Dict[str, Tensor]] = None,\n) -&gt; Tensor\n</code></pre> <p>Compute the Gauss-Newton matrix, i.e.</p> <p>$$ \\sum_{i=1}^N \\nabla_{\\theta}\\ell(m(x_i; \\theta), y)     \\nabla_{\\theta}\\ell(m(x_i; \\theta), y)^t,$$ for a  loss function \\(\\ell\\) and a model \\(m\\) with model parameters \\(\\theta\\).</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that computes the loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>data_loader</code> <p>A PyTorch DataLoader providing batches of input data and corresponding output data.</p> <p> TYPE: <code>DataLoader</code> </p> <code>restrict_to</code> <p>The parameters to restrict the differentiation to, i.e. the corresponding sub-matrix of the Jacobian. If None, the full Jacobian is used.</p> <p> TYPE: <code>Optional[Dict[str, Tensor]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The Gauss-Newton matrix.</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def gauss_newton(\n    model: torch.nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    data_loader: DataLoader,\n    restrict_to: Optional[Dict[str, torch.Tensor]] = None,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the Gauss-Newton matrix, i.e.\n\n    $$ \\sum_{i=1}^N \\nabla_{\\theta}\\ell(m(x_i; \\theta), y)\n        \\nabla_{\\theta}\\ell(m(x_i; \\theta), y)^t,$$\n    for a  loss function $\\ell$ and a model $m$ with model parameters $\\theta$.\n\n    Args:\n        model: The PyTorch model.\n        loss: A callable that computes the loss.\n        data_loader: A PyTorch DataLoader providing batches of input data and\n            corresponding output data.\n        restrict_to: The parameters to restrict the differentiation to,\n            i.e. the corresponding sub-matrix of the Jacobian. If None, the full\n            Jacobian is used.\n\n    Returns:\n        The Gauss-Newton matrix.\n    \"\"\"\n\n    per_sample_grads = create_per_sample_gradient_function(model, loss)\n\n    params = restrict_to\n    if params is None:\n        params = get_model_parameters(model)\n\n    def generate_batch_matrices():\n        for x, y in data_loader:\n            grads = flatten_dimensions(\n                per_sample_grads(params, x, y).values(), shape=(x.shape[0], -1)\n            )\n            batch_mat = grads.t() @ grads\n            yield batch_mat.detach()\n\n    n_points = 0\n    tensors = generate_batch_matrices()\n    result = next(tensors)\n\n    for t in tensors:\n        result += t\n        n_points += t.shape[0]\n\n    return result / n_points\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.hessian","title":"hessian","text":"<pre><code>hessian(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    data_loader: DataLoader,\n    use_hessian_avg: bool = True,\n    track_gradients: bool = False,\n    restrict_to: Optional[Dict[str, Tensor]] = None,\n) -&gt; Tensor\n</code></pre> <p>Computes the Hessian matrix for a given model and loss function.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model for which the Hessian is computed.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that computes the loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>data_loader</code> <p>DataLoader providing batches of input data and corresponding ground truths.</p> <p> TYPE: <code>DataLoader</code> </p> <code>use_hessian_avg</code> <p>Flag to indicate whether the average Hessian across mini-batches should be computed. If False, the empirical loss across the entire dataset is used.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>track_gradients</code> <p>Whether to track gradients for the resulting tensor of the hessian vector products.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>restrict_to</code> <p>The parameters to restrict the second order differentiation to, i.e. the corresponding sub-matrix of the Hessian. If None, the full Hessian is computed.</p> <p> TYPE: <code>Optional[Dict[str, Tensor]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor representing the Hessian matrix. The shape of the tensor will be (n_parameters, n_parameters), where n_parameters is the number of trainable parameters in the model.</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def hessian(\n    model: torch.nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    data_loader: DataLoader,\n    use_hessian_avg: bool = True,\n    track_gradients: bool = False,\n    restrict_to: Optional[Dict[str, torch.Tensor]] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the Hessian matrix for a given model and loss function.\n\n    Args:\n        model: The PyTorch model for which the Hessian is computed.\n        loss: A callable that computes the loss.\n        data_loader: DataLoader providing batches of input data and corresponding\n            ground truths.\n        use_hessian_avg: Flag to indicate whether the average Hessian across\n            mini-batches should be computed.\n            If False, the empirical loss across the entire dataset is used.\n        track_gradients: Whether to track gradients for the resulting tensor of\n            the hessian vector products.\n        restrict_to: The parameters to restrict the second order differentiation to,\n            i.e. the corresponding sub-matrix of the Hessian. If None, the full Hessian\n            is computed.\n\n    Returns:\n        A tensor representing the Hessian matrix. The shape of the tensor will be\n            (n_parameters, n_parameters), where n_parameters is the number of trainable\n            parameters in the model.\n    \"\"\"\n    params = restrict_to\n\n    if params is None:\n        params = get_model_parameters(model, detach=not track_gradients)\n    n_parameters = sum([p.numel() for p in params.values()])\n    model_dtype = next((p.dtype for p in params.values()))\n\n    flat_params = flatten_dimensions(params.values())\n\n    if use_hessian_avg:\n        n_samples = 0\n        hessian_mat = to_model_device(\n            torch.zeros((n_parameters, n_parameters), dtype=model_dtype), model\n        )\n        batch_loss = create_batch_loss_function(model, loss)\n\n        def flat_input_batch_loss(\n            p: torch.Tensor, t_x: torch.Tensor, t_y: torch.Tensor\n        ):\n            return batch_loss(align_structure(params, p), t_x, t_y)\n\n        for x, y in iter(data_loader):\n            n_samples += x.shape[0]\n            batch_hessian = torch.func.hessian(flat_input_batch_loss)(\n                flat_params, to_model_device(x, model), to_model_device(y, model)\n            )\n            if not track_gradients and batch_hessian.requires_grad:\n                batch_hessian = batch_hessian.detach()\n            hessian_mat += x.shape[0] * batch_hessian\n\n        hessian_mat /= n_samples\n    else:\n\n        def flat_input_empirical_loss(p: torch.Tensor):\n            return create_empirical_loss_function(model, loss, data_loader)(\n                align_with_model(p, model)\n            )\n\n        hessian_mat = torch.func.jacrev(torch.func.jacrev(flat_input_empirical_loss))(\n            flat_params\n        )\n\n    return hessian_mat\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.hvp","title":"hvp","text":"<pre><code>hvp(\n    func: Callable[[Dict[str, Tensor]], Tensor],\n    params: Dict[str, Tensor],\n    vec: Dict[str, Tensor],\n    reverse_only: bool = True,\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>Computes the Hessian-vector product (HVP) for a given function at the given parameters, i.e.</p> \\[\\nabla_{\\theta} \\nabla_{\\theta} f (\\theta)\\cdot v\\] <p>This function can operate in two modes, either reverse-mode autodiff only or both forward- and reverse-mode autodiff.</p> PARAMETER DESCRIPTION <code>func</code> <p>The scalar-valued function for which the HVP is computed.</p> <p> TYPE: <code>Callable[[Dict[str, Tensor]], Tensor]</code> </p> <code>params</code> <p>The parameters at which the HVP is computed.</p> <p> TYPE: <code>Dict[str, Tensor]</code> </p> <code>vec</code> <p>The vector with which the Hessian is multiplied.</p> <p> TYPE: <code>Dict[str, Tensor]</code> </p> <code>reverse_only</code> <p>Whether to use only reverse-mode autodiff (True, default) or both forward- and reverse-mode autodiff (False).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>The HVP of the function at the given parameters with the given vector.</p> Example <pre><code>&gt;&gt;&gt; def f(z): return torch.sum(z**2)\n&gt;&gt;&gt; u = torch.ones(10, requires_grad=True)\n&gt;&gt;&gt; v = torch.ones(10)\n&gt;&gt;&gt; hvp_vec = hvp(f, u, v)\n&gt;&gt;&gt; assert torch.allclose(hvp_vec, torch.full((10, ), 2.0))\n</code></pre> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def hvp(\n    func: Callable[[Dict[str, torch.Tensor]], torch.Tensor],\n    params: Dict[str, torch.Tensor],\n    vec: Dict[str, torch.Tensor],\n    reverse_only: bool = True,\n) -&gt; Dict[str, torch.Tensor]:\n    r\"\"\"\n    Computes the Hessian-vector product (HVP) for a given function at the given\n    parameters, i.e.\n\n    \\[\\nabla_{\\theta} \\nabla_{\\theta} f (\\theta)\\cdot v\\]\n\n    This function can operate in two modes, either reverse-mode autodiff only or both\n    forward- and reverse-mode autodiff.\n\n    Args:\n        func: The scalar-valued function for which the HVP is computed.\n        params: The parameters at which the HVP is computed.\n        vec: The vector with which the Hessian is multiplied.\n        reverse_only: Whether to use only reverse-mode autodiff\n            (True, default) or both forward- and reverse-mode autodiff (False).\n\n    Returns:\n        The HVP of the function at the given parameters with the given vector.\n\n    ??? Example\n\n        ```pycon\n        &gt;&gt;&gt; def f(z): return torch.sum(z**2)\n        &gt;&gt;&gt; u = torch.ones(10, requires_grad=True)\n        &gt;&gt;&gt; v = torch.ones(10)\n        &gt;&gt;&gt; hvp_vec = hvp(f, u, v)\n        &gt;&gt;&gt; assert torch.allclose(hvp_vec, torch.full((10, ), 2.0))\n        ```\n    \"\"\"\n\n    output: Dict[str, torch.Tensor]\n\n    if reverse_only:\n        _, vjp_fn = vjp(grad(func), params)\n        output = vjp_fn(vec)[0]\n    else:\n        output = jvp(grad(func), (params,), (vec,))[1]\n\n    return output\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.model_hessian_nystroem_approximation","title":"model_hessian_nystroem_approximation","text":"<pre><code>model_hessian_nystroem_approximation(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    data_loader: DataLoader,\n    rank: int,\n    shift_func: Optional[Callable[[Tensor], Tensor]] = None,\n) -&gt; LowRankProductRepresentation\n</code></pre> <p>Given a model, loss and a data_loader, computes a random Nystr\u00f6m low rank approximation of the corresponding Hessian matrix in factored form, i.e.</p> \\[ H_{\\text{nys}} = (H \\Omega)(\\Omega^T H \\Omega)^{+}(H \\Omega)^T = U \\Sigma U^T \\] PARAMETER DESCRIPTION <code>model</code> <p>A PyTorch model instance. The Hessian will be calculated with respect to this model's parameters.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that computes the loss.</p> <p> </p> <code>data_loader</code> <p>A DataLoader instance that provides the model's training data. Used in calculating the Hessian-vector products.</p> <p> TYPE: <code>DataLoader</code> </p> <code>rank</code> <p>rank of the approximation</p> <p> TYPE: <code>int</code> </p> <code>shift_func</code> <p>optional function for computing the stabilizing shift in the construction of the randomized nystroem approximation, defaults to</p> \\[ \\sqrt{\\operatorname{\\text{input_dim}}} \\cdot     \\varepsilon(\\operatorname{\\text{input_type}}) \\cdot \\|A\\Omega\\|_2,\\] <p>where \\(\\varepsilon(\\operatorname{\\text{input_type}})\\) is the value of the machine precision corresponding to the data type.</p> <p> TYPE: <code>Optional[Callable[[Tensor], Tensor]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LowRankProductRepresentation</code> <p>object containing, \\(U\\) and \\(\\Sigma\\)</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def model_hessian_nystroem_approximation(\n    model: torch.nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    data_loader: DataLoader,\n    rank: int,\n    shift_func: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n) -&gt; LowRankProductRepresentation:\n    r\"\"\"\n    Given a model, loss and a data_loader, computes a random Nystr\u00f6m low rank approximation of\n    the corresponding Hessian matrix in factored form, i.e.\n\n    $$ H_{\\text{nys}} = (H \\Omega)(\\Omega^T H \\Omega)^{+}(H \\Omega)^T\n    = U \\Sigma U^T $$\n\n    Args:\n        model: A PyTorch model instance. The Hessian will be calculated with respect to\n            this model's parameters.\n        loss : A callable that computes the loss.\n        data_loader: A DataLoader instance that provides the model's training data.\n            Used in calculating the Hessian-vector products.\n        rank: rank of the approximation\n        shift_func: optional function for computing the stabilizing shift in the\n            construction of the randomized nystroem approximation, defaults to\n\n            $$ \\sqrt{\\operatorname{\\text{input_dim}}} \\cdot\n                \\varepsilon(\\operatorname{\\text{input_type}}) \\cdot \\|A\\Omega\\|_2,$$\n\n            where $\\varepsilon(\\operatorname{\\text{input_type}})$ is the value of the\n            machine precision corresponding to the data type.\n\n    Returns:\n        object containing, $U$ and $\\Sigma$\n    \"\"\"\n\n    model_hvp = create_hvp_function(\n        model, loss, data_loader, precompute_grad=False, use_average=True\n    )\n    device = next((p.device for p in model.parameters()))\n    dtype = next((p.dtype for p in model.parameters()))\n    in_dim = sum((p.numel() for p in model.parameters() if p.requires_grad))\n\n    def model_hessian_mat_mat_prod(x: torch.Tensor):\n        return torch.func.vmap(model_hvp, in_dims=1, randomness=\"same\")(x).t()\n\n    return randomized_nystroem_approximation(\n        model_hessian_mat_mat_prod,\n        in_dim,\n        rank,\n        dtype,\n        shift_func=shift_func,\n        mat_vec_device=device,\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.operator_nystroem_approximation","title":"operator_nystroem_approximation","text":"<pre><code>operator_nystroem_approximation(\n    operator: \"TensorOperator\",\n    rank: int,\n    shift_func: Optional[Callable[[Tensor], Tensor]] = None,\n) -&gt; LowRankProductRepresentation\n</code></pre> <p>Given an operator (representing a symmetric positive definite matrix \\(A\\) ), computes a random Nystr\u00f6m low rank approximation of \\(A\\) in factored form, i.e.</p> \\[ A_{\\text{nys}} = (A \\Omega)(\\Omega^T A \\Omega)^{\\dagger}(A \\Omega)^T = U \\Sigma U^T \\] <p>where \\(\\Omega\\) is a standard normal random matrix.</p> PARAMETER DESCRIPTION <code>operator</code> <p>the operator to approximate</p> <p> TYPE: <code>'TensorOperator'</code> </p> <code>rank</code> <p>rank of the approximation</p> <p> TYPE: <code>int</code> </p> <code>shift_func</code> <p>optional function for computing the stabilizing shift in the construction of the randomized nystroem approximation, defaults to</p> \\[ \\sqrt{\\operatorname{\\text{input_dim}}} \\cdot     \\varepsilon(\\operatorname{\\text{input_type}}) \\cdot \\|A\\Omega\\|_2,\\] <p>where \\(\\varepsilon(\\operatorname{\\text{input_type}})\\) is the value of the machine precision corresponding to the data type.</p> <p> TYPE: <code>Optional[Callable[[Tensor], Tensor]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LowRankProductRepresentation</code> <p>object containing, \\(U\\) and \\(\\Sigma\\)</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def operator_nystroem_approximation(\n    operator: \"TensorOperator\",\n    rank: int,\n    shift_func: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n) -&gt; LowRankProductRepresentation:\n    r\"\"\"\n    Given an operator (representing a symmetric positive definite\n    matrix $A$ ), computes a random Nystr\u00f6m low rank approximation of\n    $A$ in factored form, i.e.\n\n    $$ A_{\\text{nys}} = (A \\Omega)(\\Omega^T A \\Omega)^{\\dagger}(A \\Omega)^T\n    = U \\Sigma U^T $$\n\n    where $\\Omega$ is a standard normal random matrix.\n\n    Args:\n        operator: the operator to approximate\n        rank: rank of the approximation\n        shift_func: optional function for computing the stabilizing shift in the\n            construction of the randomized nystroem approximation, defaults to\n\n            $$ \\sqrt{\\operatorname{\\text{input_dim}}} \\cdot\n                \\varepsilon(\\operatorname{\\text{input_type}}) \\cdot \\|A\\Omega\\|_2,$$\n\n            where $\\varepsilon(\\operatorname{\\text{input_type}})$ is the value of the\n            machine precision corresponding to the data type.\n\n    Returns:\n        object containing, $U$ and $\\Sigma$\n    \"\"\"\n\n    def mat_mat_prod(x: torch.Tensor):\n        return operator.apply(x.t()).t()\n\n    return randomized_nystroem_approximation(\n        mat_mat_prod,\n        operator.input_size,\n        rank,\n        operator.dtype,\n        shift_func=shift_func,\n        mat_vec_device=operator.device,\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.operator_spectral_approximation","title":"operator_spectral_approximation","text":"<pre><code>operator_spectral_approximation(\n    operator: \"TensorOperator\",\n    rank: int = 10,\n    krylov_dimension: Optional[int] = None,\n    tol: float = 1e-06,\n    max_iter: Optional[int] = None,\n    eigen_computation_on_gpu: bool = False,\n) -&gt; \"LowRankProductRepresentation\"\n</code></pre> <p>Calculates a low-rank approximation of an operator \\(H\\) using the implicitly restarted Lanczos algorithm, i.e.:</p> \\[ H_{\\text{approx}} = V D V^T\\] <p>where \\(D\\) is a diagonal matrix with the top (in absolute value) <code>rank</code> eigenvalues of the Hessian and \\(V\\) contains the corresponding eigenvectors.</p> PARAMETER DESCRIPTION <code>operator</code> <p>The operator to approximate.</p> <p> TYPE: <code>'TensorOperator'</code> </p> <code>rank</code> <p>The number of eigenvalues and corresponding eigenvectors to compute. Represents the desired rank of the Hessian approximation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>krylov_dimension</code> <p>The number of Krylov vectors to use for the Lanczos method. If not provided, it defaults to \\( \\min(\\text{model.n_parameters},     \\max(2 \\times \\text{rank_estimate} + 1, 20)) \\).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>tol</code> <p>The stopping criteria for the Lanczos algorithm, which stops when the difference in the approximated eigenvalue is less than <code>tol</code>. Defaults to 1e-6.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>max_iter</code> <p>The maximum number of iterations for the Lanczos method. If not provided, it defaults to \\( 10 \\cdot \\text{model.n_parameters}\\).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>eigen_computation_on_gpu</code> <p>If True, tries to execute the eigen pair approximation on the provided device via cupy implementation. Ensure that either your model is small enough, or you use a small rank_estimate to fit your device's memory. If False, the eigen pair approximation is executed on the CPU with scipy's wrapper to ARPACK.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>'LowRankProductRepresentation'</code> <p>LowRankProductRepresentation instance that contains the top (up until rank_estimate) eigenvalues and corresponding eigenvectors of the Hessian.</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def operator_spectral_approximation(\n    operator: \"TensorOperator\",\n    rank: int = 10,\n    krylov_dimension: Optional[int] = None,\n    tol: float = 1e-6,\n    max_iter: Optional[int] = None,\n    eigen_computation_on_gpu: bool = False,\n) -&gt; \"LowRankProductRepresentation\":\n    r\"\"\"\n    Calculates a low-rank approximation of an operator $H$ using the implicitly\n    restarted Lanczos algorithm, i.e.:\n\n    \\[ H_{\\text{approx}} = V D V^T\\]\n\n    where \\(D\\) is a diagonal matrix with the top (in absolute value) `rank`\n    eigenvalues of the Hessian and \\(V\\) contains the corresponding eigenvectors.\n\n    Args:\n        operator: The operator to approximate.\n        rank: The number of eigenvalues and corresponding eigenvectors\n            to compute. Represents the desired rank of the Hessian approximation.\n        krylov_dimension: The number of Krylov vectors to use for the Lanczos\n            method. If not provided, it defaults to\n            \\( \\min(\\text{model.n_parameters},\n                \\max(2 \\times \\text{rank_estimate} + 1, 20)) \\).\n        tol: The stopping criteria for the Lanczos algorithm, which stops when\n            the difference in the approximated eigenvalue is less than `tol`.\n            Defaults to 1e-6.\n        max_iter: The maximum number of iterations for the Lanczos method. If\n            not provided, it defaults to \\( 10 \\cdot \\text{model.n_parameters}\\).\n        eigen_computation_on_gpu: If True, tries to execute the eigen pair\n            approximation on the provided device via [cupy](https://cupy.dev/)\n            implementation. Ensure that either your model is small enough, or you\n            use a small rank_estimate to fit your device's memory. If False, the\n            eigen pair approximation is executed on the CPU with scipy's wrapper to\n            ARPACK.\n\n    Returns:\n        [LowRankProductRepresentation]\n            [pydvl.influence.torch.functional.LowRankProductRepresentation]\n            instance that contains the top (up until rank_estimate) eigenvalues\n            and corresponding eigenvectors of the Hessian.\n    \"\"\"\n\n    if operator.input_size == 1:\n        # in the trivial case, return early\n        eigen_vec = torch.ones((1, 1), dtype=operator.dtype, device=operator.device)\n        eigen_val = operator.apply(eigen_vec).squeeze()\n        return LowRankProductRepresentation(eigen_val, eigen_vec)\n\n    torch_dtype = operator.dtype\n\n    if eigen_computation_on_gpu:\n        try:\n            import cupy as cp\n            from cupyx.scipy.sparse.linalg import LinearOperator, eigsh\n            from torch.utils.dlpack import from_dlpack, to_dlpack\n        except ImportError as e:\n            raise ImportError(\n                \"Missing cupy, check the installation instructions \"\n                \"at https://docs.cupy.dev/en/stable/install.html \"\n                \"or set eigen_computation_on_gpu \"\n                f\"to False: {e}\"\n            )\n\n        def to_torch_conversion_function(x: cp.NDArray) -&gt; torch.Tensor:\n            return from_dlpack(x.toDlpack()).to(torch_dtype)\n\n        def mv(x):\n            x = to_torch_conversion_function(x)\n            y = operator.apply(x)\n            return cp.from_dlpack(to_dlpack(y))\n\n    else:\n        from scipy.sparse.linalg import LinearOperator, eigsh\n\n        def mv(x):\n            x_torch = torch.as_tensor(x, device=operator.device, dtype=torch_dtype)\n            y = operator.apply(x_torch).detach().cpu().numpy()\n            return y\n\n        to_torch_conversion_function = partial(torch.as_tensor, dtype=torch_dtype)\n\n    try:\n        matrix_shape = (operator.input_size, operator.input_size)\n        eigen_vals, eigen_vecs = eigsh(\n            LinearOperator(matrix_shape, matvec=mv),\n            k=min(rank, operator.input_size - 1),\n            maxiter=max_iter,\n            tol=tol,\n            ncv=krylov_dimension,\n            return_eigenvectors=True,\n        )\n\n    except ArpackNoConvergence as e:\n        warnings.warn(\n            f\"ARPACK did not converge for parameters {max_iter=}, {tol=}, \"\n            f\"{krylov_dimension=}, {rank=}. \\n \"\n            f\"Returning the best approximation found so far. \"\n            f\"Use those with care or modify parameters.\\n Original error: {e}\"\n        )\n\n        eigen_vals, eigen_vecs = e.eigenvalues, e.eigenvectors\n\n    eigen_vals = to_torch_conversion_function(eigen_vals)\n    eigen_vecs = to_torch_conversion_function(eigen_vecs)\n\n    return LowRankProductRepresentation(eigen_vals, eigen_vecs)\n</code></pre>"},{"location":"api/pydvl/influence/torch/functional/#pydvl.influence.torch.functional.randomized_nystroem_approximation","title":"randomized_nystroem_approximation","text":"<pre><code>randomized_nystroem_approximation(\n    mat_mat_prod: Union[Tensor, Callable[[Tensor], Tensor]],\n    input_dim: int,\n    rank: int,\n    input_type: dtype,\n    shift_func: Optional[Callable[[Tensor], Tensor]] = None,\n    mat_vec_device: device = device(\"cpu\"),\n) -&gt; LowRankProductRepresentation\n</code></pre> <p>Given a matrix vector product function (representing a symmetric positive definite matrix \\(A\\) ), computes a random Nystr\u00f6m low rank approximation of \\(A\\) in factored form, i.e.</p> \\[ A_{\\text{nys}} = (A \\Omega)(\\Omega^T A \\Omega)^{\\dagger}(A \\Omega)^T = U \\Sigma U^T \\] <p>where \\(\\Omega\\) is a standard normal random matrix.</p> PARAMETER DESCRIPTION <code>mat_mat_prod</code> <p>A callable representing the matrix vector product</p> <p> TYPE: <code>Union[Tensor, Callable[[Tensor], Tensor]]</code> </p> <code>input_dim</code> <p>dimension of the input for the matrix vector product</p> <p> TYPE: <code>int</code> </p> <code>input_type</code> <p>data_type of inputs</p> <p> TYPE: <code>dtype</code> </p> <code>rank</code> <p>rank of the approximation</p> <p> TYPE: <code>int</code> </p> <code>shift_func</code> <p>optional function for computing the stabilizing shift in the construction of the randomized nystroem approximation, defaults to</p> \\[ \\sqrt{\\operatorname{\\text{input_dim}}} \\cdot     \\varepsilon(\\operatorname{\\text{input_type}}) \\cdot \\|A\\Omega\\|_2,\\] <p>where \\(\\varepsilon(\\operatorname{\\text{input_type}})\\) is the value of the machine precision corresponding to the data type.</p> <p> TYPE: <code>Optional[Callable[[Tensor], Tensor]]</code> DEFAULT: <code>None</code> </p> <code>mat_vec_device</code> <p>device where the matrix vector product has to be executed</p> <p> TYPE: <code>device</code> DEFAULT: <code>device('cpu')</code> </p> RETURNS DESCRIPTION <code>LowRankProductRepresentation</code> <p>object containing, \\(U\\) and \\(\\Sigma\\)</p> Source code in <code>src/pydvl/influence/torch/functional.py</code> <pre><code>def randomized_nystroem_approximation(\n    mat_mat_prod: Union[torch.Tensor, Callable[[torch.Tensor], torch.Tensor]],\n    input_dim: int,\n    rank: int,\n    input_type: torch.dtype,\n    shift_func: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n    mat_vec_device: torch.device = torch.device(\"cpu\"),\n) -&gt; LowRankProductRepresentation:\n    r\"\"\"\n    Given a matrix vector product function (representing a symmetric positive definite\n    matrix $A$ ), computes a random Nystr\u00f6m low rank approximation of\n    $A$ in factored form, i.e.\n\n    $$ A_{\\text{nys}} = (A \\Omega)(\\Omega^T A \\Omega)^{\\dagger}(A \\Omega)^T\n    = U \\Sigma U^T $$\n\n    where $\\Omega$ is a standard normal random matrix.\n\n    Args:\n        mat_mat_prod: A callable representing the matrix vector product\n        input_dim: dimension of the input for the matrix vector product\n        input_type: data_type of inputs\n        rank: rank of the approximation\n        shift_func: optional function for computing the stabilizing shift in the\n            construction of the randomized nystroem approximation, defaults to\n\n            $$ \\sqrt{\\operatorname{\\text{input_dim}}} \\cdot\n                \\varepsilon(\\operatorname{\\text{input_type}}) \\cdot \\|A\\Omega\\|_2,$$\n\n            where $\\varepsilon(\\operatorname{\\text{input_type}})$ is the value of the\n            machine precision corresponding to the data type.\n        mat_vec_device: device where the matrix vector product has to be executed\n\n    Returns:\n        object containing, $U$ and $\\Sigma$\n    \"\"\"\n\n    if shift_func is None:\n\n        def shift_func(x: torch.Tensor):\n            return (\n                torch.sqrt(torch.as_tensor(input_dim))\n                * torch.finfo(x.dtype).eps\n                * torch.linalg.norm(x)\n            )\n\n    _mat_mat_prod: Callable[[torch.Tensor], torch.Tensor]\n\n    if isinstance(mat_mat_prod, torch.Tensor):\n\n        def _mat_mat_prod(x: torch.Tensor):\n            return mat_mat_prod @ x\n\n    else:\n        _mat_mat_prod = mat_mat_prod\n\n    random_sample_matrix = torch.randn(\n        input_dim, rank, device=mat_vec_device, dtype=input_type\n    )\n    random_sample_matrix, _ = torch.linalg.qr(random_sample_matrix)\n\n    sketch_mat = _mat_mat_prod(random_sample_matrix)\n\n    shift = shift_func(sketch_mat)\n    sketch_mat += shift * random_sample_matrix\n    cholesky_mat = torch.matmul(random_sample_matrix.t(), sketch_mat)\n    try:\n        triangular_mat = torch.linalg.cholesky(cholesky_mat)\n    except _LinAlgError as e:\n        logger.warning(\n            f\"Encountered error in cholesky decomposition: {e}.\\n \"\n            f\"Increasing shift by smallest eigenvalue and re-compute\"\n        )\n        eigen_vals, eigen_vectors = torch.linalg.eigh(cholesky_mat)\n        shift += torch.abs(torch.min(eigen_vals))\n        eigen_vals += shift\n        triangular_mat = torch.linalg.cholesky(\n            torch.mm(eigen_vectors, torch.mm(torch.diag(eigen_vals), eigen_vectors.T))\n        )\n\n    svd_input = torch.linalg.solve_triangular(\n        triangular_mat.t(), sketch_mat, upper=True, left=False\n    )\n    left_singular_vecs, singular_vals, _ = torch.linalg.svd(\n        svd_input, full_matrices=False\n    )\n    singular_vals = torch.clamp(singular_vals**2 - shift, min=0)\n\n    return LowRankProductRepresentation(singular_vals, left_singular_vecs)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/","title":"Influence function model","text":""},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model","title":"pydvl.influence.torch.influence_function_model","text":"<p>This module implements several implementations of InfluenceFunctionModel utilizing PyTorch.</p>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.ArnoldiInfluence","title":"ArnoldiInfluence","text":"<pre><code>ArnoldiInfluence(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    regularization: Optional[Union[float, Dict[str, Optional[float]]]] = None,\n    rank: int = 10,\n    krylov_dimension: Optional[int] = None,\n    tol: float = 1e-06,\n    max_iter: Optional[int] = None,\n    eigen_computation_on_gpu: bool = False,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = FULL,\n    second_order_mode: SecondOrderMode = HESSIAN,\n    use_woodbury: bool = False,\n)\n</code></pre> <p>               Bases: <code>TorchComposableInfluence[LowRankOperator]</code></p> <p>Solves the linear system Hx = b, where H is the Hessian of the model's loss function and b is the given right-hand side vector. It employs the [implicitly restarted Arnoldi method] (https://en.wikipedia.org/wiki/Arnoldi_iteration) for computing a partial eigen decomposition, which is used fo the inversion i.e.</p> \\[x = V D^{-1} V^T b\\] <p>where \\(D\\) is a diagonal matrix with the top (in absolute value) <code>rank_estimate</code> eigenvalues of the Hessian and \\(V\\) contains the corresponding eigenvectors. For more information, see Arnoldi.</p> PARAMETER DESCRIPTION <code>model</code> <p>A PyTorch model. The Hessian will be calculated with respect to this model's parameters.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that takes the model's output and target as input and returns   the scalar loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>regularization</code> <p>The regularization parameter. In case a dictionary is provided, the keys must be a subset of the block identifiers.</p> <p> TYPE: <code>Optional[Union[float, Dict[str, Optional[float]]]]</code> DEFAULT: <code>None</code> </p> <code>rank</code> <p>The number of eigenvalues and corresponding eigenvectors to compute. Represents the desired rank of the Hessian approximation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>krylov_dimension</code> <p>The number of Krylov vectors to use for the Lanczos method. Defaults to min(model's number of parameters, max(2 times rank + 1, 20)).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>tol</code> <p>The stopping criteria for the Lanczos algorithm.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>max_iter</code> <p>The maximum number of iterations for the Lanczos method.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>eigen_computation_on_gpu</code> <p>If True, tries to execute the eigen pair approximation on the model's device via a cupy implementation. Ensure the model size or rank_estimate is appropriate for device memory. If False, the eigen pair approximation is executed on the CPU by the scipy wrapper to ARPACK.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_woodbury</code> <p>If True, uses the Sherman\u2013Morrison\u2013Woodbury formula for the computation of the inverse action, which is more precise but needs additional computation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    regularization: Optional[Union[float, Dict[str, Optional[float]]]] = None,\n    rank: int = 10,\n    krylov_dimension: Optional[int] = None,\n    tol: float = 1e-6,\n    max_iter: Optional[int] = None,\n    eigen_computation_on_gpu: bool = False,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = BlockMode.FULL,\n    second_order_mode: SecondOrderMode = SecondOrderMode.HESSIAN,\n    use_woodbury: bool = False,\n):\n    super().__init__(model, block_structure, regularization)\n    self.use_woodbury = use_woodbury\n    self.second_order_mode = second_order_mode\n    self.loss = loss\n    self.rank = rank\n    self.tol = tol\n    self.max_iter = max_iter\n    self.krylov_dimension = krylov_dimension\n    self.eigen_computation_on_gpu = eigen_computation_on_gpu\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.fit","title":"fit","text":"<pre><code>fit(data: DataLoaderType) -&gt; InfluenceFunctionModel\n</code></pre> <p>Fitting to provided data, by internally creating a block mapper instance from it. Args:     data: iterable of tensors</p> RETURNS DESCRIPTION <code>InfluenceFunctionModel</code> <p>Fitted instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef fit(self, data: DataLoaderType) -&gt; InfluenceFunctionModel:\n    \"\"\"\n    Fitting to provided data, by internally creating a block mapper instance from\n    it.\n    Args:\n        data: iterable of tensors\n\n    Returns:\n        Fitted instance\n    \"\"\"\n    self.block_mapper = self._create_block_mapper(data)\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: TensorType, y: TensorType) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influence_factors(self, x: TensorType, y: TensorType) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    return self._influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influence_factors_by_block","title":"influence_factors_by_block","text":"<pre><code>influence_factors_by_block(\n    x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise</p> <code>OrderedDict[str, TensorType]</code> <p>approximate inverse Hessian matrix vector products per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influence_factors_by_block(\n    self, x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise\n        approximate inverse Hessian matrix vector products per block.\n\n    \"\"\"\n    return self.block_mapper.transformed_grads(self._create_batch(x, y))\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences","title":"influences","text":"<pre><code>influences(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    if x is None and y is not None:\n        raise ValueError(\n            \"Providing labels y, without providing model input x is not supported\"\n        )\n\n    if x is not None and y is None:\n        raise ValueError(\n            \"Providing model input x, without providing labels y is not supported\"\n        )\n\n    return self._influences(x_test, y_test, x, y, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences_by_block","title":"influences_by_block","text":"<pre><code>influences_by_block(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise influence values for the provided data, i.e. an approximation of</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of the approximation of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_by_block(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise influence values for the provided data, i.e. an\n    approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of the approximation of\n            $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block.\n\n    \"\"\"\n    left_batch = self._create_batch(x_test, y_test)\n\n    if x is None:\n        if y is not None:\n            raise ValueError(\n                \"Providing labels y, without providing model input x \"\n                \"is not supported\"\n            )\n        right_batch = left_batch\n    else:\n        if y is None:\n            raise ValueError(\n                \"Providing model input x, without providing labels y \"\n                \"is not supported\"\n            )\n        right_batch = self._create_batch(x, y)\n\n    return self.block_mapper.interactions(left_batch, right_batch, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors(\n    self,\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    tensors = self.block_mapper.generate_interactions_from_transformed_grads(\n        z_test_factors,\n        self._create_batch(x, y),\n        mode,\n    )\n    result: TensorType = next(tensors)\n    for tensor in tensors:\n        result = result + tensor\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences_from_factors_by_block","title":"influences_from_factors_by_block","text":"<pre><code>influences_from_factors_by_block(\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Block-wise computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>OrderedDict[str, TensorType]</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors_by_block(\n    self,\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Block-wise computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block\n\n    \"\"\"\n    return self.block_mapper.interactions_from_transformed_grads(\n        z_test_factors, self._create_batch(x, y), mode\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.CgInfluence","title":"CgInfluence","text":"<pre><code>CgInfluence(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    regularization: Optional[Union[float, Dict[str, Optional[float]]]] = None,\n    rtol: float = 0.0001,\n    atol: float = 1e-06,\n    maxiter: Optional[int] = None,\n    progress: bool = False,\n    precompute_grad: bool = False,\n    preconditioner: Optional[Preconditioner] = None,\n    solve_simultaneously: bool = False,\n    warn_on_max_iteration: bool = True,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = FULL,\n    second_order_mode: SecondOrderMode = HESSIAN,\n)\n</code></pre> <p>               Bases: <code>TorchComposableInfluence[CgOperator]</code></p> <p>Given a model and training data, it uses conjugate gradient to calculate the inverse of the Hessian Vector Product. More precisely, it finds x such that \\(Hx = b\\), with \\(H\\) being the model hessian. For more info, see Conjugate Gradient.</p> PARAMETER DESCRIPTION <code>model</code> <p>A PyTorch model. The Hessian will be calculated with respect to this model's parameters.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that takes the model's output and target as input and returns   the scalar loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>regularization</code> <p>Optional regularization parameter added to the Hessian-vector product for numerical stability.</p> <p> TYPE: <code>Optional[Union[float, Dict[str, Optional[float]]]]</code> DEFAULT: <code>None</code> </p> <code>rtol</code> <p>Maximum relative tolerance of result.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0001</code> </p> <code>atol</code> <p>Absolute tolerance of result.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>maxiter</code> <p>Maximum number of iterations. If None, defaults to 10*len(b).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, display progress bars for computing in the non-block mode (use_block_cg=False).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>preconditioner</code> <p>Optional preconditioner to improve convergence of conjugate gradient method</p> <p> TYPE: <code>Optional[Preconditioner]</code> DEFAULT: <code>None</code> </p> <code>solve_simultaneously</code> <p>If True, use a variant of conjugate gradient method to simultaneously solve for several right hand sides.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>warn_on_max_iteration</code> <p>If True, logs a warning, if the desired tolerance is not achieved within <code>maxiter</code> iterations. If False, the log level for this information is <code>logging.DEBUG</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>block_structure</code> <p>Union[BlockMode, OrderedDict[str, List[str]]] = BlockMode.FULL,</p> <p> TYPE: <code>Union[BlockMode, OrderedDict[str, List[str]]]</code> DEFAULT: <code>FULL</code> </p> <code>second_order_mode</code> <p>SecondOrderMode = SecondOrderMode.HESSIAN,</p> <p> TYPE: <code>SecondOrderMode</code> DEFAULT: <code>HESSIAN</code> </p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    regularization: Optional[Union[float, Dict[str, Optional[float]]]] = None,\n    rtol: float = 1e-4,\n    atol: float = 1e-6,\n    maxiter: Optional[int] = None,\n    progress: bool = False,\n    precompute_grad: bool = False,\n    preconditioner: Optional[Preconditioner] = None,\n    solve_simultaneously: bool = False,\n    warn_on_max_iteration: bool = True,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = BlockMode.FULL,\n    second_order_mode: SecondOrderMode = SecondOrderMode.HESSIAN,\n):\n    super().__init__(model, block_structure, regularization)\n    self.loss = loss\n    self.warn_on_max_iteration = warn_on_max_iteration\n    self.solve_simultaneously = solve_simultaneously\n    self.preconditioner = preconditioner\n    self.precompute_grad = precompute_grad\n    self.progress = progress\n    self.maxiter = maxiter\n    self.atol = atol\n    self.rtol = rtol\n    self.second_order_mode = second_order_mode\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.CgInfluence.fit","title":"fit","text":"<pre><code>fit(data: DataLoaderType) -&gt; InfluenceFunctionModel\n</code></pre> <p>Fitting to provided data, by internally creating a block mapper instance from it. Args:     data: iterable of tensors</p> RETURNS DESCRIPTION <code>InfluenceFunctionModel</code> <p>Fitted instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef fit(self, data: DataLoaderType) -&gt; InfluenceFunctionModel:\n    \"\"\"\n    Fitting to provided data, by internally creating a block mapper instance from\n    it.\n    Args:\n        data: iterable of tensors\n\n    Returns:\n        Fitted instance\n    \"\"\"\n    self.block_mapper = self._create_block_mapper(data)\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.CgInfluence.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.CgInfluence.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: TensorType, y: TensorType) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influence_factors(self, x: TensorType, y: TensorType) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    return self._influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.CgInfluence.influence_factors_by_block","title":"influence_factors_by_block","text":"<pre><code>influence_factors_by_block(\n    x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise</p> <code>OrderedDict[str, TensorType]</code> <p>approximate inverse Hessian matrix vector products per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influence_factors_by_block(\n    self, x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise\n        approximate inverse Hessian matrix vector products per block.\n\n    \"\"\"\n    return self.block_mapper.transformed_grads(self._create_batch(x, y))\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.CgInfluence.influences","title":"influences","text":"<pre><code>influences(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    if x is None and y is not None:\n        raise ValueError(\n            \"Providing labels y, without providing model input x is not supported\"\n        )\n\n    if x is not None and y is None:\n        raise ValueError(\n            \"Providing model input x, without providing labels y is not supported\"\n        )\n\n    return self._influences(x_test, y_test, x, y, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.CgInfluence.influences_by_block","title":"influences_by_block","text":"<pre><code>influences_by_block(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise influence values for the provided data, i.e. an approximation of</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of the approximation of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_by_block(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise influence values for the provided data, i.e. an\n    approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of the approximation of\n            $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block.\n\n    \"\"\"\n    left_batch = self._create_batch(x_test, y_test)\n\n    if x is None:\n        if y is not None:\n            raise ValueError(\n                \"Providing labels y, without providing model input x \"\n                \"is not supported\"\n            )\n        right_batch = left_batch\n    else:\n        if y is None:\n            raise ValueError(\n                \"Providing model input x, without providing labels y \"\n                \"is not supported\"\n            )\n        right_batch = self._create_batch(x, y)\n\n    return self.block_mapper.interactions(left_batch, right_batch, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.CgInfluence.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors(\n    self,\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    tensors = self.block_mapper.generate_interactions_from_transformed_grads(\n        z_test_factors,\n        self._create_batch(x, y),\n        mode,\n    )\n    result: TensorType = next(tensors)\n    for tensor in tensors:\n        result = result + tensor\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.CgInfluence.influences_from_factors_by_block","title":"influences_from_factors_by_block","text":"<pre><code>influences_from_factors_by_block(\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Block-wise computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>OrderedDict[str, TensorType]</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors_by_block(\n    self,\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Block-wise computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block\n\n    \"\"\"\n    return self.block_mapper.interactions_from_transformed_grads(\n        z_test_factors, self._create_batch(x, y), mode\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.CgInfluence.with_regularization","title":"with_regularization","text":"<pre><code>with_regularization(\n    regularization: Union[float, Dict[str, Optional[float]]]\n) -&gt; TorchComposableInfluence\n</code></pre> <p>Update the regularization parameter. Args:     regularization: Either a positive float or a dictionary with the         block names as keys and the regularization values as values.</p> RETURNS DESCRIPTION <code>TorchComposableInfluence</code> <p>The modified instance</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def with_regularization(\n    self, regularization: Union[float, Dict[str, Optional[float]]]\n) -&gt; TorchComposableInfluence:\n    \"\"\"\n    Update the regularization parameter.\n    Args:\n        regularization: Either a positive float or a dictionary with the\n            block names as keys and the regularization values as values.\n\n    Returns:\n        The modified instance\n\n    \"\"\"\n    self._regularization_dict = self._build_regularization_dict(regularization)\n    for k, reg in self._regularization_dict.items():\n        self.block_mapper.composable_block_dict[k].op.regularization = reg\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.DirectInfluence","title":"DirectInfluence","text":"<pre><code>DirectInfluence(\n    model: Module,\n    loss: LossType,\n    regularization: Optional[Union[float, Dict[str, Optional[float]]]] = None,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = FULL,\n    second_order_mode: SecondOrderMode = HESSIAN,\n)\n</code></pre> <p>               Bases: <code>TorchComposableInfluence[DirectSolveOperator]</code></p> <p>Given a model and training data, it finds x such that \\(Hx = b\\), with \\(H\\) being the model hessian or Gauss-Newton matrix.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>The loss function.</p> <p> TYPE: <code>LossType</code> </p> <code>regularization</code> <p>The regularization parameter. In case a dictionary is provided, the keys must be a subset of the block identifiers.</p> <p> TYPE: <code>Optional[Union[float, Dict[str, Optional[float]]]]</code> DEFAULT: <code>None</code> </p> <code>block_structure</code> <p>The blocking structure, either a pre-defined enum or a custom block structure, see the information regarding block-diagonal approximation.</p> <p> TYPE: <code>Union[BlockMode, OrderedDict[str, List[str]]]</code> DEFAULT: <code>FULL</code> </p> <code>second_order_mode</code> <p>The second order mode, either <code>SecondOrderMode.HESSIAN</code> or <code>SecondOrderMode.GAUSS_NEWTON</code>.</p> <p> TYPE: <code>SecondOrderMode</code> DEFAULT: <code>HESSIAN</code> </p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    loss: LossType,\n    regularization: Optional[Union[float, Dict[str, Optional[float]]]] = None,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = BlockMode.FULL,\n    second_order_mode: SecondOrderMode = SecondOrderMode.HESSIAN,\n):\n    super().__init__(\n        model,\n        block_structure=block_structure,\n        regularization=regularization,\n    )\n    self.second_order_mode = second_order_mode\n    self.loss = loss\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.DirectInfluence.fit","title":"fit","text":"<pre><code>fit(data: DataLoaderType) -&gt; InfluenceFunctionModel\n</code></pre> <p>Fitting to provided data, by internally creating a block mapper instance from it. Args:     data: iterable of tensors</p> RETURNS DESCRIPTION <code>InfluenceFunctionModel</code> <p>Fitted instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef fit(self, data: DataLoaderType) -&gt; InfluenceFunctionModel:\n    \"\"\"\n    Fitting to provided data, by internally creating a block mapper instance from\n    it.\n    Args:\n        data: iterable of tensors\n\n    Returns:\n        Fitted instance\n    \"\"\"\n    self.block_mapper = self._create_block_mapper(data)\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.DirectInfluence.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.DirectInfluence.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: TensorType, y: TensorType) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influence_factors(self, x: TensorType, y: TensorType) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    return self._influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.DirectInfluence.influence_factors_by_block","title":"influence_factors_by_block","text":"<pre><code>influence_factors_by_block(\n    x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise</p> <code>OrderedDict[str, TensorType]</code> <p>approximate inverse Hessian matrix vector products per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influence_factors_by_block(\n    self, x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise\n        approximate inverse Hessian matrix vector products per block.\n\n    \"\"\"\n    return self.block_mapper.transformed_grads(self._create_batch(x, y))\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.DirectInfluence.influences","title":"influences","text":"<pre><code>influences(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    if x is None and y is not None:\n        raise ValueError(\n            \"Providing labels y, without providing model input x is not supported\"\n        )\n\n    if x is not None and y is None:\n        raise ValueError(\n            \"Providing model input x, without providing labels y is not supported\"\n        )\n\n    return self._influences(x_test, y_test, x, y, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.DirectInfluence.influences_by_block","title":"influences_by_block","text":"<pre><code>influences_by_block(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise influence values for the provided data, i.e. an approximation of</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of the approximation of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_by_block(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise influence values for the provided data, i.e. an\n    approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of the approximation of\n            $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block.\n\n    \"\"\"\n    left_batch = self._create_batch(x_test, y_test)\n\n    if x is None:\n        if y is not None:\n            raise ValueError(\n                \"Providing labels y, without providing model input x \"\n                \"is not supported\"\n            )\n        right_batch = left_batch\n    else:\n        if y is None:\n            raise ValueError(\n                \"Providing model input x, without providing labels y \"\n                \"is not supported\"\n            )\n        right_batch = self._create_batch(x, y)\n\n    return self.block_mapper.interactions(left_batch, right_batch, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.DirectInfluence.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors(\n    self,\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    tensors = self.block_mapper.generate_interactions_from_transformed_grads(\n        z_test_factors,\n        self._create_batch(x, y),\n        mode,\n    )\n    result: TensorType = next(tensors)\n    for tensor in tensors:\n        result = result + tensor\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.DirectInfluence.influences_from_factors_by_block","title":"influences_from_factors_by_block","text":"<pre><code>influences_from_factors_by_block(\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Block-wise computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>OrderedDict[str, TensorType]</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors_by_block(\n    self,\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Block-wise computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block\n\n    \"\"\"\n    return self.block_mapper.interactions_from_transformed_grads(\n        z_test_factors, self._create_batch(x, y), mode\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.DirectInfluence.with_regularization","title":"with_regularization","text":"<pre><code>with_regularization(\n    regularization: Union[float, Dict[str, Optional[float]]]\n) -&gt; TorchComposableInfluence\n</code></pre> <p>Update the regularization parameter. Args:     regularization: Either a positive float or a dictionary with the         block names as keys and the regularization values as values.</p> RETURNS DESCRIPTION <code>TorchComposableInfluence</code> <p>The modified instance</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def with_regularization(\n    self, regularization: Union[float, Dict[str, Optional[float]]]\n) -&gt; TorchComposableInfluence:\n    \"\"\"\n    Update the regularization parameter.\n    Args:\n        regularization: Either a positive float or a dictionary with the\n            block names as keys and the regularization values as values.\n\n    Returns:\n        The modified instance\n\n    \"\"\"\n    self._regularization_dict = self._build_regularization_dict(regularization)\n    for k, reg in self._regularization_dict.items():\n        self.block_mapper.composable_block_dict[k].op.regularization = reg\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence","title":"EkfacInfluence","text":"<pre><code>EkfacInfluence(\n    model: Module,\n    update_diagonal: bool = False,\n    hessian_regularization: float = 0.0,\n    progress: bool = False,\n)\n</code></pre> <p>               Bases: <code>TorchInfluenceFunctionModel</code></p> <p>Approximately solves the linear system Hx = b, where H is the Hessian of a model with the empirical categorical cross entropy as loss function and b is the given right-hand side vector. It employs the EK-FAC method, which is based on the kronecker factorization of the Hessian.</p> <p>Contrary to the other influence function methods, this implementation can only be used for classification tasks with a cross entropy loss function. However, it is much faster than the other methods and can be used efficiently for very large datasets and models. For more information, see Eigenvalue Corrected K-FAC.</p> PARAMETER DESCRIPTION <code>model</code> <p>A PyTorch model. The Hessian will be calculated with respect to this model's parameters.</p> <p> TYPE: <code>Module</code> </p> <code>update_diagonal</code> <p>If True, the diagonal values in the ekfac representation are refitted from the training data after calculating the KFAC blocks. This provides a more accurate approximation of the Hessian, but it is computationally more expensive.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>hessian_regularization</code> <p>Regularization of the hessian.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>progress</code> <p>If True, display progress bars.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    update_diagonal: bool = False,\n    hessian_regularization: float = 0.0,\n    progress: bool = False,\n):\n    super().__init__(model, torch.nn.functional.cross_entropy)\n    self.hessian_regularization = hessian_regularization\n    self.update_diagonal = update_diagonal\n    self.active_layers = self._parse_active_layers()\n    self.progress = progress\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence._get_kfac_blocks","title":"_get_kfac_blocks","text":"<pre><code>_get_kfac_blocks(\n    data: DataLoader,\n) -&gt; Tuple[Dict[str, Tensor], Dict[str, Tensor]]\n</code></pre> <p>Compute the KFAC blocks for each layer of the model, using the provided data. Returns the average forward and backward KFAC blocks for each layer in dictionaries.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def _get_kfac_blocks(\n    self,\n    data: DataLoader,\n) -&gt; Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n    \"\"\"\n    Compute the KFAC blocks for each layer of the model, using the provided data.\n    Returns the average forward and backward KFAC blocks for each layer in\n    dictionaries.\n    \"\"\"\n    forward_x = {}\n    grad_y = {}\n    hooks = []\n    data_len = 0\n\n    for m_name, module in self.active_layers.items():\n        forward_x[m_name], grad_y[m_name] = self._init_layer_kfac_blocks(module)\n        layer_input_hook, layer_grad_hook = self._get_layer_kfac_hooks(\n            m_name, module, forward_x, grad_y\n        )\n        hooks.append(module.register_forward_hook(layer_input_hook))\n        hooks.append(module.register_full_backward_hook(layer_grad_hook))\n\n    for x, *_ in tqdm(\n        data, disable=not self.progress, desc=\"K-FAC blocks - batch progress\"\n    ):\n        data_len += x.shape[0]\n        pred_y = self.model(x.to(self.model_device))\n        loss = empirical_cross_entropy_loss_fn(pred_y)\n        loss.backward()\n\n    for key in forward_x.keys():\n        forward_x[key] /= data_len\n        grad_y[key] /= data_len\n\n    for hook in hooks:\n        hook.remove()\n\n    return forward_x, grad_y\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence._get_layer_diag_hooks","title":"_get_layer_diag_hooks","text":"<pre><code>_get_layer_diag_hooks(\n    m_name: str,\n    module: Module,\n    last_x_kfe: Dict[str, Tensor],\n    diags: Dict[str, Tensor],\n) -&gt; Tuple[Callable, Callable]\n</code></pre> <p>Create the hooks that will be used to update the diagonal values of the layer. The hooks are registered to the layer and will be called during the forward and backward passes. At each pass, the hooks will update the tensor that stores the updated diagonal values of the layer. This tensor is stored in the diags dictionary.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def _get_layer_diag_hooks(\n    self,\n    m_name: str,\n    module: torch.nn.Module,\n    last_x_kfe: Dict[str, torch.Tensor],\n    diags: Dict[str, torch.Tensor],\n) -&gt; Tuple[Callable, Callable]:\n    \"\"\"\n    Create the hooks that will be used to update the diagonal values of the layer.\n    The hooks are registered to the layer and will be called during the forward and\n    backward passes. At each pass, the hooks will update the tensor that stores the\n    updated diagonal values of the layer. This tensor is stored in the diags\n    dictionary.\n    \"\"\"\n    evecs_a, evecs_g = self.ekfac_representation.get_layer_evecs()\n    if isinstance(module, nn.Linear):\n        with_bias = module.bias is not None\n\n        def input_hook(m, x, y):\n            x = x[0].reshape(-1, module.in_features)\n            if with_bias:\n                x = torch.cat(\n                    (x, torch.ones((x.shape[0], 1), device=module.weight.device)),\n                    dim=1,\n                )\n            last_x_kfe[m_name] = torch.mm(x, evecs_a[m_name])\n\n        def grad_hook(m, m_grad, m_out):\n            m_out = m_out[0].reshape(-1, module.out_features)\n            gy_kfe = torch.mm(m_out, evecs_g[m_name])\n            diags[m_name] += torch.mm(\n                gy_kfe.t() ** 2, last_x_kfe[m_name] ** 2\n            ).view(-1)\n\n    else:\n        raise NotImplementedLayerRepresentationException(module_id=str(module))\n    return input_hook, grad_hook\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence._get_layer_kfac_hooks","title":"_get_layer_kfac_hooks  <code>staticmethod</code>","text":"<pre><code>_get_layer_kfac_hooks(\n    m_name: str,\n    module: Module,\n    forward_x: Dict[str, Tensor],\n    grad_y: Dict[str, Tensor],\n) -&gt; Tuple[Callable, Callable]\n</code></pre> <p>Create the hooks that will be used to compute the forward and backward KFAC blocks for the layer. The hooks are registered to the layer and will be called during the forward and backward passes. At each pass, the hooks will update the tensors that store the cumulative forward and backward KFAC blocks for the layer. These tensors are stored in the forward_x and grad_y dictionaries.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>@staticmethod\ndef _get_layer_kfac_hooks(\n    m_name: str,\n    module: torch.nn.Module,\n    forward_x: Dict[str, torch.Tensor],\n    grad_y: Dict[str, torch.Tensor],\n) -&gt; Tuple[Callable, Callable]:\n    \"\"\"\n    Create the hooks that will be used to compute the forward and backward KFAC\n    blocks for the layer. The hooks are registered to the layer and will be called\n    during the forward and backward passes. At each pass, the hooks will update the\n    tensors that store the cumulative forward and backward KFAC blocks for the layer.\n    These tensors are stored in the forward_x and grad_y dictionaries.\n    \"\"\"\n    if isinstance(module, nn.Linear):\n        with_bias = module.bias is not None\n\n        def input_hook(m, x, y):\n            x = x[0].reshape(-1, module.in_features)\n            if with_bias:\n                x = torch.cat(\n                    (x, torch.ones((x.shape[0], 1), device=module.weight.device)),\n                    dim=1,\n                )\n            forward_x[m_name] += torch.mm(x.t(), x)\n\n        def grad_hook(m, m_grad, m_out):\n            m_out = m_out[0].reshape(-1, module.out_features)\n            grad_y[m_name] += torch.mm(m_out.t(), m_out)\n\n    else:\n        raise NotImplementedLayerRepresentationException(module_id=str(module))\n    return input_hook, grad_hook\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence._init_layer_diag","title":"_init_layer_diag  <code>staticmethod</code>","text":"<pre><code>_init_layer_diag(module: Module) -&gt; Tensor\n</code></pre> <p>Initialize the tensor that will store the updated diagonal values of the layer.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>@staticmethod\ndef _init_layer_diag(module: torch.nn.Module) -&gt; torch.Tensor:\n    \"\"\"\n    Initialize the tensor that will store the updated diagonal values of the layer.\n    \"\"\"\n    if isinstance(module, nn.Linear):\n        with_bias = module.bias is not None\n        sG = module.out_features\n        sA = module.in_features + int(with_bias)\n        layer_diag = torch.zeros((sA * sG), device=module.weight.device)\n    else:\n        raise NotImplementedLayerRepresentationException(module_id=str(module))\n    return layer_diag\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence._init_layer_kfac_blocks","title":"_init_layer_kfac_blocks  <code>staticmethod</code>","text":"<pre><code>_init_layer_kfac_blocks(module: Module) -&gt; Tuple[Tensor, Tensor]\n</code></pre> <p>Initialize the tensors that will store the cumulative forward and backward KFAC blocks for the layer.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>@staticmethod\ndef _init_layer_kfac_blocks(\n    module: torch.nn.Module,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Initialize the tensors that will store the cumulative forward and\n    backward KFAC blocks for the layer.\n    \"\"\"\n    if isinstance(module, nn.Linear):\n        with_bias = module.bias is not None\n        sG = module.out_features\n        sA = module.in_features + int(with_bias)\n        forward_x_layer = torch.zeros((sA, sA), device=module.weight.device)\n        grad_y_layer = torch.zeros((sG, sG), device=module.weight.device)\n    else:\n        raise NotImplementedLayerRepresentationException(module_id=str(module))\n    return forward_x_layer, grad_y_layer\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence._non_symmetric_values_by_layer","title":"_non_symmetric_values_by_layer","text":"<pre><code>_non_symmetric_values_by_layer(\n    x_test: Tensor,\n    y_test: Tensor,\n    x: Tensor,\n    y: Tensor,\n    mode: InfluenceMode = Up,\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>Similar to <code>_non_symmetric_values</code>, but computes the influence for each layer separately. Returns a dictionary containing the influence for each layer, with the layer name as key.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def _non_symmetric_values_by_layer(\n    self,\n    x_test: torch.Tensor,\n    y_test: torch.Tensor,\n    x: torch.Tensor,\n    y: torch.Tensor,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Similar to `_non_symmetric_values`, but computes the influence for each\n    layer separately. Returns a dictionary containing the influence for each\n    layer, with the layer name as key.\n    \"\"\"\n    if mode == InfluenceMode.Up:\n        if x_test.shape[0] &lt;= x.shape[0]:\n            fac = self.influence_factors_by_layer(x_test, y_test)\n            values = self.influences_from_factors_by_layer(fac, x, y, mode=mode)\n        else:\n            fac = self.influence_factors_by_layer(x, y)\n            values = self.influences_from_factors_by_layer(\n                fac, x_test, y_test, mode=mode\n            )\n    elif mode == InfluenceMode.Perturbation:\n        fac = self.influence_factors_by_layer(x_test, y_test)\n        values = self.influences_from_factors_by_layer(fac, x, y, mode=mode)\n    else:\n        raise UnsupportedInfluenceModeException(mode)\n    return values\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence._parse_active_layers","title":"_parse_active_layers","text":"<pre><code>_parse_active_layers() -&gt; Dict[str, Module]\n</code></pre> <p>Find all layers of the model that have parameters that require grad and return them in a dictionary. If a layer has some parameters that require grad and some that do not, raise an error.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def _parse_active_layers(self) -&gt; Dict[str, torch.nn.Module]:\n    \"\"\"\n    Find all layers of the model that have parameters that require grad\n    and return them in a dictionary. If a layer has some parameters that require\n    grad and some that do not, raise an error.\n    \"\"\"\n    active_layers: Dict[str, torch.nn.Module] = {}\n    for m_name, module in self.model.named_modules():\n        if len(list(module.children())) == 0 and len(list(module.parameters())) &gt; 0:\n            layer_requires_grad = [\n                param.requires_grad for param in module.parameters()\n            ]\n            if all(layer_requires_grad):\n                active_layers[m_name] = module\n            elif any(layer_requires_grad):\n                raise ValueError(\n                    f\"Layer {m_name} has some parameters that require grad and some that do not.\"\n                    f\"This is not supported. Please set all parameters of the layer to require grad.\"\n                )\n    return active_layers\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence._solve_hvp_by_layer","title":"_solve_hvp_by_layer  <code>staticmethod</code>","text":"<pre><code>_solve_hvp_by_layer(\n    rhs: Tensor,\n    ekfac_representation: EkfacRepresentation,\n    hessian_regularization: float,\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>Compute the Hessian Vector Product for each layer of the model, using the provided ekfac representation and hessian regularization. It returns a dictionary containing the Hessian Vector Product for each layer.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>@staticmethod\ndef _solve_hvp_by_layer(\n    rhs: torch.Tensor,\n    ekfac_representation: EkfacRepresentation,\n    hessian_regularization: float,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Compute the Hessian Vector Product for each layer of the model, using the\n    provided ekfac representation and hessian regularization. It returns a\n    dictionary containing the Hessian Vector Product for each layer.\n    \"\"\"\n    hvp_layers = {}\n    start_idx = 0\n    for layer_id, (_, evecs_a, evecs_g, diag) in ekfac_representation:\n        end_idx = start_idx + diag.shape[0]\n        rhs_layer = rhs[:, start_idx : end_idx - evecs_g.shape[0]].reshape(\n            rhs.shape[0], evecs_g.shape[0], -1\n        )\n        bias_layer_b = rhs[:, end_idx - evecs_g.shape[0] : end_idx]\n        rhs_layer = torch.cat([rhs_layer, bias_layer_b.unsqueeze(2)], dim=2)\n        v_kfe = torch.einsum(\n            \"bij,jk-&gt;bik\",\n            torch.einsum(\"ij,bjk-&gt;bik\", evecs_g.t(), rhs_layer),\n            evecs_a,\n        )\n        inv_diag = 1 / (diag.reshape(*v_kfe.shape[1:]) + hessian_regularization)\n        inv_kfe = torch.einsum(\"bij,ij-&gt;bij\", v_kfe, inv_diag)\n        inv = torch.einsum(\n            \"bij,jk-&gt;bik\",\n            torch.einsum(\"ij,bjk-&gt;bik\", evecs_g, inv_kfe),\n            evecs_a.t(),\n        )\n        hvp_layers[layer_id] = torch.cat(\n            [inv[:, :, :-1].reshape(rhs.shape[0], -1), inv[:, :, -1]], dim=1\n        )\n        start_idx = end_idx\n    return hvp_layers\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence._symmetric_values_by_layer","title":"_symmetric_values_by_layer","text":"<pre><code>_symmetric_values_by_layer(\n    x: Tensor, y: Tensor, mode: InfluenceMode\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>Similar to <code>_symmetric_values</code>, but computes the influence for each layer separately. Returns a dictionary containing the influence for each layer, with the layer name as key.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def _symmetric_values_by_layer(\n    self, x: torch.Tensor, y: torch.Tensor, mode: InfluenceMode\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Similar to `_symmetric_values`, but computes the influence for each layer\n    separately. Returns a dictionary containing the influence for each layer,\n    with the layer name as key.\n    \"\"\"\n    grad = self._loss_grad(x, y)\n    fac = self._solve_hvp_by_layer(\n        grad, self.ekfac_representation, self.hessian_regularization\n    )\n\n    if mode == InfluenceMode.Up:\n        values = {}\n        start_idx = 0\n        for layer_id, layer_fac in fac.items():\n            end_idx = start_idx + layer_fac.shape[1]\n            values[layer_id] = layer_fac @ grad[:, start_idx:end_idx].T\n            start_idx = end_idx\n    elif mode == InfluenceMode.Perturbation:\n        values = self.influences_from_factors_by_layer(fac, x, y, mode=mode)\n    else:\n        raise UnsupportedInfluenceModeException(mode)\n    return values\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence._update_diag","title":"_update_diag","text":"<pre><code>_update_diag(data: DataLoader) -&gt; EkfacInfluence\n</code></pre> <p>Compute the updated diagonal values for each layer of the model, using the provided data. It then updates the EkfacRepresentation object that stores the KFAC blocks for each layer, their eigenvalue decomposition and diagonal values.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def _update_diag(\n    self,\n    data: DataLoader,\n) -&gt; EkfacInfluence:\n    \"\"\"\n    Compute the updated diagonal values for each layer of the model, using the\n    provided data. It then updates the EkfacRepresentation object that stores the\n    KFAC blocks for each layer, their eigenvalue decomposition and diagonal values.\n    \"\"\"\n    if not self.is_fitted:\n        raise ValueError(\n            \"EkfacInfluence must be fitted before updating the diagonal.\"\n        )\n    diags = {}\n    last_x_kfe: Dict[str, torch.Tensor] = {}\n    hooks = []\n    data_len = 0\n\n    for m_name, module in self.active_layers.items():\n        diags[m_name] = self._init_layer_diag(module)\n        input_hook, grad_hook = self._get_layer_diag_hooks(\n            m_name, module, last_x_kfe, diags\n        )\n        hooks.append(module.register_forward_hook(input_hook))\n        hooks.append(module.register_full_backward_hook(grad_hook))\n\n    for x, *_ in tqdm(\n        data, disable=not self.progress, desc=\"Update Diagonal - batch progress\"\n    ):\n        data_len += x.shape[0]\n        pred_y = self.model(x.to(self.model_device))\n        loss = empirical_cross_entropy_loss_fn(pred_y)\n        loss.backward()\n\n    for key in diags.keys():\n        diags[key] /= data_len\n\n    for hook in hooks:\n        hook.remove()\n\n    self.ekfac_representation = EkfacRepresentation(\n        self.ekfac_representation.layer_names,\n        self.ekfac_representation.layers_module,\n        self.ekfac_representation.evecs_a,\n        self.ekfac_representation.evecs_g,\n        diags.values(),\n    )\n\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence.explore_hessian_regularization","title":"explore_hessian_regularization","text":"<pre><code>explore_hessian_regularization(\n    x: Tensor, y: Tensor, regularization_values: List[float]\n) -&gt; Dict[float, Dict[str, Tensor]]\n</code></pre> <p>Efficiently computes the influence for input x and label y for each layer of the model, for different values of the hessian regularization parameter. This is done by computing the gradient of the loss function for the input x and label y only once and then solving the Hessian Vector Product for each regularization value. This is useful for finding the optimal regularization value and for exploring how robust the influence values are to changes in the regularization value.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Tensor</code> </p> <code>regularization_values</code> <p>list of regularization values to use</p> <p> TYPE: <code>List[float]</code> </p> RETURNS DESCRIPTION <code>Dict[float, Dict[str, Tensor]]</code> <p>A dictionary containing with keys being the regularization values and values</p> <code>Dict[float, Dict[str, Tensor]]</code> <p>being dictionaries containing the influences for each layer of the model,</p> <code>Dict[float, Dict[str, Tensor]]</code> <p>with the layer name as key.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def explore_hessian_regularization(\n    self,\n    x: torch.Tensor,\n    y: torch.Tensor,\n    regularization_values: List[float],\n) -&gt; Dict[float, Dict[str, torch.Tensor]]:\n    \"\"\"\n    Efficiently computes the influence for input x and label y for each layer of the\n    model, for different values of the hessian regularization parameter. This is done\n    by computing the gradient of the loss function for the input x and label y only once\n    and then solving the Hessian Vector Product for each regularization value. This is\n    useful for finding the optimal regularization value and for exploring\n    how robust the influence values are to changes in the regularization value.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n        regularization_values: list of regularization values to use\n\n    Returns:\n        A dictionary containing with keys being the regularization values and values\n        being dictionaries containing the influences for each layer of the model,\n        with the layer name as key.\n    \"\"\"\n    grad = self._loss_grad(x.to(self.model_device), y.to(self.model_device))\n    influences_by_reg_value = {}\n    for reg_value in regularization_values:\n        reg_factors = self._solve_hvp_by_layer(\n            grad, self.ekfac_representation, reg_value\n        )\n        values = {}\n        start_idx = 0\n        for layer_id, layer_fac in reg_factors.items():\n            end_idx = start_idx + layer_fac.shape[1]\n            values[layer_id] = layer_fac @ grad[:, start_idx:end_idx].T\n            start_idx = end_idx\n        influences_by_reg_value[reg_value] = values\n    return influences_by_reg_value\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence.fit","title":"fit","text":"<pre><code>fit(data: DataLoader) -&gt; EkfacInfluence\n</code></pre> <p>Compute the KFAC blocks for each layer of the model, using the provided data. It then creates an EkfacRepresentation object that stores the KFAC blocks for each layer, their eigenvalue decomposition and diagonal values.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef fit(self, data: DataLoader) -&gt; EkfacInfluence:\n    \"\"\"\n    Compute the KFAC blocks for each layer of the model, using the provided data.\n    It then creates an EkfacRepresentation object that stores the KFAC blocks for\n    each layer, their eigenvalue decomposition and diagonal values.\n    \"\"\"\n    forward_x, grad_y = self._get_kfac_blocks(data)\n    layers_evecs_a = {}\n    layers_evect_g = {}\n    layers_diags = {}\n    for key in self.active_layers.keys():\n        evals_a, evecs_a = safe_torch_linalg_eigh(forward_x[key])\n        evals_g, evecs_g = safe_torch_linalg_eigh(grad_y[key])\n        layers_evecs_a[key] = evecs_a\n        layers_evect_g[key] = evecs_g\n        layers_diags[key] = torch.kron(evals_g.view(-1, 1), evals_a.view(-1, 1))\n\n    self.ekfac_representation = EkfacRepresentation(\n        self.active_layers.keys(),\n        self.active_layers.values(),\n        layers_evecs_a.values(),\n        layers_evect_g.values(),\n        layers_diags.values(),\n    )\n    if self.update_diagonal:\n        self._update_diag(data)\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: Tensor, y: Tensor) -&gt; Tensor\n</code></pre> <p>Compute approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension (in case, you want to provide a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def influence_factors(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension (in case, you want to provide\n    a single sample z, call z.unsqueeze(0) if no batch dimension is present).\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    return super().influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence.influence_factors_by_layer","title":"influence_factors_by_layer","text":"<pre><code>influence_factors_by_layer(x: Tensor, y: Tensor) -&gt; Dict[str, Tensor]\n</code></pre> <p>Computes the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>for each layer of the model separately.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dictionary containing the influence factors for each layer of the model,</p> <code>Dict[str, Tensor]</code> <p>with the layer name as key.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def influence_factors_by_layer(\n    self,\n    x: torch.Tensor,\n    y: torch.Tensor,\n) -&gt; Dict[str, torch.Tensor]:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    for each layer of the model separately.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        A dictionary containing the influence factors for each layer of the model,\n        with the layer name as key.\n    \"\"\"\n    if not self.is_fitted:\n        raise ValueError(\n            \"Instance must be fitted before calling influence methods on it\"\n        )\n\n    return self._solve_hvp_by_layer(\n        self._loss_grad(x.to(self.model_device), y.to(self.model_device)),\n        self.ekfac_representation,\n        self.hessian_regularization,\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences","title":"influences","text":"<pre><code>influences(\n    x_test: Tensor,\n    y_test: Tensor,\n    x: Optional[Tensor] = None,\n    y: Optional[Tensor] = None,\n    mode: InfluenceMode = Up,\n) -&gt; Tensor\n</code></pre> <p>Compute the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}})), \\nabla_{\\theta} \\ell(y, f_{\\theta}(x))\\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. For all input tensors it is assumed, that the first dimension is the batch dimension (in case, you want to provide a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>Tensor</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Tensor</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: torch.Tensor,\n    y_test: torch.Tensor,\n    x: Optional[torch.Tensor] = None,\n    y: Optional[torch.Tensor] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the approximation of\n\n    \\[\n    \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n    f_{\\theta}(x_{\\text{test}})), \\nabla_{\\theta} \\ell(y, f_{\\theta}(x))\\rangle\n    \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[\n    \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle\n    \\]\n\n    for the perturbation type influence case. For all input tensors it is assumed,\n    that the first dimension is the batch dimension (in case, you want to provide\n    a single sample z, call z.unsqueeze(0) if no batch dimension is present).\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n                f_{\\theta}(x_{\\text{test}}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    t: torch.Tensor = super().influences(x_test, y_test, x, y, mode=mode)\n    return t\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_by_layer","title":"influences_by_layer","text":"<pre><code>influences_by_layer(\n    x_test: Tensor,\n    y_test: Tensor,\n    x: Optional[Tensor] = None,\n    y: Optional[Tensor] = None,\n    mode: InfluenceMode = Up,\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>Compute the influence of the data on the test data for each layer of the model.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>Tensor</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Tensor</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dictionary containing the influence of the data on the test data for each</p> <code>Dict[str, Tensor]</code> <p>layer of the model, with the layer name as key.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def influences_by_layer(\n    self,\n    x_test: torch.Tensor,\n    y_test: torch.Tensor,\n    x: Optional[torch.Tensor] = None,\n    y: Optional[torch.Tensor] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; Dict[str, torch.Tensor]:\n    r\"\"\"\n    Compute the influence of the data on the test data for each layer of the model.\n\n    Args:\n        x_test: model input to use in the gradient computations of\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n                f_{\\theta}(x_{\\text{test}}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        A dictionary containing the influence of the data on the test data for each\n        layer of the model, with the layer name as key.\n    \"\"\"\n    if not self.is_fitted:\n        raise ValueError(\n            \"Instance must be fitted before calling influence methods on it\"\n        )\n\n    if x is None:\n        if y is not None:\n            raise ValueError(\n                \"Providing labels y, without providing model input x \"\n                \"is not supported\"\n            )\n\n        return self._symmetric_values_by_layer(\n            x_test.to(self.model_device),\n            y_test.to(self.model_device),\n            mode,\n        )\n\n    if y is None:\n        raise ValueError(\n            \"Providing model input x without providing labels y is not supported\"\n        )\n\n    return self._non_symmetric_values_by_layer(\n        x_test.to(self.model_device),\n        y_test.to(self.model_device),\n        x.to(self.model_device),\n        y.to(self.model_device),\n        mode,\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: Tensor, x: Tensor, y: Tensor, mode: InfluenceMode = Up\n) -&gt; Tensor\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension (in case, you want to provide a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed tensor, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>Tensor</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Tensor</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def influences_from_factors(\n    self,\n    z_test_factors: torch.Tensor,\n    x: torch.Tensor,\n    y: torch.Tensor,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$. For all input tensors it is assumed,\n    that the first dimension is the batch dimension (in case, you want to provide\n    a single sample z, call z.unsqueeze(0) if no batch dimension is present).\n\n    Args:\n        z_test_factors: pre-computed tensor, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if mode == InfluenceMode.Up:\n        return (\n            z_test_factors.to(self.model_device)\n            @ self._loss_grad(x.to(self.model_device), y.to(self.model_device)).T\n        )\n    elif mode == InfluenceMode.Perturbation:\n        return torch.einsum(\n            \"ia,j...a-&gt;ij...\",\n            z_test_factors.to(self.model_device),\n            self._flat_loss_mixed_grad(\n                x.to(self.model_device), y.to(self.model_device)\n            ),\n        )\n    else:\n        raise UnsupportedInfluenceModeException(mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_from_factors_by_layer","title":"influences_from_factors_by_layer","text":"<pre><code>influences_from_factors_by_layer(\n    z_test_factors: Dict[str, Tensor],\n    x: Tensor,\n    y: Tensor,\n    mode: InfluenceMode = Up,\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case for each layer of the model separately. The gradients are meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed tensor, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>Dict[str, Tensor]</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Tensor</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dictionary containing the influence of the data on the test data</p> <code>Dict[str, Tensor]</code> <p>for each layer of the model, with the layer name as key.</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def influences_from_factors_by_layer(\n    self,\n    z_test_factors: Dict[str, torch.Tensor],\n    x: torch.Tensor,\n    y: torch.Tensor,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; Dict[str, torch.Tensor]:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case for each layer of the model\n    separately. The gradients are meant to be per sample of the batch $(x,\n    y)$.\n\n    Args:\n        z_test_factors: pre-computed tensor, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        A dictionary containing the influence of the data on the test data\n        for each layer of the model, with the layer name as key.\n    \"\"\"\n    if mode == InfluenceMode.Up:\n        total_grad = self._loss_grad(\n            x.to(self.model_device), y.to(self.model_device)\n        )\n        start_idx = 0\n        influences = {}\n        for layer_id, layer_z_test in z_test_factors.items():\n            end_idx = start_idx + layer_z_test.shape[1]\n            influences[layer_id] = (\n                layer_z_test.to(self.model_device)\n                @ total_grad[:, start_idx:end_idx].T\n            )\n            start_idx = end_idx\n        return influences\n    elif mode == InfluenceMode.Perturbation:\n        total_mixed_grad = self._flat_loss_mixed_grad(\n            x.to(self.model_device), y.to(self.model_device)\n        )\n        start_idx = 0\n        influences = {}\n        for layer_id, layer_z_test in z_test_factors.items():\n            end_idx = start_idx + layer_z_test.shape[1]\n            influences[layer_id] = torch.einsum(\n                \"ia,j...a-&gt;ij...\",\n                layer_z_test.to(self.model_device),\n                total_mixed_grad[:, start_idx:end_idx],\n            )\n            start_idx = end_idx\n        return influences\n    else:\n        raise UnsupportedInfluenceModeException(mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.InverseHarmonicMeanInfluence","title":"InverseHarmonicMeanInfluence","text":"<pre><code>InverseHarmonicMeanInfluence(\n    model: Module,\n    loss: LossType,\n    regularization: Union[float, Dict[str, float]],\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = FULL,\n)\n</code></pre> <p>               Bases: <code>TorchComposableInfluence[InverseHarmonicMeanOperator]</code></p> <p>This implementation replaces the inverse Hessian matrix in the influence computation with an approximation of the inverse Gauss-Newton vector product.</p> <p>Viewing the damped Gauss-newton matrix</p> \\[\\begin{align*}     G_{\\lambda}(\\theta) &amp;=     \\frac{1}{N}\\sum_{i}^N\\nabla_{\\theta}\\ell (x_i,y_i; \\theta)         \\nabla_{\\theta}\\ell (x_i, y_i; \\theta)^t + \\lambda \\operatorname{I}, \\\\\\     \\ell(x,y; \\theta) &amp;= \\text{loss}(\\text{model}(x; \\theta), y) \\end{align*}\\] <p>as an arithmetic mean of the rank-\\(1\\) updates, this implementation replaces it with the harmonic mean of the rank-\\(1\\) updates, i.e.</p> \\[ \\tilde{G}_{\\lambda}(\\theta) =     \\left(N \\cdot \\sum_{i=1}^N  \\left( \\nabla_{\\theta}\\ell (x_i,y_i; \\theta)         \\nabla_{\\theta}\\ell (x_i,y_i; \\theta)^t +         \\lambda \\operatorname{I}\\right)^{-1}         \\right)^{-1}\\] <p>and uses the matrix</p> \\[ \\tilde{G}_{\\lambda}^{-1}(\\theta)\\] <p>instead of the inverse Hessian.</p> <p>In other words, it switches the order of summation and inversion, which resolves to the <code>inverse harmonic mean</code> of the rank-\\(1\\) updates. The results are averaged over the batches provided by the data loader.</p> <p>The inverses of the rank-\\(1\\) updates are not calculated explicitly, but instead a vectorized version of the Sherman\u2013Morrison formula is applied.</p> <p>For more information, see Inverse Harmonic Mean.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>The loss function.</p> <p> TYPE: <code>LossType</code> </p> <code>regularization</code> <p>The regularization parameter. In case a dictionary is provided, the keys must match the blocking structure and the specification must be complete, so every block needs a positive regularization value, which differs from the description in block-diagonal approximation.</p> <p> TYPE: <code>Union[float, Dict[str, float]]</code> </p> <code>block_structure</code> <p>The blocking structure, either a pre-defined enum or a custom block structure, see the information regarding block-diagonal approximation.</p> <p> TYPE: <code>Union[BlockMode, OrderedDict[str, List[str]]]</code> DEFAULT: <code>FULL</code> </p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    loss: LossType,\n    regularization: Union[float, Dict[str, float]],\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = BlockMode.FULL,\n):\n    super().__init__(\n        model,\n        block_structure,\n        regularization=cast(\n            Union[float, Dict[str, Optional[float]]], regularization\n        ),\n    )\n    self.loss = loss\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.InverseHarmonicMeanInfluence.fit","title":"fit","text":"<pre><code>fit(data: DataLoaderType) -&gt; InfluenceFunctionModel\n</code></pre> <p>Fitting to provided data, by internally creating a block mapper instance from it. Args:     data: iterable of tensors</p> RETURNS DESCRIPTION <code>InfluenceFunctionModel</code> <p>Fitted instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef fit(self, data: DataLoaderType) -&gt; InfluenceFunctionModel:\n    \"\"\"\n    Fitting to provided data, by internally creating a block mapper instance from\n    it.\n    Args:\n        data: iterable of tensors\n\n    Returns:\n        Fitted instance\n    \"\"\"\n    self.block_mapper = self._create_block_mapper(data)\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.InverseHarmonicMeanInfluence.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.InverseHarmonicMeanInfluence.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: TensorType, y: TensorType) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influence_factors(self, x: TensorType, y: TensorType) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    return self._influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.InverseHarmonicMeanInfluence.influence_factors_by_block","title":"influence_factors_by_block","text":"<pre><code>influence_factors_by_block(\n    x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise</p> <code>OrderedDict[str, TensorType]</code> <p>approximate inverse Hessian matrix vector products per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influence_factors_by_block(\n    self, x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise\n        approximate inverse Hessian matrix vector products per block.\n\n    \"\"\"\n    return self.block_mapper.transformed_grads(self._create_batch(x, y))\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.InverseHarmonicMeanInfluence.influences","title":"influences","text":"<pre><code>influences(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    if x is None and y is not None:\n        raise ValueError(\n            \"Providing labels y, without providing model input x is not supported\"\n        )\n\n    if x is not None and y is None:\n        raise ValueError(\n            \"Providing model input x, without providing labels y is not supported\"\n        )\n\n    return self._influences(x_test, y_test, x, y, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.InverseHarmonicMeanInfluence.influences_by_block","title":"influences_by_block","text":"<pre><code>influences_by_block(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise influence values for the provided data, i.e. an approximation of</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of the approximation of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_by_block(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise influence values for the provided data, i.e. an\n    approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of the approximation of\n            $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block.\n\n    \"\"\"\n    left_batch = self._create_batch(x_test, y_test)\n\n    if x is None:\n        if y is not None:\n            raise ValueError(\n                \"Providing labels y, without providing model input x \"\n                \"is not supported\"\n            )\n        right_batch = left_batch\n    else:\n        if y is None:\n            raise ValueError(\n                \"Providing model input x, without providing labels y \"\n                \"is not supported\"\n            )\n        right_batch = self._create_batch(x, y)\n\n    return self.block_mapper.interactions(left_batch, right_batch, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.InverseHarmonicMeanInfluence.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors(\n    self,\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    tensors = self.block_mapper.generate_interactions_from_transformed_grads(\n        z_test_factors,\n        self._create_batch(x, y),\n        mode,\n    )\n    result: TensorType = next(tensors)\n    for tensor in tensors:\n        result = result + tensor\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.InverseHarmonicMeanInfluence.influences_from_factors_by_block","title":"influences_from_factors_by_block","text":"<pre><code>influences_from_factors_by_block(\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Block-wise computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>OrderedDict[str, TensorType]</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors_by_block(\n    self,\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Block-wise computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block\n\n    \"\"\"\n    return self.block_mapper.interactions_from_transformed_grads(\n        z_test_factors, self._create_batch(x, y), mode\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.InverseHarmonicMeanInfluence.with_regularization","title":"with_regularization","text":"<pre><code>with_regularization(\n    regularization: Union[float, Dict[str, Optional[float]]]\n) -&gt; TorchComposableInfluence\n</code></pre> <p>Update the regularization parameter. Args:     regularization: Either a positive float or a dictionary with the         block names as keys and the regularization values as values.</p> RETURNS DESCRIPTION <code>TorchComposableInfluence</code> <p>The modified instance</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def with_regularization(\n    self, regularization: Union[float, Dict[str, Optional[float]]]\n) -&gt; TorchComposableInfluence:\n    \"\"\"\n    Update the regularization parameter.\n    Args:\n        regularization: Either a positive float or a dictionary with the\n            block names as keys and the regularization values as values.\n\n    Returns:\n        The modified instance\n\n    \"\"\"\n    self._regularization_dict = self._build_regularization_dict(regularization)\n    for k, reg in self._regularization_dict.items():\n        self.block_mapper.composable_block_dict[k].op.regularization = reg\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.LissaInfluence","title":"LissaInfluence","text":"<pre><code>LissaInfluence(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    regularization: Optional[Union[float, Dict[str, Optional[float]]]] = None,\n    maxiter: int = 1000,\n    dampen: float = 0.0,\n    scale: float = 10.0,\n    rtol: float = 0.0001,\n    progress: bool = False,\n    warn_on_max_iteration: bool = True,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = FULL,\n    second_order_mode: SecondOrderMode = HESSIAN,\n)\n</code></pre> <p>               Bases: <code>TorchComposableInfluence[LissaOperator[BatchOperationType]]</code></p> <p>Uses LISSA, Linear time Stochastic Second-Order Algorithm, to iteratively approximate the inverse Hessian. More precisely, it finds x s.t. \\(Hx = b\\), with \\(H\\) being the model's second derivative wrt. the parameters. This is done with the update</p> \\[H^{-1}_{j+1} b = b + (I - d) \\ H - \\frac{H^{-1}_j b}{s},\\] <p>where \\(I\\) is the identity matrix, \\(d\\) is a dampening term and \\(s\\) a scaling factor that are applied to help convergence. For details, see Linear time Stochastic Second-Order Approximation (LiSSA)</p> PARAMETER DESCRIPTION <code>model</code> <p>A PyTorch model. The Hessian will be calculated with respect to this model's parameters.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that takes the model's output and target as input and returns   the scalar loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>regularization</code> <p>Optional regularization parameter added to the Hessian-vector product for numerical stability.</p> <p> TYPE: <code>Optional[Union[float, Dict[str, Optional[float]]]]</code> DEFAULT: <code>None</code> </p> <code>maxiter</code> <p>Maximum number of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>dampen</code> <p>Dampening factor, defaults to 0 for no dampening.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>scale</code> <p>Scaling factor, defaults to 10.</p> <p> TYPE: <code>float</code> DEFAULT: <code>10.0</code> </p> <code>rtol</code> <p>tolerance to use for early stopping</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0001</code> </p> <code>progress</code> <p>If True, display progress bars.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>warn_on_max_iteration</code> <p>If True, logs a warning, if the desired tolerance is not achieved within <code>maxiter</code> iterations. If False, the log level for this information is <code>logging.DEBUG</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>block_structure</code> <p>The blocking structure, either a pre-defined enum or a custom block structure, see the information regarding block-diagonal approximation.</p> <p> TYPE: <code>Union[BlockMode, OrderedDict[str, List[str]]]</code> DEFAULT: <code>FULL</code> </p> <code>second_order_mode</code> <p>The second order mode, either <code>SecondOrderMode.HESSIAN</code> or <code>SecondOrderMode.GAUSS_NEWTON</code>.</p> <p> TYPE: <code>SecondOrderMode</code> DEFAULT: <code>HESSIAN</code> </p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    regularization: Optional[Union[float, Dict[str, Optional[float]]]] = None,\n    maxiter: int = 1000,\n    dampen: float = 0.0,\n    scale: float = 10.0,\n    rtol: float = 1e-4,\n    progress: bool = False,\n    warn_on_max_iteration: bool = True,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = BlockMode.FULL,\n    second_order_mode: SecondOrderMode = SecondOrderMode.HESSIAN,\n):\n    super().__init__(model, block_structure, regularization)\n    self.maxiter = maxiter\n    self.progress = progress\n    self.rtol = rtol\n    self.scale = scale\n    self.dampen = dampen\n    self.loss = loss\n    self.second_order_mode = second_order_mode\n    self.warn_on_max_iteration = warn_on_max_iteration\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.LissaInfluence.fit","title":"fit","text":"<pre><code>fit(data: DataLoaderType) -&gt; InfluenceFunctionModel\n</code></pre> <p>Fitting to provided data, by internally creating a block mapper instance from it. Args:     data: iterable of tensors</p> RETURNS DESCRIPTION <code>InfluenceFunctionModel</code> <p>Fitted instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef fit(self, data: DataLoaderType) -&gt; InfluenceFunctionModel:\n    \"\"\"\n    Fitting to provided data, by internally creating a block mapper instance from\n    it.\n    Args:\n        data: iterable of tensors\n\n    Returns:\n        Fitted instance\n    \"\"\"\n    self.block_mapper = self._create_block_mapper(data)\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.LissaInfluence.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.LissaInfluence.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: TensorType, y: TensorType) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influence_factors(self, x: TensorType, y: TensorType) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    return self._influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.LissaInfluence.influence_factors_by_block","title":"influence_factors_by_block","text":"<pre><code>influence_factors_by_block(\n    x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise</p> <code>OrderedDict[str, TensorType]</code> <p>approximate inverse Hessian matrix vector products per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influence_factors_by_block(\n    self, x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise\n        approximate inverse Hessian matrix vector products per block.\n\n    \"\"\"\n    return self.block_mapper.transformed_grads(self._create_batch(x, y))\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.LissaInfluence.influences","title":"influences","text":"<pre><code>influences(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    if x is None and y is not None:\n        raise ValueError(\n            \"Providing labels y, without providing model input x is not supported\"\n        )\n\n    if x is not None and y is None:\n        raise ValueError(\n            \"Providing model input x, without providing labels y is not supported\"\n        )\n\n    return self._influences(x_test, y_test, x, y, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.LissaInfluence.influences_by_block","title":"influences_by_block","text":"<pre><code>influences_by_block(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise influence values for the provided data, i.e. an approximation of</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of the approximation of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_by_block(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise influence values for the provided data, i.e. an\n    approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of the approximation of\n            $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block.\n\n    \"\"\"\n    left_batch = self._create_batch(x_test, y_test)\n\n    if x is None:\n        if y is not None:\n            raise ValueError(\n                \"Providing labels y, without providing model input x \"\n                \"is not supported\"\n            )\n        right_batch = left_batch\n    else:\n        if y is None:\n            raise ValueError(\n                \"Providing model input x, without providing labels y \"\n                \"is not supported\"\n            )\n        right_batch = self._create_batch(x, y)\n\n    return self.block_mapper.interactions(left_batch, right_batch, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.LissaInfluence.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors(\n    self,\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    tensors = self.block_mapper.generate_interactions_from_transformed_grads(\n        z_test_factors,\n        self._create_batch(x, y),\n        mode,\n    )\n    result: TensorType = next(tensors)\n    for tensor in tensors:\n        result = result + tensor\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.LissaInfluence.influences_from_factors_by_block","title":"influences_from_factors_by_block","text":"<pre><code>influences_from_factors_by_block(\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Block-wise computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>OrderedDict[str, TensorType]</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors_by_block(\n    self,\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Block-wise computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block\n\n    \"\"\"\n    return self.block_mapper.interactions_from_transformed_grads(\n        z_test_factors, self._create_batch(x, y), mode\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.LissaInfluence.with_regularization","title":"with_regularization","text":"<pre><code>with_regularization(\n    regularization: Union[float, Dict[str, Optional[float]]]\n) -&gt; TorchComposableInfluence\n</code></pre> <p>Update the regularization parameter. Args:     regularization: Either a positive float or a dictionary with the         block names as keys and the regularization values as values.</p> RETURNS DESCRIPTION <code>TorchComposableInfluence</code> <p>The modified instance</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def with_regularization(\n    self, regularization: Union[float, Dict[str, Optional[float]]]\n) -&gt; TorchComposableInfluence:\n    \"\"\"\n    Update the regularization parameter.\n    Args:\n        regularization: Either a positive float or a dictionary with the\n            block names as keys and the regularization values as values.\n\n    Returns:\n        The modified instance\n\n    \"\"\"\n    self._regularization_dict = self._build_regularization_dict(regularization)\n    for k, reg in self._regularization_dict.items():\n        self.block_mapper.composable_block_dict[k].op.regularization = reg\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence","title":"NystroemSketchInfluence","text":"<pre><code>NystroemSketchInfluence(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    regularization: Union[float, Dict[str, float]],\n    rank: int,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = FULL,\n    second_order_mode: SecondOrderMode = HESSIAN,\n)\n</code></pre> <p>               Bases: <code>TorchComposableInfluence[LowRankOperator]</code></p> <p>Given a model and training data, it uses a low-rank approximation of the Hessian (derived via random projection Nystr\u00f6m approximation) in combination with the Sherman\u2013Morrison\u2013Woodbury formula to calculate the inverse of the Hessian Vector Product. More concrete, it computes a low-rank approximation</p> \\[\\begin{align*}     H_{\\text{nys}} &amp;= (H\\Omega)(\\Omega^TH\\Omega)^{+}(H\\Omega)^T \\\\\\                    &amp;= U \\Lambda U^T \\end{align*}\\] <p>in factorized form and approximates the action of the inverse Hessian via</p> \\[ (H_{\\text{nys}} + \\lambda I)^{-1} = U(\\Lambda+\\lambda I)U^T +     \\frac{1}{\\lambda}(I\u2212UU^T). \\] PARAMETER DESCRIPTION <code>model</code> <p>A PyTorch model. The Hessian will be calculated with respect to this model's parameters.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>A callable that takes the model's output and target as input and returns   the scalar loss.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>regularization</code> <p>Optional regularization parameter added to the Hessian-vector product for numerical stability.</p> <p> TYPE: <code>Union[float, Dict[str, float]]</code> </p> <code>rank</code> <p>rank of the low-rank approximation</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    regularization: Union[float, Dict[str, float]],\n    rank: int,\n    block_structure: Union[BlockMode, OrderedDict[str, List[str]]] = BlockMode.FULL,\n    second_order_mode: SecondOrderMode = SecondOrderMode.HESSIAN,\n):\n    super().__init__(\n        model,\n        block_structure,\n        regularization=cast(\n            Union[float, Dict[str, Optional[float]]], regularization\n        ),\n    )\n    self.second_order_mode = second_order_mode\n    self.rank = rank\n    self.loss = loss\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.fit","title":"fit","text":"<pre><code>fit(data: DataLoaderType) -&gt; InfluenceFunctionModel\n</code></pre> <p>Fitting to provided data, by internally creating a block mapper instance from it. Args:     data: iterable of tensors</p> RETURNS DESCRIPTION <code>InfluenceFunctionModel</code> <p>Fitted instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@log_duration(log_level=logging.INFO)\ndef fit(self, data: DataLoaderType) -&gt; InfluenceFunctionModel:\n    \"\"\"\n    Fitting to provided data, by internally creating a block mapper instance from\n    it.\n    Args:\n        data: iterable of tensors\n\n    Returns:\n        Fitted instance\n    \"\"\"\n    self.block_mapper = self._create_block_mapper(data)\n    return self\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: TensorType, y: TensorType) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension.</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influence_factors(self, x: TensorType, y: TensorType) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    return self._influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influence_factors_by_block","title":"influence_factors_by_block","text":"<pre><code>influence_factors_by_block(\n    x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise</p> <code>OrderedDict[str, TensorType]</code> <p>approximate inverse Hessian matrix vector products per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influence_factors_by_block(\n    self, x: TensorType, y: TensorType\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise\n        approximate inverse Hessian matrix vector products per block.\n\n    \"\"\"\n    return self.block_mapper.transformed_grads(self._create_batch(x, y))\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences","title":"influences","text":"<pre><code>influences(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computes the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computes the approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    if x is None and y is not None:\n        raise ValueError(\n            \"Providing labels y, without providing model input x is not supported\"\n        )\n\n    if x is not None and y is None:\n        raise ValueError(\n            \"Providing model input x, without providing labels y is not supported\"\n        )\n\n    return self._influences(x_test, y_test, x, y, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences_by_block","title":"influences_by_block","text":"<pre><code>influences_by_block(\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Compute the block-wise influence values for the provided data, i.e. an approximation of</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}})),     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case.</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of the approximation of \\(H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{test}\\)</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[TensorType]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block.</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_by_block(\n    self,\n    x_test: TensorType,\n    y_test: TensorType,\n    x: Optional[TensorType] = None,\n    y: Optional[TensorType] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Compute the block-wise influence values for the provided data, i.e. an\n    approximation of\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{\\text{test}},\n        f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case.\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of the approximation of\n            $H^{-1}\\nabla_{theta} \\ell(y_{test}, f_{\\theta}(x_{test}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{test}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block.\n\n    \"\"\"\n    left_batch = self._create_batch(x_test, y_test)\n\n    if x is None:\n        if y is not None:\n            raise ValueError(\n                \"Providing labels y, without providing model input x \"\n                \"is not supported\"\n            )\n        right_batch = left_batch\n    else:\n        if y is None:\n            raise ValueError(\n                \"Providing model input x, without providing labels y \"\n                \"is not supported\"\n            )\n        right_batch = self._create_batch(x, y)\n\n    return self.block_mapper.interactions(left_batch, right_batch, mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; TensorType\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors(\n    self,\n    z_test_factors: TensorType,\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; TensorType:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    tensors = self.block_mapper.generate_interactions_from_transformed_grads(\n        z_test_factors,\n        self._create_batch(x, y),\n        mode,\n    )\n    result: TensorType = next(tensors)\n    for tensor in tensors:\n        result = result + tensor\n    return result\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences_from_factors_by_block","title":"influences_from_factors_by_block","text":"<pre><code>influences_from_factors_by_block(\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = Up,\n) -&gt; OrderedDict[str, TensorType]\n</code></pre> <p>Block-wise computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed array, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>OrderedDict[str, TensorType]</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>TensorType</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>TensorType</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>OrderedDict[str, TensorType]</code> <p>Ordered dictionary of tensors representing the element-wise scalar products</p> <code>OrderedDict[str, TensorType]</code> <p>for the provided batch per block</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@InfluenceFunctionModel.fit_required\ndef influences_from_factors_by_block(\n    self,\n    z_test_factors: OrderedDict[str, TensorType],\n    x: TensorType,\n    y: TensorType,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; OrderedDict[str, TensorType]:\n    r\"\"\"\n    Block-wise computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$.\n\n    Args:\n        z_test_factors: pre-computed array, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Ordered dictionary of tensors representing the element-wise scalar products\n        for the provided batch per block\n\n    \"\"\"\n    return self.block_mapper.interactions_from_transformed_grads(\n        z_test_factors, self._create_batch(x, y), mode\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel","title":"TorchInfluenceFunctionModel","text":"<pre><code>TorchInfluenceFunctionModel(\n    model: Module, loss: Callable[[Tensor, Tensor], Tensor]\n)\n</code></pre> <p>               Bases: <code>InfluenceFunctionModel[Tensor, DataLoader]</code>, <code>ABC</code></p> <p>Abstract base class for influence computation related to torch models</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n):\n    self.loss = loss\n    self.model = model\n    self._n_parameters = sum(\n        [p.numel() for p in model.parameters() if p.requires_grad]\n    )\n    self._model_device = next(\n        (p.device for p in model.parameters() if p.requires_grad)\n    )\n    self._model_params = {\n        k: p.detach() for k, p in self.model.named_parameters() if p.requires_grad\n    }\n    self._model_dtype = next(\n        (p.dtype for p in model.parameters() if p.requires_grad)\n    )\n    super().__init__()\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.is_fitted","title":"is_fitted  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>is_fitted\n</code></pre> <p>Override this, to expose the fitting status of the instance.</p>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(data: DataLoaderType) -&gt; InfluenceFunctionModel\n</code></pre> <p>Override this method to fit the influence function model to training data, e.g. pre-compute hessian matrix or matrix decompositions</p> PARAMETER DESCRIPTION <code>data</code> <p> TYPE: <code>DataLoaderType</code> </p> RETURNS DESCRIPTION <code>InfluenceFunctionModel</code> <p>The fitted instance</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@abstractmethod\ndef fit(self, data: DataLoaderType) -&gt; InfluenceFunctionModel:\n    \"\"\"\n    Override this method to fit the influence function model to training data,\n    e.g. pre-compute hessian matrix or matrix decompositions\n\n    Args:\n        data:\n\n    Returns:\n        The fitted instance\n    \"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.fit_required","title":"fit_required  <code>staticmethod</code>","text":"<pre><code>fit_required(method)\n</code></pre> <p>Decorator to enforce the fitted check</p> Source code in <code>src/pydvl/influence/base_influence_function_model.py</code> <pre><code>@staticmethod\ndef fit_required(method):\n    \"\"\"Decorator to enforce the fitted check\"\"\"\n\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_fitted:\n            raise NotFittedException(type(self))\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influence_factors","title":"influence_factors","text":"<pre><code>influence_factors(x: Tensor, y: Tensor) -&gt; Tensor\n</code></pre> <p>Compute approximation of</p> \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\] <p>where the gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension (in case, you want to provide a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p> PARAMETER DESCRIPTION <code>x</code> <p>model input to use in the gradient computations</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor representing the element-wise inverse Hessian matrix vector products</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def influence_factors(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute approximation of\n\n    \\[ H^{-1}\\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\]\n\n    where the gradient is meant to be per sample of the batch $(x, y)$.\n    For all input tensors it is assumed,\n    that the first dimension is the batch dimension (in case, you want to provide\n    a single sample z, call z.unsqueeze(0) if no batch dimension is present).\n\n    Args:\n        x: model input to use in the gradient computations\n        y: label tensor to compute gradients\n\n    Returns:\n        Tensor representing the element-wise inverse Hessian matrix vector products\n\n    \"\"\"\n    return super().influence_factors(x, y)\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influences","title":"influences","text":"<pre><code>influences(\n    x_test: Tensor,\n    y_test: Tensor,\n    x: Optional[Tensor] = None,\n    y: Optional[Tensor] = None,\n    mode: InfluenceMode = Up,\n) -&gt; Tensor\n</code></pre> <p>Compute the approximation of</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}})), \\nabla_{\\theta} \\ell(y, f_{\\theta}(x))\\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}})),     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. For all input tensors it is assumed, that the first dimension is the batch dimension (in case, you want to provide a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p> PARAMETER DESCRIPTION <code>x_test</code> <p>model input to use in the gradient computations of \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},     f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>Tensor</code> </p> <code>y_test</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Tensor</code> </p> <code>x</code> <p>optional model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), if None, use \\(x=x_{\\text{test}}\\)</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>y</code> <p>optional label tensor to compute gradients</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def influences(\n    self,\n    x_test: torch.Tensor,\n    y_test: torch.Tensor,\n    x: Optional[torch.Tensor] = None,\n    y: Optional[torch.Tensor] = None,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the approximation of\n\n    \\[\n    \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n    f_{\\theta}(x_{\\text{test}})), \\nabla_{\\theta} \\ell(y, f_{\\theta}(x))\\rangle\n    \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[\n    \\langle H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}})),\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle\n    \\]\n\n    for the perturbation type influence case. For all input tensors it is assumed,\n    that the first dimension is the batch dimension (in case, you want to provide\n    a single sample z, call z.unsqueeze(0) if no batch dimension is present).\n\n    Args:\n        x_test: model input to use in the gradient computations\n            of $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n                f_{\\theta}(x_{\\text{test}}))$\n        y_test: label tensor to compute gradients\n        x: optional model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            if None, use $x=x_{\\text{test}}$\n        y: optional label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    t: torch.Tensor = super().influences(x_test, y_test, x, y, mode=mode)\n    return t\n</code></pre>"},{"location":"api/pydvl/influence/torch/influence_function_model/#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influences_from_factors","title":"influences_from_factors","text":"<pre><code>influences_from_factors(\n    z_test_factors: Tensor, x: Tensor, y: Tensor, mode: InfluenceMode = Up\n) -&gt; Tensor\n</code></pre> <p>Computation of</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the case of up-weighting influence, resp.</p> \\[ \\langle z_{\\text{test_factors}},     \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\] <p>for the perturbation type influence case. The gradient is meant to be per sample of the batch \\((x, y)\\). For all input tensors it is assumed, that the first dimension is the batch dimension (in case, you want to provide a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p> PARAMETER DESCRIPTION <code>z_test_factors</code> <p>pre-computed tensor, approximating \\(H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}}, f_{\\theta}(x_{\\text{test}}))\\)</p> <p> TYPE: <code>Tensor</code> </p> <code>x</code> <p>model input to use in the gradient computations \\(\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\), resp. \\(\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))\\)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>label tensor to compute gradients</p> <p> TYPE: <code>Tensor</code> </p> <code>mode</code> <p>enum value of InfluenceMode</p> <p> TYPE: <code>InfluenceMode</code> DEFAULT: <code>Up</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor representing the element-wise scalar products for the provided batch</p> Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code> <pre><code>def influences_from_factors(\n    self,\n    z_test_factors: torch.Tensor,\n    x: torch.Tensor,\n    y: torch.Tensor,\n    mode: InfluenceMode = InfluenceMode.Up,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Computation of\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the case of up-weighting influence, resp.\n\n    \\[ \\langle z_{\\text{test_factors}},\n        \\nabla_{x} \\nabla_{\\theta} \\ell(y, f_{\\theta}(x)) \\rangle \\]\n\n    for the perturbation type influence case. The gradient is meant to be per sample\n    of the batch $(x, y)$. For all input tensors it is assumed,\n    that the first dimension is the batch dimension (in case, you want to provide\n    a single sample z, call z.unsqueeze(0) if no batch dimension is present).\n\n    Args:\n        z_test_factors: pre-computed tensor, approximating\n            $H^{-1}\\nabla_{\\theta} \\ell(y_{\\text{test}},\n            f_{\\theta}(x_{\\text{test}}))$\n        x: model input to use in the gradient computations\n            $\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$,\n            resp. $\\nabla_{x}\\nabla_{\\theta}\\ell(y, f_{\\theta}(x))$\n        y: label tensor to compute gradients\n        mode: enum value of [InfluenceMode]\n            [pydvl.influence.base_influence_function_model.InfluenceMode]\n\n    Returns:\n        Tensor representing the element-wise scalar products for the provided batch\n\n    \"\"\"\n    if mode == InfluenceMode.Up:\n        return (\n            z_test_factors.to(self.model_device)\n            @ self._loss_grad(x.to(self.model_device), y.to(self.model_device)).T\n        )\n    elif mode == InfluenceMode.Perturbation:\n        return torch.einsum(\n            \"ia,j...a-&gt;ij...\",\n            z_test_factors.to(self.model_device),\n            self._flat_loss_mixed_grad(\n                x.to(self.model_device), y.to(self.model_device)\n            ),\n        )\n    else:\n        raise UnsupportedInfluenceModeException(mode)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/","title":"Operator","text":""},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator","title":"pydvl.influence.torch.operator","text":""},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.CgOperator","title":"CgOperator","text":"<pre><code>CgOperator(\n    operator: TensorOperator,\n    regularization: Optional[float] = None,\n    rtol: float = 1e-07,\n    atol: float = 1e-07,\n    maxiter: Optional[int] = None,\n    progress: bool = False,\n    preconditioner: Optional[Preconditioner] = None,\n    use_block_cg: bool = False,\n    warn_on_max_iteration: bool = True,\n)\n</code></pre> <p>               Bases: <code>TensorOperator</code></p> <p>Given an operator , it uses conjugate gradient to calculate the action of its inverse. More precisely, it finds x such that \\(Ax = A\\), with \\(A\\) being the matrix represented by the operator. For more info, see Conjugate Gradient.</p> PARAMETER DESCRIPTION <code>operator</code> <p> TYPE: <code>TensorOperator</code> </p> <code>regularization</code> <p>Optional regularization parameter added to the matrix vector product for numerical stability.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>rtol</code> <p>Maximum relative tolerance of result.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-07</code> </p> <code>atol</code> <p>Absolute tolerance of result.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-07</code> </p> <code>maxiter</code> <p>Maximum number of iterations. If None, defaults to 10*len(b).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, display progress bars for computing in the non-block mode (use_block_cg=False).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>preconditioner</code> <p>Optional pre-conditioner to improve convergence of conjugate gradient method</p> <p> TYPE: <code>Optional[Preconditioner]</code> DEFAULT: <code>None</code> </p> <code>use_block_cg</code> <p>If True, use block variant of conjugate gradient method, which solves several right hand sides simultaneously</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>warn_on_max_iteration</code> <p>If True, logs a warning, if the desired tolerance is not achieved within <code>maxiter</code> iterations. If False, the log level for this information is <code>logging.DEBUG</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/influence/torch/operator.py</code> <pre><code>def __init__(\n    self,\n    operator: TensorOperator,\n    regularization: Optional[float] = None,\n    rtol: float = 1e-7,\n    atol: float = 1e-7,\n    maxiter: Optional[int] = None,\n    progress: bool = False,\n    preconditioner: Optional[Preconditioner] = None,\n    use_block_cg: bool = False,\n    warn_on_max_iteration: bool = True,\n):\n    if regularization is not None and regularization &lt; 0:\n        raise ValueError(\"regularization must be non-negative\")\n\n    self.progress = progress\n    self.warn_on_max_iteration = warn_on_max_iteration\n    self.use_block_cg = use_block_cg\n    self.preconditioner = preconditioner\n    self.maxiter = maxiter\n    self.atol = atol\n    self.rtol = rtol\n    self._regularization = regularization\n    self.operator = operator\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.CgOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.DirectSolveOperator","title":"DirectSolveOperator","text":"<pre><code>DirectSolveOperator(\n    matrix: Tensor,\n    regularization: Optional[float] = None,\n    in_place_regularization: bool = False,\n)\n</code></pre> <p>               Bases: <code>TensorOperator</code></p> <p>Given a matrix \\(A\\) and an optional regularization parameter \\(\\lambda\\), computes the solution of the system \\((A+\\lambda I)x = b\\), where \\(b\\) is a vector or a matrix. Internally, it uses the routine torch.linalg.solve.</p> PARAMETER DESCRIPTION <code>matrix</code> <p>the system matrix</p> <p> TYPE: <code>Tensor</code> </p> <code>regularization</code> <p>the regularization parameter</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>in_place_regularization</code> <p>If True, the input matrix is modified in-place, by adding the regularization value to the diagonal.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/influence/torch/operator.py</code> <pre><code>def __init__(\n    self,\n    matrix: torch.Tensor,\n    regularization: Optional[float] = None,\n    in_place_regularization: bool = False,\n):\n    if regularization is None:\n        self.matrix = matrix\n    else:\n        self.matrix = self._update_diagonal(\n            matrix if in_place_regularization else matrix.clone(), regularization\n        )\n    self._regularization = regularization\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.DirectSolveOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.GaussNewtonOperator","title":"GaussNewtonOperator","text":"<pre><code>GaussNewtonOperator(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    dataloader: DataLoader,\n    restrict_to: Optional[Dict[str, Parameter]] = None,\n)\n</code></pre> <p>               Bases: <code>_AveragingBatchOperator[GaussNewtonBatchOperation, PointAveraging]</code></p> <p>Given a model and loss function computes the Gauss-Newton vector or matrix product with respect to the model parameters on a batch, i.e.</p> \\[\\begin{align*}     G(\\text{model}, \\text{loss}, b, \\theta) &amp;\\cdot v, \\\\\\     G(\\text{model}, \\text{loss}, b, \\theta) &amp;=     \\frac{1}{|b|}\\sum_{(x, y) \\in b}\\nabla_{\\theta}\\ell (x,y; \\theta)         \\nabla_{\\theta}\\ell (x,y; \\theta)^t, \\\\\\     \\ell(x,y; \\theta) &amp;= \\text{loss}(\\text{model}(x; \\theta), y) \\end{align*}\\] <p>where model is a torch.nn.Module and \\(v\\) is a vector or matrix, and average the results over the batches provided by the data loader.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>The loss function.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>dataloader</code> <p>The data loader providing batches of data.</p> <p> TYPE: <code>DataLoader</code> </p> <code>restrict_to</code> <p>The parameters to restrict the differentiation to, i.e. the corresponding sub-matrix of the Jacobian. If None, the full Jacobian is used. Make sure the input matches the corrct dimension, i.e. the last dimension must be equal to the property <code>input_size</code>.</p> <p> TYPE: <code>Optional[Dict[str, Parameter]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/influence/torch/operator.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    dataloader: DataLoader,\n    restrict_to: Optional[Dict[str, nn.Parameter]] = None,\n):\n    batch_op = GaussNewtonBatchOperation(\n        model,\n        loss,\n        restrict_to=restrict_to,\n    )\n    averaging = PointAveraging()\n    super().__init__(batch_op, dataloader, averaging)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.GaussNewtonOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.GaussNewtonOperator.apply_to_dict","title":"apply_to_dict","text":"<pre><code>apply_to_dict(mat: Dict[str, Tensor]) -&gt; Dict[str, Tensor]\n</code></pre> <p>Applies the operator to a dictionary of tensors, compatible to the structure defined by the property <code>input_dict_structure</code>.</p> PARAMETER DESCRIPTION <code>mat</code> <p>dictionary of tensors, whose keys and shapes match the property <code>input_dict_structure</code>.</p> <p> TYPE: <code>Dict[str, Tensor]</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dictionary of tensors after applying the operator</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def apply_to_dict(self, mat: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Applies the operator to a dictionary of tensors, compatible to the structure\n    defined by the property `input_dict_structure`.\n\n    Args:\n        mat: dictionary of tensors, whose keys and shapes match the property\n            `input_dict_structure`.\n\n    Returns:\n        A dictionary of tensors after applying the operator\n    \"\"\"\n\n    if not self._validate_mat_dict(mat):\n        raise ValueError(\n            f\"Incompatible input structure, expected (excluding batch\"\n            f\"dimension): \\n {self.input_dict_structure}\"\n        )\n\n    return self._apply_to_dict(self._dict_to_device(mat))\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.HessianOperator","title":"HessianOperator","text":"<pre><code>HessianOperator(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    dataloader: DataLoader,\n    restrict_to: Optional[Dict[str, Parameter]] = None,\n)\n</code></pre> <p>               Bases: <code>_AveragingBatchOperator[HessianBatchOperation, ChunkAveraging]</code></p> <p>Given a model and loss function computes the Hessian vector or matrix product with respect to the model parameters for a given batch, i.e.</p> \\[\\begin{align*}     &amp;\\nabla^2_{\\theta} L(b;\\theta) \\cdot v \\\\\\     &amp;L(b;\\theta) = \\left( \\frac{1}{|b|} \\sum_{(x,y) \\in b}     \\text{loss}(\\text{model}(x; \\theta), y)\\right), \\end{align*}\\] <p>where model is a torch.nn.Module and \\(v\\) is a vector or matrix, and average the results over the batches provided by the data loader.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>The loss function.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>dataloader</code> <p>The data loader providing batches of data.</p> <p> TYPE: <code>DataLoader</code> </p> <code>restrict_to</code> <p>The parameters to restrict the second order differentiation to, i.e. the corresponding sub-matrix of the Hessian. If None, the full Hessian is used. Make sure the input matches the corrct dimension, i.e. the last dimension must be equal to the property <code>input_size</code>.</p> <p> TYPE: <code>Optional[Dict[str, Parameter]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/influence/torch/operator.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    dataloader: DataLoader,\n    restrict_to: Optional[Dict[str, nn.Parameter]] = None,\n):\n    batch_op = HessianBatchOperation(model, loss, restrict_to=restrict_to)\n    averaging = ChunkAveraging()\n    super().__init__(batch_op, dataloader, averaging)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.HessianOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.HessianOperator.apply_to_dict","title":"apply_to_dict","text":"<pre><code>apply_to_dict(mat: Dict[str, Tensor]) -&gt; Dict[str, Tensor]\n</code></pre> <p>Applies the operator to a dictionary of tensors, compatible to the structure defined by the property <code>input_dict_structure</code>.</p> PARAMETER DESCRIPTION <code>mat</code> <p>dictionary of tensors, whose keys and shapes match the property <code>input_dict_structure</code>.</p> <p> TYPE: <code>Dict[str, Tensor]</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dictionary of tensors after applying the operator</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def apply_to_dict(self, mat: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Applies the operator to a dictionary of tensors, compatible to the structure\n    defined by the property `input_dict_structure`.\n\n    Args:\n        mat: dictionary of tensors, whose keys and shapes match the property\n            `input_dict_structure`.\n\n    Returns:\n        A dictionary of tensors after applying the operator\n    \"\"\"\n\n    if not self._validate_mat_dict(mat):\n        raise ValueError(\n            f\"Incompatible input structure, expected (excluding batch\"\n            f\"dimension): \\n {self.input_dict_structure}\"\n        )\n\n    return self._apply_to_dict(self._dict_to_device(mat))\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.InverseHarmonicMeanOperator","title":"InverseHarmonicMeanOperator","text":"<pre><code>InverseHarmonicMeanOperator(\n    model: Module,\n    loss: Callable[[Tensor, Tensor], Tensor],\n    dataloader: DataLoader,\n    regularization: float,\n    restrict_to: Optional[Dict[str, Parameter]] = None,\n)\n</code></pre> <p>               Bases: <code>_AveragingBatchOperator[InverseHarmonicMeanBatchOperation, PointAveraging]</code></p> <p>Given a model and loss function computes an approximation of the inverse Gauss-Newton vector or matrix product per batch and averages the results.</p> <p>Viewing the damped Gauss-newton matrix</p> \\[\\begin{align*}     G_{\\lambda}(\\text{model}, \\text{loss}, b, \\theta) &amp;=     \\frac{1}{|b|}\\sum_{(x, y) \\in b}\\nabla_{\\theta}\\ell (x,y; \\theta)         \\nabla_{\\theta}\\ell (x,y; \\theta)^t + \\lambda \\operatorname{I}, \\\\\\     \\ell(x,y; \\theta) &amp;= \\text{loss}(\\text{model}(x; \\theta), y) \\end{align*}\\] <p>as an arithmetic mean of the rank-\\(1\\) updates, this operator replaces it with the harmonic mean of the rank-\\(1\\) updates, i.e.</p> \\[ \\tilde{G}_{\\lambda}(\\text{model}, \\text{loss}, b, \\theta) =     \\left(n \\sum_{(x, y) \\in b}  \\left( \\nabla_{\\theta}\\ell (x,y; \\theta)         \\nabla_{\\theta}\\ell (x,y; \\theta)^t + \\lambda \\operatorname{I}\\right)^{-1}         \\right)^{-1}\\] <p>and computes</p> \\[ \\tilde{G}_{\\lambda}^{-1}(\\text{model}, \\text{loss}, b, \\theta) \\cdot v.\\] <p>for any given batch \\(b\\), where model is a torch.nn.Module and \\(v\\) is a vector or matrix.</p> <p>In other words, it switches the order of summation and inversion, which resolves to the <code>inverse harmonic mean</code> of the rank-\\(1\\) updates. The results are averaged over the batches provided by the data loader.</p> <p>The inverses of the rank-\\(1\\) updates are not calculated explicitly, but instead a vectorized version of the Sherman\u2013Morrison formula is applied.</p> <p>For more information, see Inverse Harmonic Mean.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model.</p> <p> TYPE: <code>Module</code> </p> <code>loss</code> <p>The loss function.</p> <p> TYPE: <code>Callable[[Tensor, Tensor], Tensor]</code> </p> <code>dataloader</code> <p>The data loader providing batches of data.</p> <p> TYPE: <code>DataLoader</code> </p> <code>restrict_to</code> <p>The parameters to restrict the differentiation to, i.e. the corresponding sub-matrix of the Jacobian. If None, the full Jacobian is used. Make sure the input matches the corrct dimension, i.e. the last dimension must be equal to the property <code>input_size</code>.</p> <p> TYPE: <code>Optional[Dict[str, Parameter]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/influence/torch/operator.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n    dataloader: DataLoader,\n    regularization: float,\n    restrict_to: Optional[Dict[str, nn.Parameter]] = None,\n):\n    if regularization &lt;= 0:\n        raise ValueError(\"regularization must be positive\")\n\n    self._regularization = regularization\n\n    batch_op = InverseHarmonicMeanBatchOperation(\n        model,\n        loss,\n        regularization,\n        restrict_to=restrict_to,\n    )\n    averaging = PointAveraging()\n    super().__init__(batch_op, dataloader, averaging)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.InverseHarmonicMeanOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.InverseHarmonicMeanOperator.apply_to_dict","title":"apply_to_dict","text":"<pre><code>apply_to_dict(mat: Dict[str, Tensor]) -&gt; Dict[str, Tensor]\n</code></pre> <p>Applies the operator to a dictionary of tensors, compatible to the structure defined by the property <code>input_dict_structure</code>.</p> PARAMETER DESCRIPTION <code>mat</code> <p>dictionary of tensors, whose keys and shapes match the property <code>input_dict_structure</code>.</p> <p> TYPE: <code>Dict[str, Tensor]</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dictionary of tensors after applying the operator</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def apply_to_dict(self, mat: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Applies the operator to a dictionary of tensors, compatible to the structure\n    defined by the property `input_dict_structure`.\n\n    Args:\n        mat: dictionary of tensors, whose keys and shapes match the property\n            `input_dict_structure`.\n\n    Returns:\n        A dictionary of tensors after applying the operator\n    \"\"\"\n\n    if not self._validate_mat_dict(mat):\n        raise ValueError(\n            f\"Incompatible input structure, expected (excluding batch\"\n            f\"dimension): \\n {self.input_dict_structure}\"\n        )\n\n    return self._apply_to_dict(self._dict_to_device(mat))\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.LissaOperator","title":"LissaOperator","text":"<pre><code>LissaOperator(\n    batch_operation: BatchOperationType,\n    data: DataLoader,\n    regularization: Optional[float] = None,\n    maxiter: int = 1000,\n    dampen: float = 0.0,\n    scale: float = 10.0,\n    rtol: float = 0.0001,\n    progress: bool = False,\n    warn_on_max_iteration: bool = True,\n)\n</code></pre> <p>               Bases: <code>TensorOperator</code>, <code>Generic[BatchOperationType]</code></p> <p>Uses LISSA, Linear time Stochastic Second-Order Algorithm, to iteratively approximate the solution of the system \\((A + \\lambda I)x = b\\). This is done with the update</p> \\[(A + \\lambda I)^{-1}_{j+1} b = b + (I - d) \\ (A + \\lambda I) -     \\frac{(A + \\lambda I)^{-1}_j b}{s},\\] <p>where \\(I\\) is the identity matrix, \\(d\\) is a dampening term and \\(s\\) a scaling factor that are applied to help convergence. For details, see Linear time Stochastic Second-Order Approximation (LiSSA)</p> PARAMETER DESCRIPTION <code>batch_operation</code> <p>The <code>BatchOperation</code> representing the action of A on a batch of the data loader.</p> <p> TYPE: <code>BatchOperationType</code> </p> <code>data</code> <p>a pytorch dataloader</p> <p> TYPE: <code>DataLoader</code> </p> <code>regularization</code> <p>Optional regularization parameter added to the Hessian-vector product for numerical stability.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>maxiter</code> <p>Maximum number of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>dampen</code> <p>Dampening factor, defaults to 0 for no dampening.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>scale</code> <p>Scaling factor, defaults to 10.</p> <p> TYPE: <code>float</code> DEFAULT: <code>10.0</code> </p> <code>rtol</code> <p>tolerance to use for early stopping</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0001</code> </p> <code>progress</code> <p>If True, display progress bars.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>warn_on_max_iteration</code> <p>If True, logs a warning, if the desired tolerance is not achieved within <code>maxiter</code> iterations. If False, the log level for this information is <code>logging.DEBUG</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/influence/torch/operator.py</code> <pre><code>def __init__(\n    self,\n    batch_operation: BatchOperationType,\n    data: DataLoader,\n    regularization: Optional[float] = None,\n    maxiter: int = 1000,\n    dampen: float = 0.0,\n    scale: float = 10.0,\n    rtol: float = 1e-4,\n    progress: bool = False,\n    warn_on_max_iteration: bool = True,\n):\n    if regularization is not None and regularization &lt; 0:\n        raise ValueError(\"regularization must be non-negative\")\n\n    self.data = data\n    self.warn_on_max_iteration = warn_on_max_iteration\n    self.progress = progress\n    self.rtol = rtol\n    self.scale = scale\n    self.dampen = dampen\n    self.maxiter = maxiter\n    self.batch_operation = batch_operation\n    self._regularization = regularization\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.LissaOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.LowRankOperator","title":"LowRankOperator","text":"<pre><code>LowRankOperator(\n    low_rank_representation: LowRankProductRepresentation,\n    regularization: Optional[float] = None,\n    exact: bool = True,\n)\n</code></pre> <p>               Bases: <code>TensorOperator</code></p> <p>Given a low rank representation of a matrix</p> \\[ A = V D V^T\\] <p>with a diagonal matrix \\(D\\) and an optional regularization parameter \\(\\lambda\\), computes</p> <p>$$ (V D V^T+\\lambda I)^{-1}b$$.</p> <p>Depending on the value of the <code>exact</code> flag, the inverse action is computed exactly using the [Sherman\u2013Morrison\u2013Woodbury formula] (https://en.wikipedia.org/wiki/Woodbury_matrix_identity). If <code>exact</code> is set to <code>False</code>, the inverse action is approximated by</p> \\[ V^T(D+\\lambda I)^{-1}Vb\\] <p>Args:</p> Source code in <code>src/pydvl/influence/torch/operator.py</code> <pre><code>def __init__(\n    self,\n    low_rank_representation: LowRankProductRepresentation,\n    regularization: Optional[float] = None,\n    exact: bool = True,\n):\n    if exact and (regularization is None or regularization &lt;= 0):\n        raise ValueError(\"regularization must be positive when exact=True\")\n    elif regularization is not None and regularization &lt; 0:\n        raise ValueError(\"regularization must be non-negative\")\n\n    self._regularization = regularization\n    self._exact = exact\n    self._low_rank_representation = low_rank_representation\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.LowRankOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.MatrixOperator","title":"MatrixOperator","text":"<pre><code>MatrixOperator(matrix: Tensor)\n</code></pre> <p>               Bases: <code>TensorOperator</code></p> <p>A simple wrapper for a torch.Tensor acting as a matrix mapping.</p> Source code in <code>src/pydvl/influence/torch/operator.py</code> <pre><code>def __init__(self, matrix: torch.Tensor):\n    self.matrix = matrix\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator.MatrixOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator._AveragingBatchOperator","title":"_AveragingBatchOperator","text":"<pre><code>_AveragingBatchOperator(\n    batch_operation: BatchOperationType,\n    dataloader: DataLoader,\n    averager: TensorAveragingType,\n)\n</code></pre> <p>               Bases: <code>TensorDictOperator</code>, <code>Generic[BatchOperationType, TensorAveragingType]</code></p> <p>Class for aggregating batch operations over a dataset using a provided data loader and aggregator.</p> <p>This class facilitates the application of a batch operation across multiple batches of data, aggregating the results using a specified sequence aggregator.</p> ATTRIBUTE DESCRIPTION <code>batch_operation</code> <p>The batch operation to apply.</p> <p> </p> <code>dataloader</code> <p>The data loader providing batches of data.</p> <p> </p> <code>averaging</code> <p>The sequence aggregator to aggregate the results of the batch operations.</p> <p> </p> Source code in <code>src/pydvl/influence/torch/operator.py</code> <pre><code>def __init__(\n    self,\n    batch_operation: BatchOperationType,\n    dataloader: DataLoader,\n    averager: TensorAveragingType,\n):\n    self.batch_operation = batch_operation\n    self.dataloader = dataloader\n    self.averaging = averager\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator._AveragingBatchOperator.apply","title":"apply","text":"<pre><code>apply(tensor: TensorType) -&gt; TensorType\n</code></pre> <p>Applies the operator to a tensor.</p> PARAMETER DESCRIPTION <code>tensor</code> <p>A tensor, whose tailing dimension must conform to the operator's input size</p> <p> TYPE: <code>TensorType</code> </p> RETURNS DESCRIPTION <code>TensorType</code> <p>A tensor representing the result of the operator application.</p> Source code in <code>src/pydvl/influence/types.py</code> <pre><code>def apply(self, tensor: TensorType) -&gt; TensorType:\n    \"\"\"\n    Applies the operator to a tensor.\n\n    Args:\n        tensor: A tensor, whose tailing dimension must conform to the\n            operator's input size\n\n    Returns:\n        A tensor representing the result of the operator application.\n    \"\"\"\n    self._validate_tensor_input(tensor)\n    return self._apply(tensor)\n</code></pre>"},{"location":"api/pydvl/influence/torch/operator/#pydvl.influence.torch.operator._AveragingBatchOperator.apply_to_dict","title":"apply_to_dict","text":"<pre><code>apply_to_dict(mat: Dict[str, Tensor]) -&gt; Dict[str, Tensor]\n</code></pre> <p>Applies the operator to a dictionary of tensors, compatible to the structure defined by the property <code>input_dict_structure</code>.</p> PARAMETER DESCRIPTION <code>mat</code> <p>dictionary of tensors, whose keys and shapes match the property <code>input_dict_structure</code>.</p> <p> TYPE: <code>Dict[str, Tensor]</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dictionary of tensors after applying the operator</p> Source code in <code>src/pydvl/influence/torch/base.py</code> <pre><code>def apply_to_dict(self, mat: Dict[str, torch.Tensor]) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Applies the operator to a dictionary of tensors, compatible to the structure\n    defined by the property `input_dict_structure`.\n\n    Args:\n        mat: dictionary of tensors, whose keys and shapes match the property\n            `input_dict_structure`.\n\n    Returns:\n        A dictionary of tensors after applying the operator\n    \"\"\"\n\n    if not self._validate_mat_dict(mat):\n        raise ValueError(\n            f\"Incompatible input structure, expected (excluding batch\"\n            f\"dimension): \\n {self.input_dict_structure}\"\n        )\n\n    return self._apply_to_dict(self._dict_to_device(mat))\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/","title":"Preconditioner","text":""},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner","title":"pydvl.influence.torch.preconditioner","text":""},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.JacobiPreconditioner","title":"JacobiPreconditioner","text":"<pre><code>JacobiPreconditioner(num_samples_estimator: int = 1)\n</code></pre> <p>               Bases: <code>Preconditioner</code></p> <p>Pre-conditioner for improving the convergence of CG for systems of the form</p> \\[ ( A + \\lambda \\operatorname{I})x = \\operatorname{rhs} \\] <p>The JacobiPreConditioner uses the diagonal information of the matrix \\(A\\). The diagonal elements are not computed directly but estimated via Hutchinson's estimator.</p> \\[ M = \\frac{1}{m} \\sum_{i=1}^m u_i \\odot Au_i + \\lambda \\operatorname{I} \\] <p>where \\(u_i\\) are i.i.d. Gaussian random vectors. Works well in the case the matrix \\(A + \\lambda \\operatorname{I}\\) is diagonal dominant. For more information, see the documentation of Conjugate Gradient Args:     num_samples_estimator: number of samples to use in computation of         Hutchinson's estimator</p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>def __init__(self, num_samples_estimator: int = 1):\n    self.num_samples_estimator = num_samples_estimator\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.JacobiPreconditioner._fit","title":"_fit","text":"<pre><code>_fit(operator: 'TensorOperator', regularization: Optional[float] = None)\n</code></pre> <p>Fits by computing an estimate of the diagonal of the matrix represented by a TensorOperator via Hutchinson's estimator</p> PARAMETER DESCRIPTION <code>operator</code> <p>The preconditioner is fitted to this operator</p> <p> TYPE: <code>'TensorOperator'</code> </p> <code>regularization</code> <p>regularization parameter \\(\\lambda\\) in \\((A+\\lambda I)x=b\\)</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>def _fit(\n    self,\n    operator: \"TensorOperator\",\n    regularization: Optional[float] = None,\n):\n    r\"\"\"\n    Fits by computing an estimate of the diagonal of the matrix represented by\n    a [TensorOperator][pydvl.influence.torch.base.TensorOperator]\n    via Hutchinson's estimator\n\n    Args:\n        operator: The preconditioner is fitted to this operator\n        regularization: regularization parameter\n            $\\lambda$ in $(A+\\lambda I)x=b$\n    \"\"\"\n    random_samples = torch.randn(\n        self.num_samples_estimator,\n        operator.input_size,\n        device=operator.device,\n        dtype=operator.dtype,\n    )\n\n    diagonal_estimate = torch.sum(\n        torch.mul(random_samples, operator.apply(random_samples)), dim=0\n    )\n\n    diagonal_estimate /= self.num_samples_estimator\n    self._diag = diagonal_estimate\n    self._reg = regularization\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.JacobiPreconditioner.fit","title":"fit","text":"<pre><code>fit(operator: 'TensorOperator', regularization: Optional[float] = None)\n</code></pre> <p>Implement this to fit the pre-conditioner to the matrix represented by the mat_mat_prod Args:     operator: The preconditioner is fitted to this operator     regularization: regularization parameter \\(\\lambda\\) in the equation         $ ( A + \\lambda \\operatorname{I})x = \\operatorname{rhs} $ Returns:     self</p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>def fit(\n    self,\n    operator: \"TensorOperator\",\n    regularization: Optional[float] = None,\n):\n    r\"\"\"\n    Implement this to fit the pre-conditioner to the matrix represented by the\n    mat_mat_prod\n    Args:\n        operator: The preconditioner is fitted to this operator\n        regularization: regularization parameter $\\lambda$ in the equation\n            $ ( A + \\lambda \\operatorname{I})x = \\operatorname{rhs} $\n    Returns:\n        self\n    \"\"\"\n    self._validate_regularization(regularization)\n    return self._fit(operator, regularization)\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.JacobiPreconditioner.solve","title":"solve","text":"<pre><code>solve(rhs: Tensor) -&gt; Tensor\n</code></pre> <p>Solve the equation \\(M@Z = \\operatorname{rhs}\\) Args:     rhs: right hand side of the equation, corresponds to the residuum vector         (or matrix) in the conjugate gradient method</p> RETURNS DESCRIPTION <code>Tensor</code> <p>solution \\(M^{-1}\\operatorname{rhs}\\)</p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>def solve(self, rhs: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Solve the equation $M@Z = \\operatorname{rhs}$\n    Args:\n        rhs: right hand side of the equation, corresponds to the residuum vector\n            (or matrix) in the conjugate gradient method\n\n    Returns:\n        solution $M^{-1}\\operatorname{rhs}$\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    return self._solve(rhs)\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.NystroemPreconditioner","title":"NystroemPreconditioner","text":"<pre><code>NystroemPreconditioner(rank: int)\n</code></pre> <p>               Bases: <code>Preconditioner</code></p> <p>Pre-conditioner for improving the convergence of CG for systems of the form</p> \\[ (A + \\lambda \\operatorname{I})x = \\operatorname{rhs} \\] <p>The NystroemPreConditioner computes a low-rank approximation</p> \\[ A_{\\text{nys}} = (A \\Omega)(\\Omega^T A \\Omega)^{\\dagger}(A \\Omega)^T = U \\Sigma U^T, \\] <p>where \\((\\cdot)^{\\dagger}\\) denotes the Moore-Penrose inverse, and uses the matrix</p> \\[ M^{-1} = (\\lambda + \\sigma_{\\text{rank}})U(\\Sigma+     \\lambda \\operatorname{I})^{-1}U^T+(\\operatorname{I} - UU^T) \\] <p>for pre-conditioning, where \\(  \\sigma_{\\text{rank}} \\) is the smallest eigenvalue of the low-rank approximation.</p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>def __init__(self, rank: int):\n    self._rank = rank\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.NystroemPreconditioner._fit","title":"_fit","text":"<pre><code>_fit(operator: 'TensorOperator', regularization: Optional[float] = None)\n</code></pre> <p>Fits by computing a low-rank approximation of the matrix represented by <code>mat_mat_prod</code> via Nystroem approximation</p> PARAMETER DESCRIPTION <code>operator</code> <p>The preconditioner is fitted to this operator</p> <p> TYPE: <code>'TensorOperator'</code> </p> <code>regularization</code> <p>regularization parameter \\(\\lambda\\)  in \\((A+\\lambda I)x=b\\)</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>def _fit(\n    self,\n    operator: \"TensorOperator\",\n    regularization: Optional[float] = None,\n):\n    r\"\"\"\n    Fits by computing a low-rank approximation of the matrix represented by\n    `mat_mat_prod` via Nystroem approximation\n\n    Args:\n        operator: The preconditioner is fitted to this operator\n        regularization: regularization parameter\n            $\\lambda$  in $(A+\\lambda I)x=b$\n    \"\"\"\n\n    self._low_rank_approx = operator_nystroem_approximation(operator, self._rank)\n    self._reg = regularization\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.NystroemPreconditioner.fit","title":"fit","text":"<pre><code>fit(operator: 'TensorOperator', regularization: Optional[float] = None)\n</code></pre> <p>Implement this to fit the pre-conditioner to the matrix represented by the mat_mat_prod Args:     operator: The preconditioner is fitted to this operator     regularization: regularization parameter \\(\\lambda\\) in the equation         $ ( A + \\lambda \\operatorname{I})x = \\operatorname{rhs} $ Returns:     self</p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>def fit(\n    self,\n    operator: \"TensorOperator\",\n    regularization: Optional[float] = None,\n):\n    r\"\"\"\n    Implement this to fit the pre-conditioner to the matrix represented by the\n    mat_mat_prod\n    Args:\n        operator: The preconditioner is fitted to this operator\n        regularization: regularization parameter $\\lambda$ in the equation\n            $ ( A + \\lambda \\operatorname{I})x = \\operatorname{rhs} $\n    Returns:\n        self\n    \"\"\"\n    self._validate_regularization(regularization)\n    return self._fit(operator, regularization)\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.NystroemPreconditioner.solve","title":"solve","text":"<pre><code>solve(rhs: Tensor) -&gt; Tensor\n</code></pre> <p>Solve the equation \\(M@Z = \\operatorname{rhs}\\) Args:     rhs: right hand side of the equation, corresponds to the residuum vector         (or matrix) in the conjugate gradient method</p> RETURNS DESCRIPTION <code>Tensor</code> <p>solution \\(M^{-1}\\operatorname{rhs}\\)</p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>def solve(self, rhs: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Solve the equation $M@Z = \\operatorname{rhs}$\n    Args:\n        rhs: right hand side of the equation, corresponds to the residuum vector\n            (or matrix) in the conjugate gradient method\n\n    Returns:\n        solution $M^{-1}\\operatorname{rhs}$\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    return self._solve(rhs)\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.Preconditioner","title":"Preconditioner","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for implementing pre-conditioners for improving the convergence of CG for systems of the form</p> \\[ ( A + \\lambda \\operatorname{I})x = \\operatorname{rhs} \\] <p>i.e. a matrix \\(M\\) such that \\(M^{-1}(A + \\lambda \\operatorname{I})\\) has a better condition number than \\(A + \\lambda \\operatorname{I}\\).</p>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.Preconditioner.fit","title":"fit","text":"<pre><code>fit(operator: 'TensorOperator', regularization: Optional[float] = None)\n</code></pre> <p>Implement this to fit the pre-conditioner to the matrix represented by the mat_mat_prod Args:     operator: The preconditioner is fitted to this operator     regularization: regularization parameter \\(\\lambda\\) in the equation         $ ( A + \\lambda \\operatorname{I})x = \\operatorname{rhs} $ Returns:     self</p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>def fit(\n    self,\n    operator: \"TensorOperator\",\n    regularization: Optional[float] = None,\n):\n    r\"\"\"\n    Implement this to fit the pre-conditioner to the matrix represented by the\n    mat_mat_prod\n    Args:\n        operator: The preconditioner is fitted to this operator\n        regularization: regularization parameter $\\lambda$ in the equation\n            $ ( A + \\lambda \\operatorname{I})x = \\operatorname{rhs} $\n    Returns:\n        self\n    \"\"\"\n    self._validate_regularization(regularization)\n    return self._fit(operator, regularization)\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.Preconditioner.solve","title":"solve","text":"<pre><code>solve(rhs: Tensor) -&gt; Tensor\n</code></pre> <p>Solve the equation \\(M@Z = \\operatorname{rhs}\\) Args:     rhs: right hand side of the equation, corresponds to the residuum vector         (or matrix) in the conjugate gradient method</p> RETURNS DESCRIPTION <code>Tensor</code> <p>solution \\(M^{-1}\\operatorname{rhs}\\)</p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>def solve(self, rhs: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Solve the equation $M@Z = \\operatorname{rhs}$\n    Args:\n        rhs: right hand side of the equation, corresponds to the residuum vector\n            (or matrix) in the conjugate gradient method\n\n    Returns:\n        solution $M^{-1}\\operatorname{rhs}$\n\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n\n    return self._solve(rhs)\n</code></pre>"},{"location":"api/pydvl/influence/torch/preconditioner/#pydvl.influence.torch.preconditioner.Preconditioner.to","title":"to  <code>abstractmethod</code>","text":"<pre><code>to(device: device) -&gt; Preconditioner\n</code></pre> <p>Implement this to move the (potentially fitted) preconditioner to a specific device</p> Source code in <code>src/pydvl/influence/torch/preconditioner.py</code> <pre><code>@abstractmethod\ndef to(self, device: torch.device) -&gt; Preconditioner:\n    \"\"\"Implement this to move the (potentially fitted) preconditioner to a\n    specific device\"\"\"\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/","title":"Util","text":""},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util","title":"pydvl.influence.torch.util","text":""},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.TorchTensorContainerType","title":"TorchTensorContainerType  <code>module-attribute</code>","text":"<pre><code>TorchTensorContainerType = Union[\n    Tensor, Collection[Tensor], Mapping[str, Tensor]\n]\n</code></pre> <p>Type for a PyTorch tensor or a container thereof.</p>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.BlockMode","title":"BlockMode","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for different modes of grouping model parameters.</p> ATTRIBUTE DESCRIPTION <code>LAYER_WISE</code> <p>Groups parameters by layers of the model.</p> <p> </p> <code>PARAMETER_WISE</code> <p>Groups parameters individually.</p> <p> </p> <code>FULL</code> <p>Groups all parameters together.</p> <p> </p>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.EkfacRepresentation","title":"EkfacRepresentation  <code>dataclass</code>","text":"<pre><code>EkfacRepresentation(\n    layer_names: Iterable[str],\n    layers_module: Iterable[Module],\n    evecs_a: Iterable[Tensor],\n    evecs_g: Iterable[Tensor],\n    diags: Iterable[Tensor],\n)\n</code></pre> <p>Container class for the EKFAC representation of the Hessian. It can be iterated over to get the layers names and their corresponding module, eigenvectors and diagonal elements of the factorized Hessian matrix.</p> PARAMETER DESCRIPTION <code>layer_names</code> <p>Names of the layers.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>layers_module</code> <p>The layers.</p> <p> TYPE: <code>Iterable[Module]</code> </p> <code>evecs_a</code> <p>The a eigenvectors of the ekfac representation.</p> <p> TYPE: <code>Iterable[Tensor]</code> </p> <code>evecs_g</code> <p>The g eigenvectors of the ekfac representation.</p> <p> TYPE: <code>Iterable[Tensor]</code> </p> <code>diags</code> <p>The diagonal elements of the factorized Hessian matrix.</p> <p> TYPE: <code>Iterable[Tensor]</code> </p>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.EkfacRepresentation.get_layer_evecs","title":"get_layer_evecs","text":"<pre><code>get_layer_evecs() -&gt; Tuple[Dict[str, Tensor], Dict[str, Tensor]]\n</code></pre> <p>It returns two dictionaries, one for the a eigenvectors and one for the g eigenvectors, with the layer names as keys. The eigenvectors are in the same order as the layers in the model.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def get_layer_evecs(\n    self,\n) -&gt; Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n    \"\"\"\n    It returns two dictionaries, one for the a eigenvectors and one for the g\n    eigenvectors, with the layer names as keys. The eigenvectors are in the same\n    order as the layers in the model.\n    \"\"\"\n    evecs_a_dict = {layer_name: evec_a for layer_name, (_, evec_a, _, _) in self}\n    evecs_g_dict = {layer_name: evec_g for layer_name, (_, _, evec_g, _) in self}\n    return evecs_a_dict, evecs_g_dict\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.ModelInfoMixin","title":"ModelInfoMixin","text":"<pre><code>ModelInfoMixin(model: Module)\n</code></pre> <p>A mixin class for classes that contain information about a model.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def __init__(self, model: torch.nn.Module):\n    self.model = model\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.ModelParameterDictBuilder","title":"ModelParameterDictBuilder  <code>dataclass</code>","text":"<pre><code>ModelParameterDictBuilder(model: Module, detach: bool = True)\n</code></pre> <p>A builder class for creating ordered dictionaries of model parameters based on specified block modes or custom blocking structures.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>The neural network model.</p> <p> TYPE: <code>Module</code> </p> <code>detach</code> <p>Whether to detach the parameters from the computation graph.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.ModelParameterDictBuilder.build","title":"build","text":"<pre><code>build(\n    block_structure: OrderedDict[str, List[str]]\n) -&gt; Dict[str, Dict[str, Parameter]]\n</code></pre> <p>Builds an ordered dictionary of model parameters based on the specified block structure represented by an ordered dictionary, where the keys are block identifiers and the values are lists of model parameter names contained in this block.</p> PARAMETER DESCRIPTION <code>block_structure</code> <p>The block structure specifying how to group the parameters.</p> <p> TYPE: <code>OrderedDict[str, List[str]]</code> </p> RETURNS DESCRIPTION <code>Dict[str, Dict[str, Parameter]]</code> <p>An ordered dictionary of ordered dictionaries, where the outer dictionary's</p> <code>Dict[str, Dict[str, Parameter]]</code> <p>keys are block identifiers and the inner dictionaries map parameter names</p> <code>Dict[str, Dict[str, Parameter]]</code> <p>to parameters.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def build(\n    self, block_structure: OrderedDict[str, List[str]]\n) -&gt; Dict[str, Dict[str, torch.nn.Parameter]]:\n    \"\"\"\n    Builds an ordered dictionary of model parameters based on the specified block\n    structure represented by an ordered dictionary, where the keys are block\n    identifiers and the values are lists of model parameter names contained in\n    this block.\n\n    Args:\n        block_structure: The block structure specifying how to group the parameters.\n\n    Returns:\n        An ordered dictionary of ordered dictionaries, where the outer dictionary's\n        keys are block identifiers and the inner dictionaries map parameter names\n        to parameters.\n    \"\"\"\n    parameter_dict = {}\n\n    for block_name, parameter_names in block_structure.items():\n        inner_ordered_dict = {}\n        for parameter_name in parameter_names:\n            parameter = self._extract_parameter_by_name(parameter_name)\n            if parameter.requires_grad:\n                inner_ordered_dict[parameter_name] = self._optional_detach(\n                    parameter\n                )\n            else:\n                warnings.warn(\n                    f\"The parameter {parameter_name} from the block \"\n                    f\"{block_name} is mark as not trainable in the model \"\n                    f\"and will be excluded from the computation.\"\n                )\n        parameter_dict[block_name] = inner_ordered_dict\n\n    return parameter_dict\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.ModelParameterDictBuilder.build_from_block_mode","title":"build_from_block_mode","text":"<pre><code>build_from_block_mode(block_mode: BlockMode) -&gt; Dict[str, Dict[str, Parameter]]\n</code></pre> <p>Builds an ordered dictionary of model parameters based on the specified block mode or custom blocking structure represented by an ordered dictionary, where the keys are block identifiers and the values are lists of model parameter names contained in this block.</p> PARAMETER DESCRIPTION <code>block_mode</code> <p>The block mode specifying how to group the parameters.</p> <p> TYPE: <code>BlockMode</code> </p> RETURNS DESCRIPTION <code>Dict[str, Dict[str, Parameter]]</code> <p>An ordered dictionary of ordered dictionaries, where the outer dictionary's</p> <code>Dict[str, Dict[str, Parameter]]</code> <p>keys are block identifiers and the inner dictionaries map parameter names</p> <code>Dict[str, Dict[str, Parameter]]</code> <p>to parameters.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def build_from_block_mode(\n    self, block_mode: BlockMode\n) -&gt; Dict[str, Dict[str, torch.nn.Parameter]]:\n    \"\"\"\n    Builds an ordered dictionary of model parameters based on the specified block\n    mode or custom blocking structure represented by an ordered dictionary, where\n    the keys are block identifiers and the values are lists of model parameter names\n    contained in this block.\n\n    Args:\n        block_mode: The block mode specifying how to group the parameters.\n\n    Returns:\n        An ordered dictionary of ordered dictionaries, where the outer dictionary's\n        keys are block identifiers and the inner dictionaries map parameter names\n        to parameters.\n    \"\"\"\n\n    block_mode_mapping = {\n        BlockMode.FULL: self._build_full,\n        BlockMode.PARAMETER_WISE: self._build_parameter_wise,\n        BlockMode.LAYER_WISE: self._build_layer_wise,\n    }\n\n    parameter_dict_func = block_mode_mapping.get(block_mode, None)\n\n    if parameter_dict_func is None:\n        raise ValueError(f\"Unknown block mode {block_mode}.\")\n\n    return self.build(parameter_dict_func())\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.NestedTorchCatAggregator","title":"NestedTorchCatAggregator","text":"<p>               Bases: <code>NestedSequenceAggregator[Tensor]</code></p> <p>An aggregator that concatenates tensors using PyTorch's torch.cat function. Concatenation is done along the first two dimensions of the chunks.</p>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.NestedTorchCatAggregator.__call__","title":"__call__","text":"<pre><code>__call__(nested_sequence_of_tensors: NestedLazyChunkSequence[Tensor]) -&gt; Tensor\n</code></pre> <p>Aggregates tensors from a nested generator structure into a single tensor by concatenating. Each inner generator is first concatenated along dimension 1 into a tensor, and then these tensors are concatenated along dimension 0 together to form the final tensor.</p> PARAMETER DESCRIPTION <code>nested_sequence_of_tensors</code> <p>Object wrapping a generator of generators, where each inner generator yields <code>torch.Tensor</code> objects.</p> <p> TYPE: <code>NestedLazyChunkSequence[Tensor]</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A single tensor formed by concatenating all tensors from the nested</p> <code>Tensor</code> <p>generators.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def __call__(\n    self, nested_sequence_of_tensors: NestedLazyChunkSequence[torch.Tensor]\n) -&gt; torch.Tensor:\n    \"\"\"\n    Aggregates tensors from a nested generator structure into a single tensor by\n    concatenating. Each inner generator is first concatenated along dimension 1 into\n    a tensor, and then these tensors are concatenated along dimension 0 together to\n    form the final tensor.\n\n    Args:\n        nested_sequence_of_tensors: Object wrapping a generator of generators,\n            where each inner generator yields `torch.Tensor` objects.\n\n    Returns:\n        A single tensor formed by concatenating all tensors from the nested\n        generators.\n\n    \"\"\"\n\n    outer_gen = cast(\n        Iterator[Iterator[torch.Tensor]],\n        nested_sequence_of_tensors.generator_factory(),\n    )\n    len_outer_generator = nested_sequence_of_tensors.len_outer_generator\n    if len_outer_generator is not None:\n        outer_gen = cast(\n            Iterator[Iterator[torch.Tensor]],\n            tqdm(outer_gen, total=len_outer_generator, desc=\"Row blocks\"),\n        )\n\n    return torch.cat(\n        list(\n            map(\n                lambda tensor_gen: torch.cat(list(tensor_gen), dim=1),\n                outer_gen,\n            )\n        )\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.TorchCatAggregator","title":"TorchCatAggregator","text":"<p>               Bases: <code>SequenceAggregator[Tensor]</code></p> <p>An aggregator that concatenates tensors using PyTorch's torch.cat function. Concatenation is done along the first dimension of the chunks.</p>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.TorchCatAggregator.__call__","title":"__call__","text":"<pre><code>__call__(tensor_sequence: LazyChunkSequence[Tensor]) -&gt; Tensor\n</code></pre> <p>Aggregates tensors from a single-level generator into a single tensor by concatenating them. This method is a straightforward way to combine a sequence of tensors into one larger tensor.</p> PARAMETER DESCRIPTION <code>tensor_sequence</code> <p>Object wrapping a generator that yields <code>torch.Tensor</code> objects.</p> <p> TYPE: <code>LazyChunkSequence[Tensor]</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A single tensor formed by concatenating all tensors from the generator. The concatenation is performed along the default dimension (0).</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def __call__(\n    self,\n    tensor_sequence: LazyChunkSequence[torch.Tensor],\n) -&gt; torch.Tensor:\n    \"\"\"\n    Aggregates tensors from a single-level generator into a single tensor by\n    concatenating them. This method is a straightforward way to combine a sequence\n    of tensors into one larger tensor.\n\n    Args:\n        tensor_sequence: Object wrapping a generator that yields `torch.Tensor`\n            objects.\n\n    Returns:\n        A single tensor formed by concatenating all tensors from the generator.\n            The concatenation is performed along the default dimension (0).\n    \"\"\"\n    t_gen = cast(Iterator[torch.Tensor], tensor_sequence.generator_factory())\n    len_generator = tensor_sequence.len_generator\n    if len_generator is not None:\n        t_gen = cast(\n            Iterator[torch.Tensor], tqdm(t_gen, total=len_generator, desc=\"Blocks\")\n        )\n\n    return torch.cat(list(t_gen))\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.TorchLinalgEighException","title":"TorchLinalgEighException","text":"<pre><code>TorchLinalgEighException(original_exception: RuntimeError)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception to wrap a RunTimeError raised by torch.linalg.eigh, when used with large matrices, see https://github.com/pytorch/pytorch/issues/92141</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def __init__(self, original_exception: RuntimeError):\n    func = torch.linalg.eigh\n    err_msg = (\n        f\"A RunTimeError occurred in '{func.__module__}.{func.__qualname__}'. \"\n        \"This might be related to known issues with \"\n        \"[torch.linalg.eigh][torch.linalg.eigh] on certain matrix sizes.\\n \"\n        \"For more details, refer to \"\n        \"https://github.com/pytorch/pytorch/issues/92141. \\n\"\n        \"In this case, consider to use a different implementation, which does not \"\n        \"depend on the usage of [torch.linalg.eigh][torch.linalg.eigh].\\n\"\n        f\" Inspect the original exception message: \\n{str(original_exception)}\"\n    )\n    super().__init__(err_msg)\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.TorchNumpyConverter","title":"TorchNumpyConverter","text":"<pre><code>TorchNumpyConverter(device: Optional[device] = None)\n</code></pre> <p>               Bases: <code>NumpyConverter[Tensor]</code></p> <p>Helper class for converting between torch.Tensor and numpy.ndarray</p> PARAMETER DESCRIPTION <code>device</code> <p>Optional device parameter to move the resulting torch tensors to the specified device</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def __init__(self, device: Optional[torch.device] = None):\n    self.device = device\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.TorchNumpyConverter.from_numpy","title":"from_numpy","text":"<pre><code>from_numpy(x: NDArray) -&gt; Tensor\n</code></pre> <p>Convert a numpy.ndarray to torch.Tensor and optionally move it to a provided device</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def from_numpy(self, x: NDArray) -&gt; torch.Tensor:\n    \"\"\"\n    Convert a [numpy.ndarray][numpy.ndarray] to [torch.Tensor][torch.Tensor] and\n    optionally move it to a provided device\n    \"\"\"\n    t = torch.from_numpy(x)\n    if self.device is not None:\n        t = t.to(self.device)\n    return t\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.TorchNumpyConverter.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy(x: Tensor) -&gt; NDArray\n</code></pre> <p>Convert a detached torch.Tensor to numpy.ndarray</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def to_numpy(self, x: torch.Tensor) -&gt; NDArray:\n    \"\"\"\n    Convert a detached [torch.Tensor][torch.Tensor] to\n    [numpy.ndarray][numpy.ndarray]\n    \"\"\"\n    arr: NDArray = x.cpu().numpy()\n    return arr\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.align_structure","title":"align_structure","text":"<pre><code>align_structure(\n    source: Mapping[str, Tensor], target: TorchTensorContainerType\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>This function transforms <code>target</code> to have the same structure as <code>source</code>, i.e., it should be a dictionary with the same keys as <code>source</code> and each corresponding value in <code>target</code> should have the same shape as the value in <code>source</code>.</p> PARAMETER DESCRIPTION <code>source</code> <p>The reference dictionary containing PyTorch tensors.</p> <p> TYPE: <code>Mapping[str, Tensor]</code> </p> <code>target</code> <p>The input to be harmonized. It can be a dictionary, tuple, or tensor.</p> <p> TYPE: <code>TorchTensorContainerType</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>The harmonized version of <code>target</code>.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>target</code> cannot be harmonized to match <code>source</code>.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def align_structure(\n    source: Mapping[str, torch.Tensor],\n    target: TorchTensorContainerType,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    This function transforms `target` to have the same structure as `source`, i.e.,\n    it should be a dictionary with the same keys as `source` and each corresponding\n    value in `target` should have the same shape as the value in `source`.\n\n    Args:\n        source: The reference dictionary containing PyTorch tensors.\n        target: The input to be harmonized. It can be a dictionary, tuple, or tensor.\n\n    Returns:\n        The harmonized version of `target`.\n\n    Raises:\n        ValueError: If `target` cannot be harmonized to match `source`.\n    \"\"\"\n\n    tangent_dict: Dict[str, torch.Tensor]\n\n    if isinstance(target, dict):\n        if list(target.keys()) != list(source.keys()):\n            raise ValueError(\"The keys in 'target' do not match the keys in 'source'.\")\n\n        if [v.shape for v in target.values()] != [v.shape for v in source.values()]:\n            raise ValueError(\n                \"The shapes of the values in 'target' do not match the shapes \"\n                \"of the values in 'source'.\"\n            )\n\n        tangent_dict = target\n\n    elif isinstance(target, tuple) or isinstance(target, list):\n        if [v.shape for v in target] != [v.shape for v in source.values()]:\n            raise ValueError(\n                \"'target' is a tuple/list but its elements' shapes do not match \"\n                \"the shapes of the values in 'source'.\"\n            )\n\n        tangent_dict = dict(zip(source.keys(), target))\n\n    elif isinstance(target, torch.Tensor):\n        try:\n            tangent_dict = dict(\n                zip(\n                    source.keys(),\n                    reshape_vector_to_tensors(\n                        target, [p.shape for p in source.values()]\n                    ),\n                )\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"'target' is a tensor but cannot be reshaped to match 'source'. \"\n                f\"Original error: {e}\"\n            )\n\n    else:\n        raise ValueError(f\"'target' is of type {type(target)} which is not supported.\")\n\n    return tangent_dict\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.align_with_model","title":"align_with_model","text":"<pre><code>align_with_model(\n    x: TorchTensorContainerType, model: Module\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>Aligns an input to the model's parameter structure, i.e. transforms it into a dict with the same keys as model.named_parameters() and matching tensor shapes</p> PARAMETER DESCRIPTION <code>x</code> <p>The input to be aligned. It can be a dictionary, tuple, or tensor.</p> <p> TYPE: <code>TorchTensorContainerType</code> </p> <code>model</code> <p>model to use for alignment</p> <p> TYPE: <code>Module</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>The aligned version of <code>x</code>.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>x</code> cannot be aligned to match the model's parameters .</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def align_with_model(\n    x: TorchTensorContainerType, model: torch.nn.Module\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Aligns an input to the model's parameter structure, i.e. transforms it into a dict\n    with the same keys as model.named_parameters() and matching tensor shapes\n\n    Args:\n        x: The input to be aligned. It can be a dictionary, tuple, or tensor.\n        model: model to use for alignment\n\n    Returns:\n        The aligned version of `x`.\n\n    Raises:\n        ValueError: If `x` cannot be aligned to match the model's parameters .\n\n    \"\"\"\n    model_params = get_model_parameters(model, detach=False)\n    return align_structure(model_params, x)\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.empirical_cross_entropy_loss_fn","title":"empirical_cross_entropy_loss_fn","text":"<pre><code>empirical_cross_entropy_loss_fn(\n    model_output: Tensor, *args, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Computes the empirical cross entropy loss of the model output. This is the cross entropy loss of the model output without the labels. The function takes all the usual arguments and keyword arguments of the cross entropy loss function, so that it is compatible with the PyTorch cross entropy loss function. However, it ignores everything except the first argument, which is the model output.</p> PARAMETER DESCRIPTION <code>model_output</code> <p>The output of the model.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def empirical_cross_entropy_loss_fn(\n    model_output: torch.Tensor, *args, **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the empirical cross entropy loss of the model output. This is the\n    cross entropy loss of the model output without the labels. The function takes\n    all the usual arguments and keyword arguments of the cross entropy loss\n    function, so that it is compatible with the PyTorch cross entropy loss\n    function. However, it ignores everything except the first argument, which is\n    the model output.\n\n    Args:\n        model_output: The output of the model.\n    \"\"\"\n    probs_ = torch.softmax(model_output, dim=1)\n    log_probs_ = torch.log(probs_)\n    log_probs_ = torch.where(\n        torch.isfinite(log_probs_), log_probs_, torch.zeros_like(log_probs_)\n    )\n    return torch.sum(log_probs_ * probs_.detach() ** 0.5)\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.flatten_dimensions","title":"flatten_dimensions","text":"<pre><code>flatten_dimensions(\n    tensors: Iterable[Tensor],\n    shape: Optional[Tuple[int, ...]] = None,\n    concat_at: int = -1,\n) -&gt; Tensor\n</code></pre> <p>Flattens the dimensions of each tensor in the given iterable and concatenates them along a specified dimension.</p> <p>This function takes an iterable of PyTorch tensors and flattens each tensor. Optionally, each tensor can be reshaped to a specified shape before concatenation. The concatenation is performed along the dimension specified by <code>concat_at</code>.</p> PARAMETER DESCRIPTION <code>tensors</code> <p>An iterable containing PyTorch tensors to be flattened and concatenated.</p> <p> TYPE: <code>Iterable[Tensor]</code> </p> <code>shape</code> <p>A tuple representing the desired shape to which each tensor is reshaped before concatenation. If None, tensors are flattened to 1D.</p> <p> TYPE: <code>Optional[Tuple[int, ...]]</code> DEFAULT: <code>None</code> </p> <code>concat_at</code> <p>The dimension along which to concatenate the tensors.</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A single tensor resulting from the concatenation of the input tensors,</p> <code>Tensor</code> <p>each either flattened or reshaped as specified.</p> Example <pre><code>&gt;&gt;&gt; tensors = [torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]])]\n&gt;&gt;&gt; flatten_dimensions(tensors)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n\n&gt;&gt;&gt; flatten_dimensions(tensors, shape=(2, 2), concat_at=0)\ntensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])\n</code></pre> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def flatten_dimensions(\n    tensors: Iterable[torch.Tensor],\n    shape: Optional[Tuple[int, ...]] = None,\n    concat_at: int = -1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Flattens the dimensions of each tensor in the given iterable and concatenates them\n    along a specified dimension.\n\n    This function takes an iterable of PyTorch tensors and flattens each tensor.\n    Optionally, each tensor can be reshaped to a specified shape before concatenation.\n    The concatenation is performed along the dimension specified by `concat_at`.\n\n    Args:\n        tensors: An iterable containing PyTorch tensors to be flattened\n            and concatenated.\n        shape: A tuple representing the desired shape to which each tensor is reshaped\n            before concatenation. If None, tensors are flattened to 1D.\n        concat_at: The dimension along which to concatenate the tensors.\n\n    Returns:\n        A single tensor resulting from the concatenation of the input tensors,\n        each either flattened or reshaped as specified.\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; tensors = [torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]])]\n        &gt;&gt;&gt; flatten_dimensions(tensors)\n        tensor([1, 2, 3, 4, 5, 6, 7, 8])\n\n        &gt;&gt;&gt; flatten_dimensions(tensors, shape=(2, 2), concat_at=0)\n        tensor([[1, 2],\n                [3, 4],\n                [5, 6],\n                [7, 8]])\n        ```\n    \"\"\"\n    return torch.cat(\n        [t.reshape(-1) if shape is None else t.reshape(*shape) for t in tensors],\n        dim=concat_at,\n    )\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.get_model_parameters","title":"get_model_parameters","text":"<pre><code>get_model_parameters(\n    model: Module, detach: bool = True, require_grad_only: bool = True\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>Returns a dictionary of model parameters, optionally restricted to parameters requiring gradients and optionally detaching them from the computation graph.</p> PARAMETER DESCRIPTION <code>model</code> <p>The neural network model.</p> <p> TYPE: <code>Module</code> </p> <code>detach</code> <p>Whether to detach the parameters from the computation graph.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>require_grad_only</code> <p>Whether to include only parameters that require gradients.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Dict[str, Tensor]</code> <p>A dict of named model parameters.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def get_model_parameters(\n    model: torch.nn.Module, detach: bool = True, require_grad_only: bool = True\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Returns a dictionary of model parameters, optionally restricted to parameters\n    requiring gradients and optionally detaching them from the computation\n    graph.\n\n    Args:\n        model: The neural network model.\n        detach: Whether to detach the parameters from the computation graph.\n        require_grad_only: Whether to include only parameters that require gradients.\n\n    Returns:\n        A dict of named model parameters.\n    \"\"\"\n\n    parameter_dict = {}\n    for k, p in model.named_parameters():\n        if require_grad_only and not p.requires_grad:\n            continue\n        parameter_dict[k] = p.detach() if detach else p\n\n    return parameter_dict\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.reshape_vector_to_tensors","title":"reshape_vector_to_tensors","text":"<pre><code>reshape_vector_to_tensors(\n    input_vector: Tensor, target_shapes: Iterable[Tuple[int, ...]]\n) -&gt; Tuple[Tensor, ...]\n</code></pre> <p>Reshape a 1D tensor into multiple tensors with specified shapes.</p> <p>This function takes a 1D tensor (input_vector) and reshapes it into a series of tensors with shapes given by 'target_shapes'. The reshaped tensors are returned as a tuple in the same order as their corresponding shapes.</p> Note <p>The total number of elements in 'input_vector' must be equal to the     sum of the products of the shapes in 'target_shapes'.</p> PARAMETER DESCRIPTION <code>input_vector</code> <p>The 1D tensor to be reshaped. Must be 1D.</p> <p> TYPE: <code>Tensor</code> </p> <code>target_shapes</code> <p>An iterable of tuples. Each tuple defines the shape of a tensor to be reshaped from the 'input_vector'.</p> <p> TYPE: <code>Iterable[Tuple[int, ...]]</code> </p> RETURNS DESCRIPTION <code>Tuple[Tensor, ...]</code> <p>A tuple of reshaped tensors.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If 'input_vector' is not a 1D tensor or if the total number of elements in 'input_vector' does not match the sum of the products of the shapes in 'target_shapes'.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def reshape_vector_to_tensors(\n    input_vector: torch.Tensor, target_shapes: Iterable[Tuple[int, ...]]\n) -&gt; Tuple[torch.Tensor, ...]:\n    \"\"\"\n    Reshape a 1D tensor into multiple tensors with specified shapes.\n\n    This function takes a 1D tensor (input_vector) and reshapes it into a series of\n    tensors with shapes given by 'target_shapes'.\n    The reshaped tensors are returned as a tuple in the same order\n    as their corresponding shapes.\n\n    Note:\n        The total number of elements in 'input_vector' must be equal to the\n            sum of the products of the shapes in 'target_shapes'.\n\n    Args:\n        input_vector: The 1D tensor to be reshaped. Must be 1D.\n        target_shapes: An iterable of tuples. Each tuple defines the shape of a tensor\n            to be reshaped from the 'input_vector'.\n\n    Returns:\n        A tuple of reshaped tensors.\n\n    Raises:\n        ValueError: If 'input_vector' is not a 1D tensor or if the total\n            number of elements in 'input_vector' does not\n            match the sum of the products of the shapes in 'target_shapes'.\n    \"\"\"\n\n    if input_vector.dim() != 1:\n        raise ValueError(\"Input vector must be a 1D tensor\")\n\n    total_elements = sum(math.prod(shape) for shape in target_shapes)\n\n    if total_elements != input_vector.shape[0]:\n        raise ValueError(\n            f\"The total elements in shapes {total_elements} \"\n            f\"does not match the vector length {input_vector.shape[0]}\"\n        )\n\n    tensors = []\n    start = 0\n    for shape in target_shapes:\n        size = math.prod(shape)  # compute the total size of the tensor with this shape\n        tensors.append(\n            input_vector[start : start + size].view(shape)\n        )  # slice the vector and reshape it\n        start += size\n    return tuple(tensors)\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.safe_torch_linalg_eigh","title":"safe_torch_linalg_eigh","text":"<pre><code>safe_torch_linalg_eigh(*args: Any, **kwargs: Any) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>A wrapper around <code>torch.linalg.eigh</code> that safely handles potential runtime errors by raising a custom <code>TorchLinalgEighException</code> with more context, especially related to the issues reported in https://github.com/pytorch/pytorch/issues/92141.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Positional arguments passed to <code>torch.linalg.eigh</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Keyword arguments passed to <code>torch.linalg.eigh</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, Tensor]</code> <p>The result of calling <code>torch.linalg.eigh</code> with the provided arguments.</p> RAISES DESCRIPTION <code>TorchLinalgEighException</code> <p>If a <code>RuntimeError</code> occurs during the execution of <code>torch.linalg.eigh</code>.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>@catch_and_raise_exception(RuntimeError, lambda e: TorchLinalgEighException(e))\ndef safe_torch_linalg_eigh(\n    *args: Any, **kwargs: Any\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    A wrapper around `torch.linalg.eigh` that safely handles potential runtime errors\n    by raising a custom `TorchLinalgEighException` with more context,\n    especially related to the issues reported in\n    [https://github.com/pytorch/pytorch/issues/92141](\n    https://github.com/pytorch/pytorch/issues/92141).\n\n    Args:\n        *args: Positional arguments passed to `torch.linalg.eigh`.\n        **kwargs: Keyword arguments passed to `torch.linalg.eigh`.\n\n    Returns:\n        The result of calling `torch.linalg.eigh` with the provided arguments.\n\n    Raises:\n        TorchLinalgEighException: If a `RuntimeError` occurs during the execution of\n            `torch.linalg.eigh`.\n    \"\"\"\n    return cast(tuple[torch.Tensor, torch.Tensor], torch.linalg.eigh(*args, **kwargs))\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.to_model_device","title":"to_model_device","text":"<pre><code>to_model_device(x: Tensor, model: Module) -&gt; Tensor\n</code></pre> <p>Returns the tensor <code>x</code> moved to the device of the <code>model</code>, if device of model is set</p> PARAMETER DESCRIPTION <code>x</code> <p>The tensor to be moved to the device of the model.</p> <p> TYPE: <code>Tensor</code> </p> <code>model</code> <p>The model whose device will be used to move the tensor.</p> <p> TYPE: <code>Module</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The tensor <code>x</code> moved to the device of the <code>model</code>, if device of model is set.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def to_model_device(x: torch.Tensor, model: torch.nn.Module) -&gt; torch.Tensor:\n    \"\"\"\n    Returns the tensor `x` moved to the device of the `model`, if device of model is set\n\n    Args:\n        x: The tensor to be moved to the device of the model.\n        model: The model whose device will be used to move the tensor.\n\n    Returns:\n        The tensor `x` moved to the device of the `model`, if device of model is set.\n    \"\"\"\n    device = next(model.parameters()).device\n    return x.to(device)\n</code></pre>"},{"location":"api/pydvl/influence/torch/util/#pydvl.influence.torch.util.torch_dataset_to_dask_array","title":"torch_dataset_to_dask_array","text":"<pre><code>torch_dataset_to_dask_array(\n    dataset: Dataset,\n    chunk_size: int,\n    total_size: Optional[int] = None,\n    resulting_dtype: Type[number] = float32,\n) -&gt; Tuple[Array, ...]\n</code></pre> <p>Construct tuple of dask arrays from a PyTorch dataset, using dask.delayed</p> PARAMETER DESCRIPTION <code>dataset</code> <p>A PyTorch dataset</p> <p> TYPE: <code>Dataset</code> </p> <code>chunk_size</code> <p>The size of the chunks for the resulting Dask arrays.</p> <p> TYPE: <code>int</code> </p> <code>total_size</code> <p>If the dataset does not implement len, provide the length via this parameter. If None the length of the dataset is inferred via accessing the dataset once.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>resulting_dtype</code> <p>The dtype of the resulting dask.array.Array</p> <p> TYPE: <code>Type[number]</code> DEFAULT: <code>float32</code> </p> Example <pre><code>import torch\nfrom torch.utils.data import TensorDataset\nx = torch.rand((20, 3))\ny = torch.rand((20, 1))\ndataset = TensorDataset(x, y)\nda_x, da_y = torch_dataset_to_dask_array(dataset, 4)\n</code></pre> RETURNS DESCRIPTION <code>Tuple[Array, ...]</code> <p>Tuple of Dask arrays corresponding to each tensor in the dataset.</p> Source code in <code>src/pydvl/influence/torch/util.py</code> <pre><code>def torch_dataset_to_dask_array(\n    dataset: Dataset,\n    chunk_size: int,\n    total_size: Optional[int] = None,\n    resulting_dtype: Type[np.number] = np.float32,\n) -&gt; Tuple[da.Array, ...]:\n    \"\"\"\n    Construct tuple of dask arrays from a PyTorch dataset, using dask.delayed\n\n    Args:\n        dataset: A PyTorch [dataset][torch.utils.data.Dataset]\n        chunk_size: The size of the chunks for the resulting Dask arrays.\n        total_size: If the dataset does not implement len, provide the length\n            via this parameter. If None\n            the length of the dataset is inferred via accessing the dataset once.\n        resulting_dtype: The dtype of the resulting [dask.array.Array][dask.array.Array]\n\n    ??? Example\n        ```python\n        import torch\n        from torch.utils.data import TensorDataset\n        x = torch.rand((20, 3))\n        y = torch.rand((20, 1))\n        dataset = TensorDataset(x, y)\n        da_x, da_y = torch_dataset_to_dask_array(dataset, 4)\n        ```\n\n    Returns:\n        Tuple of Dask arrays corresponding to each tensor in the dataset.\n    \"\"\"\n\n    def _infer_data_len(d_set: Dataset):\n        try:\n            n_data = len(d_set)  # type:ignore\n            if total_size is not None and n_data != total_size:\n                raise ValueError(\n                    f\"The number of samples in the dataset ({n_data}), derived \"\n                    f\"from calling \u00b4len\u00b4, does not match the provided \"\n                    f\"total number of samples ({total_size}). \"\n                    f\"Call the function without total_size.\"\n                )\n            return n_data\n        except TypeError as e:\n            err_msg = (\n                f\"Could not infer the number of samples in the dataset from \"\n                f\"calling \u00b4len\u00b4. Original error: {e}.\"\n            )\n            if total_size is not None:\n                logger.warning(\n                    err_msg\n                    + f\" Using the provided total number of samples {total_size}.\"\n                )\n                return total_size\n            else:\n                logger.warning(\n                    err_msg + \" Infer the number of samples from the dataset, \"\n                    \"via iterating the dataset once. \"\n                    \"This might induce severe overhead, so consider\"\n                    \"providing total_size, if you know the number of samples \"\n                    \"beforehand.\"\n                )\n                idx = 0\n                while True:\n                    try:\n                        t = d_set[idx]\n                        if all(_t.numel() == 0 for _t in t):\n                            return idx\n                        idx += 1\n\n                    except IndexError:\n                        return idx\n\n    sample = dataset[0]\n    if not isinstance(sample, tuple):\n        sample = (sample,)\n\n    def _get_chunk(\n        start_idx: int, stop_idx: int, d_set: Dataset\n    ) -&gt; Tuple[torch.Tensor, ...]:\n        try:\n            t = d_set[start_idx:stop_idx]\n            if not isinstance(t, tuple):\n                t = (t,)\n            return t  # type:ignore\n        except Exception:\n            nested_tensor_list = [\n                [d_set[idx][k] for idx in range(start_idx, stop_idx)]\n                for k in range(len(sample))\n            ]\n            return tuple(map(torch.stack, nested_tensor_list))\n\n    n_samples = _infer_data_len(dataset)\n    chunk_indices = [\n        (i, min(i + chunk_size, n_samples)) for i in range(0, n_samples, chunk_size)\n    ]\n    delayed_dataset = dask.delayed(dataset)\n    delayed_chunks = [\n        dask.delayed(partial(_get_chunk, start, stop))(delayed_dataset)\n        for (start, stop) in chunk_indices\n    ]\n\n    delayed_arrays_dict: Dict[int, List[da.Array]] = {k: [] for k in range(len(sample))}\n\n    for chunk, (start, stop) in zip(delayed_chunks, chunk_indices):\n        for tensor_idx, sample_tensor in enumerate(sample):\n            delayed_tensor = da.from_delayed(\n                dask.delayed(lambda t: t.cpu().numpy())(chunk[tensor_idx]),\n                shape=(stop - start, *sample_tensor.shape),\n                dtype=resulting_dtype,\n            )\n\n            delayed_arrays_dict[tensor_idx].append(delayed_tensor)\n\n    return tuple(\n        da.concatenate(array_list) for array_list in delayed_arrays_dict.values()\n    )\n</code></pre>"},{"location":"api/pydvl/reporting/","title":"Reporting","text":""},{"location":"api/pydvl/reporting/#pydvl.reporting","title":"pydvl.reporting","text":""},{"location":"api/pydvl/reporting/plots/","title":"Plots","text":""},{"location":"api/pydvl/reporting/plots/#pydvl.reporting.plots","title":"pydvl.reporting.plots","text":""},{"location":"api/pydvl/reporting/plots/#pydvl.reporting.plots.plot_ci_array","title":"plot_ci_array","text":"<pre><code>plot_ci_array(\n    data: NDArray,\n    level: float,\n    type: Literal[\"normal\", \"t\", \"auto\"] = \"normal\",\n    abscissa: Optional[Sequence[str]] = None,\n    mean_color: Optional[str] = \"dodgerblue\",\n    shade_color: Optional[str] = \"lightblue\",\n    ax: Optional[Axes] = None,\n    **kwargs: Any\n) -&gt; Axes\n</code></pre> <p>Plot values and a confidence interval from a 2D array.</p> <p>Supported intervals are based on the normal and the t distributions.</p> PARAMETER DESCRIPTION <code>data</code> <p>A 2D array with M different values for each of the N indices.</p> <p> TYPE: <code>NDArray</code> </p> <code>level</code> <p>The confidence level.</p> <p> TYPE: <code>float</code> </p> <code>type</code> <p>The type of confidence interval to use.</p> <p> TYPE: <code>Literal['normal', 't', 'auto']</code> DEFAULT: <code>'normal'</code> </p> <code>abscissa</code> <p>The values for the x-axis. Leave empty to use increasing integers.</p> <p> TYPE: <code>Optional[Sequence[str]]</code> DEFAULT: <code>None</code> </p> <code>mean_color</code> <p>The color of the mean line.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dodgerblue'</code> </p> <code>shade_color</code> <p>The color of the confidence interval.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'lightblue'</code> </p> <code>ax</code> <p>If passed, axes object into which to insert the figure. Otherwise, a new figure is created and the axes returned.</p> <p> TYPE: <code>Optional[Axes]</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments to pass to the plot function.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Axes</code> <p>The matplotlib axes.</p> Source code in <code>src/pydvl/reporting/plots.py</code> <pre><code>def plot_ci_array(\n    data: NDArray,\n    level: float,\n    type: Literal[\"normal\", \"t\", \"auto\"] = \"normal\",\n    abscissa: Optional[Sequence[str]] = None,\n    mean_color: Optional[str] = \"dodgerblue\",\n    shade_color: Optional[str] = \"lightblue\",\n    ax: Optional[plt.Axes] = None,\n    **kwargs: Any,\n) -&gt; plt.Axes:\n    \"\"\"Plot values and a confidence interval from a 2D array.\n\n    Supported intervals are based on the normal and the t distributions.\n\n    Args:\n        data: A 2D array with M different values for each of the N indices.\n        level: The confidence level.\n        type: The type of confidence interval to use.\n        abscissa: The values for the x-axis. Leave empty to use increasing\n            integers.\n        mean_color: The color of the mean line.\n        shade_color: The color of the confidence interval.\n        ax: If passed, axes object into which to insert the figure. Otherwise,\n            a new figure is created and the axes returned.\n        **kwargs: Additional arguments to pass to the plot function.\n\n    Returns:\n        The matplotlib axes.\n    \"\"\"\n\n    m, n = data.shape\n\n    means = np.mean(data, axis=0)\n    variances = np.var(data, axis=0, ddof=1)\n\n    dummy = ValuationResult(\n        algorithm=\"dummy\",\n        values=means,\n        variances=variances,\n        counts=np.ones_like(means, dtype=np.int_) * m,\n        indices=np.arange(n),\n        data_names=(\n            np.array(abscissa, dtype=str)\n            if abscissa is not None\n            else np.arange(n, dtype=str)\n        ),\n    )\n    dummy.sort(key=\"index\")\n\n    return plot_ci_values(\n        dummy,\n        level=level,\n        type=type,\n        abscissa=abscissa,\n        mean_color=mean_color,\n        shade_color=shade_color,\n        ax=ax,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/pydvl/reporting/plots/#pydvl.reporting.plots.plot_ci_values","title":"plot_ci_values","text":"<pre><code>plot_ci_values(\n    values: ValuationResult,\n    level: float,\n    type: Literal[\"normal\", \"t\", \"auto\"] = \"auto\",\n    abscissa: Optional[Sequence[Any]] = None,\n    mean_color: Optional[str] = \"dodgerblue\",\n    shade_color: Optional[str] = \"lightblue\",\n    ax: Optional[Axes] = None,\n    **kwargs: Any\n) -&gt; Axes\n</code></pre> <p>Plot values and a confidence interval.</p> <p>Supported intervals are based on the normal and the t distributions.</p> PARAMETER DESCRIPTION <code>values</code> <p>The valuation result. The object must be sorted by calling <code>ValuationResult.sort()</code>.</p> <p> TYPE: <code>ValuationResult</code> </p> <code>level</code> <p>The confidence level.</p> <p> TYPE: <code>float</code> </p> <code>type</code> <p>The type of confidence interval to use. If \"auto\", uses \"norm\" if the minimum number of updates for all indices is greater than 30, otherwise uses \"t\".</p> <p> TYPE: <code>Literal['normal', 't', 'auto']</code> DEFAULT: <code>'auto'</code> </p> <code>abscissa</code> <p>The values for the x-axis. Leave empty to use increasing integers.</p> <p> TYPE: <code>Optional[Sequence[Any]]</code> DEFAULT: <code>None</code> </p> <code>mean_color</code> <p>The color of the mean line.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dodgerblue'</code> </p> <code>shade_color</code> <p>The color of the confidence interval.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'lightblue'</code> </p> <code>ax</code> <p>If passed, axes object into which to insert the figure. Otherwise, a new figure is created and the axes returned.</p> <p> TYPE: <code>Optional[Axes]</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments to pass to the plot function.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Axes</code> <p>The matplotlib axes.</p> Source code in <code>src/pydvl/reporting/plots.py</code> <pre><code>def plot_ci_values(\n    values: ValuationResult,\n    level: float,\n    type: Literal[\"normal\", \"t\", \"auto\"] = \"auto\",\n    abscissa: Optional[Sequence[Any]] = None,\n    mean_color: Optional[str] = \"dodgerblue\",\n    shade_color: Optional[str] = \"lightblue\",\n    ax: Optional[plt.Axes] = None,\n    **kwargs: Any,\n) -&gt; plt.Axes:\n    \"\"\"Plot values and a confidence interval.\n\n    Supported intervals are based on the normal and the t distributions.\n\n    Args:\n        values: The valuation result. The object must be sorted by calling\n            `ValuationResult.sort()`.\n        level: The confidence level.\n        type: The type of confidence interval to use. If \"auto\", uses \"norm\" if\n            the minimum number of updates for all indices is greater than 30,\n            otherwise uses \"t\".\n        abscissa: The values for the x-axis. Leave empty to use increasing\n            integers.\n        mean_color: The color of the mean line.\n        shade_color: The color of the confidence interval.\n        ax: If passed, axes object into which to insert the figure. Otherwise,\n            a new figure is created and the axes returned.\n        **kwargs: Additional arguments to pass to the plot function.\n\n    Returns:\n        The matplotlib axes.\n    \"\"\"\n    assert values._sort_order is not None, \"Values must be sorted first.\"\n\n    ppfs = {\n        \"normal\": norm.ppf,\n        \"t\": partial(t.ppf, df=values.counts - 1),\n        \"auto\": (\n            norm.ppf\n            if np.min(values.counts) &gt; 30\n            else partial(t.ppf, df=values.counts - 1)\n        ),\n    }\n\n    try:\n        score = ppfs[type](1 - level / 2)\n    except KeyError:\n        raise ValueError(\n            f\"Unknown confidence interval type requested: {type}.\"\n        ) from None\n\n    if abscissa is None:\n        abscissa = range(len(values))\n\n    bound = score * values.stderr\n\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    ax.fill_between(\n        abscissa,\n        values.values - bound,\n        values.values + bound,\n        alpha=0.3,\n        color=shade_color,\n    )\n    ax.plot(abscissa, values.values, color=mean_color, **kwargs)\n    ax.set_xlim(left=min(abscissa), right=max(abscissa))\n    return ax\n</code></pre>"},{"location":"api/pydvl/reporting/plots/#pydvl.reporting.plots.plot_influence_distribution","title":"plot_influence_distribution","text":"<pre><code>plot_influence_distribution(\n    influences: NDArray[float64], index: int, title_extra: str = \"\"\n) -&gt; Axes\n</code></pre> <p>Plots the histogram of the influence that all samples in the training set have over a single sample index.</p> PARAMETER DESCRIPTION <code>influences</code> <p>array of influences (training samples x test samples)</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>index</code> <p>Index of the test sample for which the influences   will be plotted.</p> <p> TYPE: <code>int</code> </p> <code>title_extra</code> <p>Additional text that will be appended to the title.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> Source code in <code>src/pydvl/reporting/plots.py</code> <pre><code>def plot_influence_distribution(\n    influences: NDArray[np.float64], index: int, title_extra: str = \"\"\n) -&gt; plt.Axes:\n    \"\"\"Plots the histogram of the influence that all samples in the training set\n    have over a single sample index.\n\n    Args:\n       influences: array of influences (training samples x test samples)\n       index: Index of the test sample for which the influences\n            will be plotted.\n       title_extra: Additional text that will be appended to the title.\n    \"\"\"\n    _, ax = plt.subplots()\n    ax.hist(influences[:, index], alpha=0.7)\n    ax.set_xlabel(\"Influence values\")\n    ax.set_ylabel(\"Number of samples\")\n    ax.set_title(f\"Distribution of influences {title_extra}\")\n    return cast(plt.Axes, ax)\n</code></pre>"},{"location":"api/pydvl/reporting/plots/#pydvl.reporting.plots.plot_influence_distribution_by_label","title":"plot_influence_distribution_by_label","text":"<pre><code>plot_influence_distribution_by_label(\n    influences: NDArray[float64],\n    labels: NDArray[float64],\n    title_extra: str = \"\",\n)\n</code></pre> <p>Plots the histogram of the influence that all samples in the training set have over a single sample index, separated by labels.</p> PARAMETER DESCRIPTION <code>influences</code> <p>array of influences (training samples x test samples)</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>labels</code> <p>labels for the training set.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>title_extra</code> <p>Additional text that will be appended to the title.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> Source code in <code>src/pydvl/reporting/plots.py</code> <pre><code>def plot_influence_distribution_by_label(\n    influences: NDArray[np.float64], labels: NDArray[np.float64], title_extra: str = \"\"\n):\n    \"\"\"Plots the histogram of the influence that all samples in the training set\n    have over a single sample index, separated by labels.\n\n    Args:\n       influences: array of influences (training samples x test samples)\n       labels: labels for the training set.\n       title_extra: Additional text that will be appended to the title.\n    \"\"\"\n    _, ax = plt.subplots()\n    unique_labels = np.unique(labels)\n    for label in unique_labels:\n        ax.hist(influences[labels == label], label=label, alpha=0.7)\n    ax.set_xlabel(\"Influence values\")\n    ax.set_ylabel(\"Number of samples\")\n    ax.set_title(f\"Distribution of influences {title_extra}\")\n    ax.legend()\n    plt.show()\n</code></pre>"},{"location":"api/pydvl/reporting/plots/#pydvl.reporting.plots.plot_shapley","title":"plot_shapley","text":"<pre><code>plot_shapley(\n    df: DataFrame,\n    *,\n    level: float = 0.05,\n    ax: Optional[Axes] = None,\n    title: Optional[str] = None,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    prefix: Optional[str] = \"data_value\"\n) -&gt; Axes\n</code></pre> <p>Plots the shapley values, as returned from compute_shapley_values, with error bars corresponding to an \\(\\alpha\\)-level Normal confidence interval.</p> PARAMETER DESCRIPTION <code>df</code> <p>dataframe with the shapley values</p> <p> TYPE: <code>DataFrame</code> </p> <code>level</code> <p>confidence level for the error bars</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>ax</code> <p>axes to plot on or None if a new subplots should be created</p> <p> TYPE: <code>Optional[Axes]</code> DEFAULT: <code>None</code> </p> <code>title</code> <p>string, title of the plot</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>xlabel</code> <p>string, x label of the plot</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>ylabel</code> <p>string, y label of the plot</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Axes</code> <p>The axes created or used</p> Source code in <code>src/pydvl/reporting/plots.py</code> <pre><code>def plot_shapley(\n    df: pd.DataFrame,\n    *,\n    level: float = 0.05,\n    ax: Optional[plt.Axes] = None,\n    title: Optional[str] = None,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    prefix: Optional[str] = \"data_value\",\n) -&gt; plt.Axes:\n    r\"\"\"Plots the shapley values, as returned from\n    [compute_shapley_values][pydvl.value.shapley.common.compute_shapley_values],\n    with error bars corresponding to an $\\alpha$-level Normal confidence\n    interval.\n\n    Args:\n        df: dataframe with the shapley values\n        level: confidence level for the error bars\n        ax: axes to plot on or None if a new subplots should be created\n        title: string, title of the plot\n        xlabel: string, x label of the plot\n        ylabel: string, y label of the plot\n\n    Returns:\n        The axes created or used\n    \"\"\"\n    if ax is None:\n        _, ax = plt.subplots()\n\n    stderr = np.sqrt(\n        df[f\"{prefix}_variances\"] / np.maximum(1.0, df[f\"{prefix}_counts\"])\n    )\n    yerr = norm.ppf(1 - level / 2) * stderr\n\n    ax.errorbar(x=df.index, y=df[prefix], yerr=yerr, fmt=\"o\", capsize=6)\n    ax.set_xlabel(xlabel or \"\")\n    ax.set_ylabel(ylabel or \"\")\n    ax.set_title(title or \"\")\n    plt.xticks(rotation=60)\n    return ax\n</code></pre>"},{"location":"api/pydvl/reporting/plots/#pydvl.reporting.plots.shaded_mean_std","title":"shaded_mean_std","text":"<pre><code>shaded_mean_std(\n    data: ndarray,\n    abscissa: Optional[Sequence[Any]] = None,\n    num_std: float = 1.0,\n    mean_color: Optional[str] = \"dodgerblue\",\n    shade_color: Optional[str] = \"lightblue\",\n    title: Optional[str] = None,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    ax: Optional[Axes] = None,\n    **kwargs: Any\n) -&gt; Axes\n</code></pre> <p>The usual mean \\(\\pm\\) std deviation plot to aggregate runs of experiments.</p> <p>Deprecation notice</p> <p>This function is bogus and will be removed in the future in favour of properly computed confidence intervals.</p> PARAMETER DESCRIPTION <code>data</code> <p>axis 0 is to be aggregated on (e.g. runs) and axis 1 is the data for each run.</p> <p> TYPE: <code>ndarray</code> </p> <code>abscissa</code> <p>values for the x-axis. Leave empty to use increasing integers.</p> <p> TYPE: <code>Optional[Sequence[Any]]</code> DEFAULT: <code>None</code> </p> <code>num_std</code> <p>number of standard deviations to shade around the mean.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>mean_color</code> <p>color for the mean</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'dodgerblue'</code> </p> <code>shade_color</code> <p>color for the shaded region</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'lightblue'</code> </p> <code>title</code> <p>Title text. To use mathematics, use LaTeX notation.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>xlabel</code> <p>Text for the horizontal axis.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>ylabel</code> <p>Text for the vertical axis</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>ax</code> <p>If passed, axes object into which to insert the figure. Otherwise, a new figure is created and returned</p> <p> TYPE: <code>Optional[Axes]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>these are forwarded to the ax.plot() call for the mean.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Axes</code> <p>The axes used (or created)</p> Source code in <code>src/pydvl/reporting/plots.py</code> <pre><code>@deprecated(target=None, deprecated_in=\"0.7.1\", remove_in=\"0.9.0\")\ndef shaded_mean_std(\n    data: np.ndarray,\n    abscissa: Optional[Sequence[Any]] = None,\n    num_std: float = 1.0,\n    mean_color: Optional[str] = \"dodgerblue\",\n    shade_color: Optional[str] = \"lightblue\",\n    title: Optional[str] = None,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    ax: Optional[Axes] = None,\n    **kwargs: Any,\n) -&gt; Axes:\n    r\"\"\"The usual mean \\(\\pm\\) std deviation plot to aggregate runs of\n    experiments.\n\n    !!! warning \"Deprecation notice\"\n        This function is bogus and will be removed in the future in favour of\n        properly computed confidence intervals.\n\n    Args:\n        data: axis 0 is to be aggregated on (e.g. runs) and axis 1 is the\n            data for each run.\n        abscissa: values for the x-axis. Leave empty to use increasing integers.\n        num_std: number of standard deviations to shade around the mean.\n        mean_color: color for the mean\n        shade_color: color for the shaded region\n        title: Title text. To use mathematics, use LaTeX notation.\n        xlabel: Text for the horizontal axis.\n        ylabel: Text for the vertical axis\n        ax: If passed, axes object into which to insert the figure. Otherwise,\n            a new figure is created and returned\n        kwargs: these are forwarded to the ax.plot() call for the mean.\n\n    Returns:\n        The axes used (or created)\n    \"\"\"\n    assert len(data.shape) == 2\n    mean = data.mean(axis=0)\n    std = num_std * data.std(axis=0)\n\n    if ax is None:\n        fig, ax = plt.subplots()\n    if abscissa is None:\n        abscissa = list(range(data.shape[1]))\n\n    ax.fill_between(abscissa, mean - std, mean + std, alpha=0.3, color=shade_color)\n    ax.plot(abscissa, mean, color=mean_color, **kwargs)\n\n    ax.set_title(title or \"\")\n    ax.set_xlabel(xlabel or \"\")\n    ax.set_ylabel(ylabel or \"\")\n    ax.set_xticks(abscissa)\n    ax.set_xticklabels(abscissa, rotation=60)\n\n    return ax\n</code></pre>"},{"location":"api/pydvl/reporting/plots/#pydvl.reporting.plots.spearman_correlation","title":"spearman_correlation","text":"<pre><code>spearman_correlation(vv: List[OrderedDict], num_values: int, pvalue: float)\n</code></pre> <p>Simple matrix plots with spearman correlation for each pair in vv.</p> PARAMETER DESCRIPTION <code>vv</code> <p>list of OrderedDicts with index: value. Spearman correlation is computed for the keys.</p> <p> TYPE: <code>List[OrderedDict]</code> </p> <code>num_values</code> <p>Use only these many values from the data (from the start of the OrderedDicts)</p> <p> TYPE: <code>int</code> </p> <code>pvalue</code> <p>correlation coefficients for which the p-value is below the threshold <code>pvalue/len(vv)</code> will be discarded.</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/pydvl/reporting/plots.py</code> <pre><code>def spearman_correlation(vv: List[OrderedDict], num_values: int, pvalue: float):\n    \"\"\"Simple matrix plots with spearman correlation for each pair in vv.\n\n    Args:\n        vv: list of OrderedDicts with index: value. Spearman correlation\n            is computed for the keys.\n        num_values: Use only these many values from the data (from the start\n            of the OrderedDicts)\n        pvalue: correlation coefficients for which the p-value is below the\n            threshold `pvalue/len(vv)` will be discarded.\n    \"\"\"\n    r: np.ndarray = np.ndarray((len(vv), len(vv)))\n    p: np.ndarray = np.ndarray((len(vv), len(vv)))\n    for i, a in enumerate(vv):\n        for j, b in enumerate(vv):\n            spearman = sp.stats.spearmanr(\n                list(a.keys())[:num_values], list(b.keys())[:num_values]\n            )\n            r[i][j] = (\n                spearman.correlation if spearman.pvalue &lt; pvalue / len(vv) else np.nan\n            )  # Bonferroni correction\n            p[i][j] = spearman.pvalue\n    fig, axs = plt.subplots(1, 2, figsize=(16, 7))\n    plot1 = axs[0].matshow(r, vmin=-1, vmax=1)\n    axs[0].set_title(f\"Spearman correlation (top {num_values} values)\")\n    axs[0].set_xlabel(\"Runs\")\n    axs[0].set_ylabel(\"Runs\")\n    fig.colorbar(plot1, ax=axs[0])\n    plot2 = axs[1].matshow(p, vmin=0, vmax=1)\n    axs[1].set_title(\"p-value\")\n    axs[1].set_xlabel(\"Runs\")\n    axs[1].set_ylabel(\"Runs\")\n    fig.colorbar(plot2, ax=axs[1])\n\n    return fig\n</code></pre>"},{"location":"api/pydvl/reporting/point_removal/","title":"Point removal","text":""},{"location":"api/pydvl/reporting/point_removal/#pydvl.reporting.point_removal","title":"pydvl.reporting.point_removal","text":"<p>This module implements the standard point removal experiment in data valuation.</p> <p>It is  a method to evaluate the usefulness and stability of a valuation method. The idea is to remove a percentage of the data points from the training set based on their valuation, and then retrain the model and evaluate it on a test set. This is done for a range of removal percentages, and the performance is measured as a function of the percentage of data removed. By repeating this process multiple times, we can get an estimate of the stability of the valuation method.</p> <p>The experiment can be run in parallel with the run_removal_experiment function. In order to call it, we need to define 3 types of factories:</p> <ol> <li>A factory that returns a train-test split of the data given a random state</li> <li>A factory that returns a utility that evaluates a model on a given test set.    This is used for the performance evaluation. The model need not be the same    as the one used for the valuation.</li> <li>A factory returning a valuation method. The training set is passed to the    factory, in case the valuation needs to train something. E.g. for Data-OOB    we need the bagging model to be fitted before the valuation is computed.</li> </ol>"},{"location":"api/pydvl/reporting/point_removal/#pydvl.reporting.point_removal.removal_job","title":"removal_job","text":"<pre><code>removal_job(\n    data_factory: DataSplitFactory,\n    valuation_factory: ValuationFactory,\n    utility_factory: UtilityFactory,\n    removal_percentages: NDArray,\n    random_state: int,\n) -&gt; tuple[dict, dict]\n</code></pre> <p>A job that computes the scores for a single run of the removal experiment.</p> PARAMETER DESCRIPTION <code>data_factory</code> <p>A callable that returns a tuple of Datasets (train, test) to use in the experiment.</p> <p> TYPE: <code>DataSplitFactory</code> </p> <code>valuation_factory</code> <p>A callable that returns a Valuation object given a train dataset and a random state. Computing values with this object is the goal of the experiment</p> <p> TYPE: <code>ValuationFactory</code> </p> <code>utility_factory</code> <p>A callable that returns a ModelUtility object given a test dataset and a random state. This object is used to evaluate the performance of the valuation method by removing data points from the training set and retraining the model, then scoring it on the test set.</p> <p> TYPE: <code>UtilityFactory</code> </p> <code>removal_percentages</code> <p>As sequence of percentages of data to remove from the training set.</p> <p> TYPE: <code>NDArray</code> </p> <code>random_state</code> <p>The random state to use in the experiment.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     A tuple of dictionaries with the scores for the low and high value removals.</p> Source code in <code>src/pydvl/reporting/point_removal.py</code> <pre><code>def removal_job(\n    data_factory: DataSplitFactory,\n    valuation_factory: ValuationFactory,\n    utility_factory: UtilityFactory,\n    removal_percentages: NDArray,\n    random_state: int,\n) -&gt; tuple[dict, dict]:\n    \"\"\"\n    A job that computes the scores for a single run of the removal experiment.\n\n    Args:\n        data_factory: A callable that returns a tuple of Datasets (train, test) to use\n            in the experiment.\n        valuation_factory: A callable that returns a Valuation object given a train\n            dataset and a random state. Computing values with this object is the goal\n            of the experiment\n        utility_factory: A callable that returns a ModelUtility object given a test\n            dataset and a random state. This object is used to evaluate the performance\n            of the valuation method by removing data points from the training set and\n            retraining the model, then scoring it on the test set.\n        removal_percentages: As sequence of percentages of data to remove from the\n            training set.\n        random_state: The random state to use in the experiment.\n    Returns:\n        A tuple of dictionaries with the scores for the low and high value removals.\n    \"\"\"\n\n    train, test = data_factory(random_state=random_state)\n    valuation = valuation_factory(train=train, random_state=random_state)\n    valuation.fit(train)\n    values = valuation.values()\n\n    utility = utility_factory(test=test, random_state=random_state)\n    low_scores: dict = compute_removal_score(\n        utility,\n        values,\n        train,\n        percentages=removal_percentages,\n        remove_best=False,\n    )\n    low_scores[\"method_name\"] = valuation.__class__.__name__\n\n    high_scores: dict = compute_removal_score(\n        utility,\n        values,\n        train,\n        percentages=removal_percentages,\n        remove_best=True,\n    )\n\n    high_scores[\"method_name\"] = valuation.__class__.__name__\n\n    return low_scores, high_scores\n</code></pre>"},{"location":"api/pydvl/reporting/point_removal/#pydvl.reporting.point_removal.run_removal_experiment","title":"run_removal_experiment","text":"<pre><code>run_removal_experiment(\n    data_factory: DataSplitFactory,\n    valuation_factories: list[ValuationFactory],\n    utility_factory: UtilityFactory,\n    removal_percentages: NDArray,\n    n_runs: int = 1,\n    n_jobs: int = 1,\n    random_state: int | None = None,\n) -&gt; tuple[DataFrame, DataFrame]\n</code></pre> <p>Run the sample removal experiment.</p> <p>Given the factories, the removal percentages, and the number of runs, this function does the following in each run:</p> <ol> <li>Sample a random state</li> <li> <p>For each valuation method, compute the values and iteratively compute the scores    after retraining on subsets of the data. This is parallelized. Each job requires    3 factories:</p> </li> <li> <p>A factory that returns a train-test split of the data given a random state</p> </li> <li>A factory returning a valuation method. The training set is passed to the      factory, in case the valuation needs to train something. E.g. for Data-OOB      we need the bagging model to be fitted before the valuation is computed.</li> <li>A factory that returns a utility that evaluates some model on a given test set.      This is used for the performance evaluation. The model need not be the same      as the one used for the valuation.</li> <li>It returns the scores in two DataFrames, one for the high value removals and one    for the low value removals.</li> </ol> PARAMETER DESCRIPTION <code>data_factory</code> <p>A callable that returns a tuple of Datasets (train, test) given a random state</p> <p> TYPE: <code>DataSplitFactory</code> </p> <code>valuation_factories</code> <p>A list of callables that return Valuation objects given a model, train data, and random state. The training data is typically not needed for construction, but bagging models may require it</p> <p> TYPE: <code>list[ValuationFactory]</code> </p> <code>utility_factory</code> <p>A callable that returns a ModelUtility object given a test dataset and a random state. This object is used to evaluate the performance of the valuation method by removing data points from the training set and retraining the model, then scoring it on the test set.</p> <p> TYPE: <code>UtilityFactory</code> </p> <code>removal_percentages</code> <p>The percentage of data to remove from the training set. This should be a list of floats between 0 and 1.</p> <p> TYPE: <code>NDArray</code> </p> <code>n_runs</code> <p>The number of repetitions of the experiment.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_jobs</code> <p>The number of parallel jobs to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>random_state</code> <p>The initial random state.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <p>Returns:     A tuple of DataFrames with the scores for the low and high value removals</p> Source code in <code>src/pydvl/reporting/point_removal.py</code> <pre><code>def run_removal_experiment(\n    data_factory: DataSplitFactory,\n    valuation_factories: list[ValuationFactory],\n    utility_factory: UtilityFactory,\n    removal_percentages: NDArray,\n    n_runs: int = 1,\n    n_jobs: int = 1,\n    random_state: int | None = None,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Run the sample removal experiment.\n\n    Given the factories, the removal percentages, and the number of runs, this function\n    does the following in each run:\n\n    1. Sample a random state\n    2. For each valuation method, compute the values and iteratively compute the scores\n       after retraining on subsets of the data. This is parallelized. Each job requires\n       3 factories:\n\n       - A factory that returns a train-test split of the data given a random state\n       - A factory returning a valuation method. The training set is passed to the\n         factory, in case the valuation needs to train something. E.g. for Data-OOB\n         we need the bagging model to be fitted before the valuation is computed.\n       - A factory that returns a utility that evaluates some model on a given test set.\n         This is used for the performance evaluation. The model need not be the same\n         as the one used for the valuation.\n    3. It returns the scores in two DataFrames, one for the high value removals and one\n       for the low value removals.\n\n    Args:\n        data_factory: A callable that returns a tuple of Datasets (train, test) given\n            a random state\n        valuation_factories: A list of callables that return Valuation objects given\n            a model, train data, and random state. The training data is typically not\n            needed for construction, but bagging models may require it\n        utility_factory: A callable that returns a ModelUtility object given a test\n            dataset and a random state. This object is used to evaluate the performance\n            of the valuation method by removing data points from the training set and\n            retraining the model, then scoring it on the test set.\n        removal_percentages: The percentage of data to remove from the training set.\n            This should be a list of floats between 0 and 1.\n        n_runs: The number of repetitions of the experiment.\n        n_jobs: The number of parallel jobs to use.\n        random_state: The initial random state.\n    Returns:\n        A tuple of DataFrames with the scores for the low and high value removals\n    \"\"\"\n    all_high_scores = []\n    all_low_scores = []\n\n    with parallel_config(n_jobs=n_jobs):\n        seed_seq = ensure_seed_sequence(random_state).generate_state(n_runs)\n        job = delayed(removal_job)\n\n        with Parallel(return_as=\"generator_unordered\") as parallel:\n            delayed_evals = parallel(\n                job(\n                    data_factory=data_factory,\n                    valuation_factory=valuation_factory,\n                    utility_factory=utility_factory,\n                    removal_percentages=removal_percentages,\n                    random_state=seed_seq[i],\n                )\n                for valuation_factory in valuation_factories\n                for i in range(n_runs)\n            )\n            for result in tqdm(\n                delayed_evals, unit=\"%\", total=len(valuation_factories) * n_runs\n            ):\n                low_scores, high_scores = result\n                all_low_scores.append(low_scores)\n                all_high_scores.append(high_scores)\n\n    low_scores_df = pd.DataFrame(all_low_scores)\n    high_scores_df = pd.DataFrame(all_high_scores)\n\n    return low_scores_df, high_scores_df\n</code></pre>"},{"location":"api/pydvl/reporting/scores/","title":"Scores","text":""},{"location":"api/pydvl/reporting/scores/#pydvl.reporting.scores","title":"pydvl.reporting.scores","text":""},{"location":"api/pydvl/reporting/scores/#pydvl.reporting.scores.compute_removal_score","title":"compute_removal_score","text":"<pre><code>compute_removal_score(\n    u: ModelUtility,\n    values: ValuationResult,\n    training_data: Dataset,\n    percentages: NDArray[float_] | Iterable[float],\n    *,\n    remove_best: bool = False,\n    progress: bool = False\n) -&gt; dict[float, float]\n</code></pre> <p>Fits a model and computes its score on a test set after incrementally removing a percentage of data points from the training set, based on their values.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, test data, and scoring function.</p> <p> TYPE: <code>ModelUtility</code> </p> <code>training_data</code> <p>Dataset from which to remove data points.</p> <p> TYPE: <code>Dataset</code> </p> <code>values</code> <p>Data values of data instances in the training set.</p> <p> TYPE: <code>ValuationResult</code> </p> <code>percentages</code> <p>Sequence of removal percentages.</p> <p> TYPE: <code>NDArray[float_] | Iterable[float]</code> </p> <code>remove_best</code> <p>If True, removes data points in order of decreasing valuation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress</code> <p>If True, display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dict[float, float]</code> <p>Dictionary that maps the percentages to their respective scores.</p> Source code in <code>src/pydvl/reporting/scores.py</code> <pre><code>def compute_removal_score(\n    u: ModelUtility,\n    values: ValuationResult,\n    training_data: Dataset,\n    percentages: NDArray[np.float_] | Iterable[float],\n    *,\n    remove_best: bool = False,\n    progress: bool = False,\n) -&gt; dict[float, float]:\n    \"\"\"Fits a model and computes its score on a test set after incrementally removing\n    a percentage of data points from the training set, based on their values.\n\n    Args:\n        u: Utility object with model, test data, and scoring function.\n        training_data: Dataset from which to remove data points.\n        values: Data values of data instances in the training set.\n        percentages: Sequence of removal percentages.\n        remove_best: If True, removes data points in order of decreasing valuation.\n        progress: If True, display a progress bar.\n\n    Returns:\n        Dictionary that maps the percentages to their respective scores.\n    \"\"\"\n    u = u.with_dataset(training_data)\n\n    # Sanity checks\n    if np.any([x &gt;= 1.0 or x &lt; 0.0 for x in percentages]):\n        raise ValueError(\"All percentages should be in the range [0.0, 1.0)\")\n\n    if len(values) != len(training_data):\n        raise ValueError(\n            f\"The number of values, {len(values)}, should be equal to the number of data points, {len(training_data)}\"\n        )\n\n    scores = {}\n\n    # We sort in descending order if we want to remove the best values\n    values.sort(reverse=remove_best)\n\n    for pct in tqdm(percentages, disable=not progress, desc=\"Removal Scores\"):\n        n_removal = int(pct * len(training_data))\n        indices = values.indices[n_removal:]\n        score = u(Sample(idx=None, subset=indices))\n        scores[pct] = score\n    return scores\n</code></pre>"},{"location":"api/pydvl/utils/","title":"Intro","text":""},{"location":"api/pydvl/utils/#pydvl.utils","title":"pydvl.utils","text":""},{"location":"api/pydvl/utils/dataset/","title":"Dataset","text":""},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset","title":"pydvl.utils.dataset","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0. For use with the methods in pydvl.valuation please use pydvl.valuation.dataset instead.</p> <p>This module contains convenience classes to handle data and groups thereof.</p> <p>Shapley and Least Core value computations require evaluation of a scoring function (the utility). This is typically the performance of the model on a test set (as an approximation to its true expected performance). It is therefore convenient to keep both the training data and the test data together to be passed around to methods in shapley and least_core. This is done with Dataset.</p> <p>This abstraction layer also seamlessly grouping data points together if one is interested in computing their value as a group, see GroupedDataset.</p> <p>Objects of both types are used to construct a Utility object.</p>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.Dataset","title":"Dataset","text":"<pre><code>Dataset(\n    x_train: Union[NDArray, DataFrame],\n    y_train: Union[NDArray, DataFrame],\n    x_test: Union[NDArray, DataFrame],\n    y_test: Union[NDArray, DataFrame],\n    feature_names: Optional[Sequence[str]] = None,\n    target_names: Optional[Sequence[str]] = None,\n    data_names: Optional[Sequence[str]] = None,\n    description: Optional[str] = None,\n    is_multi_output: bool = False,\n)\n</code></pre> <p>A convenience class to handle datasets.</p> <p>It holds a dataset, split into training and test data, together with several labels on feature names, data point names and a description.</p> <p>Constructs a Dataset from data and labels.</p> PARAMETER DESCRIPTION <code>x_train</code> <p>training data</p> <p> TYPE: <code>Union[NDArray, DataFrame]</code> </p> <code>y_train</code> <p>labels for training data</p> <p> TYPE: <code>Union[NDArray, DataFrame]</code> </p> <code>x_test</code> <p>test data</p> <p> TYPE: <code>Union[NDArray, DataFrame]</code> </p> <code>y_test</code> <p>labels for test data</p> <p> TYPE: <code>Union[NDArray, DataFrame]</code> </p> <code>feature_names</code> <p>name of the features of input data</p> <p> TYPE: <code>Optional[Sequence[str]]</code> DEFAULT: <code>None</code> </p> <code>target_names</code> <p>names of the features of target data</p> <p> TYPE: <code>Optional[Sequence[str]]</code> DEFAULT: <code>None</code> </p> <code>data_names</code> <p>names assigned to data points. For example, if the dataset is a time series, each entry can be a timestamp which can be referenced directly instead of using a row number.</p> <p> TYPE: <code>Optional[Sequence[str]]</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A textual description of the dataset.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>is_multi_output</code> <p>set to <code>False</code> if labels are scalars, or to <code>True</code> if they are vectors of dimension &gt; 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>def __init__(\n    self,\n    x_train: Union[NDArray, pd.DataFrame],\n    y_train: Union[NDArray, pd.DataFrame],\n    x_test: Union[NDArray, pd.DataFrame],\n    y_test: Union[NDArray, pd.DataFrame],\n    feature_names: Optional[Sequence[str]] = None,\n    target_names: Optional[Sequence[str]] = None,\n    data_names: Optional[Sequence[str]] = None,\n    description: Optional[str] = None,\n    # FIXME: use same parameter name as in check_X_y()\n    is_multi_output: bool = False,\n):\n    \"\"\"Constructs a Dataset from data and labels.\n\n    Args:\n        x_train: training data\n        y_train: labels for training data\n        x_test: test data\n        y_test: labels for test data\n        feature_names: name of the features of input data\n        target_names: names of the features of target data\n        data_names: names assigned to data points.\n            For example, if the dataset is a time series, each entry can be a\n            timestamp which can be referenced directly instead of using a row\n            number.\n        description: A textual description of the dataset.\n        is_multi_output: set to `False` if labels are scalars, or to\n            `True` if they are vectors of dimension &gt; 1.\n    \"\"\"\n    self.x_train, self.y_train = check_X_y(\n        x_train, y_train, multi_output=is_multi_output\n    )\n    self.x_test, self.y_test = check_X_y(\n        x_test, y_test, multi_output=is_multi_output\n    )\n\n    if x_train.shape[-1] != x_test.shape[-1]:\n        raise ValueError(\n            f\"Mismatching number of features: \"\n            f\"{x_train.shape[-1]} and {x_test.shape[-1]}\"\n        )\n    if x_train.shape[0] != y_train.shape[0]:\n        raise ValueError(\n            f\"Mismatching number of samples: \"\n            f\"{x_train.shape[-1]} and {x_test.shape[-1]}\"\n        )\n    if x_test.shape[0] != y_test.shape[0]:\n        raise ValueError(\n            f\"Mismatching number of samples: \"\n            f\"{x_test.shape[-1]} and {y_test.shape[-1]}\"\n        )\n\n    def make_names(s: str, a: np.ndarray) -&gt; List[str]:\n        n = a.shape[1] if len(a.shape) &gt; 1 else 1\n        return [f\"{s}{i:0{1 + int(np.log10(n))}d}\" for i in range(1, n + 1)]\n\n    self.feature_names = feature_names\n    self.target_names = target_names\n\n    if self.feature_names is None:\n        if isinstance(x_train, pd.DataFrame):\n            self.feature_names = x_train.columns.tolist()\n        else:\n            self.feature_names = make_names(\"x\", x_train)\n\n    if self.target_names is None:\n        if isinstance(y_train, pd.DataFrame):\n            self.target_names = y_train.columns.tolist()\n        else:\n            self.target_names = make_names(\"y\", y_train)\n\n    if len(self.x_train.shape) &gt; 1:\n        if (\n            len(self.feature_names) != self.x_train.shape[-1]\n            or len(self.feature_names) != self.x_test.shape[-1]\n        ):\n            raise ValueError(\"Mismatching number of features and names\")\n    if len(self.y_train.shape) &gt; 1:\n        if (\n            len(self.target_names) != self.y_train.shape[-1]\n            or len(self.target_names) != self.y_test.shape[-1]\n        ):\n            raise ValueError(\"Mismatching number of targets and names\")\n\n    self.description = description or \"No description\"\n    self._indices: NDArray[np.int_] = np.arange(len(self.x_train), dtype=np.int_)\n    self._data_names: NDArray[np.object_] = (\n        np.array(data_names, dtype=object)\n        if data_names is not None\n        else self._indices.astype(object)\n    )\n</code></pre>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.Dataset.data_names","title":"data_names  <code>property</code>","text":"<pre><code>data_names: NDArray[object_]\n</code></pre> <p>Names of each individual datapoint.</p> <p>Used for reporting Shapley values.</p>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.Dataset.dim","title":"dim  <code>property</code>","text":"<pre><code>dim: int\n</code></pre> <p>Returns the number of dimensions of a sample.</p>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.Dataset.indices","title":"indices  <code>property</code>","text":"<pre><code>indices: NDArray[int_]\n</code></pre> <p>Index of positions in data.x_train.</p> <p>Contiguous integers from 0 to len(Dataset).</p>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.Dataset.from_arrays","title":"from_arrays  <code>classmethod</code>","text":"<pre><code>from_arrays(\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any\n) -&gt; Dataset\n</code></pre> <p>Constructs a Dataset object from X and y numpy arrays  as returned by the <code>make_*</code> functions in sklearn generated datasets.</p> Example <pre><code>&gt;&gt;&gt; from pydvl.utils import Dataset\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression()\n&gt;&gt;&gt; dataset = Dataset.from_arrays(X, y)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>numpy array of shape (n_samples, n_features)</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>numpy array of shape (n_samples,)</p> <p> TYPE: <code>NDArray</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code></p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>random_state</code> <p>seed for train / test split</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stratify_by_target</code> <p>If <code>True</code>, data is split in a stratified fashion, using the y variable as labels. Read more in sklearn's user guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Dataset constructor. Use this to pass e.g. <code>feature_names</code> or <code>target_names</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>Object with the passed X and y arrays split across training and test sets.</p> <p>New in version 0.4.0</p> <p>Changed in version 0.6.0</p> <p>Added kwargs to pass to the Dataset constructor.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>@classmethod\ndef from_arrays(\n    cls,\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any,\n) -&gt; \"Dataset\":\n    \"\"\"Constructs a [Dataset][pydvl.utils.Dataset] object from X and y numpy arrays  as\n    returned by the `make_*` functions in [sklearn generated datasets](https://scikit-learn.org/stable/datasets/sample_generators.html).\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from pydvl.utils import Dataset\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression()\n        &gt;&gt;&gt; dataset = Dataset.from_arrays(X, y)\n        ```\n\n    Args:\n        X: numpy array of shape (n_samples, n_features)\n        y: numpy array of shape (n_samples,)\n        train_size: size of the training dataset. Used in `train_test_split`\n        random_state: seed for train / test split\n        stratify_by_target: If `True`, data is split in a stratified fashion,\n            using the y variable as labels. Read more in [sklearn's user\n            guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.utils.Dataset] constructor. Use this to pass e.g. `feature_names`\n            or `target_names`.\n\n    Returns:\n        Object with the passed X and y arrays split across training and test sets.\n\n    !!! tip \"New in version 0.4.0\"\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added kwargs to pass to the [Dataset][pydvl.utils.Dataset] constructor.\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(\n        X,\n        y,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=y if stratify_by_target else None,\n    )\n    return cls(x_train, y_train, x_test, y_test, **kwargs)\n</code></pre>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.Dataset.from_sklearn","title":"from_sklearn  <code>classmethod</code>","text":"<pre><code>from_sklearn(\n    data: Bunch,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any\n) -&gt; Dataset\n</code></pre> <p>Constructs a Dataset object from a sklearn.utils.Bunch, as returned by the <code>load_*</code> functions in scikit-learn toy datasets.</p> Example <pre><code>&gt;&gt;&gt; from pydvl.utils import Dataset\n&gt;&gt;&gt; from sklearn.datasets import load_boston\n&gt;&gt;&gt; dataset = Dataset.from_sklearn(load_boston())\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>scikit-learn Bunch object. The following attributes are supported:</p> <ul> <li><code>data</code>: covariates.</li> <li><code>target</code>: target variables (labels).</li> <li><code>feature_names</code> (optional): the feature names.</li> <li><code>target_names</code> (optional): the target names.</li> <li><code>DESCR</code> (optional): a description.</li> </ul> <p> TYPE: <code>Bunch</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code></p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>random_state</code> <p>seed for train / test split</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stratify_by_target</code> <p>If <code>True</code>, data is split in a stratified fashion, using the target variable as labels. Read more in scikit-learn's user guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Dataset constructor. Use this to pass e.g. <code>is_multi_output</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>Object with the sklearn dataset</p> <p>Changed in version 0.6.0</p> <p>Added kwargs to pass to the Dataset constructor.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>@classmethod\ndef from_sklearn(\n    cls,\n    data: Bunch,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any,\n) -&gt; \"Dataset\":\n    \"\"\"Constructs a [Dataset][pydvl.utils.Dataset] object from a\n    [sklearn.utils.Bunch][], as returned by the `load_*`\n    functions in [scikit-learn toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html).\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from pydvl.utils import Dataset\n        &gt;&gt;&gt; from sklearn.datasets import load_boston\n        &gt;&gt;&gt; dataset = Dataset.from_sklearn(load_boston())\n        ```\n\n    Args:\n        data: scikit-learn Bunch object. The following attributes are supported:\n\n            - `data`: covariates.\n            - `target`: target variables (labels).\n            - `feature_names` (**optional**): the feature names.\n            - `target_names` (**optional**): the target names.\n            - `DESCR` (**optional**): a description.\n        train_size: size of the training dataset. Used in `train_test_split`\n        random_state: seed for train / test split\n        stratify_by_target: If `True`, data is split in a stratified\n            fashion, using the target variable as labels. Read more in\n            [scikit-learn's user guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.utils.Dataset] constructor. Use this to pass e.g. `is_multi_output`.\n\n    Returns:\n        Object with the sklearn dataset\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added kwargs to pass to the [Dataset][pydvl.utils.Dataset] constructor.\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(\n        data.data,\n        data.target,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=data.target if stratify_by_target else None,\n    )\n    return cls(\n        x_train,\n        y_train,\n        x_test,\n        y_test,\n        feature_names=data.get(\"feature_names\"),\n        target_names=data.get(\"target_names\"),\n        description=data.get(\"DESCR\"),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.Dataset.get_test_data","title":"get_test_data","text":"<pre><code>get_test_data(\n    indices: Optional[Iterable[int]] = None,\n) -&gt; Tuple[NDArray, NDArray]\n</code></pre> <p>Returns the entire test set regardless of the passed indices.</p> <p>The passed indices will not be used because for data valuation we generally want to score the trained model on the entire test data.</p> <p>Additionally, the way this method is used in the Utility class, the passed indices will be those of the training data and would not work on the test data.</p> <p>There may be cases where it is desired to use parts of the test data. In those cases, it is recommended to inherit from Dataset and override get_test_data().</p> <p>For example, the following snippet shows how one could go about mapping the training data indices into test data indices inside get_test_data():</p> Example <pre><code>&gt;&gt;&gt; from pydvl.utils import Dataset\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; class DatasetWithTestDataIndices(Dataset):\n...    def get_test_data(self, indices=None):\n...        if indices is None:\n...            return self.x_test, self.y_test\n...        fraction = len(list(indices)) / len(self)\n...        mapped_indices = len(self.x_test) / len(self) * np.asarray(indices)\n...        mapped_indices = np.unique(mapped_indices.astype(int))\n...        return self.x_test[mapped_indices], self.y_test[mapped_indices]\n...\n&gt;&gt;&gt; X = np.random.rand(100, 10)\n&gt;&gt;&gt; y = np.random.randint(0, 2, 100)\n&gt;&gt;&gt; dataset = DatasetWithTestDataIndices.from_arrays(X, y)\n&gt;&gt;&gt; indices = np.random.choice(dataset.indices, 30, replace=False)\n&gt;&gt;&gt; _ = dataset.get_training_data(indices)\n&gt;&gt;&gt; _ = dataset.get_test_data(indices)\n</code></pre> PARAMETER DESCRIPTION <code>indices</code> <p>Optional indices into the test data. This argument is unused left for compatibility with get_training_data().</p> <p> TYPE: <code>Optional[Iterable[int]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[NDArray, NDArray]</code> <p>The entire test data.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>def get_test_data(\n    self, indices: Optional[Iterable[int]] = None\n) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"Returns the entire test set regardless of the passed indices.\n\n    The passed indices will not be used because for data valuation\n    we generally want to score the trained model on the entire test data.\n\n    Additionally, the way this method is used in the\n    [Utility][pydvl.utils.utility.Utility] class, the passed indices will\n    be those of the training data and would not work on the test data.\n\n    There may be cases where it is desired to use parts of the test data.\n    In those cases, it is recommended to inherit from\n    [Dataset][pydvl.utils.dataset.Dataset] and override\n    [get_test_data()][pydvl.utils.dataset.Dataset.get_test_data].\n\n    For example, the following snippet shows how one could go about\n    mapping the training data indices into test data indices\n    inside [get_test_data()][pydvl.utils.dataset.Dataset.get_test_data]:\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from pydvl.utils import Dataset\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; class DatasetWithTestDataIndices(Dataset):\n        ...    def get_test_data(self, indices=None):\n        ...        if indices is None:\n        ...            return self.x_test, self.y_test\n        ...        fraction = len(list(indices)) / len(self)\n        ...        mapped_indices = len(self.x_test) / len(self) * np.asarray(indices)\n        ...        mapped_indices = np.unique(mapped_indices.astype(int))\n        ...        return self.x_test[mapped_indices], self.y_test[mapped_indices]\n        ...\n        &gt;&gt;&gt; X = np.random.rand(100, 10)\n        &gt;&gt;&gt; y = np.random.randint(0, 2, 100)\n        &gt;&gt;&gt; dataset = DatasetWithTestDataIndices.from_arrays(X, y)\n        &gt;&gt;&gt; indices = np.random.choice(dataset.indices, 30, replace=False)\n        &gt;&gt;&gt; _ = dataset.get_training_data(indices)\n        &gt;&gt;&gt; _ = dataset.get_test_data(indices)\n        ```\n\n    Args:\n        indices: Optional indices into the test data. This argument is\n            unused left for compatibility with\n            [get_training_data()][pydvl.utils.dataset.Dataset.get_training_data].\n\n    Returns:\n        The entire test data.\n    \"\"\"\n    return self.x_test, self.y_test\n</code></pre>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.Dataset.get_training_data","title":"get_training_data","text":"<pre><code>get_training_data(\n    indices: Optional[Iterable[int]] = None,\n) -&gt; Tuple[NDArray, NDArray]\n</code></pre> <p>Given a set of indices, returns the training data that refer to those indices.</p> <p>This is used mainly by Utility to retrieve subsets of the data from indices. It is typically not needed in algorithms.</p> PARAMETER DESCRIPTION <code>indices</code> <p>Optional indices that will be used to select points from the training data. If <code>None</code>, the entire training data will be returned.</p> <p> TYPE: <code>Optional[Iterable[int]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[NDArray, NDArray]</code> <p>If <code>indices</code> is not <code>None</code>, the selected x and y arrays from the training data. Otherwise, the entire dataset.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>def get_training_data(\n    self, indices: Optional[Iterable[int]] = None\n) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"Given a set of indices, returns the training data that refer to those\n    indices.\n\n    This is used mainly by [Utility][pydvl.utils.utility.Utility] to retrieve\n    subsets of the data from indices. It is typically **not needed in\n    algorithms**.\n\n    Args:\n        indices: Optional indices that will be used to select points from\n            the training data. If `None`, the entire training data will be\n            returned.\n\n    Returns:\n        If `indices` is not `None`, the selected x and y arrays from the\n            training data. Otherwise, the entire dataset.\n    \"\"\"\n    if indices is None:\n        return self.x_train, self.y_train\n    x = self.x_train[indices]\n    y = self.y_train[indices]\n    return x, y\n</code></pre>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.GroupedDataset","title":"GroupedDataset","text":"<pre><code>GroupedDataset(\n    x_train: NDArray,\n    y_train: NDArray,\n    x_test: NDArray,\n    y_test: NDArray,\n    data_groups: Sequence,\n    feature_names: Optional[Sequence[str]] = None,\n    target_names: Optional[Sequence[str]] = None,\n    group_names: Optional[Sequence[str]] = None,\n    description: Optional[str] = None,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Class for grouping datasets.</p> <p>Used for calculating Shapley values of subsets of the data considered as logical units. For instance, one can group by value of a categorical feature, by bin into which a continuous feature falls, or by label.</p> PARAMETER DESCRIPTION <code>x_train</code> <p>training data</p> <p> TYPE: <code>NDArray</code> </p> <code>y_train</code> <p>labels of training data</p> <p> TYPE: <code>NDArray</code> </p> <code>x_test</code> <p>test data</p> <p> TYPE: <code>NDArray</code> </p> <code>y_test</code> <p>labels of test data</p> <p> TYPE: <code>NDArray</code> </p> <code>data_groups</code> <p>Iterable of the same length as <code>x_train</code> containing a group label for each training data point. The label can be of any type, e.g. <code>str</code> or <code>int</code>. Data points with the same label will then be grouped by this object and considered as one for effects of valuation.</p> <p> TYPE: <code>Sequence</code> </p> <code>feature_names</code> <p>names of the covariates' features.</p> <p> TYPE: <code>Optional[Sequence[str]]</code> DEFAULT: <code>None</code> </p> <code>target_names</code> <p>names of the labels or targets y</p> <p> TYPE: <code>Optional[Sequence[str]]</code> DEFAULT: <code>None</code> </p> <code>group_names</code> <p>names of the groups. If not provided, the labels from <code>data_groups</code> will be used.</p> <p> TYPE: <code>Optional[Sequence[str]]</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A textual description of the dataset</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Dataset constructor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Changed in version 0.6.0</p> <p>Added <code>group_names</code> and forwarding of <code>kwargs</code></p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>def __init__(\n    self,\n    x_train: NDArray,\n    y_train: NDArray,\n    x_test: NDArray,\n    y_test: NDArray,\n    data_groups: Sequence,\n    feature_names: Optional[Sequence[str]] = None,\n    target_names: Optional[Sequence[str]] = None,\n    group_names: Optional[Sequence[str]] = None,\n    description: Optional[str] = None,\n    **kwargs: Any,\n):\n    \"\"\"Class for grouping datasets.\n\n    Used for calculating Shapley values of subsets of the data considered\n    as logical units. For instance, one can group by value of a categorical\n    feature, by bin into which a continuous feature falls, or by label.\n\n    Args:\n        x_train: training data\n        y_train: labels of training data\n        x_test: test data\n        y_test: labels of test data\n        data_groups: Iterable of the same length as `x_train` containing\n            a group label for each training data point. The label can be of any\n            type, e.g. `str` or `int`. Data points with the same label will\n            then be grouped by this object and considered as one for effects of\n            valuation.\n        feature_names: names of the covariates' features.\n        target_names: names of the labels or targets y\n        group_names: names of the groups. If not provided, the labels\n            from `data_groups` will be used.\n        description: A textual description of the dataset\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.utils.Dataset] constructor.\n\n    !!! tip \"Changed in version 0.6.0\"\n    Added `group_names` and forwarding of `kwargs`\n    \"\"\"\n    super().__init__(\n        x_train=x_train,\n        y_train=y_train,\n        x_test=x_test,\n        y_test=y_test,\n        feature_names=feature_names,\n        target_names=target_names,\n        description=description,\n        **kwargs,\n    )\n\n    if len(data_groups) != len(x_train):\n        raise ValueError(\n            f\"data_groups and x_train must have the same length.\"\n            f\"Instead got {len(data_groups)=} and {len(x_train)=}\"\n        )\n\n    self.groups: OrderedDict[Any, List[int]] = OrderedDict(\n        {k: [] for k in set(data_groups)}\n    )\n    for idx, group in enumerate(data_groups):\n        self.groups[group].append(idx)\n    self.group_items = list(self.groups.items())\n    self._indices = np.arange(len(self.groups.keys()))\n    self._data_names = (\n        np.array(group_names, dtype=object)\n        if group_names is not None\n        else np.array(list(self.groups.keys()), dtype=object)\n    )\n</code></pre>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.GroupedDataset.data_names","title":"data_names  <code>property</code>","text":"<pre><code>data_names\n</code></pre> <p>Names of the groups.</p>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.GroupedDataset.dim","title":"dim  <code>property</code>","text":"<pre><code>dim: int\n</code></pre> <p>Returns the number of dimensions of a sample.</p>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.GroupedDataset.indices","title":"indices  <code>property</code>","text":"<pre><code>indices\n</code></pre> <p>Indices of the groups.</p>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.GroupedDataset.from_arrays","title":"from_arrays  <code>classmethod</code>","text":"<pre><code>from_arrays(\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    data_groups: Optional[Sequence] = None,\n    **kwargs: Any\n) -&gt; Dataset\n</code></pre> <p>Constructs a GroupedDataset object from X and y numpy arrays as returned by the <code>make_*</code> functions in scikit-learn generated datasets.</p> Example <pre><code>&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; from pydvl.utils import GroupedDataset\n&gt;&gt;&gt; X, y = make_classification(\n...     n_samples=100,\n...     n_features=4,\n...     n_informative=2,\n...     n_redundant=0,\n...     random_state=0,\n...     shuffle=False\n... )\n&gt;&gt;&gt; data_groups = X[:, 0] // 0.5\n&gt;&gt;&gt; dataset = GroupedDataset.from_arrays(X, y, data_groups=data_groups)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>array of shape (n_samples, n_features)</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>array of shape (n_samples,)</p> <p> TYPE: <code>NDArray</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>random_state</code> <p>seed for train / test split.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stratify_by_target</code> <p>If <code>True</code>, data is split in a stratified fashion, using the y variable as labels. Read more in sklearn's user guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>data_groups</code> <p>an array holding the group index or name for each data point. The length of this array must be equal to the number of data points in the dataset.</p> <p> TYPE: <code>Optional[Sequence]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments that will be passed to the Dataset constructor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>Dataset with the passed X and y arrays split across training and test sets.</p> <p>New in version 0.4.0</p> <p>Changed in version 0.6.0</p> <p>Added kwargs to pass to the Dataset constructor.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>@classmethod\ndef from_arrays(\n    cls,\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    data_groups: Optional[Sequence] = None,\n    **kwargs: Any,\n) -&gt; \"Dataset\":\n    \"\"\"Constructs a [GroupedDataset][pydvl.utils.GroupedDataset] object from X and y numpy arrays\n    as returned by the `make_*` functions in\n    [scikit-learn generated datasets](https://scikit-learn.org/stable/datasets/sample_generators.html).\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from sklearn.datasets import make_classification\n        &gt;&gt;&gt; from pydvl.utils import GroupedDataset\n        &gt;&gt;&gt; X, y = make_classification(\n        ...     n_samples=100,\n        ...     n_features=4,\n        ...     n_informative=2,\n        ...     n_redundant=0,\n        ...     random_state=0,\n        ...     shuffle=False\n        ... )\n        &gt;&gt;&gt; data_groups = X[:, 0] // 0.5\n        &gt;&gt;&gt; dataset = GroupedDataset.from_arrays(X, y, data_groups=data_groups)\n        ```\n\n    Args:\n        X: array of shape (n_samples, n_features)\n        y: array of shape (n_samples,)\n        train_size: size of the training dataset. Used in `train_test_split`.\n        random_state: seed for train / test split.\n        stratify_by_target: If `True`, data is split in a stratified\n            fashion, using the y variable as labels. Read more in\n            [sklearn's user guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        data_groups: an array holding the group index or name for each data\n            point. The length of this array must be equal to the number of\n            data points in the dataset.\n        kwargs: Additional keyword arguments that will be passed to the\n            [Dataset][pydvl.utils.Dataset] constructor.\n\n    Returns:\n        Dataset with the passed X and y arrays split across training and\n            test sets.\n\n    !!! tip \"New in version 0.4.0\"\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added kwargs to pass to the [Dataset][pydvl.utils.Dataset] constructor.\n    \"\"\"\n    if data_groups is None:\n        raise ValueError(\n            \"data_groups must be provided when constructing a GroupedDataset\"\n        )\n    x_train, x_test, y_train, y_test, data_groups_train, _ = train_test_split(\n        X,\n        y,\n        data_groups,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=y if stratify_by_target else None,\n    )\n    dataset = Dataset(\n        x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, **kwargs\n    )\n    return cls.from_dataset(dataset, data_groups_train)\n</code></pre>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.GroupedDataset.from_dataset","title":"from_dataset  <code>classmethod</code>","text":"<pre><code>from_dataset(dataset: Dataset, data_groups: Sequence[Any]) -&gt; GroupedDataset\n</code></pre> <p>Creates a GroupedDataset object from the data a Dataset object and a mapping of data groups.</p> Example <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pydvl.utils import Dataset, GroupedDataset\n&gt;&gt;&gt; dataset = Dataset.from_arrays(\n...     X=np.asarray([[1, 2], [3, 4], [5, 6], [7, 8]]),\n...     y=np.asarray([0, 1, 0, 1]),\n... )\n&gt;&gt;&gt; dataset = GroupedDataset.from_dataset(dataset, data_groups=[0, 0, 1, 1])\n</code></pre> PARAMETER DESCRIPTION <code>dataset</code> <p>The original data.</p> <p> TYPE: <code>Dataset</code> </p> <code>data_groups</code> <p>An array holding the group index or name for each data point. The length of this array must be equal to the number of data points in the dataset.</p> <p> TYPE: <code>Sequence[Any]</code> </p> RETURNS DESCRIPTION <code>GroupedDataset</code> <p>A GroupedDataset with the initial Dataset grouped by data_groups.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>@classmethod\ndef from_dataset(\n    cls, dataset: Dataset, data_groups: Sequence[Any]\n) -&gt; \"GroupedDataset\":\n    \"\"\"Creates a [GroupedDataset][pydvl.utils.GroupedDataset] object from the data a\n    [Dataset][pydvl.utils.Dataset] object and a mapping of data groups.\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from pydvl.utils import Dataset, GroupedDataset\n        &gt;&gt;&gt; dataset = Dataset.from_arrays(\n        ...     X=np.asarray([[1, 2], [3, 4], [5, 6], [7, 8]]),\n        ...     y=np.asarray([0, 1, 0, 1]),\n        ... )\n        &gt;&gt;&gt; dataset = GroupedDataset.from_dataset(dataset, data_groups=[0, 0, 1, 1])\n        ```\n\n    Args:\n        dataset: The original data.\n        data_groups: An array holding the group index or name for each data\n            point. The length of this array must be equal to the number of\n            data points in the dataset.\n\n    Returns:\n        A [GroupedDataset][pydvl.utils.GroupedDataset] with the initial\n            [Dataset][pydvl.utils.Dataset] grouped by data_groups.\n    \"\"\"\n    return cls(\n        x_train=dataset.x_train,\n        y_train=dataset.y_train,\n        x_test=dataset.x_test,\n        y_test=dataset.y_test,\n        data_groups=data_groups,\n        feature_names=dataset.feature_names,\n        target_names=dataset.target_names,\n        description=dataset.description,\n    )\n</code></pre>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.GroupedDataset.from_sklearn","title":"from_sklearn  <code>classmethod</code>","text":"<pre><code>from_sklearn(\n    data: Bunch,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    data_groups: Optional[Sequence] = None,\n    **kwargs: Any\n) -&gt; GroupedDataset\n</code></pre> <p>Constructs a GroupedDataset object from a sklearn.utils.Bunch as returned by the <code>load_*</code> functions in scikit-learn toy datasets and groups it.</p> Example <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from pydvl.utils import GroupedDataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; data_groups = iris.data[:, 0] // 0.5\n&gt;&gt;&gt; dataset = GroupedDataset.from_sklearn(iris, data_groups=data_groups)\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>scikit-learn Bunch object. The following attributes are supported:</p> <ul> <li><code>data</code>: covariates.</li> <li><code>target</code>: target variables (labels).</li> <li><code>feature_names</code> (optional): the feature names.</li> <li><code>target_names</code> (optional): the target names.</li> <li><code>DESCR</code> (optional): a description.</li> </ul> <p> TYPE: <code>Bunch</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>random_state</code> <p>seed for train / test split.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stratify_by_target</code> <p>If <code>True</code>, data is split in a stratified fashion, using the target variable as labels. Read more in sklearn's user guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>data_groups</code> <p>an array holding the group index or name for each data point. The length of this array must be equal to the number of data points in the dataset.</p> <p> TYPE: <code>Optional[Sequence]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Dataset constructor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>GroupedDataset</code> <p>Dataset with the selected sklearn data</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>@classmethod\ndef from_sklearn(\n    cls,\n    data: Bunch,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    data_groups: Optional[Sequence] = None,\n    **kwargs: Any,\n) -&gt; \"GroupedDataset\":\n    \"\"\"Constructs a [GroupedDataset][pydvl.utils.GroupedDataset] object from a\n    [sklearn.utils.Bunch][sklearn.utils.Bunch] as returned by the `load_*` functions in\n    [scikit-learn toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) and groups\n    it.\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from pydvl.utils import GroupedDataset\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; data_groups = iris.data[:, 0] // 0.5\n        &gt;&gt;&gt; dataset = GroupedDataset.from_sklearn(iris, data_groups=data_groups)\n        ```\n\n    Args:\n        data: scikit-learn Bunch object. The following attributes are supported:\n\n            - `data`: covariates.\n            - `target`: target variables (labels).\n            - `feature_names` (**optional**): the feature names.\n            - `target_names` (**optional**): the target names.\n            - `DESCR` (**optional**): a description.\n        train_size: size of the training dataset. Used in `train_test_split`.\n        random_state: seed for train / test split.\n        stratify_by_target: If `True`, data is split in a stratified\n            fashion, using the target variable as labels. Read more in\n            [sklearn's user guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        data_groups: an array holding the group index or name for each\n            data point. The length of this array must be equal to the number of\n            data points in the dataset.\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.utils.Dataset] constructor.\n\n    Returns:\n        Dataset with the selected sklearn data\n    \"\"\"\n    if data_groups is None:\n        raise ValueError(\n            \"data_groups must be provided when constructing a GroupedDataset\"\n        )\n\n    x_train, x_test, y_train, y_test, data_groups_train, _ = train_test_split(\n        data.data,\n        data.target,\n        data_groups,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=data.target if stratify_by_target else None,\n    )\n\n    dataset = Dataset(\n        x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, **kwargs\n    )\n    return cls.from_dataset(dataset, data_groups_train)  # type: ignore\n</code></pre>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.GroupedDataset.get_test_data","title":"get_test_data","text":"<pre><code>get_test_data(\n    indices: Optional[Iterable[int]] = None,\n) -&gt; Tuple[NDArray, NDArray]\n</code></pre> <p>Returns the entire test set regardless of the passed indices.</p> <p>The passed indices will not be used because for data valuation we generally want to score the trained model on the entire test data.</p> <p>Additionally, the way this method is used in the Utility class, the passed indices will be those of the training data and would not work on the test data.</p> <p>There may be cases where it is desired to use parts of the test data. In those cases, it is recommended to inherit from Dataset and override get_test_data().</p> <p>For example, the following snippet shows how one could go about mapping the training data indices into test data indices inside get_test_data():</p> Example <pre><code>&gt;&gt;&gt; from pydvl.utils import Dataset\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; class DatasetWithTestDataIndices(Dataset):\n...    def get_test_data(self, indices=None):\n...        if indices is None:\n...            return self.x_test, self.y_test\n...        fraction = len(list(indices)) / len(self)\n...        mapped_indices = len(self.x_test) / len(self) * np.asarray(indices)\n...        mapped_indices = np.unique(mapped_indices.astype(int))\n...        return self.x_test[mapped_indices], self.y_test[mapped_indices]\n...\n&gt;&gt;&gt; X = np.random.rand(100, 10)\n&gt;&gt;&gt; y = np.random.randint(0, 2, 100)\n&gt;&gt;&gt; dataset = DatasetWithTestDataIndices.from_arrays(X, y)\n&gt;&gt;&gt; indices = np.random.choice(dataset.indices, 30, replace=False)\n&gt;&gt;&gt; _ = dataset.get_training_data(indices)\n&gt;&gt;&gt; _ = dataset.get_test_data(indices)\n</code></pre> PARAMETER DESCRIPTION <code>indices</code> <p>Optional indices into the test data. This argument is unused left for compatibility with get_training_data().</p> <p> TYPE: <code>Optional[Iterable[int]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[NDArray, NDArray]</code> <p>The entire test data.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>def get_test_data(\n    self, indices: Optional[Iterable[int]] = None\n) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"Returns the entire test set regardless of the passed indices.\n\n    The passed indices will not be used because for data valuation\n    we generally want to score the trained model on the entire test data.\n\n    Additionally, the way this method is used in the\n    [Utility][pydvl.utils.utility.Utility] class, the passed indices will\n    be those of the training data and would not work on the test data.\n\n    There may be cases where it is desired to use parts of the test data.\n    In those cases, it is recommended to inherit from\n    [Dataset][pydvl.utils.dataset.Dataset] and override\n    [get_test_data()][pydvl.utils.dataset.Dataset.get_test_data].\n\n    For example, the following snippet shows how one could go about\n    mapping the training data indices into test data indices\n    inside [get_test_data()][pydvl.utils.dataset.Dataset.get_test_data]:\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from pydvl.utils import Dataset\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; class DatasetWithTestDataIndices(Dataset):\n        ...    def get_test_data(self, indices=None):\n        ...        if indices is None:\n        ...            return self.x_test, self.y_test\n        ...        fraction = len(list(indices)) / len(self)\n        ...        mapped_indices = len(self.x_test) / len(self) * np.asarray(indices)\n        ...        mapped_indices = np.unique(mapped_indices.astype(int))\n        ...        return self.x_test[mapped_indices], self.y_test[mapped_indices]\n        ...\n        &gt;&gt;&gt; X = np.random.rand(100, 10)\n        &gt;&gt;&gt; y = np.random.randint(0, 2, 100)\n        &gt;&gt;&gt; dataset = DatasetWithTestDataIndices.from_arrays(X, y)\n        &gt;&gt;&gt; indices = np.random.choice(dataset.indices, 30, replace=False)\n        &gt;&gt;&gt; _ = dataset.get_training_data(indices)\n        &gt;&gt;&gt; _ = dataset.get_test_data(indices)\n        ```\n\n    Args:\n        indices: Optional indices into the test data. This argument is\n            unused left for compatibility with\n            [get_training_data()][pydvl.utils.dataset.Dataset.get_training_data].\n\n    Returns:\n        The entire test data.\n    \"\"\"\n    return self.x_test, self.y_test\n</code></pre>"},{"location":"api/pydvl/utils/dataset/#pydvl.utils.dataset.GroupedDataset.get_training_data","title":"get_training_data","text":"<pre><code>get_training_data(\n    indices: Optional[Iterable[int]] = None,\n) -&gt; Tuple[NDArray, NDArray]\n</code></pre> <p>Returns the data and labels of all samples in the given groups.</p> PARAMETER DESCRIPTION <code>indices</code> <p>group indices whose elements to return. If <code>None</code>, all data from all groups are returned.</p> <p> TYPE: <code>Optional[Iterable[int]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[NDArray, NDArray]</code> <p>Tuple of training data x and labels y.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>def get_training_data(\n    self, indices: Optional[Iterable[int]] = None\n) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"Returns the data and labels of all samples in the given groups.\n\n    Args:\n        indices: group indices whose elements to return. If `None`,\n            all data from all groups are returned.\n\n    Returns:\n        Tuple of training data x and labels y.\n    \"\"\"\n    if indices is None:\n        indices = self.indices\n    data_indices = [\n        idx for group_id in indices for idx in self.group_items[group_id][1]\n    ]\n    return super().get_training_data(data_indices)\n</code></pre>"},{"location":"api/pydvl/utils/exceptions/","title":"Exceptions","text":""},{"location":"api/pydvl/utils/exceptions/#pydvl.utils.exceptions","title":"pydvl.utils.exceptions","text":""},{"location":"api/pydvl/utils/exceptions/#pydvl.utils.exceptions.catch_and_raise_exception","title":"catch_and_raise_exception","text":"<pre><code>catch_and_raise_exception(\n    catch_exception_type: Type[CatchExceptionType],\n    raise_exception_factory: Callable[[CatchExceptionType], RaiseExceptionType],\n) -&gt; Callable\n</code></pre> <p>A decorator that catches exceptions of a specified exception type and raises another specified exception.</p> PARAMETER DESCRIPTION <code>catch_exception_type</code> <p>The type of the exception to catch.</p> <p> TYPE: <code>Type[CatchExceptionType]</code> </p> <code>raise_exception_factory</code> <p>A factory function that creates a new exception.</p> <p> TYPE: <code>Callable[[CatchExceptionType], RaiseExceptionType]</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>A decorator function that wraps the target function.</p> Example <pre><code>@catch_and_raise_exception(RuntimeError, lambda e: TorchLinalgEighException(e))\ndef safe_torch_linalg_eigh(*args, **kwargs):\n    '''\n    A wrapper around `torch.linalg.eigh` that safely handles potential runtime errors\n    by raising a custom `TorchLinalgEighException` with more context,\n    especially related to the issues reported in\n    https://github.com/pytorch/pytorch/issues/92141.\n\n    Args:\n    *args: Positional arguments passed to `torch.linalg.eigh`.\n    **kwargs: Keyword arguments passed to `torch.linalg.eigh`.\n\n    Returns:\n    The result of calling `torch.linalg.eigh` with the provided arguments.\n\n    Raises:\n    TorchLinalgEighException: If a `RuntimeError` occurs during the execution of\n    `torch.linalg.eigh`.\n    '''\n    return torch.linalg.eigh(*args, **kwargs)\n</code></pre> Source code in <code>src/pydvl/utils/exceptions.py</code> <pre><code>def catch_and_raise_exception(\n    catch_exception_type: Type[CatchExceptionType],\n    raise_exception_factory: Callable[[CatchExceptionType], RaiseExceptionType],\n) -&gt; Callable:\n    \"\"\"\n    A decorator that catches exceptions of a specified exception type and raises\n    another specified exception.\n\n    Args:\n        catch_exception_type: The type of the exception to catch.\n        raise_exception_factory: A factory function that creates a new exception.\n\n    Returns:\n        A decorator function that wraps the target function.\n\n    ??? Example\n\n        ```python\n        @catch_and_raise_exception(RuntimeError, lambda e: TorchLinalgEighException(e))\n        def safe_torch_linalg_eigh(*args, **kwargs):\n            '''\n            A wrapper around `torch.linalg.eigh` that safely handles potential runtime errors\n            by raising a custom `TorchLinalgEighException` with more context,\n            especially related to the issues reported in\n            https://github.com/pytorch/pytorch/issues/92141.\n\n            Args:\n            *args: Positional arguments passed to `torch.linalg.eigh`.\n            **kwargs: Keyword arguments passed to `torch.linalg.eigh`.\n\n            Returns:\n            The result of calling `torch.linalg.eigh` with the provided arguments.\n\n            Raises:\n            TorchLinalgEighException: If a `RuntimeError` occurs during the execution of\n            `torch.linalg.eigh`.\n            '''\n            return torch.linalg.eigh(*args, **kwargs)\n        ```\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except catch_exception_type as e:\n                raise raise_exception_factory(e) from e\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/pydvl/utils/functional/","title":"Functional","text":""},{"location":"api/pydvl/utils/functional/#pydvl.utils.functional","title":"pydvl.utils.functional","text":"<p>Supporting utilities for manipulating functions.</p>"},{"location":"api/pydvl/utils/functional/#pydvl.utils.functional._accept_additional_argument","title":"_accept_additional_argument","text":"<pre><code>_accept_additional_argument(\n    *args: Any, fun: Callable[..., R], arg: str, **kwargs: Any\n) -&gt; R\n</code></pre> <p>Calls the given function with the given positional and keyword arguments, removing <code>arg</code> from the keyword arguments.</p> PARAMETER DESCRIPTION <code>args</code> <p>Positional arguments to pass to the function.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>fun</code> <p>The function to call.</p> <p> TYPE: <code>Callable[..., R]</code> </p> <code>arg</code> <p>The name of the argument to remove.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>Keyword arguments to pass to the function.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>R</code> <p>The return value of the function.</p> Source code in <code>src/pydvl/utils/functional.py</code> <pre><code>def _accept_additional_argument(\n    *args: Any, fun: Callable[..., R], arg: str, **kwargs: Any\n) -&gt; R:\n    \"\"\"Calls the given function with the given positional and keyword arguments,\n    removing `arg` from the keyword arguments.\n\n    Args:\n        args: Positional arguments to pass to the function.\n        fun: The function to call.\n        arg: The name of the argument to remove.\n        kwargs: Keyword arguments to pass to the function.\n\n    Returns:\n        The return value of the function.\n    \"\"\"\n    try:\n        del kwargs[arg]\n    except KeyError:\n        pass\n\n    return fun(*args, **kwargs)\n</code></pre>"},{"location":"api/pydvl/utils/functional/#pydvl.utils.functional.free_arguments","title":"free_arguments","text":"<pre><code>free_arguments(fun: Union[Callable, partial]) -&gt; Set[str]\n</code></pre> <p>Computes the set of free arguments for a function or [[functools.partial]] object.</p> <p>All arguments of a function are considered free unless they are set by a partial. For example, if <code>f = partial(g, a=1)</code>, then <code>a</code> is not a free argument of <code>f</code>.</p> PARAMETER DESCRIPTION <code>fun</code> <p>A callable or a partial object.</p> <p> TYPE: <code>Union[Callable, partial]</code> </p> RETURNS DESCRIPTION <code>Set[str]</code> <p>The set of free arguments of <code>fun</code>.</p> <p>New in version 0.7.0</p> Source code in <code>src/pydvl/utils/functional.py</code> <pre><code>def free_arguments(fun: Union[Callable, functools.partial]) -&gt; Set[str]:\n    \"\"\"Computes the set of free arguments for a function or [[functools.partial]]\n    object.\n\n    All arguments of a function are considered free unless they are set by a\n    partial. For example, if `f = partial(g, a=1)`, then `a` is not a free\n    argument of `f`.\n\n    Args:\n        fun: A callable or a [partial object][functools.partial].\n\n    Returns:\n        The set of free arguments of `fun`.\n\n    !!! tip \"New in version 0.7.0\"\n    \"\"\"\n    args_set_by_partial: Set[str] = set()\n\n    def _rec_unroll_partial_function_args(\n        g: Union[Callable, functools.partial],\n    ) -&gt; Callable:\n        \"\"\"Stores arguments and recursively call itself if `g` is a\n        [functools.partial][] object. In the end, returns the initially wrapped\n        function.\n\n        This handles the construct `partial(_accept_additional_argument, *args,\n        **kwargs)` that is used by `maybe_add_argument`.\n\n        Args:\n            g: A partial or a function to unroll.\n\n        Returns:\n            Initial wrapped function.\n        \"\"\"\n        nonlocal args_set_by_partial\n\n        if isinstance(g, functools.partial) and g.func == _accept_additional_argument:\n            arg = g.keywords[\"arg\"]\n            if arg in args_set_by_partial:\n                args_set_by_partial.remove(arg)\n            return _rec_unroll_partial_function_args(g.keywords[\"fun\"])\n        elif isinstance(g, functools.partial):\n            args_set_by_partial.update(g.keywords.keys())\n            args_set_by_partial.update(g.args)\n            return _rec_unroll_partial_function_args(g.func)\n        else:\n            return g\n\n    wrapped_fn = _rec_unroll_partial_function_args(fun)\n    sig = inspect.signature(wrapped_fn)\n    return args_set_by_partial | set(sig.parameters.keys())\n</code></pre>"},{"location":"api/pydvl/utils/functional/#pydvl.utils.functional.maybe_add_argument","title":"maybe_add_argument","text":"<pre><code>maybe_add_argument(fun: Callable, new_arg: str) -&gt; Callable\n</code></pre> <p>Wraps a function to accept the given keyword parameter if it doesn't already.</p> <p>If <code>fun</code> already takes a keyword parameter of name <code>new_arg</code>, then it is returned as is. Otherwise, a wrapper is returned which merely ignores the argument.</p> PARAMETER DESCRIPTION <code>fun</code> <p>The function to wrap</p> <p> TYPE: <code>Callable</code> </p> <code>new_arg</code> <p>The name of the argument that the new function will accept (and ignore).</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>A new function accepting one more keyword argument.</p> <p>Changed in version 0.7.0</p> <p>Ability to work with partials.</p> Source code in <code>src/pydvl/utils/functional.py</code> <pre><code>def maybe_add_argument(fun: Callable, new_arg: str) -&gt; Callable:\n    \"\"\"Wraps a function to accept the given keyword parameter if it doesn't\n    already.\n\n    If `fun` already takes a keyword parameter of name `new_arg`, then it is\n    returned as is. Otherwise, a wrapper is returned which merely ignores the\n    argument.\n\n    Args:\n        fun: The function to wrap\n        new_arg: The name of the argument that the new function will accept\n            (and ignore).\n\n    Returns:\n        A new function accepting one more keyword argument.\n\n    !!! tip \"Changed in version 0.7.0\"\n        Ability to work with partials.\n    \"\"\"\n    if new_arg in free_arguments(fun):\n        return fun\n\n    return functools.partial(_accept_additional_argument, fun=fun, arg=new_arg)\n</code></pre>"},{"location":"api/pydvl/utils/functional/#pydvl.utils.functional.suppress_warnings","title":"suppress_warnings","text":"<pre><code>suppress_warnings(fun: Callable[P, R]) -&gt; Callable[P, R]\n</code></pre><pre><code>suppress_warnings(\n    fun: None = None,\n    *,\n    categories: Sequence[Type[Warning]] = (Warning,),\n    flag: str = \"\"\n) -&gt; Callable[[Callable[P, R]], Callable[P, R]]\n</code></pre><pre><code>suppress_warnings(\n    fun: Callable[P, R],\n    *,\n    categories: Sequence[Type[Warning]] = (Warning,),\n    flag: str = \"\"\n) -&gt; Callable[P, R]\n</code></pre> <pre><code>suppress_warnings(\n    fun: Callable[P, R] | None = None,\n    *,\n    categories: Sequence[Type[Warning]] = (Warning,),\n    flag: str = \"\"\n) -&gt; Union[Callable[[Callable[P, R]], Callable[P, R]], Callable[P, R]]\n</code></pre> <p>Decorator for class methods to conditionally suppress warnings.</p> <p>The decorated method will execute with warnings suppressed for the specified   categories. If the instance has the attribute named by <code>flag</code>, and it's a boolean   evaluating to <code>False</code>, warnings will be ignored. If the attribute is a string, then   it is interpreted as an \"action\" to be performed on the categories specified.   Allowed values are as per warnings.simplefilter, which are: <code>default</code>, <code>error</code>, <code>ignore</code>, <code>always</code>, <code>all</code>, <code>module</code>, <code>once</code></p> <p>??? Example \"Suppress all warnings\"       <pre><code>class A:\n    @suppress_warnings\n    def method(self, ...):\n        ...\n</code></pre>   ??? Example \"Suppress only <code>UserWarning</code>\"       <pre><code>class A:\n    @suppress_warnings(categories=(UserWarning,))\n    def method(self, ...):\n        ...\n</code></pre>   ??? Example \"Configuring behaviour at runtime\"       <pre><code>class A:\n    def __init__(self, warn_enabled: bool):\n        self.warn_enabled = warn_enabled\n\n    @suppress_warnings(flag=\"warn_enabled\")\n    def method(self, ...):\n        ...\n</code></pre></p> <p>??? Example \"Raising on RuntimeWarning\"       <pre><code>class A:\n    def __init__(self, warnings: str = \"error\"):\n        self.warnings = warnings\n\n    @suppress_warnings(flag=\"warnings\")\n    def method(self, ...):\n        ...\n\nA().method()  # Raises RuntimeWarning\n</code></pre></p> <p>Args:       fun: Optional callable to decorate. If provided, the decorator is applied inline.       categories: Sequence of warning categories to suppress.       flag: Name of an instance attribute to check for enabling warnings. If the             attribute exists and evaluates to <code>False</code>, warnings will be ignored. If             it evaluates to a str, then this action will be performed on the categories             specified. Allowed values are as per warnings.simplefilter, which are:             <code>default</code>, <code>error</code>, <code>ignore</code>, <code>always</code>, <code>all</code>, <code>module</code>, <code>once</code></p> <p>Returns:       Either a decorator (if no function is provided) or the decorated callable.</p> Source code in <code>src/pydvl/utils/functional.py</code> <pre><code>def suppress_warnings(\n    fun: Callable[P, R] | None = None,\n    *,\n    categories: Sequence[Type[Warning]] = (Warning,),\n    flag: str = \"\",\n) -&gt; Union[Callable[[Callable[P, R]], Callable[P, R]], Callable[P, R]]:\n    \"\"\"Decorator for class methods to conditionally suppress warnings.\n\n      The decorated method will execute with warnings suppressed for the specified\n      categories. If the instance has the attribute named by `flag`, and it's a boolean\n      evaluating to `False`, warnings will be ignored. If the attribute is a string, then\n      it is interpreted as an \"action\" to be performed on the categories specified.\n      Allowed values are as per [warnings.simplefilter][], which are:\n    `default`, `error`, `ignore`, `always`, `all`, `module`, `once`\n\n      ??? Example \"Suppress all warnings\"\n          ```python\n          class A:\n              @suppress_warnings\n              def method(self, ...):\n                  ...\n          ```\n      ??? Example \"Suppress only `UserWarning`\"\n          ```python\n          class A:\n              @suppress_warnings(categories=(UserWarning,))\n              def method(self, ...):\n                  ...\n          ```\n      ??? Example \"Configuring behaviour at runtime\"\n          ```python\n          class A:\n              def __init__(self, warn_enabled: bool):\n                  self.warn_enabled = warn_enabled\n\n              @suppress_warnings(flag=\"warn_enabled\")\n              def method(self, ...):\n                  ...\n          ```\n\n      ??? Example \"Raising on RuntimeWarning\"\n          ```python\n          class A:\n              def __init__(self, warnings: str = \"error\"):\n                  self.warnings = warnings\n\n              @suppress_warnings(flag=\"warnings\")\n              def method(self, ...):\n                  ...\n\n          A().method()  # Raises RuntimeWarning\n          ```\n\n\n      Args:\n          fun: Optional callable to decorate. If provided, the decorator is applied inline.\n          categories: Sequence of warning categories to suppress.\n          flag: Name of an instance attribute to check for enabling warnings. If the\n                attribute exists and evaluates to `False`, warnings will be ignored. If\n                it evaluates to a str, then this action will be performed on the categories\n                specified. Allowed values are as per [warnings.simplefilter][], which are:\n                `default`, `error`, `ignore`, `always`, `all`, `module`, `once`\n\n      Returns:\n          Either a decorator (if no function is provided) or the decorated callable.\n    \"\"\"\n\n    def decorator(fn: Callable[P, R]) -&gt; Callable[P, R]:\n        # Use a simple heuristic: if the first parameter is \"self\", assume it's a method.\n        sig = inspect.signature(fn)\n        params = list(sig.parameters)\n        if not params or params[0] != \"self\":\n            if flag:\n                raise ValueError(\"Cannot use suppress_warnings flag with non-methods\")\n\n            @functools.wraps(fn)\n            def suppress_warnings_wrapper(*args: Any, **kwargs: Any) -&gt; R:\n                with warnings.catch_warnings():\n                    for category in categories:\n                        warnings.simplefilter(\"ignore\", category=category)\n                    return fn(*args, **kwargs)\n\n            return cast(Callable[P, R], suppress_warnings_wrapper)\n        else:\n\n            @functools.wraps(fn)\n            def suppress_warnings_wrapper(self, *args: Any, **kwargs: Any) -&gt; R:\n                if flag and not hasattr(self, flag):\n                    raise AttributeError(\n                        f\"Instance has no attribute '{flag}' for suppress_warnings\"\n                    )\n                if flag and getattr(self, flag, False) is True:\n                    return fn(self, *args, **kwargs)\n                # flag is either False or a string\n                with warnings.catch_warnings():\n                    if (action := getattr(self, flag, \"ignore\")) is False:\n                        action = \"ignore\"\n                    elif not isinstance(action, str):\n                        raise TypeError(\n                            f\"Expected a boolean or string for flag '{flag}', got {type(action).__name__}\"\n                        )\n                    for category in categories:\n                        warnings.simplefilter(action, category=category)  # type: ignore\n                    return fn(self, *args, **kwargs)\n\n            return cast(Callable[P, R], suppress_warnings_wrapper)\n\n    if fun is None:\n        return decorator\n    return decorator(fun)\n</code></pre>"},{"location":"api/pydvl/utils/functional/#pydvl.utils.functional.timed","title":"timed","text":"<pre><code>timed(fun: Callable[P, R]) -&gt; TimedCallable[P, R]\n</code></pre><pre><code>timed(\n    fun: None = None, *, accumulate: bool = False, logger: Logger | None = None\n) -&gt; Callable[[Callable[P, R]], TimedCallable[P, R]]\n</code></pre><pre><code>timed(\n    fun: Callable[P, R],\n    *,\n    accumulate: bool = False,\n    logger: Logger | None = None\n) -&gt; TimedCallable[P, R]\n</code></pre> <pre><code>timed(\n    fun: Callable[P, R] | None = None,\n    *,\n    accumulate: bool = False,\n    logger: Logger | None = None\n) -&gt; Union[\n    Callable[[Callable[P, R]], TimedCallable[P, R]], TimedCallable[P, R]\n]\n</code></pre> <p>A decorator that measures the execution time of the wrapped function. Optionally logs the time taken.</p> Decorator usage <pre><code>@timed\ndef fun(...):\n    ...\n\n@timed(accumulate=True, logger=getLogger(__name__))\ndef fun(...):\n    ...\n</code></pre> Inline usage <pre><code>timed_fun = timed(fun)\nfun(...)\nprint(timed_fun.execution_time)\n\naccum_time = timed(fun, accumulate=True)\nfun(...)\nfun(...)\nprint(accum_time.execution_time)\n</code></pre> PARAMETER DESCRIPTION <code>fun</code> <p> TYPE: <code>Callable[P, R] | None</code> DEFAULT: <code>None</code> </p> <code>accumulate</code> <p>If <code>True</code>, the total execution time will be accumulated across all calls.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>logger</code> <p>If provided, the execution time will be logged at the logger's level.</p> <p> TYPE: <code>Logger | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[Callable[[Callable[P, R]], TimedCallable[P, R]], TimedCallable[P, R]]</code> <p>A decorator that wraps a function, measuring and optionally logging its</p> <code>Union[Callable[[Callable[P, R]], TimedCallable[P, R]], TimedCallable[P, R]]</code> <p>execution time. The function will have an attribute <code>execution_time</code> where</p> <code>Union[Callable[[Callable[P, R]], TimedCallable[P, R]], TimedCallable[P, R]]</code> <p>either the time of the last execution or the accumulated total is stored.</p> Source code in <code>src/pydvl/utils/functional.py</code> <pre><code>def timed(\n    fun: Callable[P, R] | None = None,\n    *,\n    accumulate: bool = False,\n    logger: Logger | None = None,\n) -&gt; Union[Callable[[Callable[P, R]], TimedCallable[P, R]], TimedCallable[P, R]]:\n    \"\"\"A decorator that measures the execution time of the wrapped function.\n    Optionally logs the time taken.\n\n    ??? Example \"Decorator usage\"\n        ```python\n        @timed\n        def fun(...):\n            ...\n\n        @timed(accumulate=True, logger=getLogger(__name__))\n        def fun(...):\n            ...\n        ```\n\n    ??? Example \"Inline usage\"\n        ```python\n        timed_fun = timed(fun)\n        fun(...)\n        print(timed_fun.execution_time)\n\n        accum_time = timed(fun, accumulate=True)\n        fun(...)\n        fun(...)\n        print(accum_time.execution_time)\n        ```\n\n    Args:\n        fun:\n        accumulate: If `True`, the total execution time will be accumulated across all\n            calls.\n        logger: If provided, the execution time will be logged at the logger's level.\n\n    Returns:\n        A decorator that wraps a function, measuring and optionally logging its\n        execution time. The function will have an attribute `execution_time` where\n        either the time of the last execution or the accumulated total is stored.\n    \"\"\"\n\n    if fun is None:\n\n        def decorator(func: Callable[P, R]) -&gt; TimedCallable[P, R]:\n            return timed(func, accumulate=accumulate, logger=logger)\n\n        return decorator\n\n    assert fun is not None\n\n    @functools.wraps(fun)\n    def timed_wrapper(*args, **kwargs) -&gt; R:\n        start = time.perf_counter()\n        try:\n            assert fun is not None\n            result = fun(*args, **kwargs)\n        finally:\n            elapsed = time.perf_counter() - start\n            if accumulate:\n                cast(TimedCallable, timed_wrapper).execution_time += elapsed\n            else:\n                cast(TimedCallable, timed_wrapper).execution_time = elapsed\n            if logger is not None:\n                assert fun is not None\n                logger.log(\n                    logger.level,\n                    f\"{fun.__module__}.{fun.__qualname__} took {elapsed:.5f} seconds\",\n                )\n        return result\n\n    cast(TimedCallable, timed_wrapper).execution_time = 0.0\n\n    return cast(TimedCallable[P, R], timed_wrapper)\n</code></pre>"},{"location":"api/pydvl/utils/monitor/","title":"Monitor","text":""},{"location":"api/pydvl/utils/monitor/#pydvl.utils.monitor","title":"pydvl.utils.monitor","text":"<p>This module implements a simple memory monitoring utility for the whole application.</p> <p>With start_memory_monitoring() one can monitor global memory usage, including the memory of child processes. The monitoring runs in a separate thread and keeps track of the maximum* memory usage observed.</p> <p>Monitoring stops automatically when the process exits or receives common termination signals (SIGINT, SIGTERM, SIGHUP). It can also be stopped manually by calling end_memory_monitoring().</p> <p>When monitoring stops, the maximum memory usage is both logged and returned (in bytes).</p> <p>Note</p> <p>This is intended to report peak memory usage for the whole application, including child processes. It is not intended to be used for profiling memory usage of individual functions or modules. Given that there exist numerous profiling tools, it probably doesn't make sense to extend this module further.</p>"},{"location":"api/pydvl/utils/monitor/#pydvl.utils.monitor._memory_monitor_thread","title":"_memory_monitor_thread","text":"<pre><code>_memory_monitor_thread() -&gt; Thread | None\n</code></pre> <p>Returns the memory monitor thread. Can be None if the monitor was never started. This is only useful for testing purposes.</p> Source code in <code>src/pydvl/utils/monitor.py</code> <pre><code>def _memory_monitor_thread() -&gt; threading.Thread | None:\n    \"\"\"Returns the memory monitor thread. Can be None if the monitor was never started.\n    This is only useful for testing purposes.\"\"\"\n    return __memory_monitor_thread\n</code></pre>"},{"location":"api/pydvl/utils/monitor/#pydvl.utils.monitor.end_memory_monitoring","title":"end_memory_monitoring","text":"<pre><code>end_memory_monitoring(log_level=DEBUG) -&gt; tuple[int, dict[int, int]]\n</code></pre> <p>Ends the memory monitoring thread and logs the maximum memory usage.</p> PARAMETER DESCRIPTION <code>log_level</code> <p>The logging level to use.</p> <p> DEFAULT: <code>DEBUG</code> </p> RETURNS DESCRIPTION <code>tuple[int, dict[int, int]]</code> <p>A tuple with the maximum memory usage observed globally, and for each pid separately as a dict. The dict will be empty if monitoring is disabled.</p> Source code in <code>src/pydvl/utils/monitor.py</code> <pre><code>def end_memory_monitoring(log_level=logging.DEBUG) -&gt; tuple[int, dict[int, int]]:\n    \"\"\"Ends the memory monitoring thread and logs the maximum memory usage.\n\n    Args:\n        log_level: The logging level to use.\n\n    Returns:\n        A tuple with the maximum memory usage observed globally, and for each pid\n            separately as a dict. The dict will be empty if monitoring is disabled.\n    \"\"\"\n    global __memory_usage\n    global __peak_memory_usage\n\n    if not __monitoring_enabled.is_set():\n        return 0, {}\n\n    __monitoring_enabled.clear()\n    assert __memory_monitor_thread is not None\n    __memory_monitor_thread.join()\n\n    with __state_lock:\n        peak_mem = __peak_memory_usage\n        mem_usage = __memory_usage.copy()\n        __memory_usage.clear()\n        __peak_memory_usage = 0\n\n    log_memory_usage_report(peak_mem, mem_usage, log_level)\n    return peak_mem, mem_usage\n</code></pre>"},{"location":"api/pydvl/utils/monitor/#pydvl.utils.monitor.log_memory_usage_report","title":"log_memory_usage_report","text":"<pre><code>log_memory_usage_report(\n    peak_mem: int, mem_usage: dict[int, int], log_level=DEBUG\n)\n</code></pre> <p>Generates a nicely tabulated memory usage report and logs it.</p> PARAMETER DESCRIPTION <code>peak_mem</code> <p>The maximum memory usage observed during the monitoring period.</p> <p> TYPE: <code>int</code> </p> <code>mem_usage</code> <p>A dictionary mapping process IDs (pid) to memory usage in bytes.</p> <p> TYPE: <code>dict[int, int]</code> </p> <code>log_level</code> <p>The log level used for logging the report.</p> <p> DEFAULT: <code>DEBUG</code> </p> Source code in <code>src/pydvl/utils/monitor.py</code> <pre><code>def log_memory_usage_report(\n    peak_mem: int, mem_usage: dict[int, int], log_level=logging.DEBUG\n):\n    \"\"\"\n    Generates a nicely tabulated memory usage report and logs it.\n\n    Args:\n        peak_mem: The maximum memory usage observed during the monitoring period.\n        mem_usage: A dictionary mapping process IDs (pid) to memory usage in bytes.\n        log_level: The log level used for logging the report.\n    \"\"\"\n    if not mem_usage:\n        logger.log(log_level, \"No memory usage data available.\")\n        return\n\n    headers = (\"PID\", \"Memory (Bytes)\", \"Memory (MB)\")\n    col_widths = (10, 20, 15)\n\n    header_line = (\n        f\"{headers[0]:&gt;{col_widths[0]}} \"\n        f\"{headers[1]:&gt;{col_widths[1]}} \"\n        f\"{headers[2]:&gt;{col_widths[2]}}\"\n    )\n    separator = \"-\" * (sum(col_widths) + 2)\n\n    summary = (\n        f\"Memory monitor: {len(mem_usage)} processes monitored. \"\n        f\"Peak memory usage: {peak_mem / (2**20):.2f} MB\"\n    )\n\n    lines = [header_line, separator, summary]\n\n    for pid, bytes_used in sorted(\n        mem_usage.items(), key=lambda item: item[1], reverse=True\n    ):\n        mb_used = bytes_used / (1024 * 1024)\n        line = (\n            f\"{pid:&gt;{col_widths[0]}} \"\n            f\"{bytes_used:&gt;{col_widths[1]},} \"\n            f\"{mb_used:&gt;{col_widths[2]}.2f}\"\n        )\n        lines.append(line)\n\n    lines.append(separator)\n\n    logger.log(log_level, \"\\n\".join(lines))\n</code></pre>"},{"location":"api/pydvl/utils/monitor/#pydvl.utils.monitor.memory_monitor_run","title":"memory_monitor_run","text":"<pre><code>memory_monitor_run(pid: int, interval: float = 0.1)\n</code></pre> <p>Monitors the memory usage of the process and its children.</p> <p>This function runs in a separate thread and updates the global variable <code>__max_memory_usage</code> with the maximum memory usage observed during the monitoring period.</p> <p>The monitoring stops when the __monitoring_enabled event is cleared, which can be achieved either by calling end_memory_monitoring(), or when the process is terminated or exits.</p> Source code in <code>src/pydvl/utils/monitor.py</code> <pre><code>def memory_monitor_run(pid: int, interval: float = 0.1):\n    \"\"\"Monitors the memory usage of the process and its children.\n\n    This function runs in a separate thread and updates the global variable\n    `__max_memory_usage` with the maximum memory usage observed during the monitoring\n    period.\n\n    The monitoring stops when the __monitoring_enabled event is cleared, which can be\n    achieved either by calling\n    [end_memory_monitoring()][pydvl.utils.monitor.end_memory_monitoring], or when the\n    process is terminated or exits.\n    \"\"\"\n    global __memory_usage\n    global __peak_memory_usage\n\n    try:\n        proc = psutil.Process(pid)\n    except psutil.NoSuchProcess:\n        logger.error(f\"Process {pid} not found. Monitoring cannot start.\")\n        return\n\n    while __monitoring_enabled.is_set():\n        total_mem = 0\n        try:\n            for p in chain([proc], proc.children(recursive=True)):\n                try:\n                    pid = p.pid\n                    rss = p.memory_info().rss\n                    total_mem += rss\n                    with __state_lock:\n                        __memory_usage[pid] = max(__memory_usage[pid], rss)\n                except psutil.NoSuchProcess:\n                    continue\n        except psutil.NoSuchProcess:  # Catch invalid proc / proc.children\n            break\n\n        with __state_lock:\n            __peak_memory_usage = max(__peak_memory_usage, total_mem)\n\n        time.sleep(interval)\n</code></pre>"},{"location":"api/pydvl/utils/monitor/#pydvl.utils.monitor.start_memory_monitoring","title":"start_memory_monitoring","text":"<pre><code>start_memory_monitoring(auto_stop: bool = True)\n</code></pre> <p>Starts a memory monitoring thread.</p> <p>The monitor runs in a separate thread and keeps track of maximum memory usage observed during the monitoring period.</p> <p>The monitoring stops by calling end_memory_monitoring() or, if <code>auto_stop</code> is <code>True</code> when the process is terminated or exits.</p> PARAMETER DESCRIPTION <code>auto_stop</code> <p>If True, the monitoring will stop when the process exits normally or receives common termination signals (SIGINT, SIGTERM, SIGHUP).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/utils/monitor.py</code> <pre><code>def start_memory_monitoring(auto_stop: bool = True):\n    \"\"\"Starts a memory monitoring thread.\n\n    The monitor runs in a separate thread and keeps track of maximum memory usage\n    observed during the monitoring period.\n\n    The monitoring stops by calling\n    [end_memory_monitoring()][pydvl.utils.monitor.end_memory_monitoring] or, if\n    `auto_stop` is `True` when the process is terminated or exits.\n\n    Args:\n        auto_stop: If True, the monitoring will stop when the process exits\n            normally or receives common termination signals (SIGINT, SIGTERM, SIGHUP).\n\n    \"\"\"\n    global __memory_usage\n    global __memory_monitor_thread\n    global __peak_memory_usage\n\n    if __monitoring_enabled.is_set():\n        logger.warning(\"Memory monitoring is already running.\")\n        return\n\n    with __state_lock:\n        __memory_usage.clear()\n        __peak_memory_usage = 0\n\n    __monitoring_enabled.set()\n    __memory_monitor_thread = threading.Thread(\n        target=memory_monitor_run, args=(psutil.Process().pid,)\n    )\n    __memory_monitor_thread.start()\n\n    if not auto_stop:\n        return\n\n    atexit.register(end_memory_monitoring)\n\n    # Register signal handlers for common termination signals, re-raising the original\n    # signal to terminate as expected\n\n    def signal_handler(signum, frame):\n        end_memory_monitoring()\n        signal.signal(signum, signal.SIG_DFL)\n        signal.raise_signal(signum)\n\n    signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C\n    signal.signal(signal.SIGTERM, signal_handler)  # Termination request\n    # SIGHUP might not be available on all platforms (e.g., Windows)\n    if hasattr(signal, \"SIGHUP\"):\n        signal.signal(signal.SIGHUP, signal_handler)  # Terminal closed\n</code></pre>"},{"location":"api/pydvl/utils/numeric/","title":"Numeric","text":""},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric","title":"pydvl.utils.numeric","text":"<p>This module contains routines for numerical computations used across the library.</p>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.complement","title":"complement","text":"<pre><code>complement(\n    include: NDArray[T], exclude: NDArray[T] | Sequence[T | None]\n) -&gt; NDArray[T]\n</code></pre> <p>Returns the complement of the set of indices excluding the given indices.</p> PARAMETER DESCRIPTION <code>include</code> <p>The set of indices to consider.</p> <p> TYPE: <code>NDArray[T]</code> </p> <code>exclude</code> <p>The indices to exclude from the complement. These must be a subset of <code>include</code>. If an index is <code>None</code> it is ignored.</p> <p> TYPE: <code>NDArray[T] | Sequence[T | None]</code> </p> RETURNS DESCRIPTION <code>NDArray[T]</code> <p>The complement of the set of indices excluding the given indices.</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def complement(\n    include: NDArray[T], exclude: NDArray[T] | Sequence[T | None]\n) -&gt; NDArray[T]:\n    \"\"\"Returns the complement of the set of indices excluding the given\n    indices.\n\n    Args:\n        include: The set of indices to consider.\n        exclude: The indices to exclude from the complement. These must be a subset\n            of `include`. If an index is `None` it is ignored.\n\n    Returns:\n        The complement of the set of indices excluding the given indices.\n    \"\"\"\n    _exclude = np.array([i for i in exclude if i is not None], dtype=include.dtype)\n    return np.setdiff1d(include, _exclude).astype(np.int_)\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.log_running_moments","title":"log_running_moments","text":"<pre><code>log_running_moments(\n    previous_log_sum_pos: float,\n    previous_log_sum_neg: float,\n    previous_log_sum2: float,\n    count: int,\n    new_log_value: float,\n    new_sign: int,\n    unbiased: bool = True,\n) -&gt; tuple[float, float, float, float, float]\n</code></pre> <p>Update running moments when the new value is provided in log space, allowing for negative values via an explicit sign.</p> <p>Here the actual value is x = new_sign * exp(new_log_value). Rather than updating the arithmetic sum S = sum(x) and S2 = sum(x^2) directly, we maintain:</p> <p>L_S+ = log(sum_{i: x_i &gt;= 0} x_i)    L_S- = log(sum_{i: x_i &lt; 0} |x_i|)    L_S2 = log(sum_i x_i^2)</p> <p>The running mean is then computed as:</p> <pre><code> mean = exp(L_S+) - exp(L_S-)\n</code></pre> <p>and the second moment is:</p> <pre><code> second_moment = exp(L_S2 - log(count))\n</code></pre> <p>so that the variance is:</p> <pre><code> variance = second_moment - mean^2\n</code></pre> <p>For the unbiased (sample) estimator, we scale the variance by count/(count-1) when count &gt; 1 (and define variance = 0 when count == 1).</p> PARAMETER DESCRIPTION <code>previous_log_sum_pos</code> <p>running log(sum of positive contributions), or -inf if none.</p> <p> TYPE: <code>float</code> </p> <code>previous_log_sum_neg</code> <p>running log(sum of negative contributions in absolute value), or -inf if none.</p> <p> TYPE: <code>float</code> </p> <code>previous_log_sum2</code> <p>running log(sum of squares) so far (or -inf if none).</p> <p> TYPE: <code>float</code> </p> <code>count</code> <p>number of points processed so far.</p> <p> TYPE: <code>int</code> </p> <code>new_log_value</code> <p>log(|x_new|), where x_new is the new value.</p> <p> TYPE: <code>float</code> </p> <code>new_sign</code> <p>sign of the new value (should be +1, 0, or -1).</p> <p> TYPE: <code>int</code> </p> <code>unbiased</code> <p>if True, compute the unbiased estimator of the variance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>new_mean</code> <p>running mean in the linear domain.</p> <p> TYPE: <code>float</code> </p> <code>new_variance</code> <p>running variance in the linear domain.</p> <p> TYPE: <code>float</code> </p> <code>new_log_sum_pos</code> <p>updated running log(sum of positive contributions).</p> <p> TYPE: <code>float</code> </p> <code>new_log_sum_neg</code> <p>updated running log(sum of negative contributions).</p> <p> TYPE: <code>float</code> </p> <code>new_log_sum2</code> <p>updated running log(sum of squares).</p> <p> TYPE: <code>float</code> </p> <code>new_count</code> <p>updated count.</p> <p> TYPE: <code>tuple[float, float, float, float, float]</code> </p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def log_running_moments(\n    previous_log_sum_pos: float,\n    previous_log_sum_neg: float,\n    previous_log_sum2: float,\n    count: int,\n    new_log_value: float,\n    new_sign: int,\n    unbiased: bool = True,\n) -&gt; tuple[float, float, float, float, float]:\n    \"\"\"\n    Update running moments when the new value is provided in log space,\n    allowing for negative values via an explicit sign.\n\n    Here the actual value is x = new_sign * exp(new_log_value). Rather than\n    updating the arithmetic sum S = sum(x) and S2 = sum(x^2) directly, we maintain:\n\n       L_S+ = log(sum_{i: x_i &gt;= 0} x_i)\n       L_S- = log(sum_{i: x_i &lt; 0} |x_i|)\n       L_S2 = log(sum_i x_i^2)\n\n    The running mean is then computed as:\n\n         mean = exp(L_S+) - exp(L_S-)\n\n    and the second moment is:\n\n         second_moment = exp(L_S2 - log(count))\n\n    so that the variance is:\n\n         variance = second_moment - mean^2\n\n    For the unbiased (sample) estimator, we scale the variance by count/(count-1)\n    when count &gt; 1 (and define variance = 0 when count == 1).\n\n    Args:\n        previous_log_sum_pos: running log(sum of positive contributions), or -inf if none.\n        previous_log_sum_neg: running log(sum of negative contributions in absolute\n            value), or -inf if none.\n        previous_log_sum2: running log(sum of squares) so far (or -inf if none).\n        count: number of points processed so far.\n        new_log_value: log(|x_new|), where x_new is the new value.\n        new_sign: sign of the new value (should be +1, 0, or -1).\n        unbiased: if True, compute the unbiased estimator of the variance.\n\n    Returns:\n        new_mean: running mean in the linear domain.\n        new_variance: running variance in the linear domain.\n        new_log_sum_pos: updated running log(sum of positive contributions).\n        new_log_sum_neg: updated running log(sum of negative contributions).\n        new_log_sum2: updated running log(sum of squares).\n        new_count: updated count.\n    \"\"\"\n\n    if count == 0:\n        if new_sign &gt;= 0:\n            new_log_sum_pos = new_log_value\n            new_log_sum_neg = -np.inf  # No negative contribution yet.\n        else:\n            new_log_sum_pos = -np.inf\n            new_log_sum_neg = new_log_value\n        new_log_sum2 = 2 * new_log_value\n    else:\n        if new_sign &gt;= 0:\n            new_log_sum_pos = logsumexp_two(previous_log_sum_pos, new_log_value)\n            new_log_sum_neg = previous_log_sum_neg\n        else:\n            new_log_sum_neg = logsumexp_two(previous_log_sum_neg, new_log_value)\n            new_log_sum_pos = previous_log_sum_pos\n        new_log_sum2 = logsumexp_two(previous_log_sum2, 2 * new_log_value)\n    new_count = count + 1\n\n    # Compute 1st and 2nd moments in the linear domain.\n    pos_sum = np.exp(new_log_sum_pos) if new_log_sum_pos != -np.inf else 0.0\n    neg_sum = np.exp(new_log_sum_neg) if new_log_sum_neg != -np.inf else 0.0\n    new_mean = (pos_sum - neg_sum) / new_count\n\n    second_moment = np.exp(new_log_sum2 - np.log(new_count))\n\n    # Compute variance using either the population or unbiased estimator.\n    if unbiased:\n        if new_count &gt; 1:\n            new_variance = new_count / (new_count - 1) * (second_moment - new_mean**2)\n        else:\n            new_variance = 0.0\n    else:\n        new_variance = second_moment - new_mean**2\n\n    return new_mean, new_variance, new_log_sum_pos, new_log_sum_neg, new_log_sum2\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.logcomb","title":"logcomb","text":"<pre><code>logcomb(n: int, k: int) -&gt; float\n</code></pre> <p>Computes the log of the binomial coefficient (n choose k).</p> \\[ \\begin{array}{rcl}     \\log\\binom{n}{k} &amp; = &amp; \\log(n!) - \\log(k!) - \\log((n-k)!) \\\\                      &amp; = &amp; \\log\\Gamma(n+1) - \\log\\Gamma(k+1) - \\log\\Gamma(n-k+1). \\end{array} \\] PARAMETER DESCRIPTION <code>n</code> <p>Total number of elements</p> <p> TYPE: <code>int</code> </p> <code>k</code> <p>Number of elements to choose</p> <p> TYPE: <code>int</code> </p> <p>Returns:     The log of the binomial coefficient</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def logcomb(n: int, k: int) -&gt; float:\n    r\"\"\"Computes the log of the binomial coefficient (n choose k).\n\n    $$\n    \\begin{array}{rcl}\n        \\log\\binom{n}{k} &amp; = &amp; \\log(n!) - \\log(k!) - \\log((n-k)!) \\\\\n                         &amp; = &amp; \\log\\Gamma(n+1) - \\log\\Gamma(k+1) - \\log\\Gamma(n-k+1).\n    \\end{array}\n    $$\n\n    Args:\n        n: Total number of elements\n        k: Number of elements to choose\n    Returns:\n        The log of the binomial coefficient\n        \"\"\"\n    if k &lt; 0 or k &gt; n or n &lt; 0:\n        raise ValueError(f\"Invalid arguments: n={n}, k={k}\")\n    return float(gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1))\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.logexp","title":"logexp","text":"<pre><code>logexp(x: float, a: float) -&gt; float\n</code></pre> <p>Computes log(x^a).</p> PARAMETER DESCRIPTION <code>x</code> <p>Base</p> <p> TYPE: <code>float</code> </p> <code>a</code> <p>Exponent</p> <p> TYPE: <code>float</code> </p> <p>Returns     a * log(x)</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def logexp(x: float, a: float) -&gt; float:\n    \"\"\"Computes log(x^a).\n\n    Args:\n        x: Base\n        a: Exponent\n    Returns\n        a * log(x)\n    \"\"\"\n    return float(a * np.log(x))\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.logsumexp_two","title":"logsumexp_two","text":"<pre><code>logsumexp_two(log_a: float, log_b: float) -&gt; float\n</code></pre> <p>Numerically stable computation of log(exp(log_a) + exp(log_b)).</p> <p>Uses standard log sum exp trick:</p> \\[ \\log(\\exp(\\log a) + \\exp(\\log b)) = m + \\log(\\exp(\\log a - m) + \\exp(\\log b - m)), \\] <p>where \\(m = \\max(\\log a, \\log b)\\).</p> PARAMETER DESCRIPTION <code>log_a</code> <p>Log of the first value</p> <p> TYPE: <code>float</code> </p> <code>log_b</code> <p>Log of the second value</p> <p> TYPE: <code>float</code> </p> <p>Returns:     The log of the sum of the exponentials</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def logsumexp_two(log_a: float, log_b: float) -&gt; float:\n    r\"\"\"Numerically stable computation of log(exp(log_a) + exp(log_b)).\n\n    Uses standard log sum exp trick:\n\n    $$\n    \\log(\\exp(\\log a) + \\exp(\\log b)) = m + \\log(\\exp(\\log a - m) + \\exp(\\log b - m)),\n    $$\n\n    where $m = \\max(\\log a, \\log b)$.\n\n    Args:\n        log_a: Log of the first value\n        log_b: Log of the second value\n    Returns:\n        The log of the sum of the exponentials\n    \"\"\"\n    assert log_a &lt; np.inf and log_b &lt; np.inf, f\"log_a={log_a}, log_b={log_b}\"\n\n    if log_a == -np.inf:\n        return log_b\n    if log_b == -np.inf:\n        return log_a\n    m = max(log_a, log_b)\n    return float(m + np.log(np.exp(log_a - m) + np.exp(log_b - m)))\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.num_samples_permutation_hoeffding","title":"num_samples_permutation_hoeffding","text":"<pre><code>num_samples_permutation_hoeffding(\n    eps: float, delta: float, u_range: float\n) -&gt; int\n</code></pre> <p>Lower bound on the number of samples required for MonteCarlo Shapley to obtain an (\u03b5,\u03b4)-approximation.</p> <p>That is: with probability 1-\u03b4, the estimated value for one data point will be \u03b5-close to the true quantity, if at least this many permutations are sampled.</p> PARAMETER DESCRIPTION <code>eps</code> <p>\u03b5 &gt; 0</p> <p> TYPE: <code>float</code> </p> <code>delta</code> <p>0 &lt; \u03b4 &lt;= 1</p> <p> TYPE: <code>float</code> </p> <code>u_range</code> <p>Range of the Utility function</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>int</code> <p>Number of permutations required to guarantee \u03b5-correct Shapley values with probability 1-\u03b4</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def num_samples_permutation_hoeffding(eps: float, delta: float, u_range: float) -&gt; int:\n    \"\"\"Lower bound on the number of samples required for MonteCarlo Shapley to\n    obtain an (\u03b5,\u03b4)-approximation.\n\n    That is: with probability 1-\u03b4, the estimated value for one data point will\n    be \u03b5-close to the true quantity, if at least this many permutations are\n    sampled.\n\n    Args:\n        eps: \u03b5 &gt; 0\n        delta: 0 &lt; \u03b4 &lt;= 1\n        u_range: Range of the [Utility][pydvl.utils.utility.Utility] function\n\n    Returns:\n        Number of _permutations_ required to guarantee \u03b5-correct Shapley\n            values with probability 1-\u03b4\n    \"\"\"\n    return int(np.ceil(np.log(2 / delta) * 2 * u_range**2 / eps**2))\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.powerset","title":"powerset","text":"<pre><code>powerset(s: NDArray[T]) -&gt; Iterator[Collection[T]]\n</code></pre> <p>Returns an iterator for the power set of the argument.</p> <p>Subsets are generated in sequence by growing size. See  random_powerset() for random  sampling.</p> Example <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pydvl.utils.numeric import powerset\n&gt;&gt;&gt; list(powerset(np.array((1,2))))\n[(), (1,), (2,), (1, 2)]\n</code></pre> PARAMETER DESCRIPTION <code>s</code> <p>The set to use</p> <p> TYPE: <code>NDArray[T]</code> </p> RETURNS DESCRIPTION <code>Iterator[Collection[T]]</code> <p>An iterator over all subsets of the set of indices <code>s</code>.</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def powerset(s: NDArray[T]) -&gt; Iterator[Collection[T]]:\n    \"\"\"Returns an iterator for the power set of the argument.\n\n     Subsets are generated in sequence by growing size. See\n     [random_powerset()][pydvl.utils.numeric.random_powerset] for random\n     sampling.\n\n    ??? Example\n        ``` pycon\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from pydvl.utils.numeric import powerset\n        &gt;&gt;&gt; list(powerset(np.array((1,2))))\n        [(), (1,), (2,), (1, 2)]\n        ```\n\n    Args:\n         s: The set to use\n\n    Returns:\n        An iterator over all subsets of the set of indices `s`.\n    \"\"\"\n    return chain.from_iterable(combinations(s, r) for r in range(len(s) + 1))\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.random_matrix_with_condition_number","title":"random_matrix_with_condition_number","text":"<pre><code>random_matrix_with_condition_number(\n    n: int, condition_number: float, seed: Optional[Seed] = None\n) -&gt; NDArray\n</code></pre> <p>Constructs a square matrix with a given condition number.</p> <p>Taken from: https://gist.github.com/bstellato/23322fe5d87bb71da922fbc41d658079#file-random_mat_condition_number-py</p> <p>Also see: https://math.stackexchange.com/questions/1351616/condition-number-of-ata.</p> PARAMETER DESCRIPTION <code>n</code> <p>size of the matrix</p> <p> TYPE: <code>int</code> </p> <code>condition_number</code> <p>duh</p> <p> TYPE: <code>float</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>An (n,n) matrix with the requested condition number.</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def random_matrix_with_condition_number(\n    n: int, condition_number: float, seed: Optional[Seed] = None\n) -&gt; NDArray:\n    \"\"\"Constructs a square matrix with a given condition number.\n\n    Taken from:\n    [https://gist.github.com/bstellato/23322fe5d87bb71da922fbc41d658079#file-random_mat_condition_number-py](\n    https://gist.github.com/bstellato/23322fe5d87bb71da922fbc41d658079#file-random_mat_condition_number-py)\n\n    Also see:\n    [https://math.stackexchange.com/questions/1351616/condition-number-of-ata](\n    https://math.stackexchange.com/questions/1351616/condition-number-of-ata).\n\n    Args:\n        n: size of the matrix\n        condition_number: duh\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        An (n,n) matrix with the requested condition number.\n    \"\"\"\n    if n &lt; 2:\n        raise ValueError(\"Matrix size must be at least 2\")\n\n    if condition_number &lt;= 1:\n        raise ValueError(\"Condition number must be greater than 1\")\n\n    rng = np.random.default_rng(seed)\n    log_condition_number = np.log(condition_number)\n    exp_vec = np.arange(\n        -log_condition_number / 4.0,\n        log_condition_number * (n + 1) / (4 * (n - 1)),\n        log_condition_number / (2.0 * (n - 1)),\n    )\n    exp_vec = exp_vec[:n]\n    s: np.ndarray = np.exp(exp_vec)\n    S = np.diag(s)\n    U, _ = np.linalg.qr((rng.uniform(size=(n, n)) - 5.0) * 200)\n    V, _ = np.linalg.qr((rng.uniform(size=(n, n)) - 5.0) * 200)\n    P: np.ndarray = U.dot(S).dot(V.T)\n    P = P.dot(P.T)\n    return P\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.random_powerset","title":"random_powerset","text":"<pre><code>random_powerset(\n    s: NDArray[T],\n    n_samples: Optional[int] = None,\n    q: float = 0.5,\n    seed: Optional[Seed] = None,\n) -&gt; Generator[NDArray[T], None, None]\n</code></pre> <p>Samples subsets from the power set of the argument, without pre-generating all subsets and in no order.</p> <p>See powerset if you wish to deterministically generate all subsets.</p> <p>To generate subsets, <code>len(s)</code> Bernoulli draws with probability <code>q</code> are drawn. The default value of <code>q = 0.5</code> provides a uniform distribution over the power set of <code>s</code>. Other choices can be used e.g. to implement owen_sampling_shapley.</p> PARAMETER DESCRIPTION <code>s</code> <p>set to sample from</p> <p> TYPE: <code>NDArray[T]</code> </p> <code>n_samples</code> <p>if set, stop the generator after this many steps. Defaults to <code>np.iinfo(np.int32).max</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>q</code> <p>Sampling probability for elements. The default 0.5 yields a uniform distribution over the power set of s.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>None</code> <p>Samples from the power set of <code>s</code>.</p> RAISES DESCRIPTION <code>ValueError</code> <p>if the element sampling probability is not in [0,1]</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def random_powerset(\n    s: NDArray[T],\n    n_samples: Optional[int] = None,\n    q: float = 0.5,\n    seed: Optional[Seed] = None,\n) -&gt; Generator[NDArray[T], None, None]:\n    \"\"\"Samples subsets from the power set of the argument, without\n    pre-generating all subsets and in no order.\n\n    See [powerset][pydvl.utils.numeric.powerset] if you wish to deterministically generate all subsets.\n\n    To generate subsets, `len(s)` Bernoulli draws with probability `q` are\n    drawn. The default value of `q = 0.5` provides a uniform distribution over\n    the power set of `s`. Other choices can be used e.g. to implement\n    [owen_sampling_shapley][pydvl.value.shapley.owen.owen_sampling_shapley].\n\n    Args:\n        s: set to sample from\n        n_samples: if set, stop the generator after this many steps.\n            Defaults to `np.iinfo(np.int32).max`\n        q: Sampling probability for elements. The default 0.5 yields a\n            uniform distribution over the power set of s.\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        Samples from the power set of `s`.\n\n    Raises:\n        ValueError: if the element sampling probability is not in [0,1]\n\n    \"\"\"\n    if q &lt; 0 or q &gt; 1:\n        raise ValueError(\"Element sampling probability must be in [0,1]\")\n\n    rng = np.random.default_rng(seed)\n    total = 1\n    if n_samples is None:\n        n_samples = np.iinfo(np.int32).max\n    while total &lt;= n_samples:\n        yield random_subset(s, q, seed=rng)\n        total += 1\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.random_powerset_label_min","title":"random_powerset_label_min","text":"<pre><code>random_powerset_label_min(\n    s: NDArray[T],\n    labels: NDArray[int_],\n    min_elements_per_label: int = 1,\n    seed: Optional[Seed] = None,\n) -&gt; Generator[NDArray[T], None, None]\n</code></pre> <p>Draws random subsets from <code>s</code>, while ensuring that at least <code>min_elements_per_label</code> elements per label are included in the draw. It can be used for classification problems to ensure that a set contains information for all labels (or not if <code>min_elements_per_label=0</code>).</p> PARAMETER DESCRIPTION <code>s</code> <p>Set to sample from</p> <p> TYPE: <code>NDArray[T]</code> </p> <code>labels</code> <p>Labels for the samples</p> <p> TYPE: <code>NDArray[int_]</code> </p> <code>min_elements_per_label</code> <p>Minimum number of elements for each label.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>None</code> <p>Generated draw from the powerset of s with <code>min_elements_per_label</code> for each</p> <code>None</code> <p>label.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>s</code> and <code>labels</code> are of different length or <code>min_elements_per_label</code> is smaller than 0.</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def random_powerset_label_min(\n    s: NDArray[T],\n    labels: NDArray[np.int_],\n    min_elements_per_label: int = 1,\n    seed: Optional[Seed] = None,\n) -&gt; Generator[NDArray[T], None, None]:\n    \"\"\"Draws random subsets from `s`, while ensuring that at least\n    `min_elements_per_label` elements per label are included in the draw. It can be used\n    for classification problems to ensure that a set contains information for all labels\n    (or not if `min_elements_per_label=0`).\n\n    Args:\n        s: Set to sample from\n        labels: Labels for the samples\n        min_elements_per_label: Minimum number of elements for each label.\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        Generated draw from the powerset of s with `min_elements_per_label` for each\n        label.\n\n    Raises:\n        ValueError: If `s` and `labels` are of different length or\n            `min_elements_per_label` is smaller than 0.\n    \"\"\"\n    if len(labels) != len(s):\n        raise ValueError(\"Set and labels have to be of same size.\")\n\n    if min_elements_per_label &lt; 0:\n        raise ValueError(\n            f\"Parameter min_elements={min_elements_per_label} needs to be bigger or \"\n            f\"equal to 0.\"\n        )\n\n    rng = np.random.default_rng(seed)\n    unique_labels = np.unique(labels)\n\n    while True:\n        subsets: list[NDArray[T]] = []\n        for label in unique_labels:\n            label_indices = np.asarray(np.where(labels == label)[0])\n            subset_size = int(\n                rng.integers(\n                    min(min_elements_per_label, len(label_indices)),\n                    len(label_indices) + 1,\n                )\n            )\n            if subset_size &gt; 0:\n                subsets.append(\n                    random_subset_of_size(s[label_indices], subset_size, seed=rng)\n                )\n\n        if len(subsets) &gt; 0:\n            subset = np.concatenate(tuple(subsets))\n            rng.shuffle(subset)\n            yield subset\n        else:\n            yield np.array([], dtype=s.dtype)\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.random_subset","title":"random_subset","text":"<pre><code>random_subset(\n    s: NDArray[T], q: float = 0.5, seed: Optional[Seed] = None\n) -&gt; NDArray[T]\n</code></pre> <p>Returns one subset at random from <code>s</code>.</p> PARAMETER DESCRIPTION <code>s</code> <p>set to sample from</p> <p> TYPE: <code>NDArray[T]</code> </p> <code>q</code> <p>Sampling probability for elements. The default 0.5 yields a uniform distribution over the power set of s.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NDArray[T]</code> <p>The subset</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def random_subset(\n    s: NDArray[T], q: float = 0.5, seed: Optional[Seed] = None\n) -&gt; NDArray[T]:\n    \"\"\"Returns one subset at random from ``s``.\n\n    Args:\n        s: set to sample from\n        q: Sampling probability for elements. The default 0.5 yields a\n            uniform distribution over the power set of s.\n        seed: Either an instance of a numpy random number generator or a seed\n            for it.\n\n    Returns:\n        The subset\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    selection = rng.uniform(size=len(s)) &lt; q\n    return s[selection]\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.random_subset_of_size","title":"random_subset_of_size","text":"<pre><code>random_subset_of_size(\n    s: NDArray[T], size: int, seed: Optional[Seed] = None\n) -&gt; NDArray[T]\n</code></pre> <p>Samples a random subset of given size uniformly from the powerset of <code>s</code>.</p> PARAMETER DESCRIPTION <code>s</code> <p>Set to sample from</p> <p> TYPE: <code>NDArray[T]</code> </p> <code>size</code> <p>Size of the subset to generate</p> <p> TYPE: <code>int</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NDArray[T]</code> <p>The subset</p> <p>Raises     ValueError: If size &gt; len(s)</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def random_subset_of_size(\n    s: NDArray[T], size: int, seed: Optional[Seed] = None\n) -&gt; NDArray[T]:\n    \"\"\"Samples a random subset of given size uniformly from the powerset\n    of `s`.\n\n    Args:\n        s: Set to sample from\n        size: Size of the subset to generate\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        The subset\n\n    Raises\n        ValueError: If size &gt; len(s)\n    \"\"\"\n    if size &gt; len(s):\n        raise ValueError(\"Cannot sample subset larger than set\")\n    rng = np.random.default_rng(seed)\n    return rng.choice(s, size=size, replace=False)\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.running_moments","title":"running_moments","text":"<pre><code>running_moments(\n    previous_avg: float,\n    previous_variance: float,\n    count: int,\n    new_value: float,\n    unbiased: bool = True,\n) -&gt; tuple[float, float]\n</code></pre> <p>Calculates running average and variance of a series of numbers.</p> <p>See Welford's algorithm in wikipedia</p> <p>Warning</p> <p>This is not really using Welford's correction for numerical stability for the variance. (FIXME)</p> <p>Todo</p> <p>This could be generalised to arbitrary moments. See this paper</p> PARAMETER DESCRIPTION <code>previous_avg</code> <p>average value at previous step.</p> <p> TYPE: <code>float</code> </p> <code>previous_variance</code> <p>variance at previous step.</p> <p> TYPE: <code>float</code> </p> <code>count</code> <p>number of points seen so far,</p> <p> TYPE: <code>int</code> </p> <code>new_value</code> <p>new value in the series of numbers.</p> <p> TYPE: <code>float</code> </p> <code>unbiased</code> <p>whether to use the unbiased variance estimator (same as <code>np.var</code> with <code>ddof=1</code>).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Returns:     new_average, new_variance, calculated with the new count</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def running_moments(\n    previous_avg: float,\n    previous_variance: float,\n    count: int,\n    new_value: float,\n    unbiased: bool = True,\n) -&gt; tuple[float, float]:\n    \"\"\"Calculates running average and variance of a series of numbers.\n\n    See [Welford's algorithm in\n    wikipedia](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm)\n\n    !!! Warning\n        This is not really using Welford's correction for numerical stability\n        for the variance. (FIXME)\n\n    !!! Todo\n        This could be generalised to arbitrary moments. See [this\n        paper](https://www.osti.gov/biblio/1028931)\n\n    Args:\n        previous_avg: average value at previous step.\n        previous_variance: variance at previous step.\n        count: number of points seen so far,\n        new_value: new value in the series of numbers.\n        unbiased: whether to use the unbiased variance estimator (same as `np.var` with\n            `ddof=1`).\n    Returns:\n        new_average, new_variance, calculated with the new count\n    \"\"\"\n    delta = new_value - previous_avg\n    new_average = previous_avg + delta / (count + 1)\n\n    if unbiased:\n        if count &gt; 0:\n            new_variance = (\n                previous_variance + delta**2 / (count + 1) - previous_variance / count\n            )\n        else:\n            new_variance = 0.0\n    else:\n        new_variance = previous_variance + (\n            delta * (new_value - new_average) - previous_variance\n        ) / (count + 1)\n\n    return new_average, new_variance\n</code></pre>"},{"location":"api/pydvl/utils/numeric/#pydvl.utils.numeric.top_k_value_accuracy","title":"top_k_value_accuracy","text":"<pre><code>top_k_value_accuracy(\n    y_true: NDArray[float64], y_pred: NDArray[float64], k: int = 3\n) -&gt; float\n</code></pre> <p>Computes the top-k accuracy for the estimated values by comparing indices of the highest k values.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>Exact/true value</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>y_pred</code> <p>Predicted/estimated value</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>k</code> <p>Number of the highest values taken into account</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> RETURNS DESCRIPTION <code>float</code> <p>Accuracy</p> Source code in <code>src/pydvl/utils/numeric.py</code> <pre><code>def top_k_value_accuracy(\n    y_true: NDArray[np.float64], y_pred: NDArray[np.float64], k: int = 3\n) -&gt; float:\n    \"\"\"Computes the top-k accuracy for the estimated values by comparing indices\n    of the highest k values.\n\n    Args:\n        y_true: Exact/true value\n        y_pred: Predicted/estimated value\n        k: Number of the highest values taken into account\n\n    Returns:\n        Accuracy\n    \"\"\"\n    top_k_exact_values = np.argsort(y_true)[-k:]\n    top_k_pred_values = np.argsort(y_pred)[-k:]\n    top_k_accuracy = len(np.intersect1d(top_k_exact_values, top_k_pred_values)) / k\n    return top_k_accuracy\n</code></pre>"},{"location":"api/pydvl/utils/progress/","title":"Progress","text":""},{"location":"api/pydvl/utils/progress/#pydvl.utils.progress","title":"pydvl.utils.progress","text":""},{"location":"api/pydvl/utils/progress/#pydvl.utils.progress.Progress","title":"Progress","text":"<pre><code>Progress(iterable: Iterable[T], is_done: StoppingCriterion, **kwargs: Any)\n</code></pre> <p>               Bases: <code>Generic[T]</code></p> <p>Displays an optional progress bar for an iterable, using StoppingCriterion.completion for the progress.</p> PARAMETER DESCRIPTION <code>iterable</code> <p>The iterable to wrap.</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>is_done</code> <p>The stopping criterion.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>kwargs</code> <p>Additional keyword arguments passed to tqdm. - <code>total</code>: The total number of items in the iterable (Default: 100) - <code>unit</code>: The unit of the progress bar. (Default: %) - <code>desc</code>: Description of the progress bar. (Default: str(is_done)) - <code>bar_format</code>: Format of the progress bar. (Default is a percentage bar) - plus anything else that tqdm accepts</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/pydvl/utils/progress.py</code> <pre><code>def __init__(\n    self,\n    iterable: Iterable[T],\n    is_done: StoppingCriterion,\n    **kwargs: Any,\n) -&gt; None:\n    self.iterable = iterable\n    self.is_done = is_done\n    self.total = kwargs.pop(\"total\", 100)\n    desc = kwargs.pop(\"desc\", str(is_done))\n    unit = kwargs.pop(\"unit\", \"%\")\n    bar_format = kwargs.pop(\n        \"bar_format\",\n        \"{desc}: {percentage:0.2f}%|{bar}| [{elapsed}&lt;{remaining}, {rate_fmt}{postfix}]\",\n    )\n    self.pbar = tqdm(\n        total=self.total,\n        desc=desc,\n        unit=unit,\n        bar_format=bar_format,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/pydvl/utils/progress/#pydvl.utils.progress.log_duration","title":"log_duration","text":"<pre><code>log_duration(_func=None, *, log_level=DEBUG)\n</code></pre> <p>Decorator to log execution time of a function with a configurable logging level. It can be used with or without specifying a log level.</p> Source code in <code>src/pydvl/utils/progress.py</code> <pre><code>def log_duration(_func=None, *, log_level=logging.DEBUG):\n    \"\"\"\n    Decorator to log execution time of a function with a configurable logging level.\n    It can be used with or without specifying a log level.\n    \"\"\"\n\n    def decorator_log_duration(func):\n        @wraps(func)\n        def wrapper_log_duration(*args, **kwargs):\n            func_name = func.__qualname__\n            logger.log(log_level, f\"Function '{func_name}' is starting.\")\n            start_time = time()\n            result = func(*args, **kwargs)\n            duration = time() - start_time\n            logger.log(\n                log_level,\n                f\"Function '{func_name}' completed. Duration: {duration:.2f} sec\",\n            )\n            return result\n\n        return wrapper_log_duration\n\n    if _func is None:\n        # If log_duration was called without arguments, return decorator\n        return decorator_log_duration\n    else:\n        # If log_duration was called with a function, apply decorator directly\n        return decorator_log_duration(_func)\n</code></pre>"},{"location":"api/pydvl/utils/progress/#pydvl.utils.progress.repeat_indices","title":"repeat_indices","text":"<pre><code>repeat_indices(\n    indices: Collection[int],\n    result: ValuationResult,\n    done: StoppingCriterion,\n    **kwargs: Any\n) -&gt; Iterator[int]\n</code></pre> <p>Helper function to cycle indefinitely over a collection of indices until the stopping criterion is satisfied while displaying progress.</p> PARAMETER DESCRIPTION <code>indices</code> <p>Collection of indices that will be cycled until done.</p> <p> TYPE: <code>Collection[int]</code> </p> <code>result</code> <p>Object containing the current results.</p> <p> TYPE: <code>ValuationResult</code> </p> <code>done</code> <p>Stopping criterion.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>kwargs</code> <p>Keyword arguments passed to tqdm.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/pydvl/utils/progress.py</code> <pre><code>@deprecated(\n    target=True,\n    deprecated_in=\"0.10.0\",\n    remove_in=\"0.12.0\",\n    template_mgs=\"%(source_name)s used only by the old value module. \"\n    \"It will be removed in %(remove_in)s.\",\n)\ndef repeat_indices(\n    indices: Collection[int],\n    result: ValuationResult,\n    done: StoppingCriterion,\n    **kwargs: Any,\n) -&gt; Iterator[int]:\n    \"\"\"Helper function to cycle indefinitely over a collection of indices\n    until the stopping criterion is satisfied while displaying progress.\n\n    Args:\n        indices: Collection of indices that will be cycled until done.\n        result: Object containing the current results.\n        done: Stopping criterion.\n        kwargs: Keyword arguments passed to tqdm.\n    \"\"\"\n    with tqdm(total=100, unit=\"%\", **kwargs) as pbar:\n        it = takewhile(lambda _: not done(result), cycle(indices))\n        for i in it:\n            yield i\n            pbar.update(100 * done.completion() - pbar.n)\n            pbar.refresh()\n</code></pre>"},{"location":"api/pydvl/utils/score/","title":"Score","text":""},{"location":"api/pydvl/utils/score/#pydvl.utils.score","title":"pydvl.utils.score","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0. For use with the methods in pydvl.valuation please use pydvl.valuation.scorers instead.</p> <p>This module provides a Scorer class that wraps scoring functions with additional information.</p> <p>Scorers are the fundamental building block of many data valuation methods. They are typically used by the Utility class to evaluate the quality of a model when trained on subsets of the training data.</p> <p>Scorers can be constructed in the same way as in scikit-learn: either from known strings or from a callable. Greater values must be better. If they are not, a negated version can be used, see scikit-learn's make_scorer().</p> <p>Scorer provides additional information about the scoring function, like its range and default values, which can be used by some data valuation methods (like group_testing_shapley()) to estimate the number of samples required for a certain quality of approximation.</p>"},{"location":"api/pydvl/utils/score/#pydvl.utils.score.squashed_r2","title":"squashed_r2  <code>module-attribute</code>","text":"<pre><code>squashed_r2 = compose_score(Scorer('r2'), _sigmoid, (0, 1), 'squashed r2')\n</code></pre> <p>A scorer that squashes the R\u00b2 score into the range [0, 1] using a sigmoid.</p>"},{"location":"api/pydvl/utils/score/#pydvl.utils.score.squashed_variance","title":"squashed_variance  <code>module-attribute</code>","text":"<pre><code>squashed_variance = compose_score(\n    Scorer(\"explained_variance\"),\n    _sigmoid,\n    (0, 1),\n    \"squashed explained variance\",\n)\n</code></pre> <p>A scorer that squashes the explained variance score into the range [0, 1] using a sigmoid.</p>"},{"location":"api/pydvl/utils/score/#pydvl.utils.score.Scorer","title":"Scorer","text":"<pre><code>Scorer(\n    scoring: Union[str, ScorerCallable],\n    default: float = nan,\n    range: Tuple = (-inf, inf),\n    name: Optional[str] = None,\n)\n</code></pre> <p>A scoring callable that takes a model, data, and labels and returns a scalar.</p> PARAMETER DESCRIPTION <code>scoring</code> <p>Either a string or callable that can be passed to get_scorer.</p> <p> TYPE: <code>Union[str, ScorerCallable]</code> </p> <code>default</code> <p>score to be used when a model cannot be fit, e.g. when too little data is passed, or errors arise.</p> <p> TYPE: <code>float</code> DEFAULT: <code>nan</code> </p> <code>range</code> <p>numerical range of the score function. Some Monte Carlo methods can use this to estimate the number of samples required for a certain quality of approximation. If not provided, it can be read from the <code>scoring</code> object if it provides it, for instance if it was constructed with compose_score().</p> <p> TYPE: <code>Tuple</code> DEFAULT: <code>(-inf, inf)</code> </p> <code>name</code> <p>The name of the scorer. If not provided, the name of the function passed will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <p>New in version 0.5.0</p> Source code in <code>src/pydvl/utils/score.py</code> <pre><code>def __init__(\n    self,\n    scoring: Union[str, ScorerCallable],\n    default: float = np.nan,\n    range: Tuple = (-np.inf, np.inf),\n    name: Optional[str] = None,\n):\n    if name is None and isinstance(scoring, str):\n        name = scoring\n    self._scorer = get_scorer(scoring)\n    self.default = default\n    # TODO: auto-fill from known scorers ?\n    self.range = np.array(range)\n    self._name = getattr(self._scorer, \"__name__\", name or \"scorer\")\n</code></pre>"},{"location":"api/pydvl/utils/score/#pydvl.utils.score.ScorerCallable","title":"ScorerCallable","text":"<p>               Bases: <code>Protocol</code></p> <p>Signature for a scorer</p>"},{"location":"api/pydvl/utils/score/#pydvl.utils.score.compose_score","title":"compose_score","text":"<pre><code>compose_score(\n    scorer: Scorer,\n    transformation: Callable[[float], float],\n    range: Tuple[float, float],\n    name: str,\n) -&gt; Scorer\n</code></pre> <p>Composes a scoring function with an arbitrary scalar transformation.</p> <p>Useful to squash unbounded scores into ranges manageable by data valuation methods.</p> <p>Example:</p> <pre><code>sigmoid = lambda x: 1/(1+np.exp(-x))\ncompose_score(Scorer(\"r2\"), sigmoid, range=(0,1), name=\"squashed r2\")\n</code></pre> PARAMETER DESCRIPTION <code>scorer</code> <p>The object to be composed.</p> <p> TYPE: <code>Scorer</code> </p> <code>transformation</code> <p>A scalar transformation</p> <p> TYPE: <code>Callable[[float], float]</code> </p> <code>range</code> <p>The range of the transformation. This will be used e.g. by Utility for the range of the composed.</p> <p> TYPE: <code>Tuple[float, float]</code> </p> <code>name</code> <p>A string representation for the composition, for <code>str()</code>.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Scorer</code> <p>The composite Scorer.</p> Source code in <code>src/pydvl/utils/score.py</code> <pre><code>def compose_score(\n    scorer: Scorer,\n    transformation: Callable[[float], float],\n    range: Tuple[float, float],\n    name: str,\n) -&gt; Scorer:\n    \"\"\"Composes a scoring function with an arbitrary scalar transformation.\n\n    Useful to squash unbounded scores into ranges manageable by data valuation\n    methods.\n\n    Example:\n\n    ```python\n    sigmoid = lambda x: 1/(1+np.exp(-x))\n    compose_score(Scorer(\"r2\"), sigmoid, range=(0,1), name=\"squashed r2\")\n    ```\n\n    Args:\n        scorer: The object to be composed.\n        transformation: A scalar transformation\n        range: The range of the transformation. This will be used e.g. by\n            [Utility][pydvl.utils.utility.Utility] for the range of the composed.\n        name: A string representation for the composition, for `str()`.\n\n    Returns:\n        The composite [Scorer][pydvl.utils.score.Scorer].\n    \"\"\"\n\n    class CompositeScorer(Scorer):\n        def __call__(self, model: SupervisedModel, X: NDArray, y: NDArray) -&gt; float:\n            score = self._scorer(model=model, X=X, y=y)\n            return transformation(score)\n\n    return CompositeScorer(scorer, range=range, name=name)\n</code></pre>"},{"location":"api/pydvl/utils/status/","title":"Status","text":""},{"location":"api/pydvl/utils/status/#pydvl.utils.status","title":"pydvl.utils.status","text":""},{"location":"api/pydvl/utils/status/#pydvl.utils.status.Status","title":"Status","text":"<p>               Bases: <code>Enum</code></p> <p>Status of a computation.</p> <p>Statuses can be combined using bitwise or (<code>|</code>) and bitwise and (<code>&amp;</code>) to get the status of a combined computation. For example, if we have two computations, one that has converged and one that has failed, then the combined status is <code>Status.Converged | Status.Failed == Status.Converged</code>, but <code>Status.Converged &amp; Status.Failed == Status.Failed</code>.</p>"},{"location":"api/pydvl/utils/status/#pydvl.utils.status.Status--or","title":"OR","text":"<p>The result of bitwise or-ing two valuation statuses with <code>|</code> is given by the following table:</p> P C F P P C P C C C C F P C F <p>where P = Pending, C = Converged, F = Failed.</p>"},{"location":"api/pydvl/utils/status/#pydvl.utils.status.Status--and","title":"AND","text":"<p>The result of bitwise and-ing two valuation statuses with <code>&amp;</code> is given by the following table:</p> P C F P P P F C P C F F F F F <p>where P = Pending, C = Converged, F = Failed.</p>"},{"location":"api/pydvl/utils/status/#pydvl.utils.status.Status--not","title":"NOT","text":"<p>The result of bitwise negation of a Status with <code>~</code> is <code>Failed</code> if the status is <code>Converged</code>, or <code>Converged</code> otherwise:</p> <pre><code>~P == C, ~C == F, ~F == C\n</code></pre>"},{"location":"api/pydvl/utils/status/#pydvl.utils.status.Status--boolean-casting","title":"Boolean casting","text":"<p>A Status evaluates to <code>True</code> iff it's <code>Converged</code> or <code>Failed</code>:</p> <pre><code>bool(Status.Pending) == False\nbool(Status.Converged) == True\nbool(Status.Failed) == True\n</code></pre> <p>Warning</p> <p>These truth values are inconsistent with the usual boolean operations. In particular the XOR of two instances of <code>Status</code> is not the same as the XOR of their boolean values.</p>"},{"location":"api/pydvl/utils/types/","title":"Types","text":""},{"location":"api/pydvl/utils/types/#pydvl.utils.types","title":"pydvl.utils.types","text":"<p>This module contains types, protocols, decorators and generic function transformations. Some of it probably belongs elsewhere.</p>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.BaggingModel","title":"BaggingModel","text":"<p>               Bases: <code>Protocol</code></p> <p>Any model with the attributes <code>n_estimators</code> and <code>max_samples</code> is considered a bagging model.</p>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.BaggingModel.fit","title":"fit","text":"<pre><code>fit(x: NDArray, y: NDArray | None)\n</code></pre> <p>Fit the model to the data</p> PARAMETER DESCRIPTION <code>x</code> <p>Independent variables</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Dependent variable</p> <p> TYPE: <code>NDArray | None</code> </p> Source code in <code>src/pydvl/utils/types.py</code> <pre><code>def fit(self, x: NDArray, y: NDArray | None):\n    \"\"\"Fit the model to the data\n\n    Args:\n        x: Independent variables\n        y: Dependent variable\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.BaggingModel.predict","title":"predict","text":"<pre><code>predict(x: NDArray) -&gt; NDArray\n</code></pre> <p>Compute predictions for the input</p> PARAMETER DESCRIPTION <code>x</code> <p>Independent variables for which to compute predictions</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>Predictions for the input</p> Source code in <code>src/pydvl/utils/types.py</code> <pre><code>def predict(self, x: NDArray) -&gt; NDArray:\n    \"\"\"Compute predictions for the input\n\n    Args:\n        x: Independent variables for which to compute predictions\n\n    Returns:\n        Predictions for the input\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.BaseModel","title":"BaseModel","text":"<p>               Bases: <code>Protocol</code></p> <p>This is the minimal model protocol with the method <code>fit()</code></p>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.BaseModel.fit","title":"fit","text":"<pre><code>fit(x: NDArray, y: NDArray | None)\n</code></pre> <p>Fit the model to the data</p> PARAMETER DESCRIPTION <code>x</code> <p>Independent variables</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Dependent variable</p> <p> TYPE: <code>NDArray | None</code> </p> Source code in <code>src/pydvl/utils/types.py</code> <pre><code>def fit(self, x: NDArray, y: NDArray | None):\n    \"\"\"Fit the model to the data\n\n    Args:\n        x: Independent variables\n        y: Dependent variable\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.SupervisedModel","title":"SupervisedModel","text":"<p>               Bases: <code>Protocol</code></p> <p>This is the standard sklearn Protocol with the methods <code>fit()</code>, <code>predict()</code> and <code>score()</code>.</p>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.SupervisedModel.fit","title":"fit","text":"<pre><code>fit(x: NDArray, y: NDArray | None)\n</code></pre> <p>Fit the model to the data</p> PARAMETER DESCRIPTION <code>x</code> <p>Independent variables</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Dependent variable</p> <p> TYPE: <code>NDArray | None</code> </p> Source code in <code>src/pydvl/utils/types.py</code> <pre><code>def fit(self, x: NDArray, y: NDArray | None):\n    \"\"\"Fit the model to the data\n\n    Args:\n        x: Independent variables\n        y: Dependent variable\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.SupervisedModel.predict","title":"predict","text":"<pre><code>predict(x: NDArray) -&gt; NDArray\n</code></pre> <p>Compute predictions for the input</p> PARAMETER DESCRIPTION <code>x</code> <p>Independent variables for which to compute predictions</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>Predictions for the input</p> Source code in <code>src/pydvl/utils/types.py</code> <pre><code>def predict(self, x: NDArray) -&gt; NDArray:\n    \"\"\"Compute predictions for the input\n\n    Args:\n        x: Independent variables for which to compute predictions\n\n    Returns:\n        Predictions for the input\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.SupervisedModel.score","title":"score","text":"<pre><code>score(x: NDArray, y: NDArray | None) -&gt; float\n</code></pre> <p>Compute the score of the model given test data</p> PARAMETER DESCRIPTION <code>x</code> <p>Independent variables</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>Dependent variable</p> <p> TYPE: <code>NDArray | None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The score of the model on <code>(x, y)</code></p> Source code in <code>src/pydvl/utils/types.py</code> <pre><code>def score(self, x: NDArray, y: NDArray | None) -&gt; float:\n    \"\"\"Compute the score of the model given test data\n\n    Args:\n        x: Independent variables\n        y: Dependent variable\n\n    Returns:\n        The score of the model on `(x, y)`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.ensure_seed_sequence","title":"ensure_seed_sequence","text":"<pre><code>ensure_seed_sequence(\n    seed: Optional[Union[Seed, SeedSequence]] = None\n) -&gt; SeedSequence\n</code></pre> <p>If the passed seed is a SeedSequence object then it is returned as is. If it is a Generator the internal protected seed sequence from the generator gets extracted. Otherwise, a new SeedSequence object is created from the passed (optional) seed.</p> PARAMETER DESCRIPTION <code>seed</code> <p>Either an int, a Generator object a SeedSequence object or None.</p> <p> TYPE: <code>Optional[Union[Seed, SeedSequence]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SeedSequence</code> <p>A SeedSequence object.</p> <p>New in version 0.7.0</p> Source code in <code>src/pydvl/utils/types.py</code> <pre><code>def ensure_seed_sequence(\n    seed: Optional[Union[Seed, SeedSequence]] = None,\n) -&gt; SeedSequence:\n    \"\"\"\n    If the passed seed is a SeedSequence object then it is returned as is. If it is\n    a Generator the internal protected seed sequence from the generator gets extracted.\n    Otherwise, a new SeedSequence object is created from the passed (optional) seed.\n\n    Args:\n        seed: Either an int, a Generator object a SeedSequence object or None.\n\n    Returns:\n        A SeedSequence object.\n\n    !!! tip \"New in version 0.7.0\"\n    \"\"\"\n    if isinstance(seed, SeedSequence):\n        return seed\n    elif isinstance(seed, Generator):\n        return cast(SeedSequence, seed.bit_generator.seed_seq)  # type: ignore\n    else:\n        return SeedSequence(seed)\n</code></pre>"},{"location":"api/pydvl/utils/types/#pydvl.utils.types.validate_number","title":"validate_number","text":"<pre><code>validate_number(\n    name: str,\n    value: Any,\n    dtype: Type[T],\n    lower: T | None = None,\n    upper: T | None = None,\n) -&gt; T\n</code></pre> <p>Ensure that the value is of the given type and within the given bounds.</p> <p>For int and float types, this function is lenient with numpy numeric types and will convert them to the appropriate Python type as long as no precision is lost.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the variable to validate.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>The value to validate.</p> <p> TYPE: <code>Any</code> </p> <code>dtype</code> <p>The type to convert the value to.</p> <p> TYPE: <code>Type[T]</code> </p> <code>lower</code> <p>The lower bound for the value (inclusive).</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> <code>upper</code> <p>The upper bound for the value (inclusive).</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If the value is not of the given type.</p> <code>ValueError</code> <p>If the value is not within the given bounds, if there is precision loss, e.g. when forcing a float to an int, or if <code>dtype</code> is not a valid scalar type.</p> Source code in <code>src/pydvl/utils/types.py</code> <pre><code>def validate_number(\n    name: str,\n    value: Any,\n    dtype: Type[T],\n    lower: T | None = None,\n    upper: T | None = None,\n) -&gt; T:\n    \"\"\"Ensure that the value is of the given type and within the given bounds.\n\n    For int and float types, this function is lenient with numpy numeric types and\n    will convert them to the appropriate Python type as long as no precision is lost.\n\n    Args:\n        name: The name of the variable to validate.\n        value: The value to validate.\n        dtype: The type to convert the value to.\n        lower: The lower bound for the value (inclusive).\n        upper: The upper bound for the value (inclusive).\n\n    Raises:\n        TypeError: If the value is not of the given type.\n        ValueError: If the value is not within the given bounds, if there is precision\n            loss, e.g. when forcing a float to an int, or if `dtype` is not a valid\n            scalar type.\n    \"\"\"\n    if not isinstance(value, (int, float, np.number)):\n        raise TypeError(f\"'{name}' is not a number, it is {type(value).__name__}\")\n    if not issubclass(dtype, (np.number, int, float)):\n        raise ValueError(f\"type '{dtype}' is not a valid scalar type\")\n\n    converted = dtype(value)\n    if not np.isnan(converted) and not np.isclose(converted, value, rtol=0, atol=0):\n        raise ValueError(\n            f\"'{name}' cannot be converted to {dtype.__name__} without precision loss\"\n        )\n    value = cast(T, converted)\n\n    if lower is not None and value &lt; lower:  # type: ignore\n        raise ValueError(f\"'{name}' is {value}, but it should be &gt;= {lower}\")\n    if upper is not None and value &gt; upper:  # type: ignore\n        raise ValueError(f\"'{name}' is {value}, but it should be &lt;= {upper}\")\n    return value\n</code></pre>"},{"location":"api/pydvl/utils/utility/","title":"Utility","text":""},{"location":"api/pydvl/utils/utility/#pydvl.utils.utility","title":"pydvl.utils.utility","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0. For use with the methods in pydvl.valuation please use any of the classes in pydvl.valuation.utility instead.</p> <p>This module contains classes to manage and learn utility functions for the computation of values. Please see the documentation on Computing Data Values for more information.</p> <p>Utility holds information about model, data and scoring function (the latter being what one usually understands under utility in the general definition of Shapley value). It is automatically cached across machines when the cache is configured and it is enabled upon construction.</p> <p>DataUtilityLearning adds support for learning the scoring function to avoid repeated re-training of the model to compute the score.</p> <p>This module also contains derived <code>Utility</code> classes for toy games that are used for testing and for demonstration purposes.</p>"},{"location":"api/pydvl/utils/utility/#pydvl.utils.utility--references","title":"References","text":"<ol> <li> <p>Wang, T., Yang, Y. and Jia, R., 2021. Improving cooperative game theory-based data valuation via data utility learning. arXiv preprint arXiv:2107.06336.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/utils/utility/#pydvl.utils.utility.DataUtilityLearning","title":"DataUtilityLearning","text":"<pre><code>DataUtilityLearning(u: Utility, training_budget: int, model: SupervisedModel)\n</code></pre> <p>Implementation of Data Utility Learning (Wang et al., 2022)<sup>1</sup>.</p> <p>This object wraps a Utility and delegates calls to it, up until a given budget (number of iterations). Every tuple of input and output (a so-called utility sample) is stored. Once the budget is exhausted, <code>DataUtilityLearning</code> fits the given model to the utility samples. Subsequent calls will use the learned model to predict the utility instead of delegating.</p> PARAMETER DESCRIPTION <code>u</code> <p>The Utility to learn.</p> <p> TYPE: <code>Utility</code> </p> <code>training_budget</code> <p>Number of utility samples to collect before fitting the given model.</p> <p> TYPE: <code>int</code> </p> <code>model</code> <p>A supervised regression model</p> <p> TYPE: <code>SupervisedModel</code> </p> Example <pre><code>&gt;&gt;&gt; from pydvl.utils import Utility, DataUtilityLearning, Dataset\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression, LogisticRegression\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; dataset = Dataset.from_sklearn(load_iris())\n&gt;&gt;&gt; u = Utility(LogisticRegression(), dataset)\n&gt;&gt;&gt; wrapped_u = DataUtilityLearning(u, 3, LinearRegression())\n... # First 3 calls will be computed normally\n&gt;&gt;&gt; for i in range(3):\n...     _ = wrapped_u((i,))\n&gt;&gt;&gt; wrapped_u((1, 2, 3)) # Subsequent calls will be computed using the fit model for DUL\n0.0\n</code></pre> Source code in <code>src/pydvl/utils/utility.py</code> <pre><code>def __init__(\n    self, u: Utility, training_budget: int, model: SupervisedModel\n) -&gt; None:\n    self.utility = u\n    self.training_budget = training_budget\n    self.model = model\n    self._current_iteration = 0\n    self._is_model_fit = False\n    self._utility_samples: Dict[FrozenSet, Tuple[NDArray[np.bool_], float]] = {}\n</code></pre>"},{"location":"api/pydvl/utils/utility/#pydvl.utils.utility.DataUtilityLearning.data","title":"data  <code>property</code>","text":"<pre><code>data: Dataset\n</code></pre> <p>Returns the wrapped utility's Dataset.</p>"},{"location":"api/pydvl/utils/utility/#pydvl.utils.utility.Utility","title":"Utility","text":"<pre><code>Utility(\n    model: SupervisedModel,\n    data: Dataset,\n    scorer: Optional[Union[str, Scorer]] = None,\n    *,\n    default_score: float = 0.0,\n    score_range: Tuple[float, float] = (-inf, inf),\n    catch_errors: bool = True,\n    show_warnings: bool = False,\n    cache_backend: Optional[CacheBackend] = None,\n    cached_func_options: Optional[CachedFuncConfig] = None,\n    clone_before_fit: bool = True\n)\n</code></pre> <p>Convenience wrapper with configurable memoization of the scoring function.</p> <p>An instance of <code>Utility</code> holds the triple of model, dataset and scoring function which determines the value of data points. This is used for the computation of all game-theoretic values like Shapley values and the Least Core.</p> <p>The Utility expect the model to fulfill the SupervisedModel interface i.e. to have <code>fit()</code>, <code>predict()</code>, and <code>score()</code> methods.</p> <p>When calling the utility, the model will be cloned if it is a Scikit-Learn model, otherwise a copy is created using copy.deepcopy</p> <p>Since evaluating the scoring function requires retraining the model and that can be time-consuming, this class wraps it and caches the results of each execution. Caching is available both locally and across nodes, but must always be enabled for your project first, see the documentation and the module documentation.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>The supervised model.</p> <p> TYPE: <code>SupervisedModel</code> </p> <code>data</code> <p>An object containing the split data.</p> <p> TYPE: <code>Dataset</code> </p> <code>scorer</code> <p>A scoring function. If None, the <code>score()</code> method of the model will be used. See score for ways to create and compose scorers, in particular how to set default values and ranges.</p> <p> TYPE: <code>Scorer</code> </p> PARAMETER DESCRIPTION <code>model</code> <p>Any supervised model. Typical choices can be found in the sci-kit learn documentation.</p> <p> TYPE: <code>SupervisedModel</code> </p> <code>data</code> <p>Dataset or GroupedDataset instance.</p> <p> TYPE: <code>Dataset</code> </p> <code>scorer</code> <p>A scoring object. If None, the <code>score()</code> method of the model will be used. See score for ways to create and compose scorers, in particular how to set default values and ranges. For convenience, a string can be passed, which will be used to construct a Scorer.</p> <p> TYPE: <code>Optional[Union[str, Scorer]]</code> DEFAULT: <code>None</code> </p> <code>default_score</code> <p>As a convenience when no <code>scorer</code> object is passed (where a default value can be provided), this argument also allows to set the default score for models that have not been fit, e.g. when too little data is passed, or errors arise.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>score_range</code> <p>As with <code>default_score</code>, this is a convenience argument for when no <code>scorer</code> argument is provided, to set the numerical range of the score function. Some Monte Carlo methods can use this to estimate the number of samples required for a certain quality of approximation.</p> <p> TYPE: <code>Tuple[float, float]</code> DEFAULT: <code>(-inf, inf)</code> </p> <code>catch_errors</code> <p>set to <code>True</code> to catch the errors when <code>fit()</code> fails. This could happen in several steps of the pipeline, e.g. when too little training data is passed, which happens often during Shapley value calculations. When this happens, the <code>default_score</code> is returned as a score and computation continues.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_warnings</code> <p>Set to <code>False</code> to suppress warnings thrown by <code>fit()</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>cache_backend</code> <p>Optional instance of CacheBackend used to wrap the _utility method of the Utility instance. By default, this is set to None and that means that the utility evaluations will not be cached.</p> <p> TYPE: <code>Optional[CacheBackend]</code> DEFAULT: <code>None</code> </p> <code>cached_func_options</code> <p>Optional configuration object for cached utility evaluation.</p> <p> TYPE: <code>Optional[CachedFuncConfig]</code> DEFAULT: <code>None</code> </p> <code>clone_before_fit</code> <p>If <code>True</code>, the model will be cloned before calling <code>fit()</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Example <pre><code>&gt;&gt;&gt; from pydvl.utils import Utility, DataUtilityLearning, Dataset\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression, LogisticRegression\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; dataset = Dataset.from_sklearn(load_iris(), random_state=16)\n&gt;&gt;&gt; u = Utility(LogisticRegression(random_state=16), dataset)\n&gt;&gt;&gt; u(dataset.indices)\n0.9\n</code></pre> <p>With caching enabled:</p> <pre><code>&gt;&gt;&gt; from pydvl.utils import Utility, DataUtilityLearning, Dataset\n&gt;&gt;&gt; from pydvl.utils.caching.memory import InMemoryCacheBackend\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression, LogisticRegression\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; dataset = Dataset.from_sklearn(load_iris(), random_state=16)\n&gt;&gt;&gt; cache_backend = InMemoryCacheBackend()\n&gt;&gt;&gt; u = Utility(LogisticRegression(random_state=16), dataset, cache_backend=cache_backend)\n&gt;&gt;&gt; u(dataset.indices)\n0.9\n</code></pre> Source code in <code>src/pydvl/utils/utility.py</code> <pre><code>def __init__(\n    self,\n    model: SupervisedModel,\n    data: Dataset,\n    scorer: Optional[Union[str, Scorer]] = None,\n    *,\n    default_score: float = 0.0,\n    score_range: Tuple[float, float] = (-np.inf, np.inf),\n    catch_errors: bool = True,\n    show_warnings: bool = False,\n    cache_backend: Optional[CacheBackend] = None,\n    cached_func_options: Optional[CachedFuncConfig] = None,\n    clone_before_fit: bool = True,\n):\n    self.model = self._clone_model(model)\n    self.data = data\n    if isinstance(scorer, str):\n        scorer = Scorer(scorer, default=default_score, range=score_range)\n    self.scorer = check_scoring(self.model, scorer)\n    self.default_score = scorer.default if scorer is not None else default_score\n    # TODO: auto-fill from known scorers ?\n    self.score_range = scorer.range if scorer is not None else np.array(score_range)\n    self.clone_before_fit = clone_before_fit\n    self.catch_errors = catch_errors\n    self.show_warnings = show_warnings\n    self.cache = cache_backend\n    if cached_func_options is None:\n        cached_func_options = CachedFuncConfig()\n    # TODO: Find a better way to do this.\n    if cached_func_options.hash_prefix is None:\n        # FIX: This does not handle reusing the same across runs.\n        cached_func_options.hash_prefix = str(hash((model, data, scorer)))\n    self.cached_func_options = cached_func_options\n    self._initialize_utility_wrapper()\n</code></pre>"},{"location":"api/pydvl/utils/utility/#pydvl.utils.utility.Utility.cache_stats","title":"cache_stats  <code>property</code>","text":"<pre><code>cache_stats: Optional[CacheStats]\n</code></pre> <p>Cache statistics are gathered when cache is enabled. See CacheStats for all fields returned.</p>"},{"location":"api/pydvl/utils/utility/#pydvl.utils.utility.Utility.__call__","title":"__call__","text":"<pre><code>__call__(indices: Iterable[int]) -&gt; float\n</code></pre> PARAMETER DESCRIPTION <code>indices</code> <p>a subset of valid indices for the <code>x_train</code> attribute of Dataset.</p> <p> TYPE: <code>Iterable[int]</code> </p> Source code in <code>src/pydvl/utils/utility.py</code> <pre><code>def __call__(self, indices: Iterable[int]) -&gt; float:\n    \"\"\"\n    Args:\n        indices: a subset of valid indices for the\n            `x_train` attribute of [Dataset][pydvl.utils.dataset.Dataset].\n    \"\"\"\n    utility: float = self._utility_wrapper(frozenset(indices))\n    return utility\n</code></pre>"},{"location":"api/pydvl/utils/utility/#pydvl.utils.utility.Utility._clone_model","title":"_clone_model  <code>staticmethod</code>","text":"<pre><code>_clone_model(model: SupervisedModel) -&gt; SupervisedModel\n</code></pre> <p>Clones the passed model to avoid the possibility of reusing a fitted estimator</p> PARAMETER DESCRIPTION <code>model</code> <p>Any supervised model. Typical choices can be found on this page</p> <p> TYPE: <code>SupervisedModel</code> </p> Source code in <code>src/pydvl/utils/utility.py</code> <pre><code>@staticmethod\ndef _clone_model(model: SupervisedModel) -&gt; SupervisedModel:\n    \"\"\"Clones the passed model to avoid the possibility\n    of reusing a fitted estimator\n\n    Args:\n        model: Any supervised model. Typical choices can be found\n            on [this page](https://scikit-learn.org/stable/supervised_learning.html)\n    \"\"\"\n    try:\n        model = clone(model)\n    except TypeError:\n        # This happens if the passed model is not an sklearn model\n        # In this case, we just make a deepcopy of the model.\n        model = clone(model, safe=False)\n    model = cast(SupervisedModel, model)\n    return model\n</code></pre>"},{"location":"api/pydvl/utils/utility/#pydvl.utils.utility.Utility._utility","title":"_utility","text":"<pre><code>_utility(indices: FrozenSet) -&gt; float\n</code></pre> <p>Clones the model, fits it on a subset of the training data and scores it on the test data.</p> <p>If an instance of CacheBackend is passed during construction, results are memoized to avoid duplicate computation. This is useful in particular when computing utilities of permutations of indices or when randomly sampling from the powerset of indices.</p> PARAMETER DESCRIPTION <code>indices</code> <p>a subset of valid indices for the <code>x_train</code> attribute of Dataset. The type must be hashable for the caching to work, e.g. wrap the argument with frozenset (rather than <code>tuple</code> since order should not matter)</p> <p> TYPE: <code>FrozenSet</code> </p> RETURNS DESCRIPTION <code>float</code> <p>0 if no indices are passed, <code>default_score</code> if we fail to fit the model or the scorer returns numpy.nan. Otherwise, the score of the model on the test data.</p> Source code in <code>src/pydvl/utils/utility.py</code> <pre><code>def _utility(self, indices: FrozenSet) -&gt; float:\n    \"\"\"Clones the model, fits it on a subset of the training data\n    and scores it on the test data.\n\n    If an instance of [CacheBackend][pydvl.utils.caching.base.CacheBackend]\n    is passed during construction, results are\n    memoized to avoid duplicate computation. This is useful in particular\n    when computing utilities of permutations of indices or when randomly\n    sampling from the powerset of indices.\n\n    Args:\n        indices: a subset of valid indices for the\n            `x_train` attribute of [Dataset][pydvl.utils.dataset.Dataset].\n            The type must be hashable for the caching to work,\n            e.g. wrap the argument with [frozenset][]\n            (rather than `tuple` since order should not matter)\n\n    Returns:\n        0 if no indices are passed, `default_score` if we fail\n            to fit the model or the scorer returns [numpy.nan][]. Otherwise, the score\n            of the model on the test data.\n    \"\"\"\n    if not indices:\n        return 0.0\n\n    x_train, y_train = self.data.get_training_data(list(indices))\n    x_test, y_test = self.data.get_test_data(list(indices))\n\n    with warnings.catch_warnings():\n        if not self.show_warnings:\n            warnings.simplefilter(\"ignore\")\n        try:\n            if self.clone_before_fit:\n                model = self._clone_model(self.model)\n            else:\n                model = self.model\n            model.fit(x_train, y_train)\n            score = float(self.scorer(model, x_test, y_test))\n            # Some scorers raise exceptions if they return NaNs, some might not\n            if np.isnan(score):\n                warnings.warn(\"Scorer returned NaN\", RuntimeWarning)\n                return self.default_score\n            return score\n        except Exception as e:\n            if self.catch_errors:\n                warnings.warn(str(e), RuntimeWarning)\n                return self.default_score\n            raise\n</code></pre>"},{"location":"api/pydvl/utils/caching/","title":"Caching","text":""},{"location":"api/pydvl/utils/caching/#pydvl.utils.caching","title":"pydvl.utils.caching","text":"<p>This module provides caching of functions.</p> <p>PyDVL can cache (memoize) the computation of the utility function and speed up some computations for data valuation.</p> <p>Warning</p> <p>Function evaluations are cached with a key based on the function's signature and code. This can lead to undesired cache hits, see Cache reuse.</p> <p>Remember not to reuse utility objects for different datasets.</p>"},{"location":"api/pydvl/utils/caching/#pydvl.utils.caching--configuration","title":"Configuration","text":"<p>Caching is disabled by default but can be enabled easily, see Setting up the cache. When enabled, it will be added to any callable used to construct a Utility (done with the wrap method of CacheBackend). Depending on the nature of the utility you might want to enable the computation of a running average of function values, see Usage with stochastic functions. You can see all configuration options under CachedFuncConfig.</p>"},{"location":"api/pydvl/utils/caching/#pydvl.utils.caching--supported-backends","title":"Supported Backends","text":"<p>pyDVL supports 3 different caching backends:</p> <ul> <li>InMemoryCacheBackend:   an in-memory cache backend that uses a dictionary to store and retrieve   cached values. This is used to share cached values between threads   in a single process.</li> <li>DiskCacheBackend:   a disk-based cache backend that uses pickled values written to and read from disk.   This is used to share cached values between processes in a single machine.</li> <li> <p>MemcachedCacheBackend:   a Memcached-based cache backend that uses pickled values written to   and read from a Memcached server. This is used to share cached values   between processes across multiple machines.</p> <p>Info</p> <p>This specific backend requires optional dependencies not installed by default. See Extra dependencies for more information.</p> </li> </ul>"},{"location":"api/pydvl/utils/caching/#pydvl.utils.caching--usage-with-stochastic-functions","title":"Usage with stochastic functions","text":"<p>In addition to standard memoization, the wrapped functions can compute running average and standard error of repeated evaluations for the same input. This can be useful for stochastic functions with high variance (e.g. model training for small sample sizes), but drastically reduces the speed benefits of memoization.</p> <p>This behaviour can be activated with the option allow_repeated_evaluations.</p>"},{"location":"api/pydvl/utils/caching/#pydvl.utils.caching--cache-reuse","title":"Cache reuse","text":"<p>When working directly with CachedFunc,  it is essential to only cache pure functions. If they have any kind of state, either internal or external (e.g. a closure over some data that may change), then the cache will fail to notice this and the same value will be returned.</p> <p>When a function is wrapped with CachedFunc for memoization, its signature (input and output names) and code are used as a key for the cache.</p> <p>If you are running experiments with the same Utility but different datasets, this will lead to evaluations of the utility on new data returning old values because utilities only use sample indices as arguments (so there is no way to tell the difference between '1' for dataset A and '1' for dataset 2 from the point of view of the cache). One solution is to empty the cache between runs by calling the <code>clear</code> method of the cache backend instance, but the preferred one is to use a different Utility object for each dataset.</p>"},{"location":"api/pydvl/utils/caching/#pydvl.utils.caching--unexpected-cache-misses","title":"Unexpected cache misses","text":"<p>Because all arguments to a function are used as part of the key for the cache, sometimes one must exclude some of them. For example, If a function is going to run across multiple processes and some reporting arguments are added (like a <code>job_id</code> for logging purposes), these will be part of the signature and make the functions distinct to the eyes of the cache. This can be avoided with the use of ignore_args option in the configuration.</p>"},{"location":"api/pydvl/utils/caching/base/","title":"Base","text":""},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base","title":"pydvl.utils.caching.base","text":""},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CacheBackend","title":"CacheBackend","text":"<pre><code>CacheBackend()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for cache backends.</p> <p>Defines interface for cache access including wrapping callables, getting/setting results, clearing cache, and combining cache keys.</p> ATTRIBUTE DESCRIPTION <code>stats</code> <p>Cache statistics tracker.</p> <p> </p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.stats = CacheStats()\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CacheBackend.clear","title":"clear  <code>abstractmethod</code>","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Abstract method to clear the entire cache.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Abstract method to clear the entire cache.\"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CacheBackend.combine_hashes","title":"combine_hashes  <code>abstractmethod</code>","text":"<pre><code>combine_hashes(*args: str) -&gt; str\n</code></pre> <p>Abstract method to combine cache keys.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>@abstractmethod\ndef combine_hashes(self, *args: str) -&gt; str:\n    \"\"\"Abstract method to combine cache keys.\"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CacheBackend.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(key: str) -&gt; Optional[CacheResult]\n</code></pre> <p>Abstract method to retrieve a cached result.</p> <p>Implemented by subclasses.</p> PARAMETER DESCRIPTION <code>key</code> <p>The cache key.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[CacheResult]</code> <p>The cached result or None if not found.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>@abstractmethod\ndef get(self, key: str) -&gt; Optional[CacheResult]:\n    \"\"\"Abstract method to retrieve a cached result.\n\n    Implemented by subclasses.\n\n    Args:\n        key: The cache key.\n\n    Returns:\n        The cached result or None if not found.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CacheBackend.set","title":"set  <code>abstractmethod</code>","text":"<pre><code>set(key: str, value: CacheResult) -&gt; None\n</code></pre> <p>Abstract method to set a cached result.</p> <p>Implemented by subclasses.</p> PARAMETER DESCRIPTION <code>key</code> <p>The cache key.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>The result to cache.</p> <p> TYPE: <code>CacheResult</code> </p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>@abstractmethod\ndef set(self, key: str, value: CacheResult) -&gt; None:\n    \"\"\"Abstract method to set a cached result.\n\n    Implemented by subclasses.\n\n    Args:\n        key: The cache key.\n        value: The result to cache.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CacheBackend.wrap","title":"wrap","text":"<pre><code>wrap(\n    func: Callable, *, config: Optional[CachedFuncConfig] = None\n) -&gt; CachedFunc\n</code></pre> <p>Wraps a function to cache its results.</p> PARAMETER DESCRIPTION <code>func</code> <p>The function to wrap.</p> <p> TYPE: <code>Callable</code> </p> <code>config</code> <p>Optional caching options for the wrapped function.</p> <p> TYPE: <code>Optional[CachedFuncConfig]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CachedFunc</code> <p>The wrapped cached function.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>def wrap(\n    self,\n    func: Callable,\n    *,\n    config: Optional[CachedFuncConfig] = None,\n) -&gt; \"CachedFunc\":\n    \"\"\"Wraps a function to cache its results.\n\n    Args:\n        func: The function to wrap.\n        config: Optional caching options for the wrapped function.\n\n    Returns:\n        The wrapped cached function.\n    \"\"\"\n    return CachedFunc(\n        func,\n        cache_backend=self,\n        config=config,\n    )\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CacheResult","title":"CacheResult  <code>dataclass</code>","text":"<pre><code>CacheResult(value: float, count: int = 1, variance: float = 0.0)\n</code></pre> <p>A class used to store the cached result of a computation as well as count and variance when using repeated evaluation.</p> ATTRIBUTE DESCRIPTION <code>value</code> <p>Cached value.</p> <p> TYPE: <code>float</code> </p> <code>count</code> <p>Number of times this value has been computed.</p> <p> TYPE: <code>int</code> </p> <code>variance</code> <p>Variance associated with the cached value.</p> <p> TYPE: <code>float</code> </p>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CacheStats","title":"CacheStats  <code>dataclass</code>","text":"<pre><code>CacheStats(\n    sets: int = 0,\n    misses: int = 0,\n    hits: int = 0,\n    timeouts: int = 0,\n    errors: int = 0,\n    reconnects: int = 0,\n)\n</code></pre> <p>Class used to store statistics gathered by cached functions.</p> ATTRIBUTE DESCRIPTION <code>sets</code> <p>Number of times a value was set in the cache.</p> <p> TYPE: <code>int</code> </p> <code>misses</code> <p>Number of times a value was not found in the cache.</p> <p> TYPE: <code>int</code> </p> <code>hits</code> <p>Number of times a value was found in the cache.</p> <p> TYPE: <code>int</code> </p> <code>timeouts</code> <p>Number of times a timeout occurred.</p> <p> TYPE: <code>int</code> </p> <code>errors</code> <p>Number of times an error occurred.</p> <p> TYPE: <code>int</code> </p> <code>reconnects</code> <p>Number of times the client reconnected to the server.</p> <p> TYPE: <code>int</code> </p>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CachedFunc","title":"CachedFunc","text":"<pre><code>CachedFunc(\n    func: Callable[..., float],\n    *,\n    cache_backend: CacheBackend,\n    config: Optional[CachedFuncConfig] = None\n)\n</code></pre> <p>Caches callable function results with a provided cache backend.</p> <p>Wraps a callable function to cache its results using a provided an instance of a subclass of CacheBackend.</p> <p>This class is heavily inspired from that of joblib.memory.MemorizedFunc.</p> <p>This class caches calls to the wrapped callable by generating a hash key based on the wrapped callable's code, the arguments passed to it and the optional hash_prefix.</p> <p>Warning</p> <p>This class only works with hashable arguments to the wrapped callable.</p> PARAMETER DESCRIPTION <code>func</code> <p>Callable to wrap.</p> <p> TYPE: <code>Callable[..., float]</code> </p> <code>cache_backend</code> <p>Instance of CacheBackendBase that handles setting and getting values.</p> <p> TYPE: <code>CacheBackend</code> </p> <code>config</code> <p>Configuration for wrapped function.</p> <p> TYPE: <code>Optional[CachedFuncConfig]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>def __init__(\n    self,\n    func: Callable[..., float],\n    *,\n    cache_backend: CacheBackend,\n    config: Optional[CachedFuncConfig] = None,\n) -&gt; None:\n    self.func = func\n    self.cache_backend = cache_backend\n    if config is None:\n        config = CachedFuncConfig()\n    self.config = config\n\n    self.__doc__ = f\"A wrapper around {func.__name__}() with caching enabled.\\n\" + (\n        CachedFunc.__doc__ or \"\"\n    )\n    self.__name__ = f\"cached_{func.__name__}\"\n    path = list(reversed(func.__qualname__.split(\".\")))\n    patched = [f\"cached_{path[0]}\"] + path[1:]\n    self.__qualname__ = \".\".join(reversed(patched))\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CachedFunc.stats","title":"stats  <code>property</code>","text":"<pre><code>stats: CacheStats\n</code></pre> <p>Cache backend statistics.</p>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CachedFunc.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs) -&gt; float\n</code></pre> <p>Call the wrapped cached function.</p> <p>Executes the wrapped function, caching and returning the result.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; float:\n    \"\"\"Call the wrapped cached function.\n\n    Executes the wrapped function, caching and returning the result.\n    \"\"\"\n    return self._cached_call(args, kwargs)\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CachedFunc._cached_call","title":"_cached_call","text":"<pre><code>_cached_call(args, kwargs) -&gt; float\n</code></pre> <p>Cached wrapped function call.</p> <p>Executes the wrapped function with cache checking/setting.</p> RETURNS DESCRIPTION <code>float</code> <p>Cached result of the wrapped function.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>def _cached_call(self, args, kwargs) -&gt; float:\n    \"\"\"Cached wrapped function call.\n\n    Executes the wrapped function with cache checking/setting.\n\n    Returns:\n        Cached result of the wrapped function.\n    \"\"\"\n    key = self._get_cache_key(*args, **kwargs)\n    cached_result = self.cache_backend.get(key)\n    if cached_result is None:\n        value, duration = self._force_call(args, kwargs)\n        result = CacheResult(value)\n        if (\n            duration &gt;= self.config.time_threshold\n            or self.config.allow_repeated_evaluations\n        ):\n            self.cache_backend.set(key, result)\n    else:\n        result = cached_result\n        if self.config.allow_repeated_evaluations:\n            error_on_average = (result.variance / result.count) ** (1 / 2)\n            if (\n                error_on_average &gt; self.config.rtol_stderr * result.value\n                or result.count &lt;= self.config.min_repetitions\n            ):\n                new_value, _ = self._force_call(args, kwargs)\n                new_avg, new_var = running_moments(\n                    result.value,\n                    result.variance,\n                    result.count,\n                    cast(float, new_value),\n                )\n                result.value = new_avg\n                result.count += 1\n                result.variance = new_var\n                self.cache_backend.set(key, result)\n    return result.value\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CachedFunc._filter_args","title":"_filter_args  <code>staticmethod</code>","text":"<pre><code>_filter_args(\n    func: Callable,\n    ignore_args: Collection[str],\n    args: Tuple[Any, ...],\n    kwargs: Dict[str, Any],\n) -&gt; Dict[str, Any]\n</code></pre> <p>Filter arguments to exclude from cache keys.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>@staticmethod\ndef _filter_args(\n    func: Callable,\n    ignore_args: Collection[str],\n    args: Tuple[Any, ...],\n    kwargs: Dict[str, Any],\n) -&gt; Dict[str, Any]:\n    \"\"\"Filter arguments to exclude from cache keys.\"\"\"\n    # Remove kwargs before calling filter_args\n    # Because some of them might not be explicitly in the function's signature\n    # and that would raise an error when calling filter_args\n    kwargs = {k: v for k, v in kwargs.items() if k not in ignore_args}  # type: ignore\n    # Update ignore_args\n    func_signature = inspect.signature(func)\n    arg_names = []\n    for param in func_signature.parameters.values():\n        if param.kind in [\n            param.POSITIONAL_ONLY,\n            param.POSITIONAL_OR_KEYWORD,\n            param.KEYWORD_ONLY,\n        ]:\n            arg_names.append(param.name)\n    ignore_args = [x for x in ignore_args if x in arg_names]\n    filtered_args: Dict[str, Any] = filter_args(func, ignore_args, args, kwargs)  # type: ignore\n    # We ignore 'self' because for our use case we only care about the method.\n    # We don't want a cache if another attribute changes in the instance.\n    try:\n        filtered_args.pop(\"self\")\n    except KeyError:\n        pass\n    return filtered_args  # type: ignore\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CachedFunc._force_call","title":"_force_call","text":"<pre><code>_force_call(args, kwargs) -&gt; Tuple[float, float]\n</code></pre> <p>Force re-evaluation of the wrapped function.</p> <p>Executes the wrapped function without caching.</p> RETURNS DESCRIPTION <code>Tuple[float, float]</code> <p>Function result and execution duration.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>def _force_call(self, args, kwargs) -&gt; Tuple[float, float]:\n    \"\"\"Force re-evaluation of the wrapped function.\n\n    Executes the wrapped function without caching.\n\n    Returns:\n        Function result and execution duration.\n    \"\"\"\n    start = time.monotonic()\n    value = self.func(*args, **kwargs)\n    end = time.monotonic()\n    duration = end - start\n    return value, duration\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CachedFunc._get_cache_key","title":"_get_cache_key","text":"<pre><code>_get_cache_key(*args, **kwargs) -&gt; str\n</code></pre> <p>Returns a string key used to identify the function and input parameter hash.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>def _get_cache_key(self, *args, **kwargs) -&gt; str:\n    \"\"\"Returns a string key used to identify the function and input parameter hash.\"\"\"\n    func_hash = self._hash_function(self.func)\n    argument_hash = self._hash_arguments(\n        self.func, self.config.ignore_args, args, kwargs\n    )\n    hashes = [func_hash, argument_hash]\n    if self.config.hash_prefix is not None:\n        hashes.insert(0, self.config.hash_prefix)\n    key = self.cache_backend.combine_hashes(*hashes)\n    return key\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CachedFunc._hash_arguments","title":"_hash_arguments  <code>staticmethod</code>","text":"<pre><code>_hash_arguments(\n    func: Callable,\n    ignore_args: Collection[str],\n    args: Tuple[Any, ...],\n    kwargs: Dict[str, Any],\n) -&gt; str\n</code></pre> <p>Create hash for function arguments.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>@staticmethod\ndef _hash_arguments(\n    func: Callable,\n    ignore_args: Collection[str],\n    args: Tuple[Any, ...],\n    kwargs: Dict[str, Any],\n) -&gt; str:\n    \"\"\"Create hash for function arguments.\"\"\"\n    args_hash: str = hashing.hash(\n        CachedFunc._filter_args(func, ignore_args, args, kwargs),\n    )\n    return args_hash\n</code></pre>"},{"location":"api/pydvl/utils/caching/base/#pydvl.utils.caching.base.CachedFunc._hash_function","title":"_hash_function  <code>staticmethod</code>","text":"<pre><code>_hash_function(func: Callable) -&gt; str\n</code></pre> <p>Create hash for wrapped function.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>@staticmethod\ndef _hash_function(func: Callable) -&gt; str:\n    \"\"\"Create hash for wrapped function.\"\"\"\n    func_hash: str = hashing.hash((func.__code__.co_code, func.__code__.co_consts))\n    return func_hash\n</code></pre>"},{"location":"api/pydvl/utils/caching/config/","title":"Config","text":""},{"location":"api/pydvl/utils/caching/config/#pydvl.utils.caching.config","title":"pydvl.utils.caching.config","text":""},{"location":"api/pydvl/utils/caching/config/#pydvl.utils.caching.config.CachedFuncConfig","title":"CachedFuncConfig  <code>dataclass</code>","text":"<pre><code>CachedFuncConfig(\n    hash_prefix: Optional[str] = None,\n    ignore_args: Collection[str] = list(),\n    time_threshold: float = 0.3,\n    allow_repeated_evaluations: bool = False,\n    rtol_stderr: float = 0.1,\n    min_repetitions: int = 3,\n)\n</code></pre> <p>Configuration for cached functions and methods, providing memoization of function calls.</p> <p>Instances of this class are typically used as arguments for the construction of a Utility.</p> PARAMETER DESCRIPTION <code>hash_prefix</code> <p>Optional string prefix that be prepended to the cache key. This can be provided in order to guarantee cache reuse across runs.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>ignore_args</code> <p>Do not take these keyword arguments into account when hashing the wrapped function for usage as key. This allows sharing the cache among different jobs for the same experiment run if the callable happens to have \"nuisance\" parameters like <code>job_id</code> which do not affect the result of the computation.</p> <p> TYPE: <code>Collection[str]</code> DEFAULT: <code>list()</code> </p> <code>time_threshold</code> <p>Computations taking less time than this many seconds are not cached. A value of 0 means that it will always cache results.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>allow_repeated_evaluations</code> <p>If <code>True</code>, repeated calls to a function with the same arguments will be allowed and outputs averaged until the running standard deviation of the mean stabilizes below <code>rtol_stderr * mean</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>rtol_stderr</code> <p>relative tolerance for repeated evaluations. More precisely, memcached() will stop evaluating the function once the standard deviation of the mean is smaller than <code>rtol_stderr * mean</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>min_repetitions</code> <p>minimum number of times that a function evaluation on the same arguments is repeated before returning cached values. Useful for stochastic functions only. If the model training is very noisy, set this number to higher values to reduce variance.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p>"},{"location":"api/pydvl/utils/caching/disk/","title":"Disk","text":""},{"location":"api/pydvl/utils/caching/disk/#pydvl.utils.caching.disk","title":"pydvl.utils.caching.disk","text":""},{"location":"api/pydvl/utils/caching/disk/#pydvl.utils.caching.disk.DiskCacheBackend","title":"DiskCacheBackend","text":"<pre><code>DiskCacheBackend(cache_dir: Optional[Union[PathLike, str]] = None)\n</code></pre> <p>               Bases: <code>CacheBackend</code></p> <p>Disk cache backend that stores results in files.</p> <p>Implements the CacheBackend interface for a disk-based cache. Stores cache entries as pickled files on disk, keyed by cache key. This allows sharing evaluations across processes in a single node/computer.</p> PARAMETER DESCRIPTION <code>cache_dir</code> <p>Base directory for cache storage.</p> <p> TYPE: <code>Optional[Union[PathLike, str]]</code> DEFAULT: <code>None</code> </p> ATTRIBUTE DESCRIPTION <code>cache_dir</code> <p>Base directory for cache storage.</p> <p> </p> Example <p>Basic usage: <pre><code>&gt;&gt;&gt; from pydvl.utils.caching.disk import DiskCacheBackend\n&gt;&gt;&gt; cache_backend = DiskCacheBackend()\n&gt;&gt;&gt; cache_backend.clear()\n&gt;&gt;&gt; value = 42\n&gt;&gt;&gt; cache_backend.set(\"key\", value)\n&gt;&gt;&gt; cache_backend.get(\"key\")\n42\n</code></pre></p> <p>Callable wrapping: <pre><code>&gt;&gt;&gt; from pydvl.utils.caching.disk import DiskCacheBackend\n&gt;&gt;&gt; cache_backend = DiskCacheBackend()\n&gt;&gt;&gt; cache_backend.clear()\n&gt;&gt;&gt; value = 42\n&gt;&gt;&gt; def foo(x: int):\n...     return x + 1\n...\n&gt;&gt;&gt; wrapped_foo = cache_backend.wrap(foo)\n&gt;&gt;&gt; wrapped_foo(value)\n43\n&gt;&gt;&gt; wrapped_foo.stats.misses\n1\n&gt;&gt;&gt; wrapped_foo.stats.hits\n0\n&gt;&gt;&gt; wrapped_foo(value)\n43\n&gt;&gt;&gt; wrapped_foo.stats.misses\n1\n&gt;&gt;&gt; wrapped_foo.stats.hits\n1\n</code></pre></p> <p>Initialize the disk cache backend.</p> PARAMETER DESCRIPTION <code>cache_dir</code> <p>Base directory for cache storage. If not provided, this defaults to a newly created temporary directory.</p> <p> TYPE: <code>Optional[Union[PathLike, str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/utils/caching/disk.py</code> <pre><code>def __init__(\n    self,\n    cache_dir: Optional[Union[os.PathLike, str]] = None,\n) -&gt; None:\n    \"\"\"Initialize the disk cache backend.\n\n    Args:\n        cache_dir: Base directory for cache storage.\n            If not provided, this defaults to a newly created\n            temporary directory.\n    \"\"\"\n    super().__init__()\n    if cache_dir is None:\n        cache_dir = tempfile.mkdtemp(prefix=\"pydvl\")\n    self.cache_dir = Path(cache_dir)\n    self.cache_dir.mkdir(exist_ok=True, parents=True)\n</code></pre>"},{"location":"api/pydvl/utils/caching/disk/#pydvl.utils.caching.disk.DiskCacheBackend.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Deletes cache directory and recreates it.</p> Source code in <code>src/pydvl/utils/caching/disk.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Deletes cache directory and recreates it.\"\"\"\n    shutil.rmtree(self.cache_dir)\n    self.cache_dir.mkdir(exist_ok=True, parents=True)\n</code></pre>"},{"location":"api/pydvl/utils/caching/disk/#pydvl.utils.caching.disk.DiskCacheBackend.combine_hashes","title":"combine_hashes","text":"<pre><code>combine_hashes(*args: str) -&gt; str\n</code></pre> <p>Join cache key components.</p> Source code in <code>src/pydvl/utils/caching/disk.py</code> <pre><code>def combine_hashes(self, *args: str) -&gt; str:\n    \"\"\"Join cache key components.\"\"\"\n    return os.pathsep.join(args)\n</code></pre>"},{"location":"api/pydvl/utils/caching/disk/#pydvl.utils.caching.disk.DiskCacheBackend.get","title":"get","text":"<pre><code>get(key: str) -&gt; Optional[Any]\n</code></pre> <p>Get a value from the cache.</p> PARAMETER DESCRIPTION <code>key</code> <p>Cache key.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[Any]</code> <p>Cached value or None if not found.</p> Source code in <code>src/pydvl/utils/caching/disk.py</code> <pre><code>def get(self, key: str) -&gt; Optional[Any]:\n    \"\"\"Get a value from the cache.\n\n    Args:\n        key: Cache key.\n\n    Returns:\n        Cached value or None if not found.\n    \"\"\"\n    cache_file = self.cache_dir / key\n    if not cache_file.exists():\n        self.stats.misses += 1\n        return None\n    self.stats.hits += 1\n    with cache_file.open(\"rb\") as f:\n        return cloudpickle.load(f)\n</code></pre>"},{"location":"api/pydvl/utils/caching/disk/#pydvl.utils.caching.disk.DiskCacheBackend.set","title":"set","text":"<pre><code>set(key: str, value: Any) -&gt; None\n</code></pre> <p>Set a value in the cache.</p> PARAMETER DESCRIPTION <code>key</code> <p>Cache key.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value to cache.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/pydvl/utils/caching/disk.py</code> <pre><code>def set(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a value in the cache.\n\n    Args:\n        key: Cache key.\n        value: Value to cache.\n    \"\"\"\n    cache_file = self.cache_dir / key\n    self.stats.sets += 1\n    with cache_file.open(\"wb\") as f:\n        cloudpickle.dump(value, f, protocol=PICKLE_VERSION)\n</code></pre>"},{"location":"api/pydvl/utils/caching/disk/#pydvl.utils.caching.disk.DiskCacheBackend.wrap","title":"wrap","text":"<pre><code>wrap(\n    func: Callable, *, config: Optional[CachedFuncConfig] = None\n) -&gt; CachedFunc\n</code></pre> <p>Wraps a function to cache its results.</p> PARAMETER DESCRIPTION <code>func</code> <p>The function to wrap.</p> <p> TYPE: <code>Callable</code> </p> <code>config</code> <p>Optional caching options for the wrapped function.</p> <p> TYPE: <code>Optional[CachedFuncConfig]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CachedFunc</code> <p>The wrapped cached function.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>def wrap(\n    self,\n    func: Callable,\n    *,\n    config: Optional[CachedFuncConfig] = None,\n) -&gt; \"CachedFunc\":\n    \"\"\"Wraps a function to cache its results.\n\n    Args:\n        func: The function to wrap.\n        config: Optional caching options for the wrapped function.\n\n    Returns:\n        The wrapped cached function.\n    \"\"\"\n    return CachedFunc(\n        func,\n        cache_backend=self,\n        config=config,\n    )\n</code></pre>"},{"location":"api/pydvl/utils/caching/memcached/","title":"Memcached","text":""},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached","title":"pydvl.utils.caching.memcached","text":""},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached.MemcachedCacheBackend","title":"MemcachedCacheBackend","text":"<pre><code>MemcachedCacheBackend(config: MemcachedClientConfig = MemcachedClientConfig())\n</code></pre> <p>               Bases: <code>CacheBackend</code></p> <p>Memcached cache backend for the distributed caching of functions.</p> <p>Implements the CacheBackend interface for a memcached based cache. This allows sharing evaluations across processes and nodes in a cluster. You can run memcached as a service, locally or remotely, see the caching documentation.</p> PARAMETER DESCRIPTION <code>config</code> <p>Memcached client configuration.</p> <p> TYPE: <code>MemcachedClientConfig</code> DEFAULT: <code>MemcachedClientConfig()</code> </p> ATTRIBUTE DESCRIPTION <code>config</code> <p>Memcached client configuration.</p> <p> </p> <code>client</code> <p>Memcached client instance.</p> <p> </p> Example <p>Basic usage: <pre><code>&gt;&gt;&gt; from pydvl.utils.caching.memcached import MemcachedCacheBackend\n&gt;&gt;&gt; cache_backend = MemcachedCacheBackend()\n&gt;&gt;&gt; cache_backend.clear()\n&gt;&gt;&gt; value = 42\n&gt;&gt;&gt; cache_backend.set(\"key\", value)\n&gt;&gt;&gt; cache_backend.get(\"key\")\n42\n</code></pre></p> <p>Callable wrapping: <pre><code>&gt;&gt;&gt; from pydvl.utils.caching.memcached import MemcachedCacheBackend\n&gt;&gt;&gt; cache_backend = MemcachedCacheBackend()\n&gt;&gt;&gt; cache_backend.clear()\n&gt;&gt;&gt; value = 42\n&gt;&gt;&gt; def foo(x: int):\n...     return x + 1\n...\n&gt;&gt;&gt; wrapped_foo = cache_backend.wrap(foo)\n&gt;&gt;&gt; wrapped_foo(value)\n43\n&gt;&gt;&gt; wrapped_foo.stats.misses\n1\n&gt;&gt;&gt; wrapped_foo.stats.hits\n0\n&gt;&gt;&gt; wrapped_foo(value)\n43\n&gt;&gt;&gt; wrapped_foo.stats.misses\n1\n&gt;&gt;&gt; wrapped_foo.stats.hits\n1\n</code></pre></p> <p>Initialize memcached cache backend.</p> PARAMETER DESCRIPTION <code>config</code> <p>Memcached client configuration.</p> <p> TYPE: <code>MemcachedClientConfig</code> DEFAULT: <code>MemcachedClientConfig()</code> </p> Source code in <code>src/pydvl/utils/caching/memcached.py</code> <pre><code>def __init__(self, config: MemcachedClientConfig = MemcachedClientConfig()) -&gt; None:\n    \"\"\"Initialize memcached cache backend.\n\n    Args:\n        config: Memcached client configuration.\n    \"\"\"\n\n    super().__init__()\n    self.config = config\n    self.client = self._connect(self.config)\n</code></pre>"},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached.MemcachedCacheBackend.__getstate__","title":"__getstate__","text":"<pre><code>__getstate__() -&gt; Dict\n</code></pre> <p>Enables pickling after a socket has been opened to the memcached server, by removing the client from the stored data.</p> Source code in <code>src/pydvl/utils/caching/memcached.py</code> <pre><code>def __getstate__(self) -&gt; Dict:\n    \"\"\"Enables pickling after a socket has been opened to the\n    memcached server, by removing the client from the stored\n    data.\"\"\"\n    odict = self.__dict__.copy()\n    del odict[\"client\"]\n    return odict\n</code></pre>"},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached.MemcachedCacheBackend.__setstate__","title":"__setstate__","text":"<pre><code>__setstate__(d: Dict)\n</code></pre> <p>Restores a client connection after loading from a pickle.</p> Source code in <code>src/pydvl/utils/caching/memcached.py</code> <pre><code>def __setstate__(self, d: Dict):\n    \"\"\"Restores a client connection after loading from a pickle.\"\"\"\n    self.config = d[\"config\"]\n    self.stats = d[\"stats\"]\n    self.client = self._connect(self.config)\n</code></pre>"},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached.MemcachedCacheBackend._connect","title":"_connect  <code>staticmethod</code>","text":"<pre><code>_connect(config: MemcachedClientConfig) -&gt; RetryingClient\n</code></pre> <p>Connect to memcached server.</p> Source code in <code>src/pydvl/utils/caching/memcached.py</code> <pre><code>@staticmethod\ndef _connect(config: MemcachedClientConfig) -&gt; RetryingClient:\n    \"\"\"Connect to memcached server.\"\"\"\n    try:\n        client = RetryingClient(\n            Client(**asdict(config)),\n            attempts=3,\n            retry_delay=0.1,\n            retry_for=[MemcacheUnexpectedCloseError],\n        )\n\n        temp_key = str(uuid.uuid4())\n        client.set(temp_key, 7)\n        assert client.get(temp_key) == 7\n        client.delete(temp_key, 0)\n        return client\n    except ConnectionRefusedError as e:\n        logger.error(  # type: ignore\n            f\"@memcached: Timeout connecting \"\n            f\"to {config.server} after \"\n            f\"{config.connect_timeout} seconds: {str(e)}. Did you start memcached?\"\n        )\n        raise\n    except AssertionError as e:\n        logger.error(  # type: ignore\n            f\"@memcached: Failure saving dummy value to {config.server}: {str(e)}\"\n        )\n        raise\n</code></pre>"},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached.MemcachedCacheBackend.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Flush all values from memcached.</p> Source code in <code>src/pydvl/utils/caching/memcached.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Flush all values from memcached.\"\"\"\n    self.client.flush_all(noreply=True)\n</code></pre>"},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached.MemcachedCacheBackend.combine_hashes","title":"combine_hashes","text":"<pre><code>combine_hashes(*args: str) -&gt; str\n</code></pre> <p>Join cache key components for Memcached.</p> Source code in <code>src/pydvl/utils/caching/memcached.py</code> <pre><code>def combine_hashes(self, *args: str) -&gt; str:\n    \"\"\"Join cache key components for Memcached.\"\"\"\n    return \":\".join(args)\n</code></pre>"},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached.MemcachedCacheBackend.get","title":"get","text":"<pre><code>get(key: str) -&gt; Optional[Any]\n</code></pre> <p>Get value from memcached.</p> PARAMETER DESCRIPTION <code>key</code> <p>Cache key.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[Any]</code> <p>Cached value or None if not found or client disconnected.</p> Source code in <code>src/pydvl/utils/caching/memcached.py</code> <pre><code>def get(self, key: str) -&gt; Optional[Any]:\n    \"\"\"Get value from memcached.\n\n    Args:\n        key: Cache key.\n\n    Returns:\n        Cached value or None if not found or client disconnected.\n    \"\"\"\n    result = None\n    try:\n        result = self.client.get(key)\n    except socket.timeout as e:\n        self.stats.timeouts += 1\n        warnings.warn(f\"{type(self).__name__}: {str(e)}\", RuntimeWarning)\n    except OSError as e:\n        self.stats.errors += 1\n        warnings.warn(f\"{type(self).__name__}: {str(e)}\", RuntimeWarning)\n    except AttributeError as e:\n        # FIXME: this depends on _recv() failing on invalid sockets\n        # See pymemcache.base.py,\n        self.stats.reconnects += 1\n        warnings.warn(f\"{type(self).__name__}: {str(e)}\", RuntimeWarning)\n        self.client = self._connect(self.config)\n    if result is None:\n        self.stats.misses += 1\n    else:\n        self.stats.hits += 1\n    return result\n</code></pre>"},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached.MemcachedCacheBackend.set","title":"set","text":"<pre><code>set(key: str, value: Any) -&gt; None\n</code></pre> <p>Set value in memcached.</p> PARAMETER DESCRIPTION <code>key</code> <p>Cache key.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value to cache.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/pydvl/utils/caching/memcached.py</code> <pre><code>def set(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set value in memcached.\n\n    Args:\n        key: Cache key.\n        value: Value to cache.\n    \"\"\"\n    self.client.set(key, value, noreply=True)\n    self.stats.sets += 1\n</code></pre>"},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached.MemcachedCacheBackend.wrap","title":"wrap","text":"<pre><code>wrap(\n    func: Callable, *, config: Optional[CachedFuncConfig] = None\n) -&gt; CachedFunc\n</code></pre> <p>Wraps a function to cache its results.</p> PARAMETER DESCRIPTION <code>func</code> <p>The function to wrap.</p> <p> TYPE: <code>Callable</code> </p> <code>config</code> <p>Optional caching options for the wrapped function.</p> <p> TYPE: <code>Optional[CachedFuncConfig]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CachedFunc</code> <p>The wrapped cached function.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>def wrap(\n    self,\n    func: Callable,\n    *,\n    config: Optional[CachedFuncConfig] = None,\n) -&gt; \"CachedFunc\":\n    \"\"\"Wraps a function to cache its results.\n\n    Args:\n        func: The function to wrap.\n        config: Optional caching options for the wrapped function.\n\n    Returns:\n        The wrapped cached function.\n    \"\"\"\n    return CachedFunc(\n        func,\n        cache_backend=self,\n        config=config,\n    )\n</code></pre>"},{"location":"api/pydvl/utils/caching/memcached/#pydvl.utils.caching.memcached.MemcachedClientConfig","title":"MemcachedClientConfig  <code>dataclass</code>","text":"<pre><code>MemcachedClientConfig(\n    server: Tuple[str, int] = (\"localhost\", 11211),\n    connect_timeout: float = 1.0,\n    timeout: float = 1.0,\n    no_delay: bool = True,\n    serde: PickleSerde = PickleSerde(pickle_version=PICKLE_VERSION),\n)\n</code></pre> <p>Configuration of the memcached client.</p> PARAMETER DESCRIPTION <code>server</code> <p>A tuple of (IP|domain name, port).</p> <p> TYPE: <code>Tuple[str, int]</code> DEFAULT: <code>('localhost', 11211)</code> </p> <code>connect_timeout</code> <p>How many seconds to wait before raising <code>ConnectionRefusedError</code> on failure to connect.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>timeout</code> <p>Duration in seconds to wait for send or recv calls on the socket connected to memcached.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>no_delay</code> <p>If True, set the <code>TCP_NODELAY</code> flag, which may help with performance in some cases.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>serde</code> <p>Serializer / Deserializer (\"serde\"). The default <code>PickleSerde</code> should work in most cases. See pymemcache.client.base.Client for details.</p> <p> TYPE: <code>PickleSerde</code> DEFAULT: <code>PickleSerde(pickle_version=PICKLE_VERSION)</code> </p>"},{"location":"api/pydvl/utils/caching/memory/","title":"Memory","text":""},{"location":"api/pydvl/utils/caching/memory/#pydvl.utils.caching.memory","title":"pydvl.utils.caching.memory","text":""},{"location":"api/pydvl/utils/caching/memory/#pydvl.utils.caching.memory.InMemoryCacheBackend","title":"InMemoryCacheBackend","text":"<pre><code>InMemoryCacheBackend()\n</code></pre> <p>               Bases: <code>CacheBackend</code></p> <p>In-memory cache backend that stores results in a dictionary.</p> <p>Implements the CacheBackend interface for an in-memory-based cache. Stores cache entries as values in a dictionary, keyed by cache key. This allows sharing evaluations across threads in a single process.</p> <p>The implementation is not thread-safe.</p> ATTRIBUTE DESCRIPTION <code>cached_values</code> <p>Dictionary used to store cached values.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> Example <p>Basic usage: <pre><code>&gt;&gt;&gt; from pydvl.utils.caching.memory import InMemoryCacheBackend\n&gt;&gt;&gt; cache_backend = InMemoryCacheBackend()\n&gt;&gt;&gt; cache_backend.clear()\n&gt;&gt;&gt; value = 42\n&gt;&gt;&gt; cache_backend.set(\"key\", value)\n&gt;&gt;&gt; cache_backend.get(\"key\")\n42\n</code></pre></p> <p>Callable wrapping: <pre><code>&gt;&gt;&gt; from pydvl.utils.caching.memory import InMemoryCacheBackend\n&gt;&gt;&gt; cache_backend = InMemoryCacheBackend()\n&gt;&gt;&gt; cache_backend.clear()\n&gt;&gt;&gt; value = 42\n&gt;&gt;&gt; def foo(x: int):\n...     return x + 1\n...\n&gt;&gt;&gt; wrapped_foo = cache_backend.wrap(foo)\n&gt;&gt;&gt; wrapped_foo(value)\n43\n&gt;&gt;&gt; wrapped_foo.stats.misses\n1\n&gt;&gt;&gt; wrapped_foo.stats.hits\n0\n&gt;&gt;&gt; wrapped_foo(value)\n43\n&gt;&gt;&gt; wrapped_foo.stats.misses\n1\n&gt;&gt;&gt; wrapped_foo.stats.hits\n1\n</code></pre></p> <p>Initialize the in-memory cache backend.</p> Source code in <code>src/pydvl/utils/caching/memory.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the in-memory cache backend.\"\"\"\n    super().__init__()\n    self.cached_values: Dict[str, Any] = {}\n</code></pre>"},{"location":"api/pydvl/utils/caching/memory/#pydvl.utils.caching.memory.InMemoryCacheBackend.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Deletes cache dictionary and recreates it.</p> Source code in <code>src/pydvl/utils/caching/memory.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Deletes cache dictionary and recreates it.\"\"\"\n    del self.cached_values\n    self.cached_values = {}\n</code></pre>"},{"location":"api/pydvl/utils/caching/memory/#pydvl.utils.caching.memory.InMemoryCacheBackend.combine_hashes","title":"combine_hashes","text":"<pre><code>combine_hashes(*args: str) -&gt; str\n</code></pre> <p>Join cache key components.</p> Source code in <code>src/pydvl/utils/caching/memory.py</code> <pre><code>def combine_hashes(self, *args: str) -&gt; str:\n    \"\"\"Join cache key components.\"\"\"\n    return os.pathsep.join(args)\n</code></pre>"},{"location":"api/pydvl/utils/caching/memory/#pydvl.utils.caching.memory.InMemoryCacheBackend.get","title":"get","text":"<pre><code>get(key: str) -&gt; Optional[Any]\n</code></pre> <p>Get a value from the cache.</p> PARAMETER DESCRIPTION <code>key</code> <p>Cache key.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[Any]</code> <p>Cached value or None if not found.</p> Source code in <code>src/pydvl/utils/caching/memory.py</code> <pre><code>def get(self, key: str) -&gt; Optional[Any]:\n    \"\"\"Get a value from the cache.\n\n    Args:\n        key: Cache key.\n\n    Returns:\n        Cached value or None if not found.\n    \"\"\"\n    value = self.cached_values.get(key, None)\n    if value is not None:\n        self.stats.hits += 1\n    else:\n        self.stats.misses += 1\n    return value\n</code></pre>"},{"location":"api/pydvl/utils/caching/memory/#pydvl.utils.caching.memory.InMemoryCacheBackend.set","title":"set","text":"<pre><code>set(key: str, value: Any) -&gt; None\n</code></pre> <p>Set a value in the cache.</p> PARAMETER DESCRIPTION <code>key</code> <p>Cache key.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value to cache.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/pydvl/utils/caching/memory.py</code> <pre><code>def set(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a value in the cache.\n\n    Args:\n        key: Cache key.\n        value: Value to cache.\n    \"\"\"\n    self.cached_values[key] = value\n    self.stats.sets += 1\n</code></pre>"},{"location":"api/pydvl/utils/caching/memory/#pydvl.utils.caching.memory.InMemoryCacheBackend.wrap","title":"wrap","text":"<pre><code>wrap(\n    func: Callable, *, config: Optional[CachedFuncConfig] = None\n) -&gt; CachedFunc\n</code></pre> <p>Wraps a function to cache its results.</p> PARAMETER DESCRIPTION <code>func</code> <p>The function to wrap.</p> <p> TYPE: <code>Callable</code> </p> <code>config</code> <p>Optional caching options for the wrapped function.</p> <p> TYPE: <code>Optional[CachedFuncConfig]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>CachedFunc</code> <p>The wrapped cached function.</p> Source code in <code>src/pydvl/utils/caching/base.py</code> <pre><code>def wrap(\n    self,\n    func: Callable,\n    *,\n    config: Optional[CachedFuncConfig] = None,\n) -&gt; \"CachedFunc\":\n    \"\"\"Wraps a function to cache its results.\n\n    Args:\n        func: The function to wrap.\n        config: Optional caching options for the wrapped function.\n\n    Returns:\n        The wrapped cached function.\n    \"\"\"\n    return CachedFunc(\n        func,\n        cache_backend=self,\n        config=config,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/","title":"Data Valuation","text":""},{"location":"api/pydvl/valuation/#pydvl.valuation","title":"pydvl.valuation","text":"<p>This module collects methods for data valuation mostly based on marginal utility computation, approximations thereof or other game-theoretic methods. For a full list, see Methods.</p> <p>As supporting modules it includes subset sampling schemes, dataset handling and objects to declare and learn utilities.</p> <p>Info</p> <p>For help on how to use this module, read the introduction to data valuation.</p>"},{"location":"api/pydvl/valuation/base/","title":"Base","text":""},{"location":"api/pydvl/valuation/base/#pydvl.valuation.base","title":"pydvl.valuation.base","text":"<p>This module declares the abstract base classes for all valuation methods. A valuation method is any function that computes a value for each data point in a dataset.</p> <p>Info</p> <p>For information on data valuation, read the introduction.</p>"},{"location":"api/pydvl/valuation/base/#pydvl.valuation.base.ModelFreeValuation","title":"ModelFreeValuation","text":"<pre><code>ModelFreeValuation(references: Iterable[Dataset])\n</code></pre> <p>               Bases: <code>Valuation</code>, <code>ABC</code></p> <p>TODO: Just a stub, probably should not inherit from Valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def __init__(self, references: Iterable[Dataset]):\n    super().__init__()\n    self.datasets = references\n</code></pre>"},{"location":"api/pydvl/valuation/base/#pydvl.valuation.base.ModelFreeValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/base/#pydvl.valuation.base.Valuation","title":"Valuation","text":"<pre><code>Valuation()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for all valuation methods.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.result: ValuationResult | None = None\n</code></pre>"},{"location":"api/pydvl/valuation/base/#pydvl.valuation.base.Valuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/","title":"Dataset","text":""},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset","title":"pydvl.valuation.dataset","text":"<p>This module contains convenience classes to handle data and groups thereof.</p> <p>Value computations with supervised models benefit from a unified interface to handle data. This module provides two classes to handle data and labels, as well as feature names and other information:</p> <ul> <li>Dataset</li> <li>GroupedDataset.</li> </ul> <p>Objects of both types can be used to construct scorers (for the valuation set) and to <code>fit</code> (most) valuation methods.</p> <p>The underlying data arrays can always be accessed (read-only) via Dataset.data(), which returns the tuple <code>(x, y)</code>.</p> <p>Logical vs data indices</p> <p>Dataset and GroupedDataset use two different types of indices: * Logical indices: These are the indices used to access the elements in the   <code>(Grouped)Dataset</code> objects when slicing or indexing them. * Data indices: These are the indices used to access the data points in the     underlying data arrays. They are the indices used in the     RawData object returned by     Dataset.data().</p>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset--slicing","title":"Slicing","text":"<p>Slicing a Dataset object, e.g. <code>dataset[0]</code>, will return a new <code>Dataset</code> with the data corresponding to that slice. Note however that the contents of the new object, i.e. <code>dataset[0].data().x</code>, may not be the same as <code>dataset.data().x[0]</code>, which is the first point in the original data array. This is in particular true for GroupedDatasets where one \"logical\" index may correspond to multiple data points.</p> <p>Slicing with <code>None</code>, i.e. <code>dataset[None]</code>, will return a copy of the whole dataset.</p>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset--grouped-datasets-and-logical-indices","title":"Grouped datasets and logical indices","text":"<p>As mentioned above, it is also possible to group data points together with GroupedDataset. In order to handle groups correctly, Datasets map \"logical\" indices to \"data\" indices and vice versa. The latter correspond to indices in the data arrays themselves, while the former may map to groups of data points.</p> <p>A call to GroupedDataset.data(indices) will return the data and labels of all samples for the given groups. But <code>grouped_data[0]</code> will return the data and labels of the first group, not the first data point and will therefore be in general different from <code>grouped_data.data([0])</code>.</p> <p>Grouping data can be useful to reduce computation time, e.g. for Shapley-based methods, or to look at the importance of certain feature sets for the model.</p> <p>Tip</p> <p>It is important to keep in mind the distinction between logical and data indices for valuation methods that require computation on individual data points, like KNNShapleyValuation or DataOOBValuation. In these cases, the logical indices are used to compute the Shapley values, while the data indices are used internally by the method.</p>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.Dataset","title":"Dataset","text":"<pre><code>Dataset(\n    x: NDArray,\n    y: NDArray,\n    feature_names: Sequence[str] | NDArray[str_] | None = None,\n    target_names: Sequence[str] | NDArray[str_] | None = None,\n    data_names: Sequence[str] | NDArray[str_] | None = None,\n    description: str | None = None,\n    multi_output: bool = False,\n)\n</code></pre> <p>A convenience class to handle datasets.</p> <p>It holds a dataset, together with info on feature names, target names, and data names. It is used to pass data around to valuation methods.</p> <p>The underlying data arrays can be accessed via Dataset.data(), which returns the tuple <code>(X, y)</code> as a read-only RawData object. The data can be accessed by indexing the object directly, e.g. <code>dataset[0]</code> will return the data point corresponding to index 0 in <code>dataset</code>. For this base class, this is the same as <code>dataset.data([0])</code>, which is the first point in the data array, but derived classes can behave differently.</p> PARAMETER DESCRIPTION <code>x</code> <p>training data</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>labels for training data</p> <p> TYPE: <code>NDArray</code> </p> <code>feature_names</code> <p>names of the features of x data</p> <p> TYPE: <code>Sequence[str] | NDArray[str_] | None</code> DEFAULT: <code>None</code> </p> <code>target_names</code> <p>names of the features of y data</p> <p> TYPE: <code>Sequence[str] | NDArray[str_] | None</code> DEFAULT: <code>None</code> </p> <code>data_names</code> <p>names assigned to data points. For example, if the dataset is a time series, each entry can be a timestamp which can be referenced directly instead of using a row number.</p> <p> TYPE: <code>Sequence[str] | NDArray[str_] | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A textual description of the dataset.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>multi_output</code> <p>set to <code>False</code> if labels are scalars, or to <code>True</code> if they are vectors of dimension &gt; 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Changed in version 0.10.0</p> <p>No longer holds split data, but only x, y.</p> <p>Changed in version 0.10.0</p> <p>Slicing now return a new <code>Dataset</code> object, not raw data.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def __init__(\n    self,\n    x: NDArray,\n    y: NDArray,\n    feature_names: Sequence[str] | NDArray[np.str_] | None = None,\n    target_names: Sequence[str] | NDArray[np.str_] | None = None,\n    data_names: Sequence[str] | NDArray[np.str_] | None = None,\n    description: str | None = None,\n    multi_output: bool = False,\n):\n    self._x, self._y = check_X_y(\n        x, y, multi_output=multi_output, estimator=\"Dataset\"\n    )\n\n    def make_names(s: str, a: np.ndarray) -&gt; list[str]:\n        n = a.shape[1] if len(a.shape) &gt; 1 else 1\n        return [f\"{s}{i:0{1 + int(np.log10(n))}d}\" for i in range(1, n + 1)]\n\n    self.feature_names = (\n        list(feature_names) if feature_names is not None else make_names(\"x\", x)\n    )\n    self.target_names = (\n        list(target_names) if target_names is not None else make_names(\"y\", y)\n    )\n\n    if len(self._x.shape) &gt; 1:\n        if len(self.feature_names) != self._x.shape[-1]:\n            raise ValueError(\"Mismatching number of features and names\")\n    if len(self._y.shape) &gt; 1:\n        if len(self.target_names) != self._y.shape[-1]:\n            raise ValueError(\"Mismatching number of targets and names\")\n\n    self.description = description or \"No description\"\n    self._indices = np.arange(len(self._x), dtype=np.int_)\n    self._data_names = (\n        np.array(data_names, dtype=np.str_)\n        if data_names is not None\n        else self._indices.astype(np.str_)\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.Dataset.indices","title":"indices  <code>property</code>","text":"<pre><code>indices: NDArray[int_]\n</code></pre> <p>Index of positions in data.x_train.</p> <p>Contiguous integers from 0 to len(Dataset).</p>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.Dataset.n_features","title":"n_features  <code>property</code>","text":"<pre><code>n_features: int\n</code></pre> <p>Returns the number of dimensions of a sample.</p>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.Dataset.names","title":"names  <code>property</code>","text":"<pre><code>names: NDArray[str_]\n</code></pre> <p>Names of each individual datapoint.</p> <p>Used for reporting Shapley values.</p>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.Dataset.data","title":"data","text":"<pre><code>data(\n    indices: int | slice | Sequence[int] | NDArray[int_] | None = None,\n) -&gt; RawData\n</code></pre> <p>Given a set of indices, returns the training data that refer to those indices, as a read-only tuple-like structure.</p> <p>This is used mainly by subclasses of UtilityBase to retrieve subsets of the data from indices.</p> PARAMETER DESCRIPTION <code>indices</code> <p>Optional indices that will be used to select points from the training data. If <code>None</code>, the entire training data will be returned.</p> <p> TYPE: <code>int | slice | Sequence[int] | NDArray[int_] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>RawData</code> <p>If <code>indices</code> is not <code>None</code>, the selected x and y arrays from the training data. Otherwise, the entire dataset.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def data(\n    self, indices: int | slice | Sequence[int] | NDArray[np.int_] | None = None\n) -&gt; RawData:\n    \"\"\"Given a set of indices, returns the training data that refer to those\n    indices, as a read-only tuple-like structure.\n\n    This is used mainly by subclasses of\n    [UtilityBase][pydvl.valuation.utility.base.UtilityBase] to retrieve subsets of\n    the data from indices.\n\n    Args:\n        indices: Optional indices that will be used to select points from\n            the training data. If `None`, the entire training data will be\n            returned.\n\n    Returns:\n        If `indices` is not `None`, the selected x and y arrays from the\n            training data. Otherwise, the entire dataset.\n    \"\"\"\n    if indices is None:\n        return RawData(self._x, self._y)\n    return RawData(self._x[indices], self._y[indices])\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.Dataset.data_indices","title":"data_indices","text":"<pre><code>data_indices(indices: Sequence[int] | None = None) -&gt; NDArray[int_]\n</code></pre> <p>Returns a subset of indices.</p> <p>This is equivalent to using <code>Dataset.indices[logical_indices]</code> but allows subclasses to define special behaviour, e.g. when indices in <code>Dataset</code> do not match the indices in the data.</p> <p>For <code>Dataset</code>, this is a simple pass-through.</p> PARAMETER DESCRIPTION <code>indices</code> <p>A set of indices held by this object</p> <p> TYPE: <code>Sequence[int] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NDArray[int_]</code> <p>The indices of the data points in the data array.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def data_indices(self, indices: Sequence[int] | None = None) -&gt; NDArray[np.int_]:\n    \"\"\"Returns a subset of indices.\n\n    This is equivalent to using `Dataset.indices[logical_indices]` but allows\n    subclasses to define special behaviour, e.g. when indices in `Dataset` do not\n    match the indices in the data.\n\n    For `Dataset`, this is a simple pass-through.\n\n    Args:\n        indices: A set of indices held by this object\n\n    Returns:\n        The indices of the data points in the data array.\n    \"\"\"\n    if indices is None:\n        return self._indices\n    return self._indices[indices]\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.Dataset.feature","title":"feature","text":"<pre><code>feature(name: str) -&gt; tuple[slice, int]\n</code></pre> <p>Returns a slice for the feature with the given name.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def feature(self, name: str) -&gt; tuple[slice, int]:\n    \"\"\"Returns a slice for the feature with the given name.\"\"\"\n    try:\n        return np.index_exp[:, self.feature_names.index(name)]  # type: ignore\n    except ValueError:\n        raise ValueError(f\"Feature {name} is not in {self.feature_names}\")\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.Dataset.from_arrays","title":"from_arrays  <code>classmethod</code>","text":"<pre><code>from_arrays(\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any\n) -&gt; tuple[Dataset, Dataset]\n</code></pre> <p>Constructs a Dataset object from X and y numpy arrays  as returned by the <code>make_*</code> functions in sklearn generated datasets.</p> Example <pre><code>&gt;&gt;&gt; from pydvl.valuation.dataset import Dataset\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression()\n&gt;&gt;&gt; dataset = Dataset.from_arrays(X, y)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>numpy array of shape (n_samples, n_features)</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>numpy array of shape (n_samples,)</p> <p> TYPE: <code>NDArray</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code></p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>random_state</code> <p>seed for train / test split</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>stratify_by_target</code> <p>If <code>True</code>, data is split in a stratified fashion, using the y variable as labels. Read more in sklearn's user guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Dataset constructor. Use this to pass e.g. <code>feature_names</code> or <code>target_names</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Dataset, Dataset]</code> <p>Object with the passed X and y arrays split across training and test sets.</p> <p>New in version 0.4.0</p> <p>Changed in version 0.6.0</p> <p>Added kwargs to pass to the Dataset constructor.</p> <p>Changed in version 0.10.0</p> <p>Returns a tuple of two Dataset objects.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>@classmethod\ndef from_arrays(\n    cls,\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any,\n) -&gt; tuple[Dataset, Dataset]:\n    \"\"\"Constructs a [Dataset][pydvl.valuation.dataset.Dataset] object from X and y numpy arrays  as\n    returned by the `make_*` functions in [sklearn generated datasets](https://scikit-learn.org/stable/datasets/sample_generators.html).\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from pydvl.valuation.dataset import Dataset\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression()\n        &gt;&gt;&gt; dataset = Dataset.from_arrays(X, y)\n        ```\n\n    Args:\n        X: numpy array of shape (n_samples, n_features)\n        y: numpy array of shape (n_samples,)\n        train_size: size of the training dataset. Used in `train_test_split`\n        random_state: seed for train / test split\n        stratify_by_target: If `True`, data is split in a stratified fashion,\n            using the y variable as labels. Read more in [sklearn's user\n            guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.valuation.dataset.Dataset] constructor. Use this to pass\n            e.g. `feature_names` or `target_names`.\n\n    Returns:\n        Object with the passed X and y arrays split across training and test sets.\n\n    !!! tip \"New in version 0.4.0\"\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added kwargs to pass to the [Dataset][pydvl.valuation.dataset.Dataset] constructor.\n\n    !!! tip \"Changed in version 0.10.0\"\n        Returns a tuple of two [Dataset][pydvl.valuation.dataset.Dataset] objects.\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(\n        X,\n        y,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=y if stratify_by_target else None,\n    )\n    return cls(x_train, y_train, **kwargs), cls(x_test, y_test, **kwargs)\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.Dataset.from_sklearn","title":"from_sklearn  <code>classmethod</code>","text":"<pre><code>from_sklearn(\n    data: Bunch,\n    train_size: int | float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    **kwargs\n) -&gt; tuple[Dataset, Dataset]\n</code></pre> <p>Constructs two Dataset objects from a sklearn.utils.Bunch, as returned by the <code>load_*</code> functions in scikit-learn toy datasets.</p> Example <pre><code>&gt;&gt;&gt; from pydvl.valuation.dataset import Dataset\n&gt;&gt;&gt; from sklearn.datasets import load_boston  # noqa\n&gt;&gt;&gt; train, test = Dataset.from_sklearn(load_boston())\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>scikit-learn Bunch object. The following attributes are supported:</p> <ul> <li><code>data</code>: covariates.</li> <li><code>target</code>: target variables (labels).</li> <li><code>feature_names</code> (optional): the feature names.</li> <li><code>target_names</code> (optional): the target names.</li> <li><code>DESCR</code> (optional): a description.</li> </ul> <p> TYPE: <code>Bunch</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code> float values represent the fraction of the dataset to include in the training split and should be in (0,1). An integer value sets the absolute number of training samples.</p> <p> TYPE: <code>int | float</code> DEFAULT: <code>0.8</code> </p> <p>the value is automatically set to the complement of the test size.     random_state: seed for train / test split     stratify_by_target: If <code>True</code>, data is split in a stratified         fashion, using the target variable as labels. Read more in         scikit-learn's user guide.     kwargs: Additional keyword arguments to pass to the         Dataset constructor. Use this to pass e.g. <code>is_multi_output</code>.</p> RETURNS DESCRIPTION <code>tuple[Dataset, Dataset]</code> <p>Object with the sklearn dataset</p> <p>Changed in version 0.6.0</p> <p>Added kwargs to pass to the Dataset constructor.</p> <p>Changed in version 0.10.0</p> <p>Returns a tuple of two Dataset objects.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>@classmethod\ndef from_sklearn(\n    cls,\n    data: Bunch,\n    train_size: int | float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    **kwargs,\n) -&gt; tuple[Dataset, Dataset]:\n    \"\"\"Constructs two [Dataset][pydvl.valuation.dataset.Dataset] objects from a\n    [sklearn.utils.Bunch][], as returned by the `load_*`\n    functions in [scikit-learn toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html).\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from pydvl.valuation.dataset import Dataset\n        &gt;&gt;&gt; from sklearn.datasets import load_boston  # noqa\n        &gt;&gt;&gt; train, test = Dataset.from_sklearn(load_boston())\n        ```\n\n    Args:\n        data: scikit-learn Bunch object. The following attributes are supported:\n\n            - `data`: covariates.\n            - `target`: target variables (labels).\n            - `feature_names` (**optional**): the feature names.\n            - `target_names` (**optional**): the target names.\n            - `DESCR` (**optional**): a description.\n        train_size: size of the training dataset. Used in `train_test_split`\n            float values represent the fraction of the dataset to include in the\n            training split and should be in (0,1). An integer value sets the\n            absolute number of training samples.\n    the value is automatically set to the complement of the test size.\n        random_state: seed for train / test split\n        stratify_by_target: If `True`, data is split in a stratified\n            fashion, using the target variable as labels. Read more in\n            [scikit-learn's user guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.valuation.dataset.Dataset] constructor. Use this to pass e.g. `is_multi_output`.\n\n    Returns:\n        Object with the sklearn dataset\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added kwargs to pass to the [Dataset][pydvl.valuation.dataset.Dataset] constructor.\n    !!! tip \"Changed in version 0.10.0\"\n        Returns a tuple of two [Dataset][pydvl.valuation.dataset.Dataset] objects.\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(\n        data.data,\n        data.target,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=data.target if stratify_by_target else None,\n    )\n    return (\n        cls(\n            x_train,\n            y_train,\n            feature_names=data.get(\"feature_names\"),\n            target_names=data.get(\"target_names\"),\n            description=data.get(\"DESCR\"),\n            **kwargs,\n        ),\n        cls(\n            x_test,\n            y_test,\n            feature_names=data.get(\"feature_names\"),\n            target_names=data.get(\"target_names\"),\n            description=data.get(\"DESCR\"),\n            **kwargs,\n        ),\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.Dataset.logical_indices","title":"logical_indices","text":"<pre><code>logical_indices(indices: Sequence[int] | None = None) -&gt; NDArray[int_]\n</code></pre> <p>Returns the indices in this <code>Dataset</code> for the given indices in the data array.</p> <p>This is equivalent to using <code>Dataset.indices[data_indices]</code> but allows subclasses to define special behaviour, e.g. when indices in <code>Dataset</code> do not match the indices in the data.</p> PARAMETER DESCRIPTION <code>indices</code> <p>A set of indices in the data array.</p> <p> TYPE: <code>Sequence[int] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NDArray[int_]</code> <p>The abstract indices for the given data indices.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def logical_indices(self, indices: Sequence[int] | None = None) -&gt; NDArray[np.int_]:\n    \"\"\"Returns the indices in this `Dataset` for the given indices in the data array.\n\n    This is equivalent to using `Dataset.indices[data_indices]` but allows\n    subclasses to define special behaviour, e.g. when indices in `Dataset` do not\n    match the indices in the data.\n\n    Args:\n        indices: A set of indices in the data array.\n\n    Returns:\n        The abstract indices for the given data indices.\n    \"\"\"\n    if indices is None:\n        return self._indices\n    return self._indices[indices]\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset","title":"GroupedDataset","text":"<pre><code>GroupedDataset(\n    x: NDArray,\n    y: NDArray,\n    data_groups: Sequence[int] | NDArray[int_],\n    feature_names: Sequence[str] | NDArray[str_] | None = None,\n    target_names: Sequence[str] | NDArray[str_] | None = None,\n    data_names: Sequence[str] | NDArray[str_] | None = None,\n    group_names: Sequence[str] | NDArray[str_] | None = None,\n    description: str | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Class for grouping datasets.</p> <p>Used for calculating values of subsets of the data considered as logical units. For instance, one can group by value of a categorical feature, by bin into which a continuous feature falls, or by label.</p> PARAMETER DESCRIPTION <code>x</code> <p>training data</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>labels of training data</p> <p> TYPE: <code>NDArray</code> </p> <code>data_groups</code> <p>Sequence of the same length as <code>x_train</code> containing a group id for each training data point. Data points with the same id will then be grouped by this object and considered as one for effects of valuation. Group ids are assumed to be zero-based consecutive integers</p> <p> TYPE: <code>Sequence[int] | NDArray[int_]</code> </p> <code>feature_names</code> <p>names of the covariates' features.</p> <p> TYPE: <code>Sequence[str] | NDArray[str_] | None</code> DEFAULT: <code>None</code> </p> <code>target_names</code> <p>names of the labels or targets y</p> <p> TYPE: <code>Sequence[str] | NDArray[str_] | None</code> DEFAULT: <code>None</code> </p> <code>data_names</code> <p>names of the data points. For example, if the dataset is a time series, each entry can be a timestamp.</p> <p> TYPE: <code>Sequence[str] | NDArray[str_] | None</code> DEFAULT: <code>None</code> </p> <code>group_names</code> <p>names of the groups. If not provided, the numerical group ids from <code>data_groups</code> will be used.</p> <p> TYPE: <code>Sequence[str] | NDArray[str_] | None</code> DEFAULT: <code>None</code> </p> <code>description</code> <p>A textual description of the dataset</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Dataset constructor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Changed in version 0.6.0</p> <p>Added <code>group_names</code> and forwarding of <code>kwargs</code></p> <p>Changed in version 0.10.0</p> <p>No longer holds split data, but only x, y and group information. Added     methods to retrieve indices for groups and vice versa.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def __init__(\n    self,\n    x: NDArray,\n    y: NDArray,\n    data_groups: Sequence[int] | NDArray[np.int_],\n    feature_names: Sequence[str] | NDArray[np.str_] | None = None,\n    target_names: Sequence[str] | NDArray[np.str_] | None = None,\n    data_names: Sequence[str] | NDArray[np.str_] | None = None,\n    group_names: Sequence[str] | NDArray[np.str_] | None = None,\n    description: str | None = None,\n    **kwargs: Any,\n):\n    \"\"\"Class for grouping datasets.\n\n    Used for calculating values of subsets of the data considered as logical units.\n    For instance, one can group by value of a categorical feature, by bin into which\n    a continuous feature falls, or by label.\n\n    Args:\n        x: training data\n        y: labels of training data\n        data_groups: Sequence of the same length as `x_train` containing\n            a group id for each training data point. Data points with the same\n            id will then be grouped by this object and considered as one for\n            effects of valuation. Group ids are assumed to be zero-based consecutive\n            integers\n        feature_names: names of the covariates' features.\n        target_names: names of the labels or targets y\n        data_names: names of the data points. For example, if the dataset is a\n            time series, each entry can be a timestamp.\n        group_names: names of the groups. If not provided, the numerical group ids\n            from `data_groups` will be used.\n        description: A textual description of the dataset\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.valuation.dataset.Dataset] constructor.\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added `group_names` and forwarding of `kwargs`\n\n    !!! tip \"Changed in version 0.10.0\"\n        No longer holds split data, but only x, y and group information. Added\n            methods to retrieve indices for groups and vice versa.\n    \"\"\"\n    super().__init__(\n        x=x,\n        y=y,\n        feature_names=feature_names,\n        target_names=target_names,\n        data_names=data_names,\n        description=description,\n        **kwargs,\n    )\n\n    if len(data_groups) != len(x):\n        raise ValueError(\n            f\"data_groups and x must have the same length.\"\n            f\"Instead got {len(data_groups)=} and {len(x)=}\"\n        )\n\n    # data index -&gt; abstract index (group id)\n    try:\n        self.data_to_group: NDArray[np.int_] = np.array(data_groups, dtype=int)\n    except ValueError as e:\n        raise ValueError(\n            \"data_groups must be a mapping from integer data indices to integer group ids\"\n        ) from e\n    # abstract index (group id) -&gt; data index\n    self.group_to_data: OrderedDict[int, list[int]] = OrderedDict(\n        {k: [] for k in set(data_groups)}\n    )\n    for data_idx, group_idx in enumerate(self.data_to_group):\n        self.group_to_data[group_idx].append(data_idx)  # type: ignore\n    self._indices = np.array(list(self.group_to_data.keys()), dtype=np.int_)\n    self._group_names = (\n        np.array(group_names, dtype=np.str_)\n        if group_names is not None\n        else np.array(list(self.group_to_data.keys()), dtype=np.str_)\n    )\n    if len(self._group_names) != len(self.group_to_data):\n        raise ValueError(\n            f\"The number of group names ({len(self._group_names)}) \"\n            f\"does not match the number of groups ({len(self.group_to_data)})\"\n        )\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset.indices","title":"indices  <code>property</code>","text":"<pre><code>indices\n</code></pre> <p>Indices of the groups.</p>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset.n_features","title":"n_features  <code>property</code>","text":"<pre><code>n_features: int\n</code></pre> <p>Returns the number of dimensions of a sample.</p>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset.names","title":"names  <code>property</code>","text":"<pre><code>names: NDArray[str_]\n</code></pre> <p>Names of the groups.</p>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset.data","title":"data","text":"<pre><code>data(\n    indices: int | slice | Sequence[int] | NDArray[int_] | None = None,\n) -&gt; RawData\n</code></pre> <p>Returns the data and labels of all samples in the given groups.</p> PARAMETER DESCRIPTION <code>indices</code> <p>group indices whose elements to return. If <code>None</code>, all data from all groups are returned.</p> <p> TYPE: <code>int | slice | Sequence[int] | NDArray[int_] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>RawData</code> <p>Tuple of training data <code>x</code> and labels <code>y</code>.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def data(\n    self, indices: int | slice | Sequence[int] | NDArray[np.int_] | None = None\n) -&gt; RawData:\n    \"\"\"Returns the data and labels of all samples in the given groups.\n\n    Args:\n        indices: group indices whose elements to return. If `None`,\n            all data from all groups are returned.\n\n    Returns:\n        Tuple of training data `x` and labels `y`.\n    \"\"\"\n    return super().data(self.data_indices(indices))\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset.data_indices","title":"data_indices","text":"<pre><code>data_indices(\n    indices: int | slice | Sequence[int] | NDArray[int_] | None = None,\n) -&gt; NDArray[int_]\n</code></pre> <p>Returns the indices of the samples in the given groups.</p> PARAMETER DESCRIPTION <code>indices</code> <p>group indices whose elements to return. If <code>None</code>, all indices from all groups are returned.</p> <p> TYPE: <code>int | slice | Sequence[int] | NDArray[int_] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NDArray[int_]</code> <p>Indices of the samples in the given groups.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def data_indices(\n    self, indices: int | slice | Sequence[int] | NDArray[np.int_] | None = None\n) -&gt; NDArray[np.int_]:\n    \"\"\"Returns the indices of the samples in the given groups.\n\n    Args:\n        indices: group indices whose elements to return. If `None`,\n            all indices from all groups are returned.\n\n    Returns:\n        Indices of the samples in the given groups.\n    \"\"\"\n    if indices is None:\n        indices = self._indices\n    if isinstance(indices, slice):\n        indices = range(*indices.indices(len(self.group_to_data)))\n    return np.concatenate([self.group_to_data[i] for i in indices], dtype=np.int_)  # type: ignore\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset.feature","title":"feature","text":"<pre><code>feature(name: str) -&gt; tuple[slice, int]\n</code></pre> <p>Returns a slice for the feature with the given name.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def feature(self, name: str) -&gt; tuple[slice, int]:\n    \"\"\"Returns a slice for the feature with the given name.\"\"\"\n    try:\n        return np.index_exp[:, self.feature_names.index(name)]  # type: ignore\n    except ValueError:\n        raise ValueError(f\"Feature {name} is not in {self.feature_names}\")\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset.from_arrays","title":"from_arrays  <code>classmethod</code>","text":"<pre><code>from_arrays(\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    **kwargs\n) -&gt; tuple[GroupedDataset, GroupedDataset]\n</code></pre><pre><code>from_arrays(\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    data_groups: Sequence[int] | None = None,\n    **kwargs\n) -&gt; tuple[GroupedDataset, GroupedDataset]\n</code></pre> <pre><code>from_arrays(\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    data_groups: Sequence[int] | None = None,\n    **kwargs: Any\n) -&gt; tuple[GroupedDataset, GroupedDataset]\n</code></pre> <p>Constructs a GroupedDataset object, and an ungrouped Dataset object from X and y numpy arrays as returned by the <code>make_*</code> functions in scikit-learn generated datasets.</p> Example <pre><code>&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; from pydvl.valuation.dataset import GroupedDataset\n&gt;&gt;&gt; X, y = make_classification(\n...     n_samples=100,\n...     n_features=4,\n...     n_informative=2,\n...     n_redundant=0,\n...     random_state=0,\n...     shuffle=False\n... )\n&gt;&gt;&gt; data_groups = X[:, 0] // 0.5\n&gt;&gt;&gt; train, test = GroupedDataset.from_arrays(X, y, data_groups=data_groups)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>array of shape (n_samples, n_features)</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>array of shape (n_samples,)</p> <p> TYPE: <code>NDArray</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>random_state</code> <p>seed for train / test split.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>stratify_by_target</code> <p>If <code>True</code>, data is split in a stratified fashion, using the y variable as labels. Read more in sklearn's user guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>data_groups</code> <p>an array holding the group index or name for each data point. The length of this array must be equal to the number of data points in the dataset.</p> <p> TYPE: <code>Sequence[int] | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments that will be passed to the GroupedDataset constructor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[GroupedDataset, GroupedDataset]</code> <p>Dataset with the passed X and y arrays split across training and test sets.</p> <p>New in version 0.4.0</p> <p>Changed in version 0.6.0</p> <p>Added kwargs to pass to the     GroupedDataset constructor.</p> <p>Changed in version 0.10.0</p> <p>Returns a tuple of two     GroupedDataset objects.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>@classmethod\ndef from_arrays(\n    cls,\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    data_groups: Sequence[int] | None = None,\n    **kwargs: Any,\n) -&gt; tuple[GroupedDataset, GroupedDataset]:\n    \"\"\"Constructs a [GroupedDataset][pydvl.valuation.dataset.GroupedDataset] object,\n    and an ungrouped [Dataset][pydvl.valuation.dataset.Dataset] object from X and y\n    numpy arrays as returned by the `make_*` functions in\n    [scikit-learn generated datasets](https://scikit-learn.org/stable/datasets/sample_generators.html).\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from sklearn.datasets import make_classification\n        &gt;&gt;&gt; from pydvl.valuation.dataset import GroupedDataset\n        &gt;&gt;&gt; X, y = make_classification(\n        ...     n_samples=100,\n        ...     n_features=4,\n        ...     n_informative=2,\n        ...     n_redundant=0,\n        ...     random_state=0,\n        ...     shuffle=False\n        ... )\n        &gt;&gt;&gt; data_groups = X[:, 0] // 0.5\n        &gt;&gt;&gt; train, test = GroupedDataset.from_arrays(X, y, data_groups=data_groups)\n        ```\n\n    Args:\n        X: array of shape (n_samples, n_features)\n        y: array of shape (n_samples,)\n        train_size: size of the training dataset. Used in `train_test_split`.\n        random_state: seed for train / test split.\n        stratify_by_target: If `True`, data is split in a stratified\n            fashion, using the y variable as labels. Read more in\n            [sklearn's user guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        data_groups: an array holding the group index or name for each data\n            point. The length of this array must be equal to the number of\n            data points in the dataset.\n        kwargs: Additional keyword arguments that will be passed to the\n            [GroupedDataset][pydvl.valuation.dataset.GroupedDataset] constructor.\n\n    Returns:\n        Dataset with the passed X and y arrays split across training and\n            test sets.\n\n    !!! tip \"New in version 0.4.0\"\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added kwargs to pass to the\n            [GroupedDataset][pydvl.valuation.dataset.GroupedDataset] constructor.\n\n    !!! tip \"Changed in version 0.10.0\"\n        Returns a tuple of two\n            [GroupedDataset][pydvl.valuation.dataset.GroupedDataset] objects.\n    \"\"\"\n\n    if data_groups is None:\n        raise ValueError(\n            \"data_groups must be provided when constructing a GroupedDataset\"\n        )\n    x_train, x_test, y_train, y_test, groups_train, groups_test = train_test_split(\n        X,\n        y,\n        data_groups,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=y if stratify_by_target else None,\n    )\n    training_set = cls(x=x_train, y=y_train, data_groups=groups_train, **kwargs)\n    test_set = cls(x=x_test, y=y_test, data_groups=groups_test, **kwargs)\n    return training_set, test_set\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset.from_dataset","title":"from_dataset  <code>classmethod</code>","text":"<pre><code>from_dataset(\n    data: Dataset,\n    data_groups: Sequence[int] | NDArray[int_],\n    group_names: Sequence[str] | NDArray[str_] | None = None,\n    **kwargs: Any\n) -&gt; GroupedDataset\n</code></pre> <p>Creates a GroupedDataset object from a Dataset object and a mapping of data groups.</p> Example <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pydvl.valuation.dataset import Dataset, GroupedDataset\n&gt;&gt;&gt; train, test = Dataset.from_arrays(\n...     X=np.asarray([[1, 2], [3, 4], [5, 6], [7, 8]]),\n...     y=np.asarray([0, 1, 0, 1]),\n... )\n&gt;&gt;&gt; grouped_train = GroupedDataset.from_dataset(train, data_groups=[0, 0, 1, 1])\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>The original data.</p> <p> TYPE: <code>Dataset</code> </p> <code>data_groups</code> <p>An array holding the group index or name for each data point. The length of this array must be equal to the number of data points in the dataset.</p> <p> TYPE: <code>Sequence[int] | NDArray[int_]</code> </p> <code>group_names</code> <p>Names of the groups. If not provided, the numerical group ids from <code>data_groups</code> will be used.</p> <p> TYPE: <code>Sequence[str] | NDArray[str_] | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional arguments to be passed to the GroupedDataset constructor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>GroupedDataset</code> <p>A GroupedDataset with the initial Dataset grouped by <code>data_groups</code>.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>@classmethod\ndef from_dataset(\n    cls,\n    data: Dataset,\n    data_groups: Sequence[int] | NDArray[np.int_],\n    group_names: Sequence[str] | NDArray[np.str_] | None = None,\n    **kwargs: Any,\n) -&gt; GroupedDataset:\n    \"\"\"Creates a [GroupedDataset][pydvl.valuation.dataset.GroupedDataset] object from a\n    [Dataset][pydvl.valuation.dataset.Dataset] object and a mapping of data groups.\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from pydvl.valuation.dataset import Dataset, GroupedDataset\n        &gt;&gt;&gt; train, test = Dataset.from_arrays(\n        ...     X=np.asarray([[1, 2], [3, 4], [5, 6], [7, 8]]),\n        ...     y=np.asarray([0, 1, 0, 1]),\n        ... )\n        &gt;&gt;&gt; grouped_train = GroupedDataset.from_dataset(train, data_groups=[0, 0, 1, 1])\n        ```\n\n    Args:\n        data: The original data.\n        data_groups: An array holding the group index or name for each data\n            point. The length of this array must be equal to the number of\n            data points in the dataset.\n        group_names: Names of the groups. If not provided, the numerical group ids\n            from `data_groups` will be used.\n        kwargs: Additional arguments to be passed to the\n            [GroupedDataset][pydvl.valuation.dataset.GroupedDataset] constructor.\n\n    Returns:\n        A [GroupedDataset][pydvl.valuation.dataset.GroupedDataset] with the initial\n            [Dataset][pydvl.valuation.dataset.Dataset] grouped by `data_groups`.\n    \"\"\"\n    return cls(\n        x=data._x,\n        y=data._y,\n        data_groups=data_groups,\n        feature_names=data.feature_names,\n        target_names=data.target_names,\n        description=data.description,\n        group_names=group_names,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset.from_sklearn","title":"from_sklearn  <code>classmethod</code>","text":"<pre><code>from_sklearn(\n    data: Bunch,\n    train_size: float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    **kwargs\n) -&gt; tuple[GroupedDataset, GroupedDataset]\n</code></pre><pre><code>from_sklearn(\n    data: Bunch,\n    train_size: float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    data_groups: Sequence[int] | None = None,\n    **kwargs\n) -&gt; tuple[GroupedDataset, GroupedDataset]\n</code></pre> <pre><code>from_sklearn(\n    data: Bunch,\n    train_size: int | float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    data_groups: Sequence[int] | None = None,\n    **kwargs: dict[str, Any]\n) -&gt; tuple[GroupedDataset, GroupedDataset]\n</code></pre> <p>Constructs a GroupedDataset object, and an ungrouped Dataset object from a sklearn.utils.Bunch as returned by the <code>load_*</code> functions in scikit-learn toy datasets and groups it.</p> Example <pre><code>&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from pydvl.valuation.dataset import GroupedDataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; data_groups = iris.test_data[:, 0] // 0.5\n&gt;&gt;&gt; train, test = GroupedDataset.from_sklearn(iris, data_groups=data_groups)\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>scikit-learn Bunch object. The following attributes are supported: - <code>data</code>: covariates. - <code>target</code>: target variables (labels). - <code>feature_names</code> (optional): the feature names. - <code>target_names</code> (optional): the target names. - <code>DESCR</code> (optional): a description.</p> <p> TYPE: <code>Bunch</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code> float values represent the fraction of the dataset to include in the training split and should be in (0,1). An integer value sets the absolute number of training samples.</p> <p> TYPE: <code>int | float</code> DEFAULT: <code>0.8</code> </p> <code>random_state</code> <p>seed for train / test split.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>stratify_by_target</code> <p>If <code>True</code>, data is split in a stratified fashion, using the target variable as labels. Read more in sklearn's user guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>data_groups</code> <p>an array holding the group index or name for each data point. The length of this array must be equal to the number of data points in the dataset.</p> <p> TYPE: <code>Sequence[int] | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Dataset constructor.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[GroupedDataset, GroupedDataset]</code> <p>Datasets with the selected sklearn data</p> <p>Changed in version 0.10.0</p> <p>Returns a tuple of two GroupedDataset     objects.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>@classmethod\ndef from_sklearn(\n    cls,\n    data: Bunch,\n    train_size: int | float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    data_groups: Sequence[int] | None = None,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[GroupedDataset, GroupedDataset]:\n    \"\"\"Constructs a [GroupedDataset][pydvl.valuation.dataset.GroupedDataset] object, and an\n    ungrouped [Dataset][pydvl.valuation.dataset.Dataset] object from a\n    [sklearn.utils.Bunch][sklearn.utils.Bunch] as returned by the `load_*` functions in\n    [scikit-learn toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) and groups\n    it.\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from sklearn.datasets import load_iris\n        &gt;&gt;&gt; from pydvl.valuation.dataset import GroupedDataset\n        &gt;&gt;&gt; iris = load_iris()\n        &gt;&gt;&gt; data_groups = iris.test_data[:, 0] // 0.5\n        &gt;&gt;&gt; train, test = GroupedDataset.from_sklearn(iris, data_groups=data_groups)\n        ```\n\n    Args:\n        data: scikit-learn Bunch object. The following attributes are supported:\n            - `data`: covariates.\n            - `target`: target variables (labels).\n            - `feature_names` (**optional**): the feature names.\n            - `target_names` (**optional**): the target names.\n            - `DESCR` (**optional**): a description.\n        train_size: size of the training dataset. Used in `train_test_split`\n            float values represent the fraction of the dataset to include in the\n            training split and should be in (0,1). An integer value sets the\n            absolute number of training samples.\n        random_state: seed for train / test split.\n        stratify_by_target: If `True`, data is split in a stratified\n            fashion, using the target variable as labels. Read more in\n            [sklearn's user guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        data_groups: an array holding the group index or name for each\n            data point. The length of this array must be equal to the number of\n            data points in the dataset.\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.valuation.dataset.Dataset] constructor.\n\n    Returns:\n        Datasets with the selected sklearn data\n\n    !!! tip \"Changed in version 0.10.0\"\n        Returns a tuple of two [GroupedDataset][pydvl.valuation.dataset.GroupedDataset]\n            objects.\n    \"\"\"\n\n    return cls.from_arrays(\n        X=data.data,\n        y=data.target,\n        train_size=train_size,\n        random_state=random_state,\n        stratify_by_target=stratify_by_target,\n        data_groups=data_groups,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.GroupedDataset.logical_indices","title":"logical_indices","text":"<pre><code>logical_indices(indices: Sequence[int] | None = None) -&gt; NDArray[int_]\n</code></pre> <p>Returns the group indices for the given data indices.</p> PARAMETER DESCRIPTION <code>indices</code> <p>indices of the data points in the data array. If <code>None</code>, the group indices for all data points are returned.</p> <p> TYPE: <code>Sequence[int] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NDArray[int_]</code> <p>Group indices for the given data indices.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def logical_indices(self, indices: Sequence[int] | None = None) -&gt; NDArray[np.int_]:\n    \"\"\"Returns the group indices for the given data indices.\n\n    Args:\n        indices: indices of the data points in the data array. If `None`,\n            the group indices for all data points are returned.\n\n    Returns:\n        Group indices for the given data indices.\n    \"\"\"\n    if indices is None:\n        return self.data_to_group\n    return self.data_to_group[indices]\n</code></pre>"},{"location":"api/pydvl/valuation/dataset/#pydvl.valuation.dataset.RawData","title":"RawData  <code>dataclass</code>","text":"<pre><code>RawData(x: NDArray, y: NDArray)\n</code></pre> <p>A view on a dataset's raw data. This is not a copy.</p>"},{"location":"api/pydvl/valuation/games/","title":"Games","text":""},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games","title":"pydvl.valuation.games","text":"<p>This module provides several predefined games used in the literature <sup>1</sup> and, depending on the game, precomputed Shapley, Least-Core, and / or Banzhaf values, for benchmarking purposes.</p> <p>The games are:</p> <ul> <li>SymmetricVotingGame</li> <li>AsymmetricVotingGame</li> <li>ShoesGame</li> <li>AirportGame</li> <li>MinimumSpanningTreeGame</li> <li>MinerGame</li> </ul>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games--references","title":"References","text":"<ol> <li> <p>Castro, J., G\u00f3mez, D. and Tejada,   J., 2009. Polynomial calculation of the Shapley value based on   sampling.   Computers &amp; Operations Research, 36(5), pp.1726-1730.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.AirportGame","title":"AirportGame","text":"<pre><code>AirportGame(n_players: int = 100)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>An airport game defined in (Castro et al., 2009)<sup>1</sup> Section 4.3</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def __init__(self, n_players: int = 100) -&gt; None:\n    if n_players != 100:\n        raise ValueError(\n            f\"{self.__class__.__name__} only supports n_players=100 but got {n_players=}.\"\n        )\n    description = \"A dummy dataset for the airport game in Castro et al. 2009\"\n    super().__init__(n_players, score_range=(0, 100), description=description)\n    ranges = [\n        range(0, 8),\n        range(8, 20),\n        range(20, 26),\n        range(26, 40),\n        range(40, 48),\n        range(48, 57),\n        range(57, 70),\n        range(70, 80),\n        range(80, 90),\n        range(90, 100),\n    ]\n    exact = [\n        0.01,\n        0.020869565,\n        0.033369565,\n        0.046883079,\n        0.063549745,\n        0.082780515,\n        0.106036329,\n        0.139369662,\n        0.189369662,\n        0.289369662,\n    ]\n    c = list(range(1, 10))\n    score_table = np.zeros(100)\n    exact_values = np.zeros(100)\n\n    for r, v in zip(ranges, exact):\n        score_table[r] = c\n        exact_values[r] = v\n\n    self.exact_values = exact_values\n    self.score_table = score_table\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.AsymmetricVotingGame","title":"AsymmetricVotingGame","text":"<pre><code>AsymmetricVotingGame(n_players: int = 51)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>An asymmetric voting game defined in (Castro et al., 2009)<sup>1</sup> Section 4.2.</p> <p>For this game the player set is \\(N = \\{1,\\dots,51\\}\\) and the utility of a coalition is given by:</p> \\[{ v(S) = \\left\\{\\begin{array}{ll} 1, &amp; \\text{ if} \\quad \\sum\\limits_{i \\in S} w_i &gt; \\sum\\limits_{j \\in N}\\frac{w_j}{2} \\\\ 0, &amp; \\text{ otherwise} \\end{array}\\right. }\\] <p>where \\(w = [w_1,\\dots, w_{51}]\\) is a list of weights associated with each player.</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> DEFAULT: <code>51</code> </p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def __init__(self, n_players: int = 51) -&gt; None:\n    if n_players != 51:\n        raise ValueError(\n            f\"{self.__class__.__name__} only supports n_players=51 but got {n_players=}.\"\n        )\n    description = \"Dummy data for the asymmetric voting game in Castro et al. 2009\"\n    super().__init__(\n        n_players,\n        score_range=(0, 1),\n        description=description,\n    )\n\n    ranges = [\n        range(0, 1),\n        range(1, 2),\n        range(2, 3),\n        range(3, 5),\n        range(5, 6),\n        range(6, 7),\n        range(7, 9),\n        range(9, 10),\n        range(10, 12),\n        range(12, 15),\n        range(15, 16),\n        range(16, 20),\n        range(20, 24),\n        range(24, 26),\n        range(26, 30),\n        range(30, 34),\n        range(34, 35),\n        range(35, 44),\n        range(44, 51),\n    ]\n\n    ranges_weights = [\n        45,\n        41,\n        27,\n        26,\n        25,\n        21,\n        17,\n        14,\n        13,\n        12,\n        11,\n        10,\n        9,\n        8,\n        7,\n        6,\n        5,\n        4,\n        3,\n    ]\n    ranges_values = [\n        \"0.08831\",\n        \"0.07973\",\n        \"0.05096\",\n        \"0.04898\",\n        \"0.047\",\n        \"0.03917\",\n        \"0.03147\",\n        \"0.02577\",\n        \"0.02388\",\n        \"0.022\",\n        \"0.02013\",\n        \"0.01827\",\n        \"0.01641\",\n        \"0.01456\",\n        \"0.01272\",\n        \"0.01088\",\n        \"0.009053\",\n        \"0.00723\",\n        \"0.005412\",\n    ]\n\n    self.weight_table = np.zeros(self.n_players)\n    exact_values = np.zeros(self.n_players)\n    for r, w, v in zip(ranges, ranges_weights, ranges_values):\n        self.weight_table[r] = w\n        exact_values[r] = v\n\n    self.exact_values = exact_values\n    self.threshold = np.sum(self.weight_table) / 2\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameDataset","title":"DummyGameDataset","text":"<pre><code>DummyGameDataset(n_players: int, description: str | None = None)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dummy game dataset.</p> <p>Initializes a dummy game dataset with <code>n_players</code> and an optional description.</p> <p>This class is used internally inside the Game class.</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> </p> <code>description</code> <p>Optional description of the dataset.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def __init__(self, n_players: int, description: str | None = None) -&gt; None:\n    x = np.arange(0, n_players, 1).reshape(-1, 1)\n    nil = np.zeros_like(x)\n    super().__init__(\n        x,\n        nil.copy(),\n        feature_names=[\"x\"],\n        target_names=[\"y\"],\n        description=description,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameDataset.indices","title":"indices  <code>property</code>","text":"<pre><code>indices: NDArray[int_]\n</code></pre> <p>Index of positions in data.x_train.</p> <p>Contiguous integers from 0 to len(Dataset).</p>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameDataset.n_features","title":"n_features  <code>property</code>","text":"<pre><code>n_features: int\n</code></pre> <p>Returns the number of dimensions of a sample.</p>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameDataset.names","title":"names  <code>property</code>","text":"<pre><code>names: NDArray[str_]\n</code></pre> <p>Names of each individual datapoint.</p> <p>Used for reporting Shapley values.</p>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameDataset.data","title":"data","text":"<pre><code>data(\n    indices: int | slice | Sequence[int] | NDArray[int_] | None = None,\n) -&gt; RawData\n</code></pre> <p>Given a set of indices, returns the training data that refer to those indices, as a read-only tuple-like structure.</p> <p>This is used mainly by subclasses of UtilityBase to retrieve subsets of the data from indices.</p> PARAMETER DESCRIPTION <code>indices</code> <p>Optional indices that will be used to select points from the training data. If <code>None</code>, the entire training data will be returned.</p> <p> TYPE: <code>int | slice | Sequence[int] | NDArray[int_] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>RawData</code> <p>If <code>indices</code> is not <code>None</code>, the selected x and y arrays from the training data. Otherwise, the entire dataset.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def data(\n    self, indices: int | slice | Sequence[int] | NDArray[np.int_] | None = None\n) -&gt; RawData:\n    \"\"\"Given a set of indices, returns the training data that refer to those\n    indices, as a read-only tuple-like structure.\n\n    This is used mainly by subclasses of\n    [UtilityBase][pydvl.valuation.utility.base.UtilityBase] to retrieve subsets of\n    the data from indices.\n\n    Args:\n        indices: Optional indices that will be used to select points from\n            the training data. If `None`, the entire training data will be\n            returned.\n\n    Returns:\n        If `indices` is not `None`, the selected x and y arrays from the\n            training data. Otherwise, the entire dataset.\n    \"\"\"\n    if indices is None:\n        return RawData(self._x, self._y)\n    return RawData(self._x[indices], self._y[indices])\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameDataset.data_indices","title":"data_indices","text":"<pre><code>data_indices(indices: Sequence[int] | None = None) -&gt; NDArray[int_]\n</code></pre> <p>Returns a subset of indices.</p> <p>This is equivalent to using <code>Dataset.indices[logical_indices]</code> but allows subclasses to define special behaviour, e.g. when indices in <code>Dataset</code> do not match the indices in the data.</p> <p>For <code>Dataset</code>, this is a simple pass-through.</p> PARAMETER DESCRIPTION <code>indices</code> <p>A set of indices held by this object</p> <p> TYPE: <code>Sequence[int] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NDArray[int_]</code> <p>The indices of the data points in the data array.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def data_indices(self, indices: Sequence[int] | None = None) -&gt; NDArray[np.int_]:\n    \"\"\"Returns a subset of indices.\n\n    This is equivalent to using `Dataset.indices[logical_indices]` but allows\n    subclasses to define special behaviour, e.g. when indices in `Dataset` do not\n    match the indices in the data.\n\n    For `Dataset`, this is a simple pass-through.\n\n    Args:\n        indices: A set of indices held by this object\n\n    Returns:\n        The indices of the data points in the data array.\n    \"\"\"\n    if indices is None:\n        return self._indices\n    return self._indices[indices]\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameDataset.feature","title":"feature","text":"<pre><code>feature(name: str) -&gt; tuple[slice, int]\n</code></pre> <p>Returns a slice for the feature with the given name.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def feature(self, name: str) -&gt; tuple[slice, int]:\n    \"\"\"Returns a slice for the feature with the given name.\"\"\"\n    try:\n        return np.index_exp[:, self.feature_names.index(name)]  # type: ignore\n    except ValueError:\n        raise ValueError(f\"Feature {name} is not in {self.feature_names}\")\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameDataset.from_arrays","title":"from_arrays  <code>classmethod</code>","text":"<pre><code>from_arrays(\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any\n) -&gt; tuple[Dataset, Dataset]\n</code></pre> <p>Constructs a Dataset object from X and y numpy arrays  as returned by the <code>make_*</code> functions in sklearn generated datasets.</p> Example <pre><code>&gt;&gt;&gt; from pydvl.valuation.dataset import Dataset\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression()\n&gt;&gt;&gt; dataset = Dataset.from_arrays(X, y)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>numpy array of shape (n_samples, n_features)</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>numpy array of shape (n_samples,)</p> <p> TYPE: <code>NDArray</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code></p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>random_state</code> <p>seed for train / test split</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>stratify_by_target</code> <p>If <code>True</code>, data is split in a stratified fashion, using the y variable as labels. Read more in sklearn's user guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Dataset constructor. Use this to pass e.g. <code>feature_names</code> or <code>target_names</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Dataset, Dataset]</code> <p>Object with the passed X and y arrays split across training and test sets.</p> <p>New in version 0.4.0</p> <p>Changed in version 0.6.0</p> <p>Added kwargs to pass to the Dataset constructor.</p> <p>Changed in version 0.10.0</p> <p>Returns a tuple of two Dataset objects.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>@classmethod\ndef from_arrays(\n    cls,\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any,\n) -&gt; tuple[Dataset, Dataset]:\n    \"\"\"Constructs a [Dataset][pydvl.valuation.dataset.Dataset] object from X and y numpy arrays  as\n    returned by the `make_*` functions in [sklearn generated datasets](https://scikit-learn.org/stable/datasets/sample_generators.html).\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from pydvl.valuation.dataset import Dataset\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression()\n        &gt;&gt;&gt; dataset = Dataset.from_arrays(X, y)\n        ```\n\n    Args:\n        X: numpy array of shape (n_samples, n_features)\n        y: numpy array of shape (n_samples,)\n        train_size: size of the training dataset. Used in `train_test_split`\n        random_state: seed for train / test split\n        stratify_by_target: If `True`, data is split in a stratified fashion,\n            using the y variable as labels. Read more in [sklearn's user\n            guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.valuation.dataset.Dataset] constructor. Use this to pass\n            e.g. `feature_names` or `target_names`.\n\n    Returns:\n        Object with the passed X and y arrays split across training and test sets.\n\n    !!! tip \"New in version 0.4.0\"\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added kwargs to pass to the [Dataset][pydvl.valuation.dataset.Dataset] constructor.\n\n    !!! tip \"Changed in version 0.10.0\"\n        Returns a tuple of two [Dataset][pydvl.valuation.dataset.Dataset] objects.\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(\n        X,\n        y,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=y if stratify_by_target else None,\n    )\n    return cls(x_train, y_train, **kwargs), cls(x_test, y_test, **kwargs)\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameDataset.from_sklearn","title":"from_sklearn  <code>classmethod</code>","text":"<pre><code>from_sklearn(\n    data: Bunch,\n    train_size: int | float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    **kwargs\n) -&gt; tuple[Dataset, Dataset]\n</code></pre> <p>Constructs two Dataset objects from a sklearn.utils.Bunch, as returned by the <code>load_*</code> functions in scikit-learn toy datasets.</p> Example <pre><code>&gt;&gt;&gt; from pydvl.valuation.dataset import Dataset\n&gt;&gt;&gt; from sklearn.datasets import load_boston  # noqa\n&gt;&gt;&gt; train, test = Dataset.from_sklearn(load_boston())\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>scikit-learn Bunch object. The following attributes are supported:</p> <ul> <li><code>data</code>: covariates.</li> <li><code>target</code>: target variables (labels).</li> <li><code>feature_names</code> (optional): the feature names.</li> <li><code>target_names</code> (optional): the target names.</li> <li><code>DESCR</code> (optional): a description.</li> </ul> <p> TYPE: <code>Bunch</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code> float values represent the fraction of the dataset to include in the training split and should be in (0,1). An integer value sets the absolute number of training samples.</p> <p> TYPE: <code>int | float</code> DEFAULT: <code>0.8</code> </p> <p>the value is automatically set to the complement of the test size.     random_state: seed for train / test split     stratify_by_target: If <code>True</code>, data is split in a stratified         fashion, using the target variable as labels. Read more in         scikit-learn's user guide.     kwargs: Additional keyword arguments to pass to the         Dataset constructor. Use this to pass e.g. <code>is_multi_output</code>.</p> RETURNS DESCRIPTION <code>tuple[Dataset, Dataset]</code> <p>Object with the sklearn dataset</p> <p>Changed in version 0.6.0</p> <p>Added kwargs to pass to the Dataset constructor.</p> <p>Changed in version 0.10.0</p> <p>Returns a tuple of two Dataset objects.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>@classmethod\ndef from_sklearn(\n    cls,\n    data: Bunch,\n    train_size: int | float = 0.8,\n    random_state: int | None = None,\n    stratify_by_target: bool = False,\n    **kwargs,\n) -&gt; tuple[Dataset, Dataset]:\n    \"\"\"Constructs two [Dataset][pydvl.valuation.dataset.Dataset] objects from a\n    [sklearn.utils.Bunch][], as returned by the `load_*`\n    functions in [scikit-learn toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html).\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from pydvl.valuation.dataset import Dataset\n        &gt;&gt;&gt; from sklearn.datasets import load_boston  # noqa\n        &gt;&gt;&gt; train, test = Dataset.from_sklearn(load_boston())\n        ```\n\n    Args:\n        data: scikit-learn Bunch object. The following attributes are supported:\n\n            - `data`: covariates.\n            - `target`: target variables (labels).\n            - `feature_names` (**optional**): the feature names.\n            - `target_names` (**optional**): the target names.\n            - `DESCR` (**optional**): a description.\n        train_size: size of the training dataset. Used in `train_test_split`\n            float values represent the fraction of the dataset to include in the\n            training split and should be in (0,1). An integer value sets the\n            absolute number of training samples.\n    the value is automatically set to the complement of the test size.\n        random_state: seed for train / test split\n        stratify_by_target: If `True`, data is split in a stratified\n            fashion, using the target variable as labels. Read more in\n            [scikit-learn's user guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.valuation.dataset.Dataset] constructor. Use this to pass e.g. `is_multi_output`.\n\n    Returns:\n        Object with the sklearn dataset\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added kwargs to pass to the [Dataset][pydvl.valuation.dataset.Dataset] constructor.\n    !!! tip \"Changed in version 0.10.0\"\n        Returns a tuple of two [Dataset][pydvl.valuation.dataset.Dataset] objects.\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(\n        data.data,\n        data.target,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=data.target if stratify_by_target else None,\n    )\n    return (\n        cls(\n            x_train,\n            y_train,\n            feature_names=data.get(\"feature_names\"),\n            target_names=data.get(\"target_names\"),\n            description=data.get(\"DESCR\"),\n            **kwargs,\n        ),\n        cls(\n            x_test,\n            y_test,\n            feature_names=data.get(\"feature_names\"),\n            target_names=data.get(\"target_names\"),\n            description=data.get(\"DESCR\"),\n            **kwargs,\n        ),\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameDataset.logical_indices","title":"logical_indices","text":"<pre><code>logical_indices(indices: Sequence[int] | None = None) -&gt; NDArray[int_]\n</code></pre> <p>Returns the indices in this <code>Dataset</code> for the given indices in the data array.</p> <p>This is equivalent to using <code>Dataset.indices[data_indices]</code> but allows subclasses to define special behaviour, e.g. when indices in <code>Dataset</code> do not match the indices in the data.</p> PARAMETER DESCRIPTION <code>indices</code> <p>A set of indices in the data array.</p> <p> TYPE: <code>Sequence[int] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NDArray[int_]</code> <p>The abstract indices for the given data indices.</p> Source code in <code>src/pydvl/valuation/dataset.py</code> <pre><code>def logical_indices(self, indices: Sequence[int] | None = None) -&gt; NDArray[np.int_]:\n    \"\"\"Returns the indices in this `Dataset` for the given indices in the data array.\n\n    This is equivalent to using `Dataset.indices[data_indices]` but allows\n    subclasses to define special behaviour, e.g. when indices in `Dataset` do not\n    match the indices in the data.\n\n    Args:\n        indices: A set of indices in the data array.\n\n    Returns:\n        The abstract indices for the given data indices.\n    \"\"\"\n    if indices is None:\n        return self._indices\n    return self._indices[indices]\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameUtility","title":"DummyGameUtility","text":"<pre><code>DummyGameUtility(\n    score: Callable[[NDArray], float], score_range: tuple[float, float]\n)\n</code></pre> <p>               Bases: <code>UtilityBase</code></p> <p>Dummy game utility</p> <p>This class is used internally inside the Game class.</p> PARAMETER DESCRIPTION <code>score</code> <p>Function to compute the score of a coalition.</p> <p> TYPE: <code>Callable[[NDArray], float]</code> </p> <code>score_range</code> <p>Minimum and maximum values of the score function.</p> <p> TYPE: <code>tuple[float, float]</code> </p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def __init__(\n    self, score: Callable[[NDArray], float], score_range: tuple[float, float]\n):\n    self.score = score\n    self.score_range = score_range\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyGameUtility.training_data","title":"training_data  <code>property</code>","text":"<pre><code>training_data: Dataset | None\n</code></pre> <p>Retrieves the training data used by this utility.</p> <p>This property is read-only. In order to set it, use with_dataset().</p>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.DummyModel","title":"DummyModel","text":"<pre><code>DummyModel()\n</code></pre> <p>               Bases: <code>SupervisedModel</code></p> <p>Dummy model class.</p> <p>A dummy supervised model used for testing purposes only.</p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def __init__(self) -&gt; None:\n    pass\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.Game","title":"Game","text":"<pre><code>Game(\n    n_players: int,\n    score_range: tuple[float, float] = (-inf, inf),\n    description: str | None = None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for games</p> <p>Any Game subclass has to implement the abstract <code>_score</code> method to assign a score to each coalition/subset and at least one of <code>shapley_values</code>, <code>least_core_values</code>, or <code>banzhaf_values</code>.</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> </p> <code>score_range</code> <p>Minimum and maximum values of the <code>_score</code> method.</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(-inf, inf)</code> </p> <code>description</code> <p>Optional string description of the dummy dataset that will be created.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> ATTRIBUTE DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> </p> <code>data</code> <p>Dummy dataset object.</p> <p> </p> <code>u</code> <p>Utility object with a dummy model and dataset.</p> <p> </p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def __init__(\n    self,\n    n_players: int,\n    score_range: tuple[float, float] = (-np.inf, np.inf),\n    description: str | None = None,\n):\n    self.n_players = n_players\n    self.data = DummyGameDataset(self.n_players, description)\n    self.u = DummyGameUtility(score=self._score, score_range=score_range)\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.MinerGame","title":"MinerGame","text":"<pre><code>MinerGame(n_players: int)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>Consider a group of n miners, who have discovered large bars of gold.</p> <p>If two miners can carry one piece of gold, then the payoff of a coalition \\(S\\) is:</p> \\[{ v(S) = \\left\\{\\begin{array}{lll} \\mid S \\mid / 2, &amp; \\text{ if} &amp; \\mid S \\mid \\text{ is even} \\\\ ( \\mid S \\mid - 1)/2, &amp; \\text{ otherwise} \\end{array}\\right. }\\] <p>If there are more than two miners and there is an even number of miners, then the core consists of the single payoff where each miner gets 1/2.</p> <p>If there is an odd number of miners, then the core is empty.</p> <p>Taken from Wikipedia</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of miners that participate in the game.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def __init__(self, n_players: int) -&gt; None:\n    if n_players &lt;= 2:\n        raise ValueError(f\"n_players, {n_players}, should be &gt; 2\")\n    description = \"Dummy data for Miner Game taken from https://en.wikipedia.org/wiki/Core_(game_theory)\"\n    super().__init__(\n        n_players,\n        score_range=(0, n_players // 2),\n        description=description,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.MinimumSpanningTreeGame","title":"MinimumSpanningTreeGame","text":"<pre><code>MinimumSpanningTreeGame(n_players: int = 100)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>A minimum spanning tree game defined in (Castro et al., 2009)<sup>1</sup>.</p> <p>Let \\(G = (N \\cup \\{0\\},E)\\) be a valued graph where \\(N = \\{1,\\dots,100\\}\\), and the cost associated to an edge \\((i, j)\\) is:</p> \\[{ c_{ij} = \\left\\{\\begin{array}{lll} 1, &amp; \\text{ if} &amp; i = j + 1 \\text{ or } i = j - 1 \\\\ &amp; &amp; \\text{ or } (i = 1 \\text{ and } j = 100) \\text{ or } (i = 100 \\text{ and } j = 1) \\\\ 101, &amp; \\text{ if} &amp; i = 0 \\text{ or } j = 0 \\\\ \\infty, &amp; \\text{ otherwise} \\end{array}\\right. }\\] <p>A minimum spanning tree game \\((N, c)\\) is a cost game, where for a given coalition \\(S \\subset N\\), \\(v(S)\\) is the sum of the edge cost of the minimum spanning tree, i.e. \\(v(S)\\) = Minimum Spanning Tree of the graph \\(G|_{S\\cup\\{0\\}}\\), which is the partial graph restricted to the players \\(S\\) and the source node \\(0\\).</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def __init__(self, n_players: int = 100) -&gt; None:\n    if n_players != 100:\n        raise ValueError(\n            f\"{self.__class__.__name__} only supports n_players=100 but got {n_players=}.\"\n        )\n    description = (\n        \"A dummy dataset for the minimum spanning tree game in Castro et al. 2009\"\n    )\n    super().__init__(n_players, score_range=(0, np.inf), description=description)\n\n    graph = np.zeros(shape=(self.n_players, self.n_players))\n\n    for i in range(self.n_players):\n        for j in range(self.n_players):\n            if (\n                i == j + 1\n                or i == j - 1\n                or (i == 1 and j == self.n_players - 1)\n                or (i == self.n_players - 1 and j == 1)\n            ):\n                graph[i, j] = 1\n            elif i == 0 or j == 0:\n                graph[i, j] = 0\n            else:\n                graph[i, j] = np.inf\n    assert np.all(graph == graph.T)\n\n    self.graph = graph\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.ShoesGame","title":"ShoesGame","text":"<pre><code>ShoesGame(left: int, right: int)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>A shoes game defined in (Castro et al., 2009)<sup>1</sup>.</p> <p>In this game, some players have a left shoe and others a right shoe.</p> <p>The payoff (utility) of a coalition \\(S\\) is:</p> \\[{ U(S) = \\min( \\mid S \\cap L \\mid, \\mid S \\cap R \\mid ) }\\] <p>Where \\(L\\), respectively \\(R\\), is the set of players with left shoes, respectively right shoes. This means that the marginal contribution of a player with a left shoe to a coalition \\(S\\) is 1 if the number of players with a left shoe in \\(S\\) is strictly less than the number of players with a right shoe in \\(S\\), and 0 otherwise. Let player \\(i\\) have a left shoe, then:</p> \\[{ U(S_{+i}) - U(S) =     \\left\\{         \\begin{array}{ll}             1, &amp; \\text{ if} \\mid S \\cap L \\mid &lt; \\mid S \\cap R \\mid \\\\             0, &amp; \\text{ otherwise}         \\end{array}     \\right. }\\] <p>The situation is analogous for players with a right shoe. In order to compute the Shapley or Banzhaf value for a player \\(i\\) with a left shoe, we need then the number of subsets \\(S\\) of \\(D_{-i}\\) such that \\(\\mid S \\cap L \\mid &lt; \\mid S \\cap R \\mid\\). This number is given by the sum:</p> \\[\\sum^{| L |}_{i = 0} \\sum_{j &gt; i}^{| R |} \\binom{| L |}{i} \\binom{| R |}{j}.\\] PARAMETER DESCRIPTION <code>left</code> <p>Number of players with a left shoe.</p> <p> TYPE: <code>int</code> </p> <code>right</code> <p>Number of players with a right shoe.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def __init__(self, left: int, right: int) -&gt; None:\n    self.left = left\n    self.right = right\n    n_players = self.left + self.right\n    description = \"Dummy data for the shoe game in Castro et al. 2009\"\n    max_score = n_players // 2\n    super().__init__(n_players, score_range=(0, max_score), description=description)\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.ShoesGame.banzhaf_values","title":"banzhaf_values  <code>cached</code>","text":"<pre><code>banzhaf_values() -&gt; ValuationResult\n</code></pre> <p>We use the fact that the marginal utility of a coalition S is 1 if |S \u2229 L| &lt; |S \u2229 R| and 0 otherwise, and simply count those sets.</p> <p>The solution for left or right shoes is symmetrical.</p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>@lru_cache\ndef banzhaf_values(self) -&gt; ValuationResult:\n    \"\"\"\n    We use the fact that the marginal utility of a coalition S is 1 if\n    |S \u2229 L| &lt; |S \u2229 R| and 0 otherwise, and simply count those sets.\n\n    The solution for left or right shoes is symmetrical.\n    \"\"\"\n    m = self.n_players - 1\n    left_value = self.n_subsets_left(self.left - 1, self.right) / 2**m\n    right_value = self.n_subsets_right(self.left, self.right - 1) / 2**m\n\n    exact_values = np.array([left_value] * self.left + [right_value] * self.right)\n    return ValuationResult(\n        algorithm=\"exact_banzhaf\",\n        status=Status.Converged,\n        indices=self.data.indices,\n        values=exact_values,\n        variances=np.zeros_like(self.data.data().x),\n        counts=np.zeros_like(self.data.data().x),\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.ShoesGame.shapley_values","title":"shapley_values  <code>cached</code>","text":"<pre><code>shapley_values() -&gt; ValuationResult\n</code></pre> <p>We use the fact that the marginal utility of a coalition S of size k is 1 if |S \u2229 L| &lt; |S \u2229 R| and 0 otherwise, and compute Shapley values with the formula that iterates over subset sizes.</p> <p>The solution for left or right shoes is symmetrical</p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>@lru_cache\ndef shapley_values(self) -&gt; ValuationResult:\n    \"\"\"\n    We use the fact that the marginal utility of a coalition S of size k is 1 if\n    |S \u2229 L| &lt; |S \u2229 R| and 0 otherwise, and compute Shapley values with the formula\n    that iterates over subset sizes.\n\n    The solution for left or right shoes is symmetrical\n    \"\"\"\n    left_value = 0.0\n    right_value = 0.0\n    m = self.n_players - 1\n    for k in range(m + 1):\n        left_value += (\n            1 / math.comb(m, k) * self.n_subsets_left(self.left - 1, self.right, k)\n        )\n        right_value += (\n            1 / math.comb(m, k) * self.n_subsets_right(self.left, self.right - 1, k)\n        )\n    left_value /= self.n_players\n    right_value /= self.n_players\n    exact_values = np.array([left_value] * self.left + [right_value] * self.right)\n    return ValuationResult(\n        algorithm=\"exact_shapley\",\n        status=Status.Converged,\n        indices=self.data.indices,\n        values=exact_values,\n        variances=np.zeros_like(self.data.data().x),\n        counts=np.zeros_like(self.data.data().x),\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games.SymmetricVotingGame","title":"SymmetricVotingGame","text":"<pre><code>SymmetricVotingGame(n_players: int)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>A symmetric voting game defined in (Castro et al., 2009)<sup>1</sup> Section 4.1</p> <p>For this game the utility of a coalition is 1 if its cardinality is greater than num_samples/2, or 0 otherwise.</p> \\[{ v(S) = \\left\\{\\begin{array}{ll} 1, &amp; \\text{ if} \\quad \\mid S \\mid &gt; \\frac{N}{2} \\\\ 0, &amp; \\text{ otherwise} \\end{array}\\right. }\\] PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def __init__(self, n_players: int) -&gt; None:\n    if n_players % 2 != 0:\n        raise ValueError(\"n_players must be an even number.\")\n    description = \"Dummy data for the symmetric voting game in Castro et al. 2009\"\n    super().__init__(\n        n_players,\n        score_range=(0, 1),\n        description=description,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/games/#pydvl.valuation.games._exact_a_lb","title":"_exact_a_lb","text":"<pre><code>_exact_a_lb(n_players)\n</code></pre> <p>Hardcoded exact A_lb matrix for testing least-core problem generation.</p> Source code in <code>src/pydvl/valuation/games.py</code> <pre><code>def _exact_a_lb(n_players):\n    \"\"\"Hardcoded exact A_lb matrix for testing least-core problem generation.\"\"\"\n    if n_players == 2:\n        a_lb = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n    elif n_players == 3:\n        a_lb = np.array(\n            [\n                [0, 0, 0],\n                [1, 0, 0],\n                [0, 1, 0],\n                [0, 0, 1],\n                [1, 1, 0],\n                [1, 0, 1],\n                [0, 1, 1],\n                [1, 1, 1],\n            ]\n        )\n    elif n_players == 4:\n        a_lb = np.array(\n            [\n                [0, 0, 0, 0],\n                [1, 0, 0, 0],\n                [0, 1, 0, 0],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1],\n                [1, 1, 0, 0],\n                [1, 0, 1, 0],\n                [1, 0, 0, 1],\n                [0, 1, 1, 0],\n                [0, 1, 0, 1],\n                [0, 0, 1, 1],\n                [1, 1, 1, 0],\n                [1, 1, 0, 1],\n                [1, 0, 1, 1],\n                [0, 1, 1, 1],\n                [1, 1, 1, 1],\n            ]\n        )\n    else:\n        raise NotImplementedError(\n            \"Exact A_lb matrix is not implemented for more than 4 players.\"\n        )\n    return a_lb.astype(float)\n</code></pre>"},{"location":"api/pydvl/valuation/parallel/","title":"Parallel","text":""},{"location":"api/pydvl/valuation/parallel/#pydvl.valuation.parallel","title":"pydvl.valuation.parallel","text":"<p>This module defines some utilities used in the parallel processing of valuation methods.</p> <p>In particular, it defines a flag that can be used to signal across parallel processes to stop computation. This is useful when utility computations are expensive or batched together.</p> <p>The flag is created by the <code>fit</code> method of valuations within a make_parallel_flag context manager, and passed to implementations of EvaluationStrategy.process. The latter calls the flag to detect if the computation should stop.</p>"},{"location":"api/pydvl/valuation/parallel/#pydvl.valuation.parallel.Flag","title":"Flag","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for flags</p> <p>To check a flag, call it as a function or check it in a boolean context. This will return <code>True</code> if the flag is set, and <code>False</code> otherwise.</p>"},{"location":"api/pydvl/valuation/parallel/#pydvl.valuation.parallel.MultiprocessingFlag","title":"MultiprocessingFlag","text":"<pre><code>MultiprocessingFlag(name: str)\n</code></pre> <p>               Bases: <code>Flag</code></p> <p>A flag for signalling across processes using shared memory.</p> Source code in <code>src/pydvl/valuation/parallel.py</code> <pre><code>def __init__(self, name: str):\n    self._flag = shared_memory.SharedMemory(name, create=False, size=1)\n</code></pre>"},{"location":"api/pydvl/valuation/parallel/#pydvl.valuation.parallel.ThreadingFlag","title":"ThreadingFlag","text":"<pre><code>ThreadingFlag()\n</code></pre> <p>               Bases: <code>Flag</code></p> <p>A trivial flag for signalling across threads.</p> Source code in <code>src/pydvl/valuation/parallel.py</code> <pre><code>def __init__(self):\n    self._flag = False\n</code></pre>"},{"location":"api/pydvl/valuation/parallel/#pydvl.valuation.parallel.make_parallel_flag","title":"make_parallel_flag","text":"<pre><code>make_parallel_flag()\n</code></pre> <p>A context manager that creates a flag for signalling across parallel processes. The type of flag created is based on the active parallel backend.</p> Source code in <code>src/pydvl/valuation/parallel.py</code> <pre><code>@contextmanager\ndef make_parallel_flag():\n    \"\"\"A context manager that creates a flag for signalling across parallel processes.\n    The type of flag created is based on the active parallel backend.\"\"\"\n    backend = _get_active_backend()[0]\n\n    if isinstance(backend, MultiprocessingBackend) or isinstance(backend, LokyBackend):\n        flag = MultiprocessingFlag.create()\n    elif isinstance(backend, ThreadingBackend):\n        flag = ThreadingFlag()\n    else:\n        raise NotImplementedError()\n\n    try:\n        yield flag\n    finally:\n        flag.unlink()\n</code></pre>"},{"location":"api/pydvl/valuation/result/","title":"Result","text":""},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result","title":"pydvl.valuation.result","text":"<p>This module collects types and methods for the inspection of the results of valuation algorithms.</p> <p>The most important class is ValuationResult, which provides access to raw values, as well as convenient behaviour as a <code>Sequence</code> with extended indexing and updating abilities, and conversion to pandas DataFrames.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result--indexing-and-slicing","title":"Indexing and slicing","text":"<p>Indexing and slicing of results is supported in a natural way and ValuationResult objects are returned. Indexing follows the sorting order. See the class documentation for more on this.</p> <p>Setting items and slices is also possible with other valuation results. Index and name clashes are detected and raise an exception. Note that any sorted state is potentially lost when setting items or slices.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result--addition","title":"Addition","text":"<p>Results can be added together with the standard <code>+</code> operator. Because values are typically running averages of iterative algorithms, addition behaves like a weighted average of the two results, with the weights being the number of updates in each result: adding two results is the same as generating one result with the mean of the values of the two results as values. The variances are updated accordingly. See ValuationResult for details.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result--comparing","title":"Comparing","text":"<p>Results can be compared with the equality operator. The comparison is \"semantic\" in the sense that it's the valuation for data indices that matters and not the order in which they are in the <code>ValuationResult</code>. Values, variances and counts are compared.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result--sorting","title":"Sorting","text":"<p>Results can also be sorted in place by value, variance or number of updates, see sort(). All the properties ValuationResult.values, ValuationResult.variances, ValuationResult.counts, ValuationResult.indices, ValuationResult.stderr, ValuationResult.names are then sorted according to the same order.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result--updating","title":"Updating","text":"<p>Updating results as new values arrive from workers in valuation algorithms can depend on the algorithm used. The most common case is to use the LogResultUpdater class, which uses the log-sum-exp trick to update the values and variances for better numerical stability. This is the default behaviour with the base IndexSampler, but other sampling schemes might require different ones. In particular, MSRResultUpdater must keep track of separate positive and negative updates.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result--factories","title":"Factories","text":"<p>Besides copy(),the most commonly used factory method is ValuationResult.zeros(), which creates a result object with all values, variances and counts set to zero.</p> <p>ValuationResult.empty() creates an empty result object, which can be used as a starting point for adding results together. Any metadata in empty results is discarded when added to other results.</p> <p>Finally, ValuationResult.from_random() samples random values uniformly.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.LogResultUpdater","title":"LogResultUpdater","text":"<pre><code>LogResultUpdater(result: ValuationResult)\n</code></pre> <p>               Bases: <code>ResultUpdater[ValueUpdateT]</code></p> <p>An object to update valuation results in log-space.</p> <p>This updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments. It also uses the log-sum-exp trick for numerical stability.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def __init__(self, result: ValuationResult):\n    super().__init__(result)\n    self._log_sum_positive = np.full_like(result.values, -np.inf)\n\n    pos = result.values &gt; 0\n    self._log_sum_positive[pos] = np.log(result.values[pos] * result.counts[pos])\n    self._log_sum_negative = np.full_like(result.values, -np.inf)\n\n    neg = result.values &lt; 0\n    self._log_sum_negative[neg] = np.log(-result.values[neg] * result.counts[neg])\n    self._log_sum2 = np.full_like(result.values, -np.inf)\n\n    nz = result.values != 0\n    x2 = (\n        result.variances[nz] * np.maximum(1, result.counts[nz] - 1) ** 2\n        + result.values[nz] ** 2 * result.counts[nz]\n    )\n    self._log_sum2[nz] = np.log(x2)\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ResultUpdater","title":"ResultUpdater","text":"<pre><code>ResultUpdater(result: ValuationResult)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[ValueUpdateT]</code></p> <p>Base class for result updaters.</p> <p>A result updater is a strategy to update a valuation result with a value update. It is used by the valuation methods to process the ValueUpdates emitted by the EvaluationStrategy corresponding to the sampler.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def __init__(self, result: ValuationResult):\n    self.result = result\n    self.n_updates = 0\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult","title":"ValuationResult","text":"<pre><code>ValuationResult(\n    *,\n    values: Sequence[float64] | NDArray[float64],\n    variances: Sequence[float64] | NDArray[float64] | None = None,\n    counts: Sequence[int_] | NDArray[int_] | None = None,\n    indices: Sequence[IndexT] | NDArray[IndexT] | None = None,\n    data_names: Sequence[NameT] | NDArray[NameT] | None = None,\n    algorithm: str = \"\",\n    status: Status = Pending,\n    sort: bool | None = None,\n    **extra_values: Any\n)\n</code></pre> <p>               Bases: <code>Sequence</code>, <code>Iterable[ValueItem]</code></p> <p>Objects of this class hold the results of valuation algorithms.</p> <p>These include indices in the original Dataset, any data names (e.g. group names in GroupedDataset), the values themselves, and variance of the computation in the case of Monte Carlo methods. <code>ValuationResults</code> can be iterated over like any <code>Sequence</code>: <code>iter(valuation_result)</code> returns a generator of ValueItem in the order in which the object is sorted.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult--indexing","title":"Indexing","text":"<p>Indexing is sort-based, when accessing any of the attributes values, variances, counts and indices, as well as when iterating over the object, or using the item access operator, both getter and setter. The \"position\" is either the original sequence in which the data was passed to the constructor, or the sequence in which the object has been sorted, see below. One can retrieve the sorted position for a given data index using the method positions().</p> <p>Some methods use data indices instead. This is the case for get().</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult--sorting","title":"Sorting","text":"<p>Results can be sorted in-place with sort(), or alternatively using python's standard <code>sorted()</code> and <code>reversed()</code> Note that sorting values affects how iterators and the object itself as <code>Sequence</code> behave: <code>values[0]</code> returns a ValueItem with the highest or lowest ranking point if this object is sorted by descending or ascending value, respectively.the methods If unsorted, <code>values[0]</code> returns the <code>ValueItem</code> at position 0, which has data index <code>indices[0]</code> in the Dataset.</p> <p>The same applies to direct indexing of the <code>ValuationResult</code>: the index is positional, according to the sorting. It does not refer to the \"data index\". To sort according to data index, use sort() with <code>key=\"index\"</code>.</p> <p>In order to access ValueItem objects by their data index, use get(), or use positions() to convert data indices to positions.</p> <p>Converting back and forth from data indices and positions</p> <p><code>data_indices = result.indices[result.positions(data_indices)]</code> is a noop.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult--operating-on-results","title":"Operating on results","text":"<p>Results can be added to each other with the <code>+</code> operator. Means and variances are correctly updated accordingly using the Welford algorithm.</p> <p>Empty objects behave in a special way, see empty().</p> PARAMETER DESCRIPTION <code>values</code> <p>An array of values. If omitted, defaults to an empty array or to an array of zeros if <code>indices</code> are given.</p> <p> TYPE: <code>Sequence[float64] | NDArray[float64]</code> </p> <code>indices</code> <p>An optional array of indices in the original dataset. If omitted, defaults to <code>np.arange(len(values))</code>. Warning: It is common to pass the indices of a Dataset here. Attention must be paid in a parallel context to copy them to the local process. Just do <code>indices=np.copy(data.indices)</code>.</p> <p> TYPE: <code>Sequence[IndexT] | NDArray[IndexT] | None</code> DEFAULT: <code>None</code> </p> <code>variances</code> <p>An optional array of variances of the marginals from which the values are computed.</p> <p> TYPE: <code>Sequence[float64] | NDArray[float64] | None</code> DEFAULT: <code>None</code> </p> <code>counts</code> <p>An optional array with the number of updates for each value. Defaults to an array of ones.</p> <p> TYPE: <code>Sequence[int_] | NDArray[int_] | None</code> DEFAULT: <code>None</code> </p> <code>data_names</code> <p>Names for the data points. Defaults to index numbers if not set.</p> <p> TYPE: <code>Sequence[NameT] | NDArray[NameT] | None</code> DEFAULT: <code>None</code> </p> <code>algorithm</code> <p>The method used.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>status</code> <p>The end status of the algorithm.</p> <p> TYPE: <code>Status</code> DEFAULT: <code>Pending</code> </p> <code>sort</code> <p>Whether to sort the indices. Defaults to <code>None</code> for no sorting. Set to <code>True</code> for ascending order by value, <code>False</code> for descending. See above how sorting affects usage as an iterable or sequence.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>extra_values</code> <p>Additional values that can be passed as keyword arguments. This can contain, for example, the least core value.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If input arrays have mismatching lengths.</p> Changed in 0.10.0 <p>Changed the behaviour of sorting, slicing, and indexing.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def __init__(\n    self,\n    *,\n    values: Sequence[np.float64] | NDArray[np.float64],\n    variances: Sequence[np.float64] | NDArray[np.float64] | None = None,\n    counts: Sequence[np.int_] | NDArray[np.int_] | None = None,\n    indices: Sequence[IndexT] | NDArray[IndexT] | None = None,\n    data_names: Sequence[NameT] | NDArray[NameT] | None = None,\n    algorithm: str = \"\",\n    status: Status = Status.Pending,\n    sort: bool | None = None,\n    **extra_values: Any,\n):\n    if variances is not None and len(variances) != len(values):\n        raise ValueError(\n            f\"Lengths of values ({len(values)}) \"\n            f\"and variances ({len(variances)}) do not match\"\n        )\n    if data_names is not None and len(data_names) != len(values):\n        raise ValueError(\n            f\"Lengths of values ({len(values)}) \"\n            f\"and data_names ({len(data_names)}) do not match\"\n        )\n    if indices is not None and len(indices) != len(values):\n        raise ValueError(\n            f\"Lengths of values ({len(values)}) \"\n            f\"and indices ({len(indices)}) do not match\"\n        )\n\n    self._algorithm = algorithm\n    self._status = Status(status)  # Just in case we are given a string\n    self._values = np.asarray(values, dtype=np.float64)\n    self._variances = (\n        np.zeros_like(values) if variances is None else np.asarray(variances)\n    )\n    self._counts = (\n        np.ones_like(values, dtype=int) if counts is None else np.asarray(counts)\n    )\n    self._sort_order = None\n    self._extra_values = extra_values or {}\n\n    # Internal indices -&gt; data indices\n    self._indices = self._create_indices_array(indices, len(self._values))\n    self._names = self._create_names_array(data_names, self._indices)\n\n    # Data indices -&gt; Internal indices\n    self._positions = {idx: pos for pos, idx in enumerate(self._indices)}\n\n    # Sorted indices -&gt; Internal indices\n    self._sort_positions = np.arange(len(self._values), dtype=np.int_)\n    if sort is not None:\n        self.sort(reverse=not sort)\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.counts","title":"counts  <code>property</code>","text":"<pre><code>counts: NDArray[int_]\n</code></pre> <p>The raw counts, possibly sorted.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.indices","title":"indices  <code>property</code>","text":"<pre><code>indices: NDArray[IndexT]\n</code></pre> <p>The indices for the values, possibly sorted.</p> <p>If the object is unsorted, then these are the same as declared at construction or <code>np.arange(len(values))</code> if none were passed.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.names","title":"names  <code>property</code>","text":"<pre><code>names: NDArray[NameT]\n</code></pre> <p>The names for the values, possibly sorted. If the object is unsorted, then these are the same as declared at construction or <code>np.arange(len(values))</code> if none were passed.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.stderr","title":"stderr  <code>property</code>","text":"<pre><code>stderr: NDArray[float64]\n</code></pre> <p>Standard errors of the value estimates, possibly sorted.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.values","title":"values  <code>property</code>","text":"<pre><code>values: NDArray[float64]\n</code></pre> <p>The values, possibly sorted.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.variances","title":"variances  <code>property</code>","text":"<pre><code>variances: NDArray[float64]\n</code></pre> <p>Variances of the marginals from which values were computed, possibly sorted.</p> <p>Note that this is not the variance of the value estimate, but the sample variance of the marginals used to compute it.</p>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.__add__","title":"__add__","text":"<pre><code>__add__(other: ValuationResult) -&gt; ValuationResult\n</code></pre> <p>Adds two ValuationResults.</p> <p>The values must have been computed with the same algorithm. An exception to this is if one argument has empty values, in which case the other argument is returned.</p> <p>Danger</p> <p>Abusing this will introduce numerical errors.</p> <p>Means and standard errors are correctly handled. Statuses are added with bit-wise <code>&amp;</code>, see Status. <code>data_names</code> are taken from the left summand, or if unavailable from the right one. The <code>algorithm</code> string is carried over if both terms have the same one or concatenated.</p> <p>It is possible to add ValuationResults of different lengths, and with different or overlapping indices. The result will have the union of indices, and the values.</p> <p>Warning</p> <p>FIXME: Arbitrary <code>extra_values</code> aren't handled.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def __add__(self, other: ValuationResult) -&gt; ValuationResult:\n    \"\"\"Adds two ValuationResults.\n\n    The values must have been computed with the same algorithm. An exception\n    to this is if one argument has empty values, in which case the other\n    argument is returned.\n\n    !!! danger\n        Abusing this will introduce numerical errors.\n\n    Means and standard errors are correctly handled. Statuses are added with\n    bit-wise `&amp;`, see [Status][pydvl.valuation.result.Status].\n    `data_names` are taken from the left summand, or if unavailable from\n    the right one. The `algorithm` string is carried over if both terms\n    have the same one or concatenated.\n\n    It is possible to add ValuationResults of different lengths, and with\n    different or overlapping indices. The result will have the union of\n    indices, and the values.\n\n    !!! Warning\n        FIXME: Arbitrary `extra_values` aren't handled.\n\n    \"\"\"\n    self._check_compatible(other)\n\n    if len(self.values) == 0:\n        return other\n    if len(other.values) == 0:\n        return self\n\n    indices = np.union1d(self._indices, other._indices).astype(self._indices.dtype)\n    this_pos = np.searchsorted(indices, self._indices)\n    other_pos = np.searchsorted(indices, other._indices)\n\n    n: NDArray[np.int_] = np.zeros_like(indices, dtype=int)\n    m: NDArray[np.int_] = np.zeros_like(indices, dtype=int)\n    xn: NDArray[np.int_] = np.zeros_like(indices, dtype=float)\n    xm: NDArray[np.int_] = np.zeros_like(indices, dtype=float)\n    vn: NDArray[np.int_] = np.zeros_like(indices, dtype=float)\n    vm: NDArray[np.int_] = np.zeros_like(indices, dtype=float)\n\n    n[this_pos] = self._counts\n    xn[this_pos] = self._values\n    vn[this_pos] = self._variances\n    m[other_pos] = other._counts\n    xm[other_pos] = other._values\n    vm[other_pos] = other._variances\n\n    # np.maximum(1, n + m) covers case n = m = 0.\n    n_m_sum = np.maximum(1, n + m)\n\n    # Sample mean of n+m samples from two means of n and m samples\n    xnm = (n * xn + m * xm) / n_m_sum\n\n    # Sample variance of n+m samples from two sample variances of n and m samples\n    vnm = (n * (vn + xn**2) + m * (vm + xm**2)) / n_m_sum - xnm**2\n\n    if np.any(vnm &lt; 0):\n        if np.any(vnm &lt; -1e-6):\n            logger.warning(\n                \"Numerical error in variance computation. \"\n                f\"Negative sample variances clipped to 0 in {vnm}\"\n            )\n        vnm[np.where(vnm &lt; 0)] = 0\n\n    # Merging of names:\n    # If an index has the same name in both results, it must be the same.\n    # If an index has a name in one result but not the other, the name is\n    # taken from the result with the name.\n    if self._names.dtype != other._names.dtype:\n        if np.can_cast(other._names.dtype, self._names.dtype, casting=\"safe\"):\n            logger.warning(\n                f\"Casting ValuationResult.names from {other._names.dtype} to {self._names.dtype}\"\n            )\n            other._names = other._names.astype(self._names.dtype)\n        else:\n            raise TypeError(\n                f\"Cannot cast ValuationResult.names from \"\n                f\"{other._names.dtype} to {self._names.dtype}\"\n            )\n\n    both_pos = np.intersect1d(this_pos, other_pos)\n\n    if len(both_pos) &gt; 0:\n        this_names: NDArray = np.empty_like(indices, dtype=np.str_)\n        other_names: NDArray = np.empty_like(indices, dtype=np.str_)\n        this_names[this_pos] = self._names\n        other_names[other_pos] = other._names\n\n        this_shared_names = np.take(this_names, both_pos)\n        other_shared_names = np.take(other_names, both_pos)\n\n        if np.any(this_shared_names != other_shared_names):\n            raise ValueError(\"Mismatching names in ValuationResults\")\n\n    names = np.empty_like(indices, dtype=self._names.dtype)\n    names[this_pos] = self._names\n    names[other_pos] = other._names\n\n    return ValuationResult(\n        algorithm=self.algorithm or other.algorithm or \"\",\n        status=self.status &amp; other.status,\n        indices=indices,\n        values=xnm,\n        variances=vnm,\n        counts=n + m,\n        data_names=names,\n        # FIXME: What to do with extra_values? This is not commutative:\n        # extra_values=self._extra_values.update(other._extra_values),\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(attr: str) -&gt; Any\n</code></pre> <p>Allows access to extra values as if they were properties of the instance.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def __getattr__(self, attr: str) -&gt; Any:\n    \"\"\"Allows access to extra values as if they were properties of the instance.\"\"\"\n    # This is here to avoid a RecursionError when copying or pickling the object\n    if attr == \"_extra_values\":\n        raise AttributeError()\n    try:\n        return self._extra_values[attr]\n    except KeyError as e:\n        raise AttributeError(\n            f\"{self.__class__.__name__} object has no attribute {attr}\"\n        ) from e\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: int) -&gt; ValuationResult\n</code></pre><pre><code>__getitem__(key: slice) -&gt; ValuationResult\n</code></pre><pre><code>__getitem__(key: Iterable[int]) -&gt; ValuationResult\n</code></pre> <pre><code>__getitem__(key: Union[slice, Iterable[int], int]) -&gt; ValuationResult\n</code></pre> <p>Get a ValuationResult for the given key.</p> <p>The key can be an integer, a slice, or an iterable of integers. The returned object is a new <code>ValuationResult</code> with all metadata copied, except for the sorting order. If the key is a slice or sequence, the returned object will contain the items in the order specified by the sequence.</p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>A new object containing only the selected items.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def __getitem__(self, key: Union[slice, Iterable[int], int]) -&gt; ValuationResult:\n    \"\"\"Get a ValuationResult for the given key.\n\n    The key can be an integer, a slice, or an iterable of integers.\n    The returned object is a new `ValuationResult` with all metadata copied, except\n    for the sorting order. If the key is a slice or sequence, the returned object\n    will contain the items **in the order specified by the sequence**.\n\n    Returns:\n        A new object containing only the selected items.\n    \"\"\"\n\n    positions = self._key_to_positions(key)\n\n    # Convert positions to original indices in the sort order\n    sort_indices = self._sort_positions[positions]\n\n    return ValuationResult(\n        values=self._values[sort_indices].copy(),\n        variances=self._variances[sort_indices].copy(),\n        counts=self._counts[sort_indices].copy(),\n        indices=self._indices[sort_indices].copy(),\n        data_names=self._names[sort_indices].copy(),\n        algorithm=self._algorithm,\n        status=self._status,\n        # sort=self._sort_order,  # makes no sense\n        **self._extra_values,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[ValueItem]\n</code></pre> <p>Iterate over the results returning ValueItem objects. To sort in place before iteration, use sort().</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def __iter__(self) -&gt; Iterator[ValueItem]:\n    \"\"\"Iterate over the results returning [ValueItem][pydvl.valuation.result.ValueItem] objects.\n    To sort in place before iteration, use [sort()][pydvl.valuation.result.ValuationResult.sort].\n    \"\"\"\n    for pos in self._sort_positions:\n        yield ValueItem(\n            self._indices[pos],\n            self._names[pos],\n            self._values[pos],\n            self._variances[pos],\n            self._counts[pos],\n        )\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key: int, value: ValuationResult) -&gt; None\n</code></pre><pre><code>__setitem__(key: slice, value: ValuationResult) -&gt; None\n</code></pre><pre><code>__setitem__(key: Iterable[int], value: ValuationResult) -&gt; None\n</code></pre> <pre><code>__setitem__(\n    key: Union[slice, Iterable[int], int], value: ValuationResult\n) -&gt; None\n</code></pre> <p>Set items in the <code>ValuationResult</code> using another <code>ValuationResult</code>.</p> <p>This method provides a symmetrical counterpart to <code>__getitem__</code>, both operating on <code>ValuationResult</code> objects.</p> <p>The key can be an integer, a slice, or an iterable of integers. The value must be a <code>ValuationResult</code> with length matching the number of positions specified by key.</p> PARAMETER DESCRIPTION <code>key</code> <p>Position(s) to set</p> <p> TYPE: <code>Union[slice, Iterable[int], int]</code> </p> <code>value</code> <p>A ValuationResult to set at the specified position(s)</p> <p> TYPE: <code>ValuationResult</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If value is not a ValuationResult</p> <code>ValueError</code> <p>If value's length doesn't match the number of positions specified by the key</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def __setitem__(\n    self, key: Union[slice, Iterable[int], int], value: ValuationResult\n) -&gt; None:\n    \"\"\"Set items in the `ValuationResult` using another `ValuationResult`.\n\n    This method provides a symmetrical counterpart to `__getitem__`, both\n    operating on `ValuationResult` objects.\n\n    The key can be an integer, a slice, or an iterable of integers.\n    The value must be a `ValuationResult` with length matching the number of\n    positions specified by key.\n\n    Args:\n        key: Position(s) to set\n        value: A ValuationResult to set at the specified position(s)\n\n    Raises:\n        TypeError: If value is not a ValuationResult\n        ValueError: If value's length doesn't match the number of positions\n            specified by the key\n    \"\"\"\n    if not isinstance(value, ValuationResult):\n        raise TypeError(\n            f\"Value must be a ValuationResult, got {type(value)}. \"\n            f\"To set individual ValueItems, use the set() method instead.\"\n        )\n\n    positions = self._key_to_positions(key)\n\n    if len(value) != len(positions):\n        raise ValueError(\n            f\"Cannot set {len(positions)} positions with a ValuationResult of length {len(value)}\"\n        )\n\n    # Convert sorted positions (user-facing) to original indices in the sort order\n    destination = self._sort_positions[positions]\n    # For the source, use the first sorted n items\n    source = list(range(len(positions)))\n\n    # Check that the operation won't result in duplicate indices or names\n    new_indices = self._indices.copy()\n    new_indices[destination] = value.indices[source]\n    new_names = self._names.copy()\n    new_names[destination] = value.names[source]\n\n    if len(np.unique(new_indices)) != len(new_indices):\n        raise ValueError(\"Operation would result in duplicate indices\")\n    if len(np.unique(new_names)) != len(new_names):\n        raise ValueError(\"Operation would result in duplicate names\")\n\n    # Update data index -&gt; internal index mapping\n    for data_idx in self._indices[destination]:\n        del self._positions[data_idx]\n    for data_idx, dest in zip(value.indices[source], destination):\n        self._positions[data_idx] = dest\n\n    self._indices[destination] = value.indices[source]\n    self._names[destination] = value.names[source]\n    self._values[destination] = value.values[source]\n    self._variances[destination] = value.variances[source]\n    self._counts[destination] = value.counts[source]\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.copy","title":"copy","text":"<pre><code>copy() -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the object.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def copy(self) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the object.\"\"\"\n    return ValuationResult(\n        values=self._values.copy(),\n        variances=self._variances.copy(),\n        counts=self._counts.copy(),\n        indices=self._indices.copy(),\n        data_names=self._names.copy(),\n        algorithm=self._algorithm,\n        status=self._status,\n        sort=self._sort_order,\n        **self._extra_values,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.empty","title":"empty  <code>classmethod</code>","text":"<pre><code>empty(\n    algorithm: str = \"\",\n    indices: IndexSetT | None = None,\n    data_names: Sequence[NameT] | NDArray[NameT] | None = None,\n    n_samples: int = 0,\n    **kwargs: dict[str, Any]\n) -&gt; ValuationResult\n</code></pre> <p>Creates an empty ValuationResult object.</p> <p>Empty results are characterised by having an empty array of values.</p> <p>Warning</p> <p>When a result is added to an empty one, the empty one is entirely discarded.</p> PARAMETER DESCRIPTION <code>algorithm</code> <p>Name of the algorithm used to compute the values</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>indices</code> <p>Optional sequence or array of indices.</p> <p> TYPE: <code>IndexSetT | None</code> DEFAULT: <code>None</code> </p> <code>data_names</code> <p>Optional sequences or array of names for the data points. Defaults to index numbers if not set.</p> <p> TYPE: <code>Sequence[NameT] | NDArray[NameT] | None</code> DEFAULT: <code>None</code> </p> <code>n_samples</code> <p>Number of valuation result entries.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>kwargs</code> <p>Additional options to pass to the constructor of ValuationResult. Use to override status, extra_values, etc.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <p>Returns:     Object with the results.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>@classmethod\ndef empty(\n    cls,\n    algorithm: str = \"\",\n    indices: IndexSetT | None = None,\n    data_names: Sequence[NameT] | NDArray[NameT] | None = None,\n    n_samples: int = 0,\n    **kwargs: dict[str, Any],\n) -&gt; ValuationResult:\n    \"\"\"Creates an empty [ValuationResult][pydvl.valuation.result.ValuationResult]\n    object.\n\n    Empty results are characterised by having an empty array of values.\n\n    !!! warning\n        When a result is added to an empty one, the empty one is entirely discarded.\n\n    Args:\n        algorithm: Name of the algorithm used to compute the values\n        indices: Optional sequence or array of indices.\n        data_names: Optional sequences or array of names for the data points.\n            Defaults to index numbers if not set.\n        n_samples: Number of valuation result entries.\n        kwargs: Additional options to pass to the constructor of\n            [ValuationResult][pydvl.valuation.result.ValuationResult]. Use to\n            override status, extra_values, etc.\n    Returns:\n        Object with the results.\n    \"\"\"\n    if indices is not None or data_names is not None or n_samples != 0:\n        options: dict[str, Any] = dict(\n            algorithm=algorithm,\n            indices=indices,\n            data_names=data_names,\n            n_samples=n_samples,\n        )\n        return cls.zeros(**(options | kwargs))\n\n    options = dict(algorithm=algorithm, status=Status.Pending, values=np.array([]))\n    return cls(**(options | kwargs))\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.from_random","title":"from_random  <code>classmethod</code>","text":"<pre><code>from_random(\n    size: int,\n    total: float | None = None,\n    seed: Seed | None = None,\n    **kwargs: dict[str, Any]\n) -&gt; ValuationResult\n</code></pre> <p>Creates a ValuationResult object and fills it with an array of random values from a uniform distribution in [-1,1]. The values can be made to sum up to a given total number (doing so will change their range).</p> PARAMETER DESCRIPTION <code>size</code> <p>Number of values to generate</p> <p> TYPE: <code>int</code> </p> <code>total</code> <p>If set, the values are normalized to sum to this number (\"efficiency\" property of Shapley values).</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Random seed to use</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional options to pass to the constructor of ValuationResult. Use to override status, names, etc.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>A valuation result with its status set to</p> <code>ValuationResult</code> <p>Status.Converged by default.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>size</code> is less than 1.</p> <p>Changed in version 0.6.0</p> <p>Added parameter <code>total</code>. Check for zero size</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>@classmethod\ndef from_random(\n    cls,\n    size: int,\n    total: float | None = None,\n    seed: Seed | None = None,\n    **kwargs: dict[str, Any],\n) -&gt; ValuationResult:\n    \"\"\"Creates a [ValuationResult][pydvl.valuation.result.ValuationResult] object\n    and fills it with an array of random values from a uniform distribution in\n    [-1,1]. The values can be made to sum up to a given total number (doing so will\n    change their range).\n\n    Args:\n        size: Number of values to generate\n        total: If set, the values are normalized to sum to this number\n            (\"efficiency\" property of Shapley values).\n        seed: Random seed to use\n        kwargs: Additional options to pass to the constructor of\n            [ValuationResult][pydvl.valuation.result.ValuationResult]. Use to\n            override status, names, etc.\n\n    Returns:\n        A valuation result with its status set to\n        [Status.Converged][pydvl.utils.status.Status] by default.\n\n    Raises:\n         ValueError: If `size` is less than 1.\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added parameter `total`. Check for zero size\n    \"\"\"\n    if size &lt; 1:\n        raise ValueError(\"Size must be a positive integer\")\n\n    rng = np.random.default_rng(seed)\n    values = rng.uniform(low=-1, high=1, size=size)\n    if total is not None:\n        values *= total / np.sum(values)\n\n    options = dict(values=values, status=Status.Converged, algorithm=\"random\")\n    options.update(kwargs)\n    return cls(**options)  # type: ignore\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.get","title":"get","text":"<pre><code>get(data_idx: IndexT) -&gt; ValueItem\n</code></pre> <p>Retrieves a ValueItem object by data index, as opposed to sort index, like the indexing operator.</p> PARAMETER DESCRIPTION <code>data_idx</code> <p>Data index of the value to retrieve.</p> <p> TYPE: <code>IndexT</code> </p> RAISES DESCRIPTION <code>IndexError</code> <p>If the index is not found.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def get(self, data_idx: IndexT) -&gt; ValueItem:\n    \"\"\"Retrieves a [ValueItem][pydvl.valuation.result.ValueItem] object by data\n    index, as opposed to sort index, like the indexing operator.\n\n    Args:\n        data_idx: Data index of the value to retrieve.\n\n    Raises:\n         IndexError: If the index is not found.\n    \"\"\"\n    try:\n        pos = self._positions[data_idx]\n    except KeyError:\n        raise IndexError(f\"Index {data_idx} not found in ValuationResult\")\n\n    return ValueItem(\n        data_idx,\n        self._names[pos],\n        self._values[pos],\n        self._variances[pos],\n        self._counts[pos],\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.positions","title":"positions","text":"<pre><code>positions(data_indices: IndexSetT | list[IndexT]) -&gt; IndexSetT\n</code></pre> <p>Return the location (indices) within the <code>ValuationResult</code> for the given data indices.</p> <p>Sorting is taken into account. This operation is the inverse of indexing the indices property:</p> <pre><code>np.all(v.indices[v.positions(data_indices)] == data_indices) == True\n</code></pre> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def positions(self, data_indices: IndexSetT | list[IndexT]) -&gt; IndexSetT:\n    \"\"\"Return the location (indices) within the `ValuationResult` for the given\n    data indices.\n\n    Sorting is taken into account. This operation is the inverse of indexing the\n    [indices][pydvl.valuation.result.ValuationResult.indices] property:\n\n    ```python\n    np.all(v.indices[v.positions(data_indices)] == data_indices) == True\n    ```\n    \"\"\"\n    indices = [self._positions[idx] for idx in data_indices]\n    return self._sort_positions[indices]\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.scale","title":"scale","text":"<pre><code>scale(factor: float, data_indices: NDArray[IndexT] | None = None)\n</code></pre> <p>Scales the values and variances of the result by a coefficient.</p> PARAMETER DESCRIPTION <code>factor</code> <p>Factor to scale by.</p> <p> TYPE: <code>float</code> </p> <code>data_indices</code> <p>Data indices to scale. If <code>None</code>, all values are scaled.</p> <p> TYPE: <code>NDArray[IndexT] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def scale(self, factor: float, data_indices: NDArray[IndexT] | None = None):\n    \"\"\"\n    Scales the values and variances of the result by a coefficient.\n\n    Args:\n        factor: Factor to scale by.\n        data_indices: Data indices to scale. If `None`, all values are scaled.\n    \"\"\"\n    if data_indices is None:\n        positions = None\n    else:\n        positions = [self._positions[idx] for idx in data_indices]\n    self._values[positions] *= factor\n    self._variances[positions] *= factor**2\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.set","title":"set","text":"<pre><code>set(data_idx: IndexT, value: ValueItem) -&gt; Self\n</code></pre> <p>Set a ValueItem in the result by its data index.</p> <p>This is the complement to the get() method and allows setting individual <code>ValueItems</code> directly by their data index rather than (sort-) position.</p> PARAMETER DESCRIPTION <code>data_idx</code> <p>Data index of the value to set</p> <p> TYPE: <code>IndexT</code> </p> <code>value</code> <p>The data to set</p> <p> TYPE: <code>ValueItem</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A reference to self for method chaining</p> RAISES DESCRIPTION <code>IndexError</code> <p>If the index is not found</p> <code>ValueError</code> <p>If the <code>ValueItem</code>'s idx doesn't match <code>data_idx</code></p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def set(self, data_idx: IndexT, value: ValueItem) -&gt; Self:\n    \"\"\"Set a [ValueItem][pydvl.valuation.result.ValueItem] in the result by its data\n    index.\n\n    This is the complement to the [get()][pydvl.valuation.result.ValuationResult.get]\n    method and allows setting individual `ValueItems` directly by their data index\n    rather than (sort-) position.\n\n    Args:\n        data_idx: Data index of the value to set\n        value: The data to set\n\n    Returns:\n        A reference to self for method chaining\n\n    Raises:\n        IndexError: If the index is not found\n        ValueError: If the `ValueItem`'s idx doesn't match `data_idx`\n    \"\"\"\n    if value.idx != data_idx:\n        raise ValueError(\n            f\"ValueItem's idx ({value.idx}) doesn't match the provided data_idx ({data_idx})\"\n        )\n\n    try:\n        pos = self._positions[data_idx]\n    except KeyError:\n        raise IndexError(f\"Index {data_idx} not found in ValuationResult\")\n\n    self._indices[pos] = value.idx\n    self._names[pos] = value.name\n    self._values[pos] = value.value\n    self._variances[pos] = value.variance\n    self._counts[pos] = value.count\n\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.sort","title":"sort","text":"<pre><code>sort(\n    reverse: bool = False,\n    key: Literal[\"value\", \"variance\", \"index\", \"name\"] = \"value\",\n) -&gt; None\n</code></pre> <p>Sorts the indices in place in ascending order by <code>key</code>.</p> <p>Once sorted, iteration over the results, and indexing of all the properties ValuationResult.values, ValuationResult.variances, ValuationResult.stderr, ValuationResult.counts, ValuationResult.indices and ValuationResult.names will follow the same order.</p> PARAMETER DESCRIPTION <code>reverse</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>key</code> <p>The key to sort by. Defaults to ValueItem.value.</p> <p> TYPE: <code>Literal['value', 'variance', 'index', 'name']</code> DEFAULT: <code>'value'</code> </p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def sort(\n    self,\n    reverse: bool = False,\n    # Need a \"Comparable\" type here\n    key: Literal[\"value\", \"variance\", \"index\", \"name\"] = \"value\",\n) -&gt; None:\n    \"\"\"Sorts the indices **in place** in ascending order by `key`.\n\n    Once sorted, iteration over the results, and indexing of all the\n    properties\n    [ValuationResult.values][pydvl.valuation.result.ValuationResult.values],\n    [ValuationResult.variances][pydvl.valuation.result.ValuationResult.variances],\n    [ValuationResult.stderr][pydvl.valuation.result.ValuationResult.stderr],\n    [ValuationResult.counts][pydvl.valuation.result.ValuationResult.counts],\n    [ValuationResult.indices][pydvl.valuation.result.ValuationResult.indices]\n    and [ValuationResult.names][pydvl.valuation.result.ValuationResult.names]\n    will follow the same order.\n\n    Args:\n        reverse: Whether to sort in descending order.\n        key: The key to sort by. Defaults to\n            [ValueItem.value][pydvl.valuation.result.ValueItem].\n    \"\"\"\n    keymap = {\n        \"index\": \"_indices\",\n        \"value\": \"_values\",\n        \"variance\": \"_variances\",\n        \"name\": \"_names\",\n        \"stderr\": \"stderr\",\n    }\n    self._sort_positions = np.argsort(getattr(self, keymap[key])).astype(int)\n    if reverse:\n        self._sort_positions = self._sort_positions[::-1]\n    self._sort_order = not reverse\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe(column: str | None = None, use_names: bool = False) -&gt; DataFrame\n</code></pre> <p>Returns values as a dataframe.</p> PARAMETER DESCRIPTION <code>column</code> <p>Name for the column holding the data value. Defaults to the name of the algorithm used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>use_names</code> <p>Whether to use data names instead of indices for the DataFrame's index.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A dataframe with three columns: <code>name</code>, <code>name_variances</code> and <code>name_counts</code>, where <code>name</code> is the value of argument <code>column</code>.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>def to_dataframe(\n    self, column: str | None = None, use_names: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"Returns values as a dataframe.\n\n    Args:\n        column: Name for the column holding the data value. Defaults to\n            the name of the algorithm used.\n        use_names: Whether to use data names instead of indices for the\n            DataFrame's index.\n\n    Returns:\n        A dataframe with three columns: `name`, `name_variances` and\n            `name_counts`, where `name` is the value of argument `column`.\n    \"\"\"\n    column = column or self._algorithm\n    df = pd.DataFrame(\n        self._values[self._sort_positions],\n        index=(\n            self._names[self._sort_positions]\n            if use_names\n            else self._indices[self._sort_positions]\n        ),\n        columns=[column],\n    )\n    df[column + \"_variances\"] = self.variances[self._sort_positions]\n    df[column + \"_counts\"] = self.counts[self._sort_positions]\n    return df\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValuationResult.zeros","title":"zeros  <code>classmethod</code>","text":"<pre><code>zeros(\n    algorithm: str = \"\",\n    indices: IndexSetT | None = None,\n    data_names: Sequence[NameT] | NDArray[NameT] | None = None,\n    n_samples: int = 0,\n    **kwargs: dict[str, Any]\n) -&gt; ValuationResult\n</code></pre> <p>Creates a ValuationResult filled with zeros.</p> <p>Empty results are characterised by having an empty array of values. When another result is added to an empty one, the empty one is ignored.</p> PARAMETER DESCRIPTION <code>algorithm</code> <p>Name of the algorithm used to compute the values</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>indices</code> <p>Data indices to use. A copy will be made. If not given, the indices will be set to the range <code>[0, n_samples)</code>.</p> <p> TYPE: <code>IndexSetT | None</code> DEFAULT: <code>None</code> </p> <code>data_names</code> <p>Data names to use. A copy will be made. If not given, the names will be set to the string representation of the indices.</p> <p> TYPE: <code>Sequence[NameT] | NDArray[NameT] | None</code> DEFAULT: <code>None</code> </p> <code>n_samples</code> <p>Number of data points whose values are computed. If not given, the length of <code>indices</code> will be used.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>kwargs</code> <p>Additional options to pass to the constructor of ValuationResult. Use to override status, extra_values, etc.</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <p>Returns:     Object with the results.</p> Source code in <code>src/pydvl/valuation/result.py</code> <pre><code>@classmethod\ndef zeros(\n    cls,\n    algorithm: str = \"\",\n    indices: IndexSetT | None = None,\n    data_names: Sequence[NameT] | NDArray[NameT] | None = None,\n    n_samples: int = 0,\n    **kwargs: dict[str, Any],\n) -&gt; ValuationResult:\n    \"\"\"Creates a [ValuationResult][pydvl.valuation.result.ValuationResult] filled\n    with zeros.\n\n    Empty results are characterised by having an empty array of values. When\n    another result is added to an empty one, the empty one is ignored.\n\n    Args:\n        algorithm: Name of the algorithm used to compute the values\n        indices: Data indices to use. A copy will be made. If not given,\n            the indices will be set to the range `[0, n_samples)`.\n        data_names: Data names to use. A copy will be made. If not given,\n            the names will be set to the string representation of the indices.\n        n_samples: Number of data points whose values are computed. If\n            not given, the length of `indices` will be used.\n        kwargs: Additional options to pass to the constructor of\n            [ValuationResult][pydvl.valuation.result.ValuationResult]. Use to\n            override status, extra_values, etc.\n    Returns:\n        Object with the results.\n    \"\"\"\n    indices = cls._create_indices_array(indices, n_samples)\n    data_names = cls._create_names_array(data_names, indices)\n\n    options: dict[str, Any] = dict(\n        algorithm=algorithm,\n        status=Status.Pending,\n        indices=indices,\n        data_names=data_names,\n        values=np.zeros(len(indices)),\n        variances=np.zeros(len(indices)),\n        counts=np.zeros(len(indices), dtype=np.int_),\n    )\n    return cls(**(options | kwargs))\n</code></pre>"},{"location":"api/pydvl/valuation/result/#pydvl.valuation.result.ValueItem","title":"ValueItem  <code>dataclass</code>","text":"<pre><code>ValueItem(\n    idx: IndexT,\n    name: NameT,\n    value: float,\n    variance: float | None,\n    count: int | None,\n)\n</code></pre> <p>The result of a value computation for one datum.</p> <p><code>ValueItems</code> can be compared with the usual operators, forming a total order. Comparisons take only the <code>idx</code>, <code>name</code> and <code>value</code> into account.</p> <p>Todo</p> <p>Maybe have a mode of comparison taking the <code>variance</code> into account.</p> ATTRIBUTE DESCRIPTION <code>idx</code> <p>Index of the sample with this value in the original Dataset</p> <p> TYPE: <code>IndexT</code> </p> <code>name</code> <p>Name of the sample if it was provided. Otherwise, <code>str(idx)</code></p> <p> TYPE: <code>NameT</code> </p> <code>value</code> <p>The value</p> <p> TYPE: <code>float</code> </p> <code>variance</code> <p>Variance of the marginals from which the value was computed.</p> <p> TYPE: <code>float | None</code> </p> <code>count</code> <p>Number of updates for this value</p> <p> TYPE: <code>int | None</code> </p>"},{"location":"api/pydvl/valuation/stopping/","title":"Stopping","text":""},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping","title":"pydvl.valuation.stopping","text":"<p>This module provides a basic set of criteria to stop valuation algorithms, in particular all semi-values. Common examples are MinUpdates, MaxTime, or HistoryDeviation.</p> <p>Stopping criteria can behave in different ways depending on the context. For example, MaxUpdates limits the number of updates to values, which depending on the algorithm may mean a different number of utility evaluations or imply other computations like solving a linear or quadratic program. In the case of SemivalueValuation, the criteria are evaluated once per batch, which might lead to different behavior depending on the batch size (e.g. for certain batch sizes it might happen that the number of updates to values after convergence is not exactly what was required, since multiple updates might happen at once).</p> <p>Stopping criteria are callables that are evaluated on a ValuationResult and return a Status object. They can be combined using boolean operators.</p> Saving a history of values <p>The special stopping criterion History can be used to store a rolling history of the values, e.g. for comparing methods as they evolve. <pre><code>from pydvl.valuation import ShapleyValuation, History\nhistory = History(n_steps=1000)\nstopping = MaxUpdates(10000) | history\nvaluation = ShapleyValuation(utility=utility, sampler=sampler, is_done=stopping)\nvaluation.fit(training_data)\nhistory.data[0]  # The last update\nhistory.data[-1]  # The 1000th update before last\n</code></pre></p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping--combining-stopping-criteria","title":"Combining stopping criteria","text":"<p>Objects of type StoppingCriterion can be combined with the binary operators <code>&amp;</code> (and), and <code>|</code> (or), following the truth tables of Status. The unary operator <code>~</code> (not) is also supported. See StoppingCriterion for details on how these operations affect the behavior of the stopping criteria.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping--how-convergence-is-determined","title":"How convergence is determined","text":"<p>Most stopping criteria keep track of the convergence of each index separately but make global decisions based on the overall convergence of some fraction of all indices. For example, if we have a stopping criterion that checks whether the standard error of 90% of values is below a threshold, then methods will keep updating all indices until 90% of them have converged, irrespective of the quality of the individual estimates, and without freezing updates for indices along the way as values individually attain low standard error.</p> <p>This has some practical implications, because some values do tend to converge sooner than others. For example, assume we use the criterion <code>AbsoluteStandardError(0.02) | MaxUpdates(1000)</code>. Then values close to 0 might be marked as \"converged\" rather quickly because they fulfill the first criterion, say after 20 iterations, despite being poor estimates (see the section on pitfalls below for commentary on stopping criteria based on standard errors). Because other indices take longer to reach a low standard error and the criterion is a global check, the \"converged\" ones keep being updated and end up being good estimates. In this case, this has been beneficial, but one might not wish for converged values to be updated, if one is sure that the criterion is adequate for individual values.</p> <p>Semi-value methods include a parameter <code>skip_converged</code> that allows to skip the computation of values that have converged. The way to avoid doing this too early is to use a more stringent check, e.g. <code>AbsoluteStandardError(1e-3) | MaxUpdates(1000)</code>. With <code>skip_converged=True</code> this check can still take less time than the first one, despite requiring more iterations for some indices.</p> Stopping criterion for finite samplers <p>Using a finite sampler naturally defines when the valuation algorithm terminates. However, in order to properly report progress, we need to use a stopping criterion that keeps track of the number of iterations. In this case, one can use NoStopping with the sampler as an argument. This quirk is due to progress reported depending on the completion attribute of a criterion. Here's how it's done:</p> <pre><code>from pydvl.valuation import ShapleyValuation, NoStopping\n\nsampler = DeterministicUniformSampler()\nstopping = NoStopping(sampler)\nvaluation = ShapleyValuation(\n    utility=utility, sampler=sampler, is_done=stopping, progress=True\n)\nwith parallel_config(n_jobs=4):\n    valuation.fit(data)\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping--choosing-a-stopping-criterion","title":"Choosing a stopping criterion","text":"<p>The choice of a stopping criterion greatly depends on the algorithm and the context. A safe bet is to combine a MaxUpdates or a MaxTime with a HistoryDeviation or an RankCorrelation. The former will ensure that the computation does not run for too long, while the latter will try to achieve results that are stable enough. Note however that if the threshold is too strict, one will always end up running until a maximum number of iterations or time. Also keep in mind that different values converge at different times, so you might want to use tight thresholds and <code>skip_converged</code> as described above for semi-values.</p> Example <p><pre><code>from pydvl.valuation import BanzhafValuation, MinUpdates, MSRSampler, RankCorrelation\n\nmodel = ... # Some sklearn-compatible model\nscorer = SupervisedScorer(\"accuracy\", test_data, default=0.0)\nutility = ModelUtility(model, scorer)\nsampler = MSRSampler(seed=seed)\nstopping = RankCorrelation(rtol=1e-2, burn_in=32) | MinUpdates(1000)\nvaluation = BanzhafValuation(utility=utility, sampler=sampler, is_done=stopping)\nwith parallel_config(n_jobs=4):\n    valuation.fit(training_data)\nresult = valuation.values()\n</code></pre> This will compute the Banzhaf semivalues for <code>utility</code> until either the change in Spearman rank correlation between updates is below <code>1e-2</code> or <code>1000</code> updates have been performed. The <code>burn_in</code> parameter is used to discard the first <code>32</code> updates from the computation of the standard error.</p> <p>Warning</p> <p>Be careful not to reuse the same stopping criterion for different computations. The object has state, which is reset by <code>fit()</code> for some valuation methods, but this is not guaranteed for all methods. If you need to reuse the same criterion, it's safer to create a new instance.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping--interactions-with-sampling-schemes-and-other-pitfalls-of-stopping-criteria","title":"Interactions with sampling schemes and other pitfalls of stopping criteria","text":"<p>When sampling over powersets with a sequential index iteration, indices' values are updated sequentially, as expected. Now, if the number of samples per index is high, it might be a long while until the next index is updated. In this case, criteria like MinUpdates will seem to stall after each index has reached the specified number of updates, even though the computation is still ongoing. A \"fix\" is to set the <code>skip_converged</code> parameter of Semi-value methods to <code>True</code>, so that as soon as the stopping criterion is fulfilled for an index, the computation continues. Note that this will probably break any desirable properties of certain samplers, for instance the StratifiedSampler.</p> Problem with 'skip_converged' <p>Alas, the above will fail under some circumstances, until we fix this bug</p> <p>Different samplers define different \"update strategies\" for values. For example, MSRSampler updates the <code>counts</code> field of a ValuationResult only for about half of the utility evaluations, because it reuses samples. This means that a stopping criterion like MaxChecks will not work as expected, because it will count the number of calls to the criterion, not the number of updates to the values. In this case, one should use MaxUpdates or, more likely, MinUpdates instead.</p> <p>Finally, stopping criteria that rely on the standard error of the values, like AbsoluteStandardError, should be used with care. The standard error is a measure of the uncertainty of the estimate, but it does not guarantee that the estimate is close to the true value. For example, if the utility function is very noisy, the standard error might be very low, but the estimate might be far from the true value. In this case, one might want to use a RankCorrelation instead, which checks whether the rank of the values is stable.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping--creating-stopping-criteria","title":"Creating stopping criteria","text":"<p>In order to create a new stopping criterion, one can subclass StoppingCriterion and implement the <code>_check</code> method. This method should return a Status and update the <code>_converged</code> attribute, which is a boolean array indicating whether the value for each index has converged. When this does not make sense for a particular stopping criterion, completion should be overridden to provide an overall completion value, since its default implementation attempts to compute the mean of <code>_converged</code>.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping--references","title":"References","text":"<ol> <li> <p>Ghorbani, A., Zou, J., 2019. Data Shapley:   Equitable Valuation of Data for Machine   Learning. In: Proceedings of   the 36th International Conference on Machine Learning, PMLR, pp. 2242\u20132251.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.AbsoluteStandardError","title":"AbsoluteStandardError","text":"<pre><code>AbsoluteStandardError(\n    threshold: float,\n    fraction: float = 1.0,\n    burn_in: int = 4,\n    modify_result: bool = True,\n)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Determine convergence based on the standard error of the values.</p> <p>If \\(s_i\\) is the standard error for datum \\(i\\), then this criterion returns Converged if \\(s_i &lt; \\epsilon\\) for all \\(i\\) and a threshold value \\(\\epsilon \\gt 0\\).</p> <p>Warning</p> <p>This criterion should be used with care. The standard error is a measure of the uncertainty of the estimate, but it does not guarantee that the estimate is close to the true value. For example, if the utility function is very noisy, the standard error might be very low, but the estimate might be far from the true value. In this case, one might want to use a RankCorrelation instead, which checks whether the rank of the values is stable.</p> PARAMETER DESCRIPTION <code>threshold</code> <p>A value is considered to have converged if the standard error is below this threshold. A way of choosing it is to pick some percentage of the range of the values. For Shapley values this is the difference between the maximum and minimum of the utility function (to see this substitute the maximum and minimum values of the utility into the marginal contribution formula).</p> <p> TYPE: <code>float</code> </p> <code>fraction</code> <p>The fraction of values that must have converged for the criterion to return Converged.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>burn_in</code> <p>The number of iterations to ignore before checking for convergence. This is required because computations typically start with zero variance, as a result of using zeros(). The default is set to an arbitrary minimum which is usually enough but may need to be increased.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>modify_result</code> <p>If <code>True</code> the status of the input ValuationResult is modified in place after the call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(\n    self,\n    threshold: float,\n    fraction: float = 1.0,\n    burn_in: int = 4,\n    modify_result: bool = True,\n):\n    super().__init__(modify_result=modify_result)\n    self.threshold = threshold\n    self.fraction = fraction\n    self.burn_in = burn_in\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.AbsoluteStandardError.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.AbsoluteStandardError.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.AbsoluteStandardError.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.AbsoluteStandardError.completion","title":"completion","text":"<pre><code>completion() -&gt; float\n</code></pre> <p>Returns a value between 0 and 1 indicating the completion of the computation.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def completion(self) -&gt; float:\n    \"\"\"Returns a value between 0 and 1 indicating the completion of the\n    computation.\"\"\"\n    if self.converged.size == 0:\n        return 0.0\n    return float(np.mean(self.converged).item())\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.History","title":"History","text":"<pre><code>History(n_steps: int, skip_steps: int = 0, modify_result: bool = False)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>A dummy stopping criterion that stores the last <code>steps</code> values in a rolling memory.</p> <p>You can access the values via indexing with the <code>[]</code> operator. Indices should always be negative, with the most recent value being <code>-1</code>, the second most recent being <code>-2</code>, and so on.</p> Typical usage <pre><code>stopping = MaxSamples(5000) | (history := History(5000))\nvaluation = TMCShapleyValuation(utility, sampler, is_done=stopping)\nwith parallel_config(n_jobs=-1):\n    valuation.fit(training_data)\n# history[-10:]  # contains the last 10 steps\n</code></pre> Comparing histories across valuation methods <p>Care must be taken when comparing the histories saved while fitting different methods. The rate at which stopping criteria are checked, and hence a <code>History</code> is updated is not guaranteed to be the same, due to differences in sampling and batching. For instance, a deterministic powerset sampler with a FiniteSequentialIndexIteration with a <code>batch_size=2**(n-1)</code> will update the history exactly <code>n</code> times (if given enough iterations by other stopping criteria), where <code>n</code> is the number of indices. If instead the batch size is <code>1</code>, the history will be updated <code>2**n</code> times, but all the values except for one index will remain constant during <code>2**(n-1)</code> iterations. Comparing any of these to, say, a PermutationSampler which results in one update to the history per permutation, requires setting the parameter <code>skip_steps</code> adequately in the respective <code>History</code> objects.</p> Example <pre><code>result = ValuationResult.from_random(size=7)\nhistory = History(n_steps=5)\nhistory(result)\nassert all(history[-1] == result)\n</code></pre> <p>Args:     n_steps: The number of steps to remember.     skip_steps: The number of steps to skip between updates. If <code>0</code>, the memory         is updated at every check of the criterion. If <code>1</code>, the memory is updated         every other step, and so on. This is useful to synchronize the memory of         methods with different update rates.     modify_result: Ignored.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(self, n_steps: int, skip_steps: int = 0, modify_result: bool = False):\n    super().__init__(modify_result=False)\n    self.memory = RollingMemory(\n        size=n_steps, skip_steps=skip_steps, default=np.inf, dtype=np.float64\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.History.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.History.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.History.data","title":"data  <code>property</code>","text":"<pre><code>data: NDArray[float64]\n</code></pre> <p>A view on the data. Rows are the steps, columns are the indices</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.History.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.History.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>The number of steps that the memory has saved. This is guaranteed to be between 0 and <code>n_steps</code>, inclusive.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"The number of steps that the memory has saved. This is guaranteed to be\n    between 0 and `n_steps`, inclusive.\"\"\"\n    return len(self.memory)\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.HistoryDeviation","title":"HistoryDeviation","text":"<pre><code>HistoryDeviation(\n    n_steps: int,\n    rtol: float,\n    pin_converged: bool = True,\n    modify_result: bool = True,\n)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>A simple check for relative distance to a previous step in the computation.</p> <p>The method used by Ghorbani and Zou, (2019)<sup>1</sup> computes the relative distances between the current values \\(v_i^t\\) and the values at the previous checkpoint \\(v_i^{t-\\tau}\\). If the sum is below a given threshold, the computation is terminated.</p> \\[\\sum_{i=1}^n \\frac{\\left| v_i^t - v_i^{t-\\tau} \\right|}{v_i^t} &lt; \\epsilon.\\] <p>When the denominator is zero, the summand is set to the value of \\(v_i^{ t-\\tau}\\).</p> <p>This implementation is slightly generalised to allow for different number of updates to individual indices, as happens with powerset samplers instead of permutations. Every subset of indices that is found to converge can be pinned to that state. Once all indices have converged the method has converged.</p> <p>Warning</p> <p>This criterion is meant for the reproduction of the results in the paper, but we do not recommend using it in practice.</p> PARAMETER DESCRIPTION <code>n_steps</code> <p>Compare values after so many steps. A step is one evaluation of the criterion, which happens once per batch.</p> <p> TYPE: <code>int</code> </p> <code>rtol</code> <p>Relative tolerance for convergence (\\(\\epsilon\\) in the formula).</p> <p> TYPE: <code>float</code> </p> <code>pin_converged</code> <p>If <code>True</code>, once an index has converged, it is pinned</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(\n    self,\n    n_steps: int,\n    rtol: float,\n    pin_converged: bool = True,\n    modify_result: bool = True,\n):\n    super().__init__(modify_result=modify_result)\n    self.memory = RollingMemory(n_steps + 1, default=np.inf, dtype=np.float64)\n    self.rtol = validate_number(\"rtol\", rtol, float, lower=0.0, upper=1.0)\n    self.update_op = np.logical_or if pin_converged else np.logical_and\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.HistoryDeviation.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.HistoryDeviation.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.HistoryDeviation.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.HistoryDeviation.completion","title":"completion","text":"<pre><code>completion() -&gt; float\n</code></pre> <p>Returns a value between 0 and 1 indicating the completion of the computation.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def completion(self) -&gt; float:\n    \"\"\"Returns a value between 0 and 1 indicating the completion of the\n    computation.\"\"\"\n    if self.converged.size == 0:\n        return 0.0\n    return float(np.mean(self.converged).item())\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxChecks","title":"MaxChecks","text":"<pre><code>MaxChecks(n_checks: int | None, modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Terminate as soon as the number of checks exceeds the threshold.</p> <p>A \"check\" is one call to the criterion. Note that this might have different interpretations depending on the sampler. For example, MSRSampler performs a single utility evaluation to update all indices, so that's <code>len(training_data)</code> checks for a single training of the model. But it also only changes the <code>counts</code> field of the ValuationResult for about half of the indices, which is what e.g. MaxUpdates checks.</p> PARAMETER DESCRIPTION <code>n_checks</code> <p>Threshold: if <code>None</code>, no check is performed, effectively creating a (never) stopping criterion that always returns <code>Pending</code>.</p> <p> TYPE: <code>int | None</code> </p> <code>modify_result</code> <p>If <code>True</code> the status of the input ValuationResult is modified in place after the call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(self, n_checks: int | None, modify_result: bool = True):\n    super().__init__(modify_result=modify_result)\n    if n_checks is not None:\n        n_checks = validate_number(\"n_checks\", n_checks, int, lower=1)\n    self.n_checks = n_checks\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxChecks.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxChecks.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxChecks.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxSamples","title":"MaxSamples","text":"<pre><code>MaxSamples(sampler: IndexSampler, n_samples: int, modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Run until the sampler has sampled the given number of samples.</p> <p>Warning</p> <p>If the sampler is batched, and the valuation method runs in parallel, the check might be off by the sampler's batch size.</p> PARAMETER DESCRIPTION <code>sampler</code> <p>The sampler to check.</p> <p> TYPE: <code>IndexSampler</code> </p> <code>n_samples</code> <p>The number of samples to run until.</p> <p> TYPE: <code>int</code> </p> <code>modify_result</code> <p>If <code>True</code> the status of the input ValuationResult is modified in place after the call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(\n    self, sampler: IndexSampler, n_samples: int, modify_result: bool = True\n):\n    super().__init__(modify_result=modify_result)\n    self.sampler = sampler\n    self.n_samples = validate_number(\"n_samples\", n_samples, int, lower=1)\n    self._completion = 0.0\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxSamples.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxSamples.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxSamples.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxTime","title":"MaxTime","text":"<pre><code>MaxTime(seconds: float | None, modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Terminate if the computation time exceeds the given number of seconds.</p> <p>Checks the elapsed time since construction.</p> PARAMETER DESCRIPTION <code>seconds</code> <p>Threshold: The computation is terminated if the elapsed time between object construction and a _check exceeds this value. If <code>None</code>, no _check is performed, effectively creating a (never) stopping criterion that always returns <code>Pending</code>.</p> <p> TYPE: <code>float | None</code> </p> <code>modify_result</code> <p>If <code>True</code> the status of the input ValuationResult is modified in place after the call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(self, seconds: float | None, modify_result: bool = True):\n    super().__init__(modify_result=modify_result)\n    if seconds is None:\n        seconds = np.inf\n    self.max_seconds = validate_number(\"seconds\", seconds, float, lower=1e-6)\n    self.start = time()\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxTime.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxTime.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxTime.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxUpdates","title":"MaxUpdates","text":"<pre><code>MaxUpdates(n_updates: int | None, modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Terminate if any number of value updates exceeds or equals the given threshold.</p> <p>Note</p> <p>If you want to ensure that all values have been updated, you probably want MinUpdates instead.</p> <p>This checks the <code>counts</code> field of a ValuationResult, i.e. the number of times that each index has been updated. For powerset samplers, the maximum of this number coincides with the maximum number of subsets sampled. For permutation samplers, it coincides with the number of permutations sampled.</p> PARAMETER DESCRIPTION <code>n_updates</code> <p>Threshold: if <code>None</code>, no check is performed, effectively creating a (never) stopping criterion that always returns <code>Pending</code>.</p> <p> TYPE: <code>int | None</code> </p> <code>modify_result</code> <p>If <code>True</code> the status of the input ValuationResult is modified in place after the call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(self, n_updates: int | None, modify_result: bool = True):\n    super().__init__(modify_result=modify_result)\n    if n_updates is not None:\n        n_updates = validate_number(\"n_updates\", n_updates, int, lower=1)\n    self.n_updates = n_updates\n    self.last_max = 0\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxUpdates.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxUpdates.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MaxUpdates.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MinUpdates","title":"MinUpdates","text":"<pre><code>MinUpdates(n_updates: int | None, modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Terminate as soon as all value updates exceed or equal the given threshold.</p> <p>This checks the <code>counts</code> field of a ValuationResult, i.e. the number of times that each index has been updated. For powerset samplers, the minimum of this number is a lower bound for the number of subsets sampled. For permutation samplers, it lower-bounds the amount of permutations sampled.</p> PARAMETER DESCRIPTION <code>n_updates</code> <p>Threshold: if <code>None</code>, no _check is performed, effectively creating a (never) stopping criterion that always returns <code>Pending</code>.</p> <p> TYPE: <code>int | None</code> </p> <code>modify_result</code> <p>If <code>True</code> the status of the input ValuationResult is modified in place after the call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(self, n_updates: int | None, modify_result: bool = True):\n    super().__init__(modify_result=modify_result)\n    if n_updates is not None:\n        n_updates = validate_number(\"n_updates\", n_updates, int, lower=1)\n    self.n_updates = n_updates\n    self.last_min = 0\n    self._actual_completion = 0.0\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MinUpdates.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MinUpdates.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.MinUpdates.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.NoStopping","title":"NoStopping","text":"<pre><code>NoStopping(sampler: IndexSampler | None = None, modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Keep running forever or until sampling stops.</p> <p>If a sampler instance is passed, and it is a finite sampler, its counter will be used to update completion status.</p> PARAMETER DESCRIPTION <code>sampler</code> <p>A sampler instance to use for completion status.</p> <p> TYPE: <code>IndexSampler | None</code> DEFAULT: <code>None</code> </p> <code>modify_result</code> <p>If <code>True</code> the status of the input ValuationResult is modified in place after the call</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(self, sampler: IndexSampler | None = None, modify_result: bool = True):\n    super().__init__(modify_result=modify_result)\n    self.sampler = sampler\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.NoStopping.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.NoStopping.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.NoStopping.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RankCorrelation","title":"RankCorrelation","text":"<pre><code>RankCorrelation(\n    rtol: float, burn_in: int, fraction: float = 1.0, modify_result: bool = True\n)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>A check for stability of Spearman correlation between checks.</p> <p>Convergence is reached when the change in rank correlation between two successive iterations is below a given threshold.</p> <p>This criterion is used in (Wang et al.)<sup>2</sup>.</p> <p>The meaning of successive iterations</p> <p>Stopping criteria in pyDVL are typically evaluated after each batch of value updates is received. This can imply very different things, depending on the configuration of the samplers. For this reason, <code>RankCorrelation</code> keeps itself track of the number of updates that each index has seen, and only checks for correlation changes when a given fraction of all indices has been updated more than <code>burn_in</code> times and once since last time the criterion was checked.</p> PARAMETER DESCRIPTION <code>rtol</code> <p>Relative tolerance for convergence (\\(\\epsilon\\) in the formula)</p> <p> TYPE: <code>float</code> </p> <code>burn_in</code> <p>The minimum number of updates an index must have seen before checking for convergence. This is required because the first correlation checks are usually meaningless.</p> <p> TYPE: <code>int</code> </p> <code>fraction</code> <p>The fraction of values that must have been updated between two correlation checks. This is to avoid comparing two results where only one value has been updated, which would have almost perfect rank correlation.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>modify_result</code> <p>If <code>True</code>, the status of the input ValuationResult is modified in place after the call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Added in 0.9.0</p> <p>Changed in 0.10.0</p> <p>The behaviour of the <code>burn_in</code> parameter was changed to look at value updates. The parameter <code>fraction</code> was added.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(\n    self,\n    rtol: float,\n    burn_in: int,\n    fraction: float = 1.0,\n    modify_result: bool = True,\n):\n    super().__init__(modify_result=modify_result)\n\n    self.rtol = validate_number(\"rtol\", rtol, float, lower=0.0, upper=1.0)\n    self.burn_in = burn_in\n    self.fraction = validate_number(\n        \"fraction\", fraction, float, lower=0.0, upper=1.0\n    )\n    self.memory = RollingMemory(size=2, default=np.nan, dtype=np.float64)\n    self.count_memory = RollingMemory(size=2, default=0, dtype=np.int_)\n    self._corr = np.nan\n    self._completion = 0.0\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RankCorrelation.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RankCorrelation.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RankCorrelation.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RollingMemory","title":"RollingMemory","text":"<pre><code>RollingMemory(\n    size: int,\n    skip_steps: int = 0,\n    default: Union[DT, int, float] = inf,\n    *,\n    dtype: Type[DT] | None = None\n)\n</code></pre> <p>               Bases: <code>Generic[DT]</code></p> <p>A simple rolling memory for the last <code>size</code> values of each index.</p> <p>Updating the memory results in new values being copied to the last column of the matrix and old ones removed from the first.</p> PARAMETER DESCRIPTION <code>size</code> <p>The number of steps to remember. The internal buffer will have shape <code>(size, n_indices)</code>, where <code>n_indices</code> is the number of indices in the ValuationResult.</p> <p> TYPE: <code>int</code> </p> <code>skip_steps</code> <p>The number of steps to skip between updates. If <code>0</code>, the memory is updated at every step. If <code>1</code>, the memory is updated every other step, and so on.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>default</code> <p>The default value to use when the memory is empty.</p> <p> TYPE: <code>Union[DT, int, float]</code> DEFAULT: <code>inf</code> </p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(\n    self,\n    size: int,\n    skip_steps: int = 0,\n    default: Union[DT, int, float] = np.inf,\n    *,\n    dtype: Type[DT] | None = None,\n):\n    if not isinstance(default, np.generic):  # convert to np scalar\n        default = cast(DT, np.array(default, dtype=np.result_type(default))[()])\n    if dtype is not None:  # user forced conversion\n        default = dtype(default)\n    self.size = validate_number(\"size\", size, int, lower=1)\n    self._skip_steps = validate_number(\"skip_steps\", skip_steps, int, lower=0)\n    self._count = 0\n    self._default = cast(DT, default)\n    self._data: NDArray[DT] = np.full(0, default, dtype=type(default))\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RollingMemory.data","title":"data  <code>property</code>","text":"<pre><code>data: NDArray[DT]\n</code></pre> <p>A view on the data. Rows are the steps, columns are the indices</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RollingMemory.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: Union[slice, Iterable[int], int]) -&gt; NDArray\n</code></pre> <p>Get items from the memory, properly handling temporal sequence.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __getitem__(self, key: Union[slice, Iterable[int], int]) -&gt; NDArray:\n    \"\"\"Get items from the memory, properly handling temporal sequence.\"\"\"\n    key = self._validate_key(key)\n    return cast(NDArray, self.data[key])\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RollingMemory.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>The number of steps that the memory has saved. This is guaranteed to be between 0 and <code>size</code>, inclusive.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"The number of steps that the memory has saved. This is guaranteed to be\n    between 0 and `size`, inclusive.\"\"\"\n    return min(self.size, self._count // (1 + self._skip_steps))\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RollingMemory.must_skip","title":"must_skip","text":"<pre><code>must_skip()\n</code></pre> <p>Check if the memory must skip the current update</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def must_skip(self):\n    \"\"\"Check if the memory must skip the current update\"\"\"\n    return self._count % (1 + self._skip_steps) &gt; 0\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RollingMemory.reset","title":"reset","text":"<pre><code>reset() -&gt; Self\n</code></pre> <p>Empty the memory</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def reset(self) -&gt; Self:\n    \"\"\"Empty the memory\"\"\"\n    self._data = np.full(0, self._default, dtype=type(self._default))\n    self._count = 0\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.RollingMemory.update","title":"update","text":"<pre><code>update(values: NDArray[DT]) -&gt; Self\n</code></pre> <p>Update the memory with the values of the current result.</p> <p>The values are appended to the memory as its last column, and the oldest values (the first column) are removed</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def update(self, values: NDArray[DT]) -&gt; Self:\n    \"\"\"Update the memory with the values of the current result.\n\n    The values are appended to the memory as its last column, and the oldest values\n    (the first column) are removed\n    \"\"\"\n    if len(self._data) == 0:\n        self._data = np.full(\n            (len(values), self.size),\n            self._default,\n            dtype=type(self._default),\n        )\n    self._count += 1\n    if self.must_skip():\n        return self\n    self._data = np.concatenate([self._data[:, 1:], values.reshape(-1, 1)], axis=1)\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.StoppingCriterion","title":"StoppingCriterion","text":"<pre><code>StoppingCriterion(modify_result: bool = True)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A composable callable object to determine whether a computation must stop.</p> <p>A <code>StoppingCriterion</code> is a callable taking a ValuationResult and returning a Status. It also keeps track of individual convergence of values with converged, and reports the overall completion of the computation with completion.</p> <p>Instances of <code>StoppingCriterion</code> can be composed with the binary operators <code>&amp;</code> (and), and <code>|</code> (or), following the truth tables of Status. The unary operator <code>~</code> (not) is also supported. These boolean operations act according to the following rules:</p> <ul> <li>The results of <code>check()</code> are combined with the operator. See   Status for the truth tables.</li> <li>The results of   converged are combined   with the operator (returning another boolean array).</li> <li>The completion   method returns the min, max, or the complement to 1 of the completions of   the operands, for AND, OR and NOT respectively. This is required for cases   where one of the criteria does not keep track of the convergence of single   values, e.g. MaxUpdates, because   completion by   default returns the mean of the boolean convergence array.</li> </ul>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.StoppingCriterion--subclassing","title":"Subclassing","text":"<p>Subclassing this class requires implementing a <code>check()</code> method that returns a Status object based on a given ValuationResult. This method should update the attribute <code>_converged</code>, which is a boolean array indicating whether the value for each index has converged. When this does not make sense for a particular stopping criterion, completion should be overridden to provide an overall completion value, since its default implementation attempts to compute the mean of <code>_converged</code>.</p> PARAMETER DESCRIPTION <code>modify_result</code> <p>If <code>True</code> the status of the input ValuationResult is modified in place after the call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __init__(self, modify_result: bool = True):\n    self.modify_result = modify_result\n    self._converged = np.full(0, False)\n    self._count = 0\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.StoppingCriterion.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.StoppingCriterion.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of times that the criterion has been checked.</p>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.StoppingCriterion.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    self._count += 1\n    if self._converged.size == 0:\n        self._converged = np.full_like(result.indices, False, dtype=bool)\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.StoppingCriterion._check","title":"_check  <code>abstractmethod</code>","text":"<pre><code>_check(result: ValuationResult) -&gt; Status\n</code></pre> <p>Check whether the computation should stop.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>@abc.abstractmethod\ndef _check(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Check whether the computation should stop.\"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping.StoppingCriterion.completion","title":"completion","text":"<pre><code>completion() -&gt; float\n</code></pre> <p>Returns a value between 0 and 1 indicating the completion of the computation.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def completion(self) -&gt; float:\n    \"\"\"Returns a value between 0 and 1 indicating the completion of the\n    computation.\"\"\"\n    if self.converged.size == 0:\n        return 0.0\n    return float(np.mean(self.converged).item())\n</code></pre>"},{"location":"api/pydvl/valuation/stopping/#pydvl.valuation.stopping._make_criterion","title":"_make_criterion","text":"<pre><code>_make_criterion(\n    check: Callable[[ValuationResult], Status],\n    criteria: list[StoppingCriterion],\n    converged: Callable[[], NDArray[bool_]],\n    completion: Callable[[], float],\n    name: str,\n) -&gt; Type[StoppingCriterion]\n</code></pre> <p>Create a new StoppingCriterion from several callables. Used to compose simpler criteria with bitwise operators</p> PARAMETER DESCRIPTION <code>check</code> <p>The callable to wrap.</p> <p> TYPE: <code>Callable[[ValuationResult], Status]</code> </p> <code>criteria</code> <p>A list of criteria that are combined into a new criterion</p> <p> TYPE: <code>list[StoppingCriterion]</code> </p> <code>converged</code> <p>A callable that returns a boolean array indicating what values have converged.</p> <p> TYPE: <code>Callable[[], NDArray[bool_]]</code> </p> <code>completion</code> <p>A callable that returns a value between 0 and 1 indicating the rate of completion of the computation. If not provided, the fraction of converged values is used.</p> <p> TYPE: <code>Callable[[], float]</code> </p> <code>name</code> <p>The name of the new criterion. If <code>None</code>, the <code>__name__</code> of the function is used.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Type[StoppingCriterion]</code> <p>A new subclass of StoppingCriterion.</p> Source code in <code>src/pydvl/valuation/stopping.py</code> <pre><code>def _make_criterion(\n    check: Callable[[ValuationResult], Status],\n    criteria: list[StoppingCriterion],\n    converged: Callable[[], NDArray[np.bool_]],\n    completion: Callable[[], float],\n    name: str,\n) -&gt; Type[StoppingCriterion]:\n    \"\"\"Create a new [StoppingCriterion][pydvl.valuation.stopping.StoppingCriterion] from\n    several callables. Used to compose simpler criteria with bitwise operators\n\n    Args:\n        check: The callable to wrap.\n        criteria: A list of criteria that are combined into a new criterion\n        converged: A callable that returns a boolean array indicating what\n            values have converged.\n        completion: A callable that returns a value between 0 and 1 indicating\n            the rate of completion of the computation. If not provided, the fraction\n            of converged values is used.\n        name: The name of the new criterion. If `None`, the `__name__` of\n            the function is used.\n\n    Returns:\n        A new subclass of [StoppingCriterion][pydvl.valuation.stopping.StoppingCriterion].\n    \"\"\"\n\n    class WrappedCriterion(StoppingCriterion):\n        def __init__(self, modify_result: bool = True):\n            super().__init__(modify_result=modify_result)\n            self._name = name or cast(\n                str, getattr(check, \"__name__\", \"WrappedCriterion\")\n            )\n            self._criteria = criteria if criteria is not None else []\n\n        @property\n        def criteria(self) -&gt; list[StoppingCriterion]:\n            return self._criteria\n\n        def increase_criteria_count(self):\n            for criterion in self._criteria:\n                if hasattr(criterion, \"_criteria\"):\n                    cast(WrappedCriterion, criterion).increase_criteria_count()\n                else:\n                    criterion._count += 1\n\n        def init_criteria_converged(self, result: ValuationResult):\n            for criterion in self._criteria:\n                if hasattr(criterion, \"_criteria\"):\n                    cast(WrappedCriterion, criterion).init_criteria_converged(result)\n                else:\n                    if criterion._converged.size == 0:\n                        criterion._converged = np.full_like(\n                            result.indices, False, dtype=bool\n                        )\n\n        def __call__(self, result: ValuationResult) -&gt; Status:\n            self.increase_criteria_count()\n            self.init_criteria_converged(result)\n            return super().__call__(result)\n\n        def _check(self, result: ValuationResult) -&gt; Status:\n            status = check(result)\n            self._converged = converged()\n            return status\n\n        def __str__(self) -&gt; str:\n            return self._name\n\n        def completion(self) -&gt; float:\n            return completion()\n\n    return WrappedCriterion\n</code></pre>"},{"location":"api/pydvl/valuation/types/","title":"Types","text":""},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types","title":"pydvl.valuation.types","text":"<p>This module contains different types used by pydvl.valuation</p> <p>If you are interested in extending valuation methods, you might need to subclass ValueUpdate, Sample or ClasswiseSample. These are the data types used for communication between the samplers on the main process and the workers.</p>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.ClasswiseSample","title":"ClasswiseSample  <code>dataclass</code>","text":"<pre><code>ClasswiseSample(\n    idx: IndexT | None,\n    subset: NDArray[IndexT],\n    label: int,\n    ooc_subset: NDArray[IndexT],\n)\n</code></pre> <p>               Bases: <code>Sample</code></p> <p>Sample class for classwise shapley valuation</p>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.ClasswiseSample.idx","title":"idx  <code>instance-attribute</code>","text":"<pre><code>idx: IndexT | None\n</code></pre> <p>Index of current sample</p>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.ClasswiseSample.label","title":"label  <code>instance-attribute</code>","text":"<pre><code>label: int\n</code></pre> <p>Label of the current sample</p>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.ClasswiseSample.ooc_subset","title":"ooc_subset  <code>instance-attribute</code>","text":"<pre><code>ooc_subset: NDArray[IndexT]\n</code></pre> <p>Indices of out-of-class elements, i.e., those with a label different from this sample's label</p>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.ClasswiseSample.subset","title":"subset  <code>instance-attribute</code>","text":"<pre><code>subset: NDArray[IndexT]\n</code></pre> <p>Indices of current sample</p>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.ClasswiseSample.with_idx","title":"with_idx","text":"<pre><code>with_idx(idx: IndexT) -&gt; Self\n</code></pre> <p>Return a copy of sample with idx changed.</p> <p>Returns the original sample if idx is the same.</p> PARAMETER DESCRIPTION <code>idx</code> <p>New value for idx.</p> <p> TYPE: <code>IndexT</code> </p> RETURNS DESCRIPTION <code>Sample</code> <p>A copy of the sample with idx changed.</p> <p> TYPE: <code>Self</code> </p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def with_idx(self, idx: IndexT) -&gt; Self:\n    \"\"\"Return a copy of sample with idx changed.\n\n    Returns the original sample if idx is the same.\n\n    Args:\n        idx: New value for idx.\n\n    Returns:\n        Sample: A copy of the sample with idx changed.\n    \"\"\"\n    if self.idx == idx:\n        return self\n\n    return replace(self, idx=idx)\n</code></pre>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.ClasswiseSample.with_idx_in_subset","title":"with_idx_in_subset","text":"<pre><code>with_idx_in_subset() -&gt; Self\n</code></pre> <p>Return a copy of sample with idx added to the subset.</p> <p>Returns the original sample if idx was already part of the subset.</p> RETURNS DESCRIPTION <code>Sample</code> <p>A copy of the sample with idx added to the subset.</p> <p> TYPE: <code>Self</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If idx is None.</p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def with_idx_in_subset(self) -&gt; Self:\n    \"\"\"Return a copy of sample with idx added to the subset.\n\n    Returns the original sample if idx was already part of the subset.\n\n    Returns:\n        Sample: A copy of the sample with idx added to the subset.\n\n    Raises:\n        ValueError: If idx is None.\n    \"\"\"\n    if self.idx in self.subset:\n        return self\n\n    if self.idx is None:\n        raise ValueError(\"Cannot add idx to subset if idx is None.\")\n\n    new_subset = np.append(self.subset, self.idx)\n    return replace(self, subset=new_subset)\n</code></pre>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.ClasswiseSample.with_subset","title":"with_subset","text":"<pre><code>with_subset(subset: NDArray[IndexT]) -&gt; Self\n</code></pre> <p>Return a copy of sample with subset changed.</p> <p>Returns the original sample if subset is the same.</p> PARAMETER DESCRIPTION <code>subset</code> <p>New value for subset.</p> <p> TYPE: <code>NDArray[IndexT]</code> </p> RETURNS DESCRIPTION <code>Sample</code> <p>A copy of the sample with subset changed.</p> <p> TYPE: <code>Self</code> </p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def with_subset(self, subset: NDArray[IndexT]) -&gt; Self:\n    \"\"\"Return a copy of sample with subset changed.\n\n    Returns the original sample if subset is the same.\n\n    Args:\n        subset: New value for subset.\n\n    Returns:\n        Sample: A copy of the sample with subset changed.\n    \"\"\"\n    if np.array_equal(self.subset, subset):\n        return self\n\n    return replace(self, subset=subset)\n</code></pre>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.Sample","title":"Sample  <code>dataclass</code>","text":"<pre><code>Sample(idx: IndexT | None, subset: NDArray[IndexT])\n</code></pre>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.Sample.idx","title":"idx  <code>instance-attribute</code>","text":"<pre><code>idx: IndexT | None\n</code></pre> <p>Index of current sample</p>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.Sample.subset","title":"subset  <code>instance-attribute</code>","text":"<pre><code>subset: NDArray[IndexT]\n</code></pre> <p>Indices of current sample</p>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.Sample.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> <p>This type must be hashable for the utility caching to work. We use hashlib.sha256 which is about 4-5x faster than hash(), and returns the same value in all processes, as opposed to hash() which is salted in each process</p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def __hash__(self):\n    \"\"\"This type must be hashable for the utility caching to work.\n    We use hashlib.sha256 which is about 4-5x faster than hash(), and returns the\n    same value in all processes, as opposed to hash() which is salted in each\n    process\n    \"\"\"\n    sha256_hash = hashlib.sha256(self.subset.tobytes()).hexdigest()\n    return int(sha256_hash, base=16)\n</code></pre>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.Sample.with_idx","title":"with_idx","text":"<pre><code>with_idx(idx: IndexT) -&gt; Self\n</code></pre> <p>Return a copy of sample with idx changed.</p> <p>Returns the original sample if idx is the same.</p> PARAMETER DESCRIPTION <code>idx</code> <p>New value for idx.</p> <p> TYPE: <code>IndexT</code> </p> RETURNS DESCRIPTION <code>Sample</code> <p>A copy of the sample with idx changed.</p> <p> TYPE: <code>Self</code> </p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def with_idx(self, idx: IndexT) -&gt; Self:\n    \"\"\"Return a copy of sample with idx changed.\n\n    Returns the original sample if idx is the same.\n\n    Args:\n        idx: New value for idx.\n\n    Returns:\n        Sample: A copy of the sample with idx changed.\n    \"\"\"\n    if self.idx == idx:\n        return self\n\n    return replace(self, idx=idx)\n</code></pre>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.Sample.with_idx_in_subset","title":"with_idx_in_subset","text":"<pre><code>with_idx_in_subset() -&gt; Self\n</code></pre> <p>Return a copy of sample with idx added to the subset.</p> <p>Returns the original sample if idx was already part of the subset.</p> RETURNS DESCRIPTION <code>Sample</code> <p>A copy of the sample with idx added to the subset.</p> <p> TYPE: <code>Self</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If idx is None.</p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def with_idx_in_subset(self) -&gt; Self:\n    \"\"\"Return a copy of sample with idx added to the subset.\n\n    Returns the original sample if idx was already part of the subset.\n\n    Returns:\n        Sample: A copy of the sample with idx added to the subset.\n\n    Raises:\n        ValueError: If idx is None.\n    \"\"\"\n    if self.idx in self.subset:\n        return self\n\n    if self.idx is None:\n        raise ValueError(\"Cannot add idx to subset if idx is None.\")\n\n    new_subset = np.append(self.subset, self.idx)\n    return replace(self, subset=new_subset)\n</code></pre>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.Sample.with_subset","title":"with_subset","text":"<pre><code>with_subset(subset: NDArray[IndexT]) -&gt; Self\n</code></pre> <p>Return a copy of sample with subset changed.</p> <p>Returns the original sample if subset is the same.</p> PARAMETER DESCRIPTION <code>subset</code> <p>New value for subset.</p> <p> TYPE: <code>NDArray[IndexT]</code> </p> RETURNS DESCRIPTION <code>Sample</code> <p>A copy of the sample with subset changed.</p> <p> TYPE: <code>Self</code> </p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def with_subset(self, subset: NDArray[IndexT]) -&gt; Self:\n    \"\"\"Return a copy of sample with subset changed.\n\n    Returns the original sample if subset is the same.\n\n    Args:\n        subset: New value for subset.\n\n    Returns:\n        Sample: A copy of the sample with subset changed.\n    \"\"\"\n    if np.array_equal(self.subset, subset):\n        return self\n\n    return replace(self, subset=subset)\n</code></pre>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.SemivalueCoefficient","title":"SemivalueCoefficient","text":"<p>               Bases: <code>Protocol</code></p>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.SemivalueCoefficient.__call__","title":"__call__","text":"<pre><code>__call__(n: int, k: int) -&gt; float\n</code></pre> <p>A semi-value coefficient is a function of the number of elements in the set, and the size of the subset for which the coefficient is being computed. Because both coefficients and sampler weights can be very large or very small, we perform all computations in log-space to avoid numerical issues.</p> PARAMETER DESCRIPTION <code>n</code> <p>Total number of elements in the set.</p> <p> TYPE: <code>int</code> </p> <code>k</code> <p>Size of the subset for which the coefficient is being computed</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The natural logarithm of the semi-value coefficient.</p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def __call__(self, n: int, k: int) -&gt; float:\n    \"\"\"A semi-value coefficient is a function of the number of elements in the set,\n    and the size of the subset for which the coefficient is being computed.\n    Because both coefficients and sampler weights can be very large or very small,\n    we perform all computations in log-space to avoid numerical issues.\n\n    Args:\n        n: Total number of elements in the set.\n        k: Size of the subset for which the coefficient is being computed\n\n    Returns:\n        The natural logarithm of the semi-value coefficient.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/types/#pydvl.valuation.types.ValueUpdate","title":"ValueUpdate  <code>dataclass</code>","text":"<pre><code>ValueUpdate(idx: IndexT | None, log_update: float, sign: int)\n</code></pre> <p>ValueUpdates are emitted by evaluation strategies.</p> <p>Typically, a value update is the product of a marginal utility, the sampler weight and the valuation's coefficient. Instead of multiplying weights, coefficients and utilities directly, the strategy works in log-space for numerical stability using the samplers' log-weights and the valuation methods' log-coefficients.</p> <p>The updates from all workers are converted back to linear space by LogResultUpdater.</p> ATTRIBUTE DESCRIPTION <code>idx</code> <p>Index of the sample the update corresponds to.</p> <p> TYPE: <code>IndexT | None</code> </p> <code>log_update</code> <p>Logarithm of the absolute value of the update.</p> <p> TYPE: <code>float</code> </p> <code>sign</code> <p>Sign of the update.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def __init__(self, idx: IndexT | None, log_update: float, sign: int):\n    object.__setattr__(self, \"idx\", idx)\n    object.__setattr__(self, \"log_update\", log_update)\n    object.__setattr__(self, \"sign\", sign)\n</code></pre>"},{"location":"api/pydvl/valuation/methods/","title":"Methods","text":""},{"location":"api/pydvl/valuation/methods/#pydvl.valuation.methods","title":"pydvl.valuation.methods","text":"<p>This module contains the implementations of all valuation methods.</p> <p>This includes semi-values, model-specific methods and any related algorithms. For several of the semi-value methods there are two implementations:</p> <ul> <li>A generic one combining some of the available samplers   with a subclass of   SemivalueValuation which   defines a coefficient function.</li> <li>A dedicated class that chooses the right sampler to avoid numerical issues when   computing otherwise necessary importance sampling corrections. You can read more about   this in Sampling strategies for semi-values.</li> </ul> <p>Info</p> <p>For a full list of algorithms see: All data valuation methods implemented.</p>"},{"location":"api/pydvl/valuation/methods/banzhaf/","title":"Banzhaf","text":""},{"location":"api/pydvl/valuation/methods/banzhaf/#pydvl.valuation.methods.banzhaf","title":"pydvl.valuation.methods.banzhaf","text":"<p>This module implements the Banzhaf valuation method, as described in Wang and Jia, (2022)<sup>1</sup>.</p> <p>Data Banzhaf was proposed as a means to counteract the inherent stochasticity of the utility function in machine learning problems. It chooses the coefficients \\(w(k)\\) of the semi-value valuation function to be constant \\(2^{n-1}\\) for all set sizes \\(k,\\) yielding:</p> \\[ v_\\text{bzf}(i) = \\frac{1}{2^{n-1}} \\sum_{S \\sim P(D_{-i})} [u(S_{+i}) - u(S)], \\] <p>Background on semi-values</p> <p>The Banzhaf valuation is a special case of the semi-value valuation method. You can read a short introduction in the documentation.</p> <p>The intuition for picking a constant weight is that for any choice of weight function \\(w\\), one can always construct a utility with higher variance where \\(w\\) is greater. Therefore, in a worst-case sense, the best one can do is to pick a constant weight.</p> <p>Data Banzhaf proves to outperform many other valuation methods in downstream tasks like best point removal.</p>"},{"location":"api/pydvl/valuation/methods/banzhaf/#pydvl.valuation.methods.banzhaf--maximum-sample-reuse-banzhaf","title":"Maximum Sample Reuse Banzhaf","text":"<p>A special sampling scheme (MSR) that reuses each sample to update every index in the dataset is shown by Wang and Jia to be optimal for the Banzhaf valuation: not only does it drastically reduce the number of sets needed, but the sampling distribution also matches the Banzhaf indices, in the sense explained in Sampling strategies for semi-values.</p> <p>In order to work with this sampler for Banzhaf values, you can use MSRBanzhafValuation. In principle, it is also possible to select the MSRSampler when instantiating BanzhafValuation, but this might introduce some numerical instability, as explained in the document linked above.</p>"},{"location":"api/pydvl/valuation/methods/banzhaf/#pydvl.valuation.methods.banzhaf--references","title":"References","text":"<ol> <li> <p> Wang, Jiachen T., and Ruoxi Jia. Data Banzhaf: A   Robust Data Valuation Framework for Machine   Learning. In Proceedings of The   26th International Conference on Artificial Intelligence and Statistics,   6388\u20136421. PMLR, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/methods/banzhaf/#pydvl.valuation.methods.banzhaf.BanzhafValuation","title":"BanzhafValuation","text":"<pre><code>BanzhafValuation(\n    utility: UtilityBase,\n    sampler: IndexSampler,\n    is_done: StoppingCriterion,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n)\n</code></pre> <p>               Bases: <code>SemivalueValuation</code></p> <p>Computes Banzhaf values.</p> Source code in <code>src/pydvl/valuation/methods/semivalue.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    sampler: IndexSampler,\n    is_done: StoppingCriterion,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n):\n    super().__init__()\n    self.utility = utility\n    self.sampler = sampler\n    self.is_done = is_done\n    self.skip_converged = skip_converged\n    if skip_converged:  # test whether the sampler supports skipping indices:\n        self.sampler.skip_indices = np.array([], dtype=np.int_)\n    self.show_warnings = show_warnings\n    self.tqdm_args: dict[str, Any] = {\"desc\": str(self)}\n    # HACK: parse additional args for the progress bar if any (we probably want\n    #  something better)\n    if isinstance(progress, bool):\n        self.tqdm_args.update({\"disable\": not progress})\n    elif isinstance(progress, dict):\n        self.tqdm_args.update(progress)\n    else:\n        raise TypeError(f\"Invalid type for progress: {type(progress)}\")\n</code></pre>"},{"location":"api/pydvl/valuation/methods/banzhaf/#pydvl.valuation.methods.banzhaf.BanzhafValuation.log_coefficient","title":"log_coefficient  <code>property</code>","text":"<pre><code>log_coefficient: SemivalueCoefficient | None\n</code></pre> <p>Returns the log-coefficient of the Banzhaf valuation.</p>"},{"location":"api/pydvl/valuation/methods/banzhaf/#pydvl.valuation.methods.banzhaf.BanzhafValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/banzhaf/#pydvl.valuation.methods.banzhaf.MSRBanzhafValuation","title":"MSRBanzhafValuation","text":"<pre><code>MSRBanzhafValuation(\n    utility: UtilityBase,\n    is_done: StoppingCriterion,\n    batch_size: int = 1,\n    seed: Seed | None = None,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n)\n</code></pre> <p>               Bases: <code>SemivalueValuation</code></p> <p>Computes Banzhaf values with Maximum Sample Reuse.</p> <p>This can be seen as a convenience class that wraps the MSRSampler but in fact it also skips importance sampling altogether, since the MSR sampling scheme already provides the correct weights for the Monte Carlo approximation. This can avoid some numerical inaccuracies that can arise, when using an <code>MSRSampler</code> with BanzhafValuation, despite the fact that the respective coefficients cancel each other out analytically.</p> Source code in <code>src/pydvl/valuation/methods/banzhaf.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    is_done: StoppingCriterion,\n    batch_size: int = 1,\n    seed: Seed | None = None,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n):\n    sampler = MSRSampler(batch_size=batch_size, seed=seed)\n    super().__init__(\n        utility, sampler, is_done, skip_converged, show_warnings, progress\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/methods/banzhaf/#pydvl.valuation.methods.banzhaf.MSRBanzhafValuation.log_coefficient","title":"log_coefficient  <code>property</code>","text":"<pre><code>log_coefficient: SemivalueCoefficient | None\n</code></pre> <p>Disable importance sampling for this method since we have a fixed sampler that already provides the correct weights for the Monte Carlo approximation.</p>"},{"location":"api/pydvl/valuation/methods/banzhaf/#pydvl.valuation.methods.banzhaf.MSRBanzhafValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/beta_shapley/","title":"Beta shapley","text":""},{"location":"api/pydvl/valuation/methods/beta_shapley/#pydvl.valuation.methods.beta_shapley","title":"pydvl.valuation.methods.beta_shapley","text":"<p>This module implements Beta-Shapley valuation as introduced in Kwon and Zou (2022)<sup>1</sup>.</p> <p>Background on semi-values</p> <p>Beta-Shapley is a special case of the semi-value valuation method. You can read a short introduction in the documentation.</p> <p>Beta(\\(\\alpha\\), \\(\\beta\\))-Shapley is a semi-value whose coefficients are given by the Beta function. The coefficients are defined as:</p> \\[ \\begin{eqnarray*}   w_{\\alpha, \\beta} (n, k) &amp; := &amp; \\int_0^1 t^{k - 1}  (1 - t)^{n - k}   \\frac{t^{\\beta - 1}  (1 - t)^{\\alpha - 1}}{\\text{Beta} (\\alpha, \\beta)}   \\mathrm{d} t\\\\   &amp; = &amp; \\frac{\\text{Beta} (k + \\beta - 1, n - k + \\alpha)}{\\text{Beta}   (\\alpha, \\beta)}. \\end{eqnarray*} \\] <p>Note that this deviates by a factor \\(n\\) from eq. (5) in Kwon and Zou (2022)<sup>1</sup> because of how we define sampler weights, but the effective coefficient remains the same when using any PowersetSampler or PermutationSampler.</p>"},{"location":"api/pydvl/valuation/methods/beta_shapley/#pydvl.valuation.methods.beta_shapley--connection-to-ame","title":"Connection to AME","text":"<p>Beta-Shapley can be seen as a special case of AME, introduced in Lin et al. (2022)<sup>2</sup>.</p> <p>Todo</p> <p>Explain sampler choices for AME and how to estimate Beta-Shapley with lasso.</p>"},{"location":"api/pydvl/valuation/methods/beta_shapley/#pydvl.valuation.methods.beta_shapley--references","title":"References","text":"<ol> <li> <p>Kwon, Yongchan, and James Zou. Beta Shapley: A   Unified and Noise-Reduced Data Valuation Framework for Machine   Learning. In Proceedings of The   25th International Conference on Artificial Intelligence and Statistics,   8780\u20138802. PMLR, 2022.\u00a0\u21a9\u21a9</p> </li> <li> <p>Lin, Jinkun, Anqi Zhang, Mathias L\u00e9cuyer, Jinyang   Li, Aurojit Panda, and Siddhartha Sen. Measuring the Effect of Training Data on   Deep Learning Predictions via Randomized   Experiments. In Proceedings of   the 39th International Conference on Machine Learning, 13468\u2013504. PMLR, 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/methods/beta_shapley/#pydvl.valuation.methods.beta_shapley.BetaShapleyValuation","title":"BetaShapleyValuation","text":"<pre><code>BetaShapleyValuation(\n    utility: UtilityBase,\n    sampler: IndexSampler,\n    is_done: StoppingCriterion,\n    alpha: float,\n    beta: float,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: bool = False,\n)\n</code></pre> <p>               Bases: <code>SemivalueValuation</code></p> <p>Computes Beta-Shapley values.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Object to compute utilities.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>sampler</code> <p>Sampling scheme to use.</p> <p> TYPE: <code>IndexSampler</code> </p> <code>is_done</code> <p>Stopping criterion to use.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>alpha</code> <p>The alpha parameter of the Beta distribution.</p> <p> TYPE: <code>float</code> </p> <code>beta</code> <p>The beta parameter of the Beta distribution.</p> <p> TYPE: <code>float</code> </p> <code>skip_converged</code> <p>Whether to skip converged indices. Convergence is determined by the stopping criterion's <code>converged</code> array.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>show_warnings</code> <p>Whether to show any runtime warnings.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <p>Whether to show a progress bar. If a dictionary, it is passed to <code>tqdm</code> as keyword arguments, and the progress bar is displayed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/valuation/methods/beta_shapley.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    sampler: IndexSampler,\n    is_done: StoppingCriterion,\n    alpha: float,\n    beta: float,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: bool = False,\n):\n    super().__init__(\n        utility,\n        sampler,\n        is_done,\n        skip_converged=skip_converged,\n        show_warnings=show_warnings,\n        progress=progress,\n    )\n\n    self.alpha = alpha\n    self.beta = beta\n</code></pre>"},{"location":"api/pydvl/valuation/methods/beta_shapley/#pydvl.valuation.methods.beta_shapley.BetaShapleyValuation.log_coefficient","title":"log_coefficient  <code>property</code>","text":"<pre><code>log_coefficient: SemivalueCoefficient | None\n</code></pre> <p>Beta-Shapley coefficient.</p> <p>Defined (up to a constant n) as eq. (5) of Kwon and Zou (2023)<sup>1</sup>.</p>"},{"location":"api/pydvl/valuation/methods/beta_shapley/#pydvl.valuation.methods.beta_shapley.BetaShapleyValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/classwise_shapley/","title":"Classwise shapley","text":""},{"location":"api/pydvl/valuation/methods/classwise_shapley/#pydvl.valuation.methods.classwise_shapley","title":"pydvl.valuation.methods.classwise_shapley","text":"<p>Class-wise Shapley (Schoch et al., 2022)<sup>1</sup> is a semi-value tailored for classification problems.</p> <p>The core intuition behind the method is that a sample might enhance the overall performance of the model, while being detrimental for the performance when the model is restricted to items of the same class, and vice versa.</p> <p>Analysis of Class-wise Shapley</p> <p>For a detailed explanation and analysis of the method, with comparison to other valuation techniques, please refer to the main documentation and to Semmler and de Benito Delgado (2024).<sup>2</sup></p>"},{"location":"api/pydvl/valuation/methods/classwise_shapley/#pydvl.valuation.methods.classwise_shapley--references","title":"References","text":"<ol> <li> <p>Schoch, Stephanie, Haifeng Xu, and Yangfeng Ji. CS-Shapley: Class-wise Shapley Values for Data Valuation in Classification. In Proc. of the Thirty-Sixth Conference on Neural Information Processing Systems (NeurIPS). New Orleans, Louisiana, USA, 2022.\u00a0\u21a9</p> </li> <li> <p>Semmler, Markus, and Miguel de Benito Delgado. [Re] Classwise-Shapley Values for Data Valuation. Transactions on Machine Learning Research, July 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/methods/classwise_shapley/#pydvl.valuation.methods.classwise_shapley.ClasswiseShapleyValuation","title":"ClasswiseShapleyValuation","text":"<pre><code>ClasswiseShapleyValuation(\n    utility: ClasswiseModelUtility,\n    sampler: ClasswiseSampler,\n    is_done: StoppingCriterion,\n    progress: dict[str, Any] | bool = False,\n    *,\n    normalize_values: bool = True\n)\n</code></pre> <p>               Bases: <code>Valuation</code></p> <p>Class to compute Class-wise Shapley values.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Class-wise utility object with model and class-wise scoring function.</p> <p> TYPE: <code>ClasswiseModelUtility</code> </p> <code>sampler</code> <p>Class-wise sampling scheme to use.</p> <p> TYPE: <code>ClasswiseSampler</code> </p> <code>is_done</code> <p>Stopping criterion to use.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>progress</code> <p>Whether to show a progress bar.</p> <p> TYPE: <code>dict[str, Any] | bool</code> DEFAULT: <code>False</code> </p> <code>normalize_values</code> <p>Whether to normalize values after valuation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/methods/classwise_shapley.py</code> <pre><code>def __init__(\n    self,\n    utility: ClasswiseModelUtility,\n    sampler: ClasswiseSampler,\n    is_done: StoppingCriterion,\n    progress: dict[str, Any] | bool = False,\n    *,\n    normalize_values: bool = True,\n):\n    super().__init__()\n    self.utility = utility\n    self.sampler = sampler\n    self.labels: NDArray | None = None\n    if not isinstance(utility.scorer, ClasswiseSupervisedScorer):\n        raise ValueError(\"scorer must be an instance of ClasswiseSupervisedScorer\")\n    self.scorer: ClasswiseSupervisedScorer = utility.scorer\n    self.is_done = is_done\n    self.tqdm_args: dict[str, Any] = {\n        \"desc\": f\"{self.__class__.__name__}: {str(is_done)}\"\n    }\n    # HACK: parse additional args for the progress bar if any (we probably want\n    #  something better)\n    if isinstance(progress, bool):\n        self.tqdm_args.update({\"disable\": not progress})\n    else:\n        self.tqdm_args.update(progress if isinstance(progress, dict) else {})\n    self.normalize_values = normalize_values\n</code></pre>"},{"location":"api/pydvl/valuation/methods/classwise_shapley/#pydvl.valuation.methods.classwise_shapley.ClasswiseShapleyValuation._normalize","title":"_normalize","text":"<pre><code>_normalize() -&gt; ValuationResult\n</code></pre> <p>Normalize a valuation result specific to classwise Shapley.</p> <p>Each value \\(v_i\\) associated with the sample \\((x_i, y_i)\\) is normalized by multiplying it with \\(a_S(D_{y_i})\\) and dividing by \\(\\sum_{j \\in D_{y_i}} v_j\\). For more details, see (Schoch et al., 2022) <sup>1 </sup>.</p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Normalized ValuationResult object.</p> Source code in <code>src/pydvl/valuation/methods/classwise_shapley.py</code> <pre><code>def _normalize(self) -&gt; ValuationResult:\n    r\"\"\"\n    Normalize a valuation result specific to classwise Shapley.\n\n    Each value $v_i$ associated with the sample $(x_i, y_i)$ is normalized by\n    multiplying it with $a_S(D_{y_i})$ and dividing by $\\sum_{j \\in D_{y_i}} v_j$.\n    For more details, see (Schoch et al., 2022) &lt;sup&gt;&lt;a\n    href=\"#schoch_csshapley_2022\"&gt;1&lt;/a&gt; &lt;/sup&gt;.\n\n    Returns:\n        Normalized ValuationResult object.\n    \"\"\"\n    if self.result is None:\n        raise ValueError(\"You must call fit before calling _normalize()\")\n\n    if self.utility.training_data is None:\n        raise ValueError(\"You should call fit before calling _normalize()\")\n\n    logger.info(\"Normalizing valuation result.\")\n    x, y = self.utility.training_data.data()\n    unique_labels = get_unique_labels(y)\n    self.utility.model.fit(x, y)\n\n    for idx_label, label in enumerate(unique_labels):\n        active_elements = y == label\n        indices_label_set = np.where(active_elements)[0]\n        indices_label_set = self.utility.training_data.indices[indices_label_set]\n\n        self.scorer.label = label\n        in_class_acc, _ = self.scorer.compute_in_and_out_of_class_scores(\n            self.utility.model\n        )\n\n        sigma = np.sum(self.result.values[indices_label_set])\n        if sigma != 0:\n            self.result.scale(in_class_acc / sigma, data_indices=indices_label_set)\n\n    return self.result\n</code></pre>"},{"location":"api/pydvl/valuation/methods/classwise_shapley/#pydvl.valuation.methods.classwise_shapley.ClasswiseShapleyValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/data_oob/","title":"Data oob","text":""},{"location":"api/pydvl/valuation/methods/data_oob/#pydvl.valuation.methods.data_oob","title":"pydvl.valuation.methods.data_oob","text":"<p>This module implements the method described in Kwon and Zou, (2023).<sup>1</sup></p> <p>Data-OOB value is tailored to bagging models. It defines a data point's value as the average loss of the estimators which were not fit on it.</p> <p>As such it is not a semi-value, and it is not based on marginal contributions.</p> <p>Info</p> <p>For details on the method and a discussion on how and whether to use it by bagging models a posteriori, see the main documentation.</p>"},{"location":"api/pydvl/valuation/methods/data_oob/#pydvl.valuation.methods.data_oob--references","title":"References","text":"<ol> <li> <p> Kwon, Yongchan, and James Zou. Data-OOB:   Out-of-bag Estimate as a Simple and Efficient Data   Value. In Proceedings of the   40th International Conference on Machine Learning, 18135\u201352. PMLR, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/methods/data_oob/#pydvl.valuation.methods.data_oob.DataOOBValuation","title":"DataOOBValuation","text":"<pre><code>DataOOBValuation(model: BaggingModel, score: PointwiseScore | None = None)\n</code></pre> <p>               Bases: <code>Valuation</code></p> <p>Computes Data Out-Of-Bag values.</p> <p>This class implements the method described in Kwon and Zou, (2023)<sup>1</sup>.</p> PARAMETER DESCRIPTION <code>model</code> <p>A fitted bagging model. Bagging models in sklearn include [[BaggingClassifier]], [[BaggingRegressor]], [[IsolationForest]], RandomForest, ExtraTrees, or any model which defines an attribute <code>estimators_</code> and uses bootstrapped subsamples to compute predictions.</p> <p> TYPE: <code>BaggingModel</code> </p> <code>score</code> <p>A callable for point-wise comparison of true values with the predictions. If <code>None</code>, uses point-wise accuracy for classifiers and negative \\(l_2\\) distance for regressors.</p> <p> TYPE: <code>PointwiseScore | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/methods/data_oob.py</code> <pre><code>def __init__(\n    self,\n    model: BaggingModel,\n    score: PointwiseScore | None = None,\n):\n    super().__init__()\n    self.model = model\n    self.score = score\n</code></pre>"},{"location":"api/pydvl/valuation/methods/data_oob/#pydvl.valuation.methods.data_oob.DataOOBValuation.fit","title":"fit","text":"<pre><code>fit(data: Dataset) -&gt; Self\n</code></pre> <p>Compute the Data-OOB values.</p> <p>This requires the bagging model passed upon construction to be fitted.</p> PARAMETER DESCRIPTION <code>data</code> <p>Data for which to compute values</p> <p> TYPE: <code>Dataset</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The fitted object.</p> Source code in <code>src/pydvl/valuation/methods/data_oob.py</code> <pre><code>def fit(self, data: Dataset) -&gt; Self:\n    \"\"\"Compute the Data-OOB values.\n\n    This requires the bagging model passed upon construction to be fitted.\n\n    Args:\n        data: Data for which to compute values\n\n    Returns:\n        The fitted object.\n    \"\"\"\n    # TODO: automate str representation for all Valuations\n    algorithm_name = f\"Data-OOB-{str(self.model)}\"\n    self.result = ValuationResult.empty(\n        algorithm=algorithm_name,\n        indices=data.indices,\n        data_names=data.names,\n    )\n\n    check_is_fitted(\n        self.model,\n        msg=\"The bagging model has to be fitted before calling the valuation method.\",\n    )\n\n    # This should always be present after fitting\n    try:\n        estimators = self.model.estimators_  # type: ignore\n    except AttributeError:\n        raise ValueError(\n            \"The model has to be an sklearn-compatible bagging model, including \"\n            \"BaggingClassifier, BaggingRegressor, IsolationForest, RandomForest*, \"\n            \"and ExtraTrees*\"\n        )\n\n    if self.score is None:\n        self.score = (\n            point_wise_accuracy if is_classifier(self.model) else neg_l2_distance\n        )\n\n    if hasattr(self.model, \"estimators_samples_\"):  # Bagging(Classifier|Regressor)\n        unsampled_indices = [\n            np.setxor1d(data.indices, np.unique(sampled))\n            for sampled in self.model.estimators_samples_\n        ]\n    else:  # RandomForest*, ExtraTrees*, IsolationForest\n        n_samples_bootstrap = _get_n_samples_bootstrap(\n            len(data), self.model.max_samples\n        )\n        unsampled_indices = [\n            _generate_unsampled_indices(\n                est.random_state, len(data.indices), n_samples_bootstrap\n            )\n            for est in estimators\n        ]\n\n    for est, oob_indices in zip(estimators, unsampled_indices):\n        subset = data[oob_indices].data()\n        score_array = self.score(y_true=subset.y, y_pred=est.predict(subset.x))\n        self.result += ValuationResult(\n            algorithm=algorithm_name,\n            indices=oob_indices,\n            names=data[oob_indices].names,\n            values=score_array,\n            counts=np.ones_like(score_array, dtype=data.indices.dtype),\n        )\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/methods/data_oob/#pydvl.valuation.methods.data_oob.DataOOBValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/data_oob/#pydvl.valuation.methods.data_oob.neg_l2_distance","title":"neg_l2_distance","text":"<pre><code>neg_l2_distance(y_true: NDArray[T], y_pred: NDArray[T]) -&gt; NDArray[T]\n</code></pre> <p>Point-wise negative \\(l_2\\) distance between two arrays.</p> <p>Higher is better.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>Array of true values (e.g. labels)</p> <p> TYPE: <code>NDArray[T]</code> </p> <code>y_pred</code> <p>Array of estimated values (e.g. model predictions)</p> <p> TYPE: <code>NDArray[T]</code> </p> RETURNS DESCRIPTION <code>NDArray[T]</code> <p>Array with point-wise negative \\(l_2\\) distances between labels and model</p> <code>NDArray[T]</code> <p>predictions</p> Source code in <code>src/pydvl/valuation/methods/data_oob.py</code> <pre><code>def neg_l2_distance(y_true: NDArray[T], y_pred: NDArray[T]) -&gt; NDArray[T]:\n    r\"\"\"Point-wise negative $l_2$ distance between two arrays.\n\n    Higher is better.\n\n    Args:\n        y_true: Array of true values (e.g. labels)\n        y_pred: Array of estimated values (e.g. model predictions)\n\n    Returns:\n        Array with point-wise negative $l_2$ distances between labels and model\n        predictions\n    \"\"\"\n    return -np.square(np.array(y_pred - y_true), dtype=y_pred.dtype)\n</code></pre>"},{"location":"api/pydvl/valuation/methods/data_oob/#pydvl.valuation.methods.data_oob.point_wise_accuracy","title":"point_wise_accuracy","text":"<pre><code>point_wise_accuracy(y_true: NDArray[T], y_pred: NDArray[T]) -&gt; NDArray[T]\n</code></pre> <p>Point-wise accuracy, or 0-1 score between two arrays.</p> <p>Higher is better.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>Array of true values (e.g. labels)</p> <p> TYPE: <code>NDArray[T]</code> </p> <code>y_pred</code> <p>Array of estimated values (e.g. model predictions)</p> <p> TYPE: <code>NDArray[T]</code> </p> RETURNS DESCRIPTION <code>NDArray[T]</code> <p>Array with point-wise 0-1 accuracy between labels and model predictions</p> Source code in <code>src/pydvl/valuation/methods/data_oob.py</code> <pre><code>def point_wise_accuracy(y_true: NDArray[T], y_pred: NDArray[T]) -&gt; NDArray[T]:\n    \"\"\"Point-wise accuracy, or 0-1 score between two arrays.\n\n    Higher is better.\n\n    Args:\n        y_true: Array of true values (e.g. labels)\n        y_pred: Array of estimated values (e.g. model predictions)\n\n    Returns:\n        Array with point-wise 0-1 accuracy between labels and model predictions\n    \"\"\"\n    return np.array(y_pred == y_true, dtype=y_pred.dtype)\n</code></pre>"},{"location":"api/pydvl/valuation/methods/delta_shapley/","title":"Delta shapley","text":""},{"location":"api/pydvl/valuation/methods/delta_shapley/#pydvl.valuation.methods.delta_shapley","title":"pydvl.valuation.methods.delta_shapley","text":"<p>This module implements the \\(\\delta\\)-Shapley valuation method, introduced by Watson et al. (2023)<sup>1</sup>.</p> <p>\\(\\delta\\)-Shapley uses a stratified sampling approach to accurately approximate Shapley values for certain model classes, based on uniform stability bounds.</p> <p>Additionally, it reduces computation by skipping the marginal utilities for set sizes outside a small range.<sup>2</sup></p> <p>Info</p> <p>See the documentation or Watson et al. (2023)<sup>1</sup> for a more detailed introduction to the method.</p>"},{"location":"api/pydvl/valuation/methods/delta_shapley/#pydvl.valuation.methods.delta_shapley--references","title":"References","text":"<ol> <li> <p>Watson, Lauren, Zeno Kujawa, Rayna Andreeva, Hao-Tsung Yang, Tariq Elahi, and Rik   Sarkar. Accelerated Shapley Value Approximation for Data   Evaluation. arXiv, 9 November 2023.\u00a0\u21a9\u21a9</p> </li> <li> <p>When this is done, the final values are off by a constant factor with respect to   the true Shapley values.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/methods/delta_shapley/#pydvl.valuation.methods.delta_shapley.DeltaShapleyValuation","title":"DeltaShapleyValuation","text":"<pre><code>DeltaShapleyValuation(\n    utility: UtilityBase,\n    sampler: StratifiedSampler | StratifiedPermutationSampler,\n    is_done: StoppingCriterion,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n)\n</code></pre> <p>               Bases: <code>SemivalueValuation</code></p> <p>Computes \\(\\delta\\)-Shapley values.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Object to compute utilities.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>sampler</code> <p>The sampling scheme to use. Must be a stratified sampler.</p> <p> TYPE: <code>StratifiedSampler | StratifiedPermutationSampler</code> </p> <code>is_done</code> <p>Stopping criterion to use.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>skip_converged</code> <p>Whether to skip converged indices, as determined by the stopping criterion's <code>converged</code> array.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>show_warnings</code> <p>Whether to show warnings.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <p>Whether to show a progress bar. If a dictionary, it is passed to <code>tqdm</code> as keyword arguments, and the progress bar is displayed.</p> <p> TYPE: <code>dict[str, Any] | bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/valuation/methods/delta_shapley.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    sampler: StratifiedSampler | StratifiedPermutationSampler,\n    is_done: StoppingCriterion,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n):\n    super().__init__(\n        utility, sampler, is_done, skip_converged, show_warnings, progress\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/methods/delta_shapley/#pydvl.valuation.methods.delta_shapley.DeltaShapleyValuation.log_coefficient","title":"log_coefficient  <code>property</code>","text":"<pre><code>log_coefficient: SemivalueCoefficient | None\n</code></pre> <p>Returns the log-coefficient of the \\(\\delta\\)-Shapley valuation.</p> <p>This is constructed to account for the sampling distribution of a StratifiedSampler and yield the Shapley coefficient as effective coefficient (truncated by the size bounds in the sampler).</p> <p>Normalization</p> <p>This coefficient differs from the one used in the original paper by a normalization factor of \\(m=\\sum_k m_k,\\) where \\(m_k\\) is the number of samples of size \\(k\\). Since, contrary to their layer-wise means, we are computing running averages of all \\(m\\) value updates, this cancels out, and we are left with the same effective coefficient.</p>"},{"location":"api/pydvl/valuation/methods/delta_shapley/#pydvl.valuation.methods.delta_shapley.DeltaShapleyValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/gt_shapley/","title":"Gt shapley","text":""},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley","title":"pydvl.valuation.methods.gt_shapley","text":"<p>This module implements Group Testing for the approximation of Shapley values, as introduced in (Jia, R. et al., 2019).<sup>1</sup></p> <p>Computation of the Shapley value is transformed into a feasibility problem, where the constraints involve certain utility evaluations of sampled subsets. The sampling is done in such a way that an approximation to the true Shapley values can be computed with guarantees.<sup>3</sup></p> <p>Warning</p> <p>Group Testing is very inefficient. Potential improvements to the implementation notwithstanding, convergence seems to be very slow (in terms of evaluations of the utility required). We recommend other Monte Carlo methods instead. See the whole list here.</p> <p>You can read about data valuation in general in the documentation.</p>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley--references","title":"References","text":"<ol> <li> <p>Jia, R. et al., 2019.   Towards Efficient Data Valuation Based on the Shapley   Value.   In: Proceedings of the 22nd International Conference on Artificial Intelligence   and Statistics, pp. 1167\u20131176. PMLR.\u00a0\u21a9</p> </li> <li> <p>Jia, R. et al., 2023. A Note on \"Towards Efficient Data Valuation Based on the Shapley Value\".\u00a0\u21a9</p> </li> <li> <p>Internally, this sampling is achieved with a   StratifiedSampler with   GroupTestingSampleSize strategy.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley.GroupTestingProblem","title":"GroupTestingProblem","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Solver agnostic representation of the group-testing problem.</p>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley.GroupTestingShapleyValuation","title":"GroupTestingShapleyValuation","text":"<pre><code>GroupTestingShapleyValuation(\n    utility: UtilityBase,\n    n_samples: int,\n    epsilon: float,\n    solver_options: dict | None = None,\n    progress: bool = True,\n    seed: Seed | None = None,\n    batch_size: int = 1,\n)\n</code></pre> <p>               Bases: <code>Valuation</code></p> <p>Class to calculate the group-testing approximation to shapley values.</p> <p>See (Jia, R. et al., 2019)<sup>1</sup> for a description of the method, and Data valuation for an overview of data valuation.</p> <p>Warning</p> <p>Group Testing is very inefficient. Potential improvements to the implementation notwithstanding, convergence seems to be very slow (in terms of evaluations of the utility required). We recommend other Monte Carlo methods instead. See the whole list here.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Utility object with model, data and scoring function.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>n_samples</code> <p>The number of samples to use. A sample size with theoretical guarantees can be computed using compute_n_samples().</p> <p> TYPE: <code>int</code> </p> <code>epsilon</code> <p>The error tolerance.</p> <p> TYPE: <code>float</code> </p> <code>solver_options</code> <p>Optional dictionary containing a CVXPY solver and options to configure it. For valid values to the \"solver\" key see this tutorial. For additional options cvxpy's documentation.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to show a progress bar during the construction of the group-testing problem.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>seed</code> <p>Seed for the random number generator.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>The number of samples to draw in each batch. Can be used to reduce parallelization overhead for fast utilities. Defaults to 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>src/pydvl/valuation/methods/gt_shapley.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    n_samples: int,\n    epsilon: float,\n    solver_options: dict | None = None,\n    progress: bool = True,\n    seed: Seed | None = None,\n    batch_size: int = 1,\n):\n    super().__init__()\n\n    self._utility = utility\n    self._n_samples = n_samples\n    self._solver_options = solver_options\n    self._progress = progress\n    self._sampler = StratifiedSampler(\n        index_iteration=NoIndexIteration,\n        sample_sizes=GroupTestingSampleSize(),\n        sample_sizes_iteration=RandomSizeIteration,\n        batch_size=batch_size,\n        seed=seed,\n    )\n    self._epsilon = epsilon\n</code></pre>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley.GroupTestingShapleyValuation.fit","title":"fit","text":"<pre><code>fit(data: Dataset) -&gt; Self\n</code></pre> <p>Calculate the group-testing valuation on a dataset.</p> <p>This method has to be called before calling <code>values()</code>.</p> <p>Calculating the least core valuation is a computationally expensive task that can be parallelized. To do so, call the <code>fit()</code> method inside a <code>joblib.parallel_config</code> context manager as follows:</p> <pre><code>from joblib import parallel_config\n\nwith parallel_config(n_jobs=4):\n    valuation.fit(data)\n</code></pre> Source code in <code>src/pydvl/valuation/methods/gt_shapley.py</code> <pre><code>def fit(self, data: Dataset) -&gt; Self:\n    \"\"\"Calculate the group-testing valuation on a dataset.\n\n    This method has to be called before calling `values()`.\n\n    Calculating the least core valuation is a computationally expensive task that\n    can be parallelized. To do so, call the `fit()` method inside a\n    `joblib.parallel_config` context manager as follows:\n\n    ```python\n    from joblib import parallel_config\n\n    with parallel_config(n_jobs=4):\n        valuation.fit(data)\n    ```\n\n    \"\"\"\n    self._utility = self._utility.with_dataset(data)\n\n    problem = create_group_testing_problem(\n        utility=self._utility,\n        sampler=self._sampler,\n        n_samples=self._n_samples,\n        progress=self._progress,\n        epsilon=self._epsilon,\n    )\n\n    solution = solve_group_testing_problem(\n        problem=problem,\n        solver_options=self._solver_options,\n        algorithm_name=self.algorithm_name,\n        data_names=data.names,\n    )\n\n    self.result = solution\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley.GroupTestingShapleyValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley._calculate_utility_differences","title":"_calculate_utility_differences","text":"<pre><code>_calculate_utility_differences(\n    utility_values: NDArray[float64], masks: NDArray[bool_], n_obs: int\n) -&gt; NDArray[float64]\n</code></pre> <p>Calculate utility differences from utility values and sample masks.</p> PARAMETER DESCRIPTION <code>utility_values</code> <p>1D array with utility values.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>masks</code> <p>2D array with sample masks.</p> <p> TYPE: <code>NDArray[bool_]</code> </p> <code>n_obs</code> <p>The number of observations.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>NDArray[float64]</code> <p>The utility differences.</p> Source code in <code>src/pydvl/valuation/methods/gt_shapley.py</code> <pre><code>def _calculate_utility_differences(\n    utility_values: NDArray[np.float64],\n    masks: NDArray[np.bool_],\n    n_obs: int,\n) -&gt; NDArray[np.float64]:\n    \"\"\"Calculate utility differences from utility values and sample masks.\n\n    Args:\n        utility_values: 1D array with utility values.\n        masks: 2D array with sample masks.\n        n_obs: The number of observations.\n\n    Returns:\n        The utility differences.\n\n    \"\"\"\n    betas: NDArray[np.int_] = masks.astype(np.int_)\n    n_samples = len(utility_values)\n\n    z = _calculate_z(n_obs)\n\n    u_differences = np.zeros(shape=(n_obs, n_obs))\n    for i in range(n_obs):\n        for j in range(i + 1, n_obs):\n            u_differences[i, j] = np.dot(utility_values, betas[:, i] - betas[:, j])\n    u_differences *= z / n_samples\n\n    return u_differences\n</code></pre>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley._calculate_z","title":"_calculate_z","text":"<pre><code>_calculate_z(n_obs: int) -&gt; float\n</code></pre> <p>Calculate the normalization constant Z.</p> Source code in <code>src/pydvl/valuation/methods/gt_shapley.py</code> <pre><code>def _calculate_z(n_obs: int) -&gt; float:\n    \"\"\"Calculate the normalization constant Z.\"\"\"\n    kk = _create_sample_sizes(n_obs)\n    z: float = 2 * np.sum(1 / kk)\n    return z\n</code></pre>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley._create_sample_sizes","title":"_create_sample_sizes","text":"<pre><code>_create_sample_sizes(n_obs: int) -&gt; NDArray[int_]\n</code></pre> <p>Create a grid of possible sample sizes for the group testing algorithm.</p> Source code in <code>src/pydvl/valuation/methods/gt_shapley.py</code> <pre><code>def _create_sample_sizes(n_obs: int) -&gt; NDArray[np.int_]:\n    \"\"\"Create a grid of possible sample sizes for the group testing algorithm.\"\"\"\n    return np.arange(1, n_obs)\n</code></pre>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley._create_sampling_probabilities","title":"_create_sampling_probabilities","text":"<pre><code>_create_sampling_probabilities(sample_sizes: NDArray[int_]) -&gt; NDArray[float64]\n</code></pre> <p>Create probabilities for each possible sample size.</p> Source code in <code>src/pydvl/valuation/methods/gt_shapley.py</code> <pre><code>def _create_sampling_probabilities(\n    sample_sizes: NDArray[np.int_],\n) -&gt; NDArray[np.float64]:\n    \"\"\"Create probabilities for each possible sample size.\"\"\"\n    weights = 1 / sample_sizes + 1 / sample_sizes[::-1]\n    probs: NDArray[np.float64] = weights / weights.sum()\n    return probs\n</code></pre>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley.compute_n_samples","title":"compute_n_samples","text":"<pre><code>compute_n_samples(epsilon: float, delta: float, n_obs: int) -&gt; int\n</code></pre> <p>Compute the minimal sample size with epsilon-delta guarantees.</p> <p>Based on the formula in Theorem 4 of (Jia, R. et al., 2023)<sup>2</sup> which gives a lower bound on the number of samples required to obtain an (\u03b5/\u221an,\u03b4/(N(N-1))-approximation to all pair-wise differences of Shapley values, wrt. \\(\\ell_2\\) norm.</p> <p>The updated version refines the lower bound of the original paper. Note that the bound is tighter than earlier versions but might still overestimate the number of samples required.</p> PARAMETER DESCRIPTION <code>epsilon</code> <p>The error tolerance.</p> <p> TYPE: <code>float</code> </p> <code>delta</code> <p>The confidence level.</p> <p> TYPE: <code>float</code> </p> <code>n_obs</code> <p>Number of data points.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The sample size.</p> Source code in <code>src/pydvl/valuation/methods/gt_shapley.py</code> <pre><code>def compute_n_samples(epsilon: float, delta: float, n_obs: int) -&gt; int:\n    r\"\"\"Compute the minimal sample size with epsilon-delta guarantees.\n\n    Based on the formula in Theorem 4 of\n    (Jia, R. et al., 2023)&lt;sup&gt;&lt;a href=\"#jia_update_2023\"&gt;2&lt;/a&gt;&lt;/sup&gt;\n    which gives a lower bound on the number of samples required to obtain an\n    (\u03b5/\u221an,\u03b4/(N(N-1))-approximation to all pair-wise differences of Shapley\n    values, wrt. $\\ell_2$ norm.\n\n    The updated version refines the lower bound of the original paper. Note that the\n    bound is tighter than earlier versions but might still overestimate the number of\n    samples required.\n\n    Args:\n        epsilon: The error tolerance.\n        delta: The confidence level.\n        n_obs: Number of data points.\n\n    Returns:\n        The sample size.\n\n    \"\"\"\n    kk = _create_sample_sizes(n_obs)\n    Z = _calculate_z(n_obs)\n\n    q = _create_sampling_probabilities(kk)\n    q_tot = (n_obs - 2) / n_obs * q[0] + np.inner(\n        q[1:], 1 + 2 * kk[1:] * (kk[1:] - n_obs) / (n_obs * (n_obs - 1))\n    )\n\n    def _h(u: float) -&gt; float:\n        return cast(float, (1 + u) * np.log(1 + u) - u)\n\n    n_samples = np.log(n_obs * (n_obs - 1) / delta)\n    n_samples /= 1 - q_tot\n    n_samples /= _h(epsilon / (2 * Z * np.sqrt(n_obs) * (1 - q_tot)))\n\n    return int(n_samples)\n</code></pre>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley.create_group_testing_problem","title":"create_group_testing_problem","text":"<pre><code>create_group_testing_problem(\n    utility: UtilityBase,\n    sampler: IndexSampler,\n    n_samples: int,\n    progress: bool,\n    epsilon: float,\n) -&gt; GroupTestingProblem\n</code></pre> <p>Create the feasibility problem that characterizes group testing shapley values.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Utility object with model, data and scoring function.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>sampler</code> <p>The sampler to use for the valuation.</p> <p> TYPE: <code>IndexSampler</code> </p> <code>n_samples</code> <p>The number of samples to use.</p> <p> TYPE: <code>int</code> </p> <code>progress</code> <p>Whether to show a progress bar.</p> <p> TYPE: <code>bool</code> </p> <code>epsilon</code> <p>The error tolerance.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>GroupTestingProblem</code> <p>A GroupTestingProblem object.</p> Source code in <code>src/pydvl/valuation/methods/gt_shapley.py</code> <pre><code>def create_group_testing_problem(\n    utility: UtilityBase,\n    sampler: IndexSampler,\n    n_samples: int,\n    progress: bool,\n    epsilon: float,\n) -&gt; GroupTestingProblem:\n    \"\"\"Create the feasibility problem that characterizes group testing shapley values.\n\n    Args:\n        utility: Utility object with model, data and scoring function.\n        sampler: The sampler to use for the valuation.\n        n_samples: The number of samples to use.\n        progress: Whether to show a progress bar.\n        epsilon: The error tolerance.\n\n    Returns:\n        A GroupTestingProblem object.\n\n    \"\"\"\n    if utility.training_data is None:\n        raise ValueError(\"Utility object must have training data.\")\n\n    u_values, masks = compute_utility_values_and_sample_masks(\n        utility=utility,\n        sampler=sampler,\n        n_samples=n_samples,\n        progress=progress,\n        extra_samples=[Sample(idx=None, subset=utility.training_data.indices)],\n    )\n\n    total_utility = u_values[-1].item()\n    u_values = u_values[:-1]\n    masks = masks[:-1]\n\n    u_differences = _calculate_utility_differences(\n        utility_values=u_values, masks=masks, n_obs=len(utility.training_data)\n    )\n\n    problem = GroupTestingProblem(\n        utility_differences=u_differences,\n        total_utility=total_utility,\n        epsilon=epsilon,\n    )\n\n    return problem\n</code></pre>"},{"location":"api/pydvl/valuation/methods/gt_shapley/#pydvl.valuation.methods.gt_shapley.solve_group_testing_problem","title":"solve_group_testing_problem","text":"<pre><code>solve_group_testing_problem(\n    problem: GroupTestingProblem,\n    solver_options: dict | None,\n    algorithm_name: str,\n    data_names: NDArray[NameT],\n) -&gt; ValuationResult\n</code></pre> <p>Solve the group testing problem and create a ValuationResult.</p> PARAMETER DESCRIPTION <code>problem</code> <p>The group testing problem.</p> <p> TYPE: <code>GroupTestingProblem</code> </p> <code>solver_options</code> <p>Optional dictionary containing a CVXPY solver and options to configure it. For valid values to the \"solver\" key see here. For additional options see here.</p> <p> TYPE: <code>dict | None</code> </p> <code>algorithm_name</code> <p>The name of the algorithm.</p> <p> TYPE: <code>str</code> </p> <code>data_names</code> <p>The names of the data columns.</p> <p> TYPE: <code>NDArray[NameT]</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>A ValuationResult object.</p> Source code in <code>src/pydvl/valuation/methods/gt_shapley.py</code> <pre><code>def solve_group_testing_problem(\n    problem: GroupTestingProblem,\n    solver_options: dict | None,\n    algorithm_name: str,\n    data_names: NDArray[NameT],\n) -&gt; ValuationResult:\n    \"\"\"Solve the group testing problem and create a ValuationResult.\n\n    Args:\n        problem: The group testing problem.\n        solver_options: Optional dictionary containing a CVXPY solver and options to\n            configure it. For valid values to the \"solver\" key see\n            [here](https://www.cvxpy.org/tutorial/solvers/index.html#choosing-a-solver).\n            For additional options see\n            [here](https://www.cvxpy.org/tutorial/solvers/index.html#setting-solver-options).\n        algorithm_name: The name of the algorithm.\n        data_names: The names of the data columns.\n\n    Returns:\n        A ValuationResult object.\n\n    \"\"\"\n\n    solver_options = {} if solver_options is None else solver_options.copy()\n\n    C = problem.utility_differences\n    total_utility = problem.total_utility\n    epsilon = problem.epsilon\n    n_obs = len(C)\n\n    v = cp.Variable(n_obs)\n    constraints = [cp.sum(v) == total_utility]\n    for i in range(n_obs):\n        for j in range(i + 1, n_obs):\n            constraints.append(v[i] - v[j] &lt;= epsilon + C[i, j])\n            constraints.append(v[j] - v[i] &lt;= epsilon - C[i, j])\n\n    cp_problem = cp.Problem(cp.Minimize(0), constraints)\n    solver = solver_options.pop(\"solver\", cp.SCS)\n    cp_problem.solve(solver=solver, **solver_options)\n\n    if cp_problem.status != \"optimal\":\n        log.warning(f\"cvxpy returned status {cp_problem.status}\")\n        values = (\n            np.nan * np.ones_like(n_obs)\n            if not hasattr(v.value, \"__len__\")\n            else cast(NDArray[np.float64], v.value)\n        )\n        status = Status.Failed\n    else:\n        values = cast(NDArray[np.float64], v.value)\n        status = Status.Converged\n\n    result = ValuationResult(\n        status=status,\n        values=values,\n        data_names=data_names,\n        solver_status=cp_problem.status,\n        algorithm=algorithm_name,\n    )\n\n    return result\n</code></pre>"},{"location":"api/pydvl/valuation/methods/knn_shapley/","title":"Knn shapley","text":""},{"location":"api/pydvl/valuation/methods/knn_shapley/#pydvl.valuation.methods.knn_shapley","title":"pydvl.valuation.methods.knn_shapley","text":"<p>This module contains Shapley computations for K-Nearest Neighbours classifier, introduced by Jia et al. (2019).<sup>1</sup></p> <p>In particular it provides KNNShapleyValuation to compute exact Shapley values for a KNN classifier in \\(O(n \\log n)\\) time per test point, as opposed to \\(O(n^2 \\log^2 n)\\) if the model were simply fed to a generic ShapleyValuation object.</p> <p>See the documentation or the paper for details.</p> <p>Todo</p> <p>Implement approximate KNN computation for sublinear complexity</p>"},{"location":"api/pydvl/valuation/methods/knn_shapley/#pydvl.valuation.methods.knn_shapley--references","title":"References","text":"<ol> <li> <p>Jia, R. et al., 2019. Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms. In: Proceedings of the VLDB Endowment, Vol. 12, No. 11, pp. 1610\u20131623.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/methods/knn_shapley/#pydvl.valuation.methods.knn_shapley.KNNShapleyValuation","title":"KNNShapleyValuation","text":"<pre><code>KNNShapleyValuation(\n    model: KNeighborsClassifier,\n    test_data: Dataset,\n    progress: bool = True,\n    clone_before_fit: bool = True,\n)\n</code></pre> <p>               Bases: <code>Valuation</code></p> <p>Computes exact Shapley values for a KNN classifier.</p> <p>This implements the method described in (Jia, R. et al., 2019)<sup>1</sup>.</p> PARAMETER DESCRIPTION <code>model</code> <p>KNeighborsClassifier model to use for valuation</p> <p> TYPE: <code>KNeighborsClassifier</code> </p> <code>test_data</code> <p>Dataset containing test data to evaluate the model.</p> <p> TYPE: <code>Dataset</code> </p> <code>progress</code> <p>Whether to display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>clone_before_fit</code> <p>Whether to clone the model before fitting.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/methods/knn_shapley.py</code> <pre><code>def __init__(\n    self,\n    model: KNeighborsClassifier,\n    test_data: Dataset,\n    progress: bool = True,\n    clone_before_fit: bool = True,\n):\n    super().__init__()\n    if not isinstance(model, KNeighborsClassifier):\n        raise TypeError(\"KNN Shapley requires a K-Nearest Neighbours model\")\n    self.model = model\n    self.test_data = test_data\n    self.progress = progress\n    self.clone_before_fit = clone_before_fit\n</code></pre>"},{"location":"api/pydvl/valuation/methods/knn_shapley/#pydvl.valuation.methods.knn_shapley.KNNShapleyValuation._compute_values_for_test_points","title":"_compute_values_for_test_points  <code>staticmethod</code>","text":"<pre><code>_compute_values_for_test_points(\n    model: NearestNeighbors, x_test: NDArray, y_test: NDArray, y_train: NDArray\n) -&gt; NDArray[float64]\n</code></pre> <p>Compute the Shapley value using a set of test points.</p> <p>The Shapley value for a training point is computed over the whole test set by averaging the Shapley values of the single test data points.</p> PARAMETER DESCRIPTION <code>model</code> <p>A fitted NearestNeighbors model.</p> <p> TYPE: <code>NearestNeighbors</code> </p> <code>x_test</code> <p>The test data points.</p> <p> TYPE: <code>NDArray</code> </p> <code>y_test</code> <p>The test labels.</p> <p> TYPE: <code>NDArray</code> </p> <code>y_train</code> <p>The labels for the training points to be valued.</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>NDArray[float64]</code> <p>The Shapley values for the training data points.</p> Source code in <code>src/pydvl/valuation/methods/knn_shapley.py</code> <pre><code>@staticmethod\ndef _compute_values_for_test_points(\n    model: NearestNeighbors,\n    x_test: NDArray,\n    y_test: NDArray,\n    y_train: NDArray,\n) -&gt; NDArray[np.float64]:\n    \"\"\"Compute the Shapley value using a set of test points.\n\n    The Shapley value for a training point is computed over the whole test set by\n    averaging the Shapley values of the single test data points.\n\n    Args:\n        model: A fitted NearestNeighbors model.\n        x_test: The test data points.\n        y_test: The test labels.\n        y_train: The labels for the training points to be valued.\n\n    Returns:\n        The Shapley values for the training data points.\n\n    \"\"\"\n    n_obs = len(y_train)\n    n_neighbors = model.get_params()[\"n_neighbors\"]\n\n    # sort data indices from close to far\n    sorted_indices = model.kneighbors(\n        x_test, n_neighbors=n_obs, return_distance=False\n    )\n\n    values = np.zeros(shape=(len(x_test), n_obs))\n\n    for query, neighbors in enumerate(sorted_indices):\n        label = y_test[query]\n        # Initialize the farthest neighbor's value\n        idx = neighbors[-1]\n        values[query][idx] = float(y_train[idx] == label) / n_obs\n        # reverse range because we want to go from far to close\n        for i in range(n_obs - 1, 0, -1):\n            prev_idx = neighbors[i]\n            idx = neighbors[i - 1]\n            values[query][idx] = values[query][prev_idx]\n            values[query][idx] += (\n                int(y_train[idx] == label) - int(y_train[prev_idx] == label)\n            ) / max(n_neighbors, i)\n            # 1/max(K, i) = 1/K * min{K, i}/i as in the paper\n\n    return cast(NDArray[np.float64], values.sum(axis=0))\n</code></pre>"},{"location":"api/pydvl/valuation/methods/knn_shapley/#pydvl.valuation.methods.knn_shapley.KNNShapleyValuation.fit","title":"fit","text":"<pre><code>fit(data: Dataset) -&gt; Self\n</code></pre> <p>Calculate exact shapley values for a KNN model on a dataset.</p> <p>This fit method bypasses direct evaluations of the utility function and calculates the Shapley values directly.</p> <p>In contrast to other data valuation models, the runtime increases linearly with the size of the dataset.</p> <p>Calculating the KNN valuation is a computationally expensive task that can be parallelized. To do so, call the <code>fit()</code> method inside a <code>joblib.parallel_config</code> context manager as follows:</p> <pre><code>from joblib import parallel_config\n\nwith parallel_config(n_jobs=4):\n    valuation.fit(data)\n</code></pre> Source code in <code>src/pydvl/valuation/methods/knn_shapley.py</code> <pre><code>def fit(self, data: Dataset) -&gt; Self:\n    \"\"\"Calculate exact shapley values for a KNN model on a dataset.\n\n    This fit method bypasses direct evaluations of the utility function and\n    calculates the Shapley values directly.\n\n    In contrast to other data valuation models, the runtime increases linearly\n    with the size of the dataset.\n\n    Calculating the KNN valuation is a computationally expensive task that\n    can be parallelized. To do so, call the `fit()` method inside a\n    `joblib.parallel_config` context manager as follows:\n\n    ```python\n    from joblib import parallel_config\n\n    with parallel_config(n_jobs=4):\n        valuation.fit(data)\n    ```\n    \"\"\"\n\n    if isinstance(data, GroupedDataset):\n        raise TypeError(\"GroupedDataset is not supported by KNNShapleyValuation\")\n\n    x_train, y_train = data.data()\n    if self.clone_before_fit:\n        self.model = cast(KNeighborsClassifier, clone(self.model))\n    self.model.fit(x_train, y_train)\n\n    n_test = len(self.test_data)\n\n    _, n_jobs = get_active_backend()\n    n_jobs = n_jobs or 1  # Handle None if outside a joblib context\n    batch_size = (n_test // n_jobs) + (1 if n_test % n_jobs else 0)\n    x_test, y_test = self.test_data.data()\n    batches = zip(chunked(x_test, batch_size), chunked(y_test, batch_size))\n\n    process = delayed(self._compute_values_for_test_points)\n    with Parallel(return_as=\"generator_unordered\") as parallel:\n        results = parallel(\n            process(self.model, x_test, y_test, y_train)\n            for x_test, y_test in batches\n        )\n        values = np.zeros(len(data))\n        # FIXME: this progress bar won't add much since we have n_jobs batches and\n        #  they will all take about the same time\n        for res in tqdm(results, total=n_jobs, disable=not self.progress):\n            values += res\n        values /= n_test\n\n    self.result = ValuationResult(\n        algorithm=\"knn_shapley\",\n        status=Status.Converged,\n        values=values,\n        data_names=data.names,\n    )\n\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/methods/knn_shapley/#pydvl.valuation.methods.knn_shapley.KNNShapleyValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/","title":"Least core","text":""},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core","title":"pydvl.valuation.methods.least_core","text":"<p>This module implements the least-core valuation method.</p> <p>Least-Core values were introduced by Yan and Procaccia (2021).<sup>1</sup> Please refer to the paper or our documentation for more details and a comparison with other methods (Benmerzoug and de Benito Delgado, 2023).<sup>2</sup></p>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core--references","title":"References","text":"<ol> <li> <p>Yan, Tom, and Ariel D. Procaccia. If You Like Shapley Then You\u2019ll Love the   Core. In Proceedings of the 35th AAAI   Conference on Artificial Intelligence, 2021, 6:5751\u201359. Virtual conference:   Association for the Advancement of Artificial Intelligence, 2021.\u00a0\u21a9</p> </li> <li> <p>Benmerzoug, Anes, and Miguel de Benito Delgado. [Re] If You like Shapley, Then   You\u2019ll Love the Core. ReScience C 9, no.   2 (31 July 2023): #32.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core.ExactLeastCoreValuation","title":"ExactLeastCoreValuation","text":"<pre><code>ExactLeastCoreValuation(\n    utility: UtilityBase,\n    non_negative_subsidy: bool = False,\n    solver_options: dict | None = None,\n    progress: bool = True,\n    batch_size: int = 1,\n)\n</code></pre> <p>               Bases: <code>LeastCoreValuation</code></p> <p>Class to calculate exact least-core values.</p> <p>Equivalent to constructing a LeastCoreValuation with a DeterministicUniformSampler and <code>n_samples=None</code>.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Utility object with model, data and scoring function.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>non_negative_subsidy</code> <p>If True, the least core subsidy \\(e\\) is constrained to be non-negative.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>solver_options</code> <p>Optional dictionary containing a CVXPY solver and options to configure it. For valid values to the \"solver\" key see here. For additional options see here.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to show a progress bar during the construction of the least-core problem.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/methods/least_core.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    non_negative_subsidy: bool = False,\n    solver_options: dict | None = None,\n    progress: bool = True,\n    batch_size: int = 1,\n):\n    super().__init__(\n        utility=utility,\n        sampler=DeterministicUniformSampler(\n            index_iteration=FiniteNoIndexIteration, batch_size=batch_size\n        ),\n        n_samples=None,\n        non_negative_subsidy=non_negative_subsidy,\n        solver_options=solver_options,\n        progress=progress,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core.ExactLeastCoreValuation.fit","title":"fit","text":"<pre><code>fit(data: Dataset) -&gt; Self\n</code></pre> <p>Calculate the least core valuation on a dataset.</p> <p>This method has to be called before calling <code>values()</code>.</p> <p>Calculating the least core valuation is a computationally expensive task that can be parallelized. To do so, call the <code>fit()</code> method inside a <code>joblib.parallel_config</code> context manager as follows:</p> <pre><code>from joblib import parallel_config\n\nwith parallel_config(n_jobs=4):\n    valuation.fit(data)\n</code></pre> Source code in <code>src/pydvl/valuation/methods/least_core.py</code> <pre><code>def fit(self, data: Dataset) -&gt; Self:\n    \"\"\"Calculate the least core valuation on a dataset.\n\n    This method has to be called before calling `values()`.\n\n    Calculating the least core valuation is a computationally expensive task that\n    can be parallelized. To do so, call the `fit()` method inside a\n    `joblib.parallel_config` context manager as follows:\n\n    ```python\n    from joblib import parallel_config\n\n    with parallel_config(n_jobs=4):\n        valuation.fit(data)\n    ```\n\n    \"\"\"\n    self._utility = self._utility.with_dataset(data)\n    if self._n_samples is None:\n        self._n_samples = _get_default_n_samples(\n            sampler=self._sampler, indices=data.indices\n        )\n\n    algorithm = str(self._sampler)\n\n    problem = create_least_core_problem(\n        u=self._utility,\n        sampler=self._sampler,\n        n_samples=self._n_samples,\n        progress=self._progress,\n    )\n\n    solution = lc_solve_problem(\n        problem=problem,\n        u=self._utility,\n        algorithm=algorithm,\n        non_negative_subsidy=self._non_negative_subsidy,\n        solver_options=self._solver_options,\n    )\n\n    self.result = solution\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core.ExactLeastCoreValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core.LeastCoreValuation","title":"LeastCoreValuation","text":"<pre><code>LeastCoreValuation(\n    utility: UtilityBase,\n    sampler: PowersetSampler,\n    n_samples: int | None = None,\n    non_negative_subsidy: bool = False,\n    solver_options: dict | None = None,\n    progress: bool = True,\n)\n</code></pre> <p>               Bases: <code>Valuation</code></p> <p>Umbrella class to calculate least-core values with multiple sampling methods.</p> <p>See the documentation for an overview.</p> <p>Different samplers correspond to different least-core methods from the literature. For those, we provide convenience subclasses of <code>LeastCoreValuation</code>. See</p> <ul> <li>ExactLeastCoreValuation</li> <li>MonteCarloLeastCoreValuation</li> </ul> <p>Other samplers allow you to create your own importance sampling method and might yield computational gains over the standard Monte Carlo method.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Utility object with model, data and scoring function.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>sampler</code> <p>The sampler to use for the valuation.</p> <p> TYPE: <code>PowersetSampler</code> </p> <code>n_samples</code> <p>The number of samples to use for the valuation. If None, it will be set to the sample limit of the chosen sampler (for finite samplers) or <code>1000 * len(data)</code> (for infinite samplers).</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>non_negative_subsidy</code> <p>If True, the least core subsidy \\(e\\) is constrained to be non-negative.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>solver_options</code> <p>Optional dictionary containing a CVXPY solver and options to configure it. For valid values to the \"solver\" key see here. For additional options see here.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to show a progress bar during the construction of the least-core problem.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/methods/least_core.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    sampler: PowersetSampler,\n    n_samples: int | None = None,\n    non_negative_subsidy: bool = False,\n    solver_options: dict | None = None,\n    progress: bool = True,\n):\n    super().__init__()\n\n    _check_sampler(sampler)\n    self._utility = utility\n    self._sampler = sampler\n    self._non_negative_subsidy = non_negative_subsidy\n    self._solver_options = solver_options\n    self._n_samples = n_samples\n    self._progress = progress\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core.LeastCoreValuation.fit","title":"fit","text":"<pre><code>fit(data: Dataset) -&gt; Self\n</code></pre> <p>Calculate the least core valuation on a dataset.</p> <p>This method has to be called before calling <code>values()</code>.</p> <p>Calculating the least core valuation is a computationally expensive task that can be parallelized. To do so, call the <code>fit()</code> method inside a <code>joblib.parallel_config</code> context manager as follows:</p> <pre><code>from joblib import parallel_config\n\nwith parallel_config(n_jobs=4):\n    valuation.fit(data)\n</code></pre> Source code in <code>src/pydvl/valuation/methods/least_core.py</code> <pre><code>def fit(self, data: Dataset) -&gt; Self:\n    \"\"\"Calculate the least core valuation on a dataset.\n\n    This method has to be called before calling `values()`.\n\n    Calculating the least core valuation is a computationally expensive task that\n    can be parallelized. To do so, call the `fit()` method inside a\n    `joblib.parallel_config` context manager as follows:\n\n    ```python\n    from joblib import parallel_config\n\n    with parallel_config(n_jobs=4):\n        valuation.fit(data)\n    ```\n\n    \"\"\"\n    self._utility = self._utility.with_dataset(data)\n    if self._n_samples is None:\n        self._n_samples = _get_default_n_samples(\n            sampler=self._sampler, indices=data.indices\n        )\n\n    algorithm = str(self._sampler)\n\n    problem = create_least_core_problem(\n        u=self._utility,\n        sampler=self._sampler,\n        n_samples=self._n_samples,\n        progress=self._progress,\n    )\n\n    solution = lc_solve_problem(\n        problem=problem,\n        u=self._utility,\n        algorithm=algorithm,\n        non_negative_subsidy=self._non_negative_subsidy,\n        solver_options=self._solver_options,\n    )\n\n    self.result = solution\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core.LeastCoreValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core.MonteCarloLeastCoreValuation","title":"MonteCarloLeastCoreValuation","text":"<pre><code>MonteCarloLeastCoreValuation(\n    utility: UtilityBase,\n    n_samples: int,\n    non_negative_subsidy: bool = False,\n    solver_options: dict | None = None,\n    progress: bool = True,\n    seed: Seed | None = None,\n    batch_size: int = 1,\n)\n</code></pre> <p>               Bases: <code>LeastCoreValuation</code></p> <p>Class to calculate exact least-core values.</p> <p>Equivalent to creating a LeastCoreValuation with a UniformSampler.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Utility object with model, data and scoring function.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>n_samples</code> <p>The number of samples to use for the valuation. If None, it will be set to <code>1000 * len(data)</code>.</p> <p> TYPE: <code>int</code> </p> <code>non_negative_subsidy</code> <p>If True, the least core subsidy \\(e\\) is constrained to be non-negative.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>solver_options</code> <p>Optional dictionary containing a CVXPY solver and options to configure it. For valid values to the \"solver\" key see here. For additional options see here.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to show a progress bar during the construction of the least-core problem.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/methods/least_core.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    n_samples: int,\n    non_negative_subsidy: bool = False,\n    solver_options: dict | None = None,\n    progress: bool = True,\n    seed: Seed | None = None,\n    batch_size: int = 1,\n):\n    super().__init__(\n        utility=utility,\n        sampler=UniformSampler(\n            index_iteration=NoIndexIteration, seed=seed, batch_size=batch_size\n        ),\n        n_samples=n_samples,\n        non_negative_subsidy=non_negative_subsidy,\n        solver_options=solver_options,\n        progress=progress,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core.MonteCarloLeastCoreValuation.fit","title":"fit","text":"<pre><code>fit(data: Dataset) -&gt; Self\n</code></pre> <p>Calculate the least core valuation on a dataset.</p> <p>This method has to be called before calling <code>values()</code>.</p> <p>Calculating the least core valuation is a computationally expensive task that can be parallelized. To do so, call the <code>fit()</code> method inside a <code>joblib.parallel_config</code> context manager as follows:</p> <pre><code>from joblib import parallel_config\n\nwith parallel_config(n_jobs=4):\n    valuation.fit(data)\n</code></pre> Source code in <code>src/pydvl/valuation/methods/least_core.py</code> <pre><code>def fit(self, data: Dataset) -&gt; Self:\n    \"\"\"Calculate the least core valuation on a dataset.\n\n    This method has to be called before calling `values()`.\n\n    Calculating the least core valuation is a computationally expensive task that\n    can be parallelized. To do so, call the `fit()` method inside a\n    `joblib.parallel_config` context manager as follows:\n\n    ```python\n    from joblib import parallel_config\n\n    with parallel_config(n_jobs=4):\n        valuation.fit(data)\n    ```\n\n    \"\"\"\n    self._utility = self._utility.with_dataset(data)\n    if self._n_samples is None:\n        self._n_samples = _get_default_n_samples(\n            sampler=self._sampler, indices=data.indices\n        )\n\n    algorithm = str(self._sampler)\n\n    problem = create_least_core_problem(\n        u=self._utility,\n        sampler=self._sampler,\n        n_samples=self._n_samples,\n        progress=self._progress,\n    )\n\n    solution = lc_solve_problem(\n        problem=problem,\n        u=self._utility,\n        algorithm=algorithm,\n        non_negative_subsidy=self._non_negative_subsidy,\n        solver_options=self._solver_options,\n    )\n\n    self.result = solution\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core.MonteCarloLeastCoreValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core._check_sampler","title":"_check_sampler","text":"<pre><code>_check_sampler(sampler: PowersetSampler)\n</code></pre> <p>Check that the sampler is compatible with the Least Core valuation.</p> Source code in <code>src/pydvl/valuation/methods/least_core.py</code> <pre><code>def _check_sampler(sampler: PowersetSampler):\n    \"\"\"Check that the sampler is compatible with the Least Core valuation.\"\"\"\n    if not issubclass(sampler._index_iterator_cls, NoIndexIteration):\n        raise ValueError(\n            \"Least core valuation only supports samplers with NoIndexIteration.\"\n        )\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core._get_default_n_samples","title":"_get_default_n_samples","text":"<pre><code>_get_default_n_samples(sampler: PowersetSampler, indices: IndexSetT) -&gt; int\n</code></pre> <p>Get a default value for n_samples based on the sampler's sample limit.</p> PARAMETER DESCRIPTION <code>sampler</code> <p>The sampler to use for the valuation.</p> <p> TYPE: <code>PowersetSampler</code> </p> <code>indices</code> <p>The indices of the dataset.</p> <p> TYPE: <code>IndexSetT</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The number of samples to use for the valuation.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/valuation/methods/least_core.py</code> <pre><code>def _get_default_n_samples(sampler: PowersetSampler, indices: IndexSetT) -&gt; int:\n    \"\"\"Get a default value for n_samples based on the sampler's sample limit.\n\n    Args:\n        sampler: The sampler to use for the valuation.\n        indices: The indices of the dataset.\n\n    Returns:\n        int: The number of samples to use for the valuation.\n\n    \"\"\"\n    sample_limit = sampler.sample_limit(indices)\n    if sample_limit is not None:\n        out = sample_limit\n    else:\n        # TODO: This is a rather arbitrary rule of thumb. The value was chosen to be\n        # larger than the number of samples used by Yan and Procaccia\n        # https://ojs.aaai.org/index.php/AAAI/article/view/16721 in a low resource\n        # setting but linear in the dataset size to avoid exploding runtimes.\n        out = 1000 * len(indices)\n\n    return out\n</code></pre>"},{"location":"api/pydvl/valuation/methods/least_core/#pydvl.valuation.methods.least_core.create_least_core_problem","title":"create_least_core_problem","text":"<pre><code>create_least_core_problem(\n    u: UtilityBase, sampler: PowersetSampler, n_samples: int, progress: bool\n) -&gt; LeastCoreProblem\n</code></pre> <p>Create a Least Core problem from a utility and a sampler.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data and scoring function.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>sampler</code> <p>The sampler to use for the valuation.</p> <p> TYPE: <code>PowersetSampler</code> </p> <code>n_samples</code> <p>The maximum number of samples to use for the valuation.</p> <p> TYPE: <code>int</code> </p> <code>progress</code> <p>Whether to show a progress bar during the construction of the least-core problem.</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>LeastCoreProblem</code> <p>The least core problem to solve.</p> <p> TYPE: <code>LeastCoreProblem</code> </p> Source code in <code>src/pydvl/valuation/methods/least_core.py</code> <pre><code>def create_least_core_problem(\n    u: UtilityBase, sampler: PowersetSampler, n_samples: int, progress: bool\n) -&gt; LeastCoreProblem:\n    \"\"\"Create a Least Core problem from a utility and a sampler.\n\n    Args:\n        u: Utility object with model, data and scoring function.\n        sampler: The sampler to use for the valuation.\n        n_samples: The maximum number of samples to use for the valuation.\n        progress: Whether to show a progress bar during the construction of the\n            least-core problem.\n\n    Returns:\n        LeastCoreProblem: The least core problem to solve.\n\n    \"\"\"\n    utility_values, masks = compute_utility_values_and_sample_masks(\n        utility=u, sampler=sampler, n_samples=n_samples, progress=progress\n    )\n\n    return LeastCoreProblem(utility_values=utility_values, A_lb=masks.astype(float))\n</code></pre>"},{"location":"api/pydvl/valuation/methods/loo/","title":"Loo","text":""},{"location":"api/pydvl/valuation/methods/loo/#pydvl.valuation.methods.loo","title":"pydvl.valuation.methods.loo","text":"<p>This module implements Leave-One-Out (LOO) valuation.</p> <p>It is defined as:</p> \\[ v_\\text{loo}(i) = u(N) - u(N_{-i}), \\] <p>where \\(u\\) is the utility function, \\(N\\) is the set of all indices, and \\(i\\) is the index of interest.</p>"},{"location":"api/pydvl/valuation/methods/loo/#pydvl.valuation.methods.loo--changing-loo","title":"Changing LOO","text":"<p>LOOValuation is preconfigured to stop once all indices have been visited once. In particular, it uses a default LOOSampler with a FiniteSequentialIndexIteration. If you want to change this behaviour, the easiest way is to subclass and replace the constructor.</p>"},{"location":"api/pydvl/valuation/methods/loo/#pydvl.valuation.methods.loo.LOOValuation","title":"LOOValuation","text":"<pre><code>LOOValuation(utility: UtilityBase, progress: bool = False)\n</code></pre> <p>               Bases: <code>SemivalueValuation</code></p> <p>Computes LOO values for a dataset.</p> Source code in <code>src/pydvl/valuation/methods/loo.py</code> <pre><code>def __init__(self, utility: UtilityBase, progress: bool = False):\n    self.result: ValuationResult | None = None\n    super().__init__(\n        utility,\n        LOOSampler(batch_size=1, index_iteration=FiniteSequentialIndexIteration),\n        # LOO is done when every index has been updated once\n        MinUpdates(n_updates=1),\n        progress=progress,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/methods/loo/#pydvl.valuation.methods.loo.LOOValuation.log_coefficient","title":"log_coefficient  <code>property</code>","text":"<pre><code>log_coefficient: SemivalueCoefficient | None\n</code></pre> <p>Disable importance sampling for this method since we have a fixed sampler that already provides the correct weights for the Monte Carlo approximation.</p>"},{"location":"api/pydvl/valuation/methods/loo/#pydvl.valuation.methods.loo.LOOValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/random/","title":"Random","text":""},{"location":"api/pydvl/valuation/methods/random/#pydvl.valuation.methods.random","title":"pydvl.valuation.methods.random","text":"<p>This module implements a trivial random valuation method.</p> <p>It exists mainly for convenience when running experiments, e.g. when comparing methods with point removal, random values are a simple (albeit weak) baseline.</p>"},{"location":"api/pydvl/valuation/methods/random/#pydvl.valuation.methods.random.RandomValuation","title":"RandomValuation","text":"<pre><code>RandomValuation(random_state: Seed)\n</code></pre> <p>               Bases: <code>Valuation</code></p> <p>A trivial valuation method that assigns random values to each data point.</p> <p>Values are in the range [0, 1), as generated by ValuationResult.from_random.</p> <p>Successive calls to fit() will generate different values.</p> Source code in <code>src/pydvl/valuation/methods/random.py</code> <pre><code>def __init__(self, random_state: Seed):\n    super().__init__()\n    self.random_state = np.random.default_rng(random_state)\n</code></pre>"},{"location":"api/pydvl/valuation/methods/random/#pydvl.valuation.methods.random.RandomValuation.fit","title":"fit","text":"<pre><code>fit(train: Dataset) -&gt; Self\n</code></pre> <p>Dummy fitting that generates a set of random values.</p> <p>Successive calls will generate different values.</p> PARAMETER DESCRIPTION <code>train</code> <p>used to determine the size of the valuation result</p> <p> TYPE: <code>Dataset</code> </p> <p>Returns:     self</p> Source code in <code>src/pydvl/valuation/methods/random.py</code> <pre><code>def fit(self, train: Dataset) -&gt; Self:\n    \"\"\"Dummy fitting that generates a set of random values.\n\n    Successive calls will generate different values.\n\n    Args:\n        train: used to determine the size of the valuation result\n    Returns:\n        self\n    \"\"\"\n    self.result = ValuationResult.from_random(\n        size=len(train), seed=self.random_state\n    )\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/methods/random/#pydvl.valuation.methods.random.RandomValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/semivalue/","title":"Semivalue","text":""},{"location":"api/pydvl/valuation/methods/semivalue/#pydvl.valuation.methods.semivalue","title":"pydvl.valuation.methods.semivalue","text":"<p>This module contains the base class for all semi-value valuation methods.</p> <p>A semi-value is any marginal contribution-based valuation method which weights the marginal contributions of a data point \\(i\\) to the utility of a subset \\(S\\) by weights \\(w(k)\\), where \\(k\\) is the size of the subset, fulfilling certain conditions. For details, please refer to the introduction to semi-values.</p>"},{"location":"api/pydvl/valuation/methods/semivalue/#pydvl.valuation.methods.semivalue--implementing-new-methods-with-importance-sampling","title":"Implementing new methods with importance sampling","text":"<p>Semi-values and importance sampling</p> <p>For a more detailed analysis of the ideas in this and the following section, please read Sampling strategies for semi-values.</p> <p>Because almost every method employs Monte Carlo sampling of subsets, our architecture allows for importance sampling. Early valuation methods chose samplers to implicitly provide the weights \\(w(k)\\) as exactly the sampling probabilities of sets \\(p(S|k)\\), e.g. permutation Shapley.</p> <p>However, this is not a requirement. In fact, other methods employ different forms of importance sampling as a means to reduce the variance both of the Monte Carlo estimates and the utility function.</p> <p>For this reason, our implementation allows mix-and-matching of any semi-value coefficient with any sampler. For importance sampling, the mechanism is as follows:</p> <ul> <li> <p>Choose a sampler to go with the semi-value. The sampler must implement the   <code>log_weight()</code> property, which returns the logarithm of the sampling probability of a   subset \\(S\\) of size \\(k\\), i.e. \\(p(S|k).\\) Note that this is not p(|S|=k).$ The sampler   also implements an EvaluationStrategy   which is used to compute the utility of the sampled subsets in subprocesses.</p> </li> <li> <p>Subclass SemivalueValuation   and implement the <code>log_coefficient()</code> method. This method should return the final   coefficient in log-space, i.e. the natural logarithm of the coefficient, for numerical   stability. The coefficient is a function of the number of elements in the set \\(n\\) and   the size of the subset \\(k\\) for which the coefficient is being computed, and of the   sampler's weight. You can combine the method's coefficient and the weight in any way.   For instance, in order to entirely compensate for the sampling distribution one simply   subtracts the log-weights from the log-coefficient.</p> </li> </ul>"},{"location":"api/pydvl/valuation/methods/semivalue/#pydvl.valuation.methods.semivalue--disabling-importance-sampling","title":"Disabling importance sampling","text":"<p>In case you have a sampler that already provides the coefficients you need implicitly as the sampling probabilities, you can override the <code>log_coefficient</code> property to return <code>None</code>.</p>"},{"location":"api/pydvl/valuation/methods/semivalue/#pydvl.valuation.methods.semivalue.SemivalueValuation","title":"SemivalueValuation","text":"<pre><code>SemivalueValuation(\n    utility: UtilityBase,\n    sampler: IndexSampler,\n    is_done: StoppingCriterion,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n)\n</code></pre> <p>               Bases: <code>Valuation</code></p> <p>Abstract class to define semi-values.</p> <p>Implementations must only provide the <code>log_coefficient()</code> property, corresponding to the semi-value coefficient.</p> <p>Note</p> <p>For implementation consistency, we slightly depart from the common definition of semi-values, which includes a factor \\(1/n\\) in the sum over subsets. Instead, we subsume this factor into the coefficient \\(w(k)\\).</p> PARAMETER DESCRIPTION <code>utility</code> <p>Object to compute utilities.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>sampler</code> <p>Sampling scheme to use.</p> <p> TYPE: <code>IndexSampler</code> </p> <code>is_done</code> <p>Stopping criterion to use.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>skip_converged</code> <p>Whether to skip converged indices, as determined by the stopping criterion's <code>converged</code> array.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>show_warnings</code> <p>Whether to show warnings.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <p>Whether to show a progress bar. If a dictionary, it is passed to <code>tqdm</code> as keyword arguments, and the progress bar is displayed.</p> <p> TYPE: <code>dict[str, Any] | bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/valuation/methods/semivalue.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    sampler: IndexSampler,\n    is_done: StoppingCriterion,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n):\n    super().__init__()\n    self.utility = utility\n    self.sampler = sampler\n    self.is_done = is_done\n    self.skip_converged = skip_converged\n    if skip_converged:  # test whether the sampler supports skipping indices:\n        self.sampler.skip_indices = np.array([], dtype=np.int_)\n    self.show_warnings = show_warnings\n    self.tqdm_args: dict[str, Any] = {\"desc\": str(self)}\n    # HACK: parse additional args for the progress bar if any (we probably want\n    #  something better)\n    if isinstance(progress, bool):\n        self.tqdm_args.update({\"disable\": not progress})\n    elif isinstance(progress, dict):\n        self.tqdm_args.update(progress)\n    else:\n        raise TypeError(f\"Invalid type for progress: {type(progress)}\")\n</code></pre>"},{"location":"api/pydvl/valuation/methods/semivalue/#pydvl.valuation.methods.semivalue.SemivalueValuation.log_coefficient","title":"log_coefficient  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>log_coefficient: SemivalueCoefficient | None\n</code></pre> <p>This property returns the function computing the semi-value coefficient.</p> <p>Return <code>None</code> in subclasses that do not need to correct for the sampling distribution probabilities because of a specific, fixed sampler choice which already yields the semi-value coefficient.</p>"},{"location":"api/pydvl/valuation/methods/semivalue/#pydvl.valuation.methods.semivalue.SemivalueValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/shapley/","title":"Shapley","text":""},{"location":"api/pydvl/valuation/methods/shapley/#pydvl.valuation.methods.shapley","title":"pydvl.valuation.methods.shapley","text":"<p>This module implements the Shapley valuation method.</p> <p>Info</p> <p>See the main documentation for a description of the algorithm and its properties.</p> <p>We provide two main ways of computing Shapley values:</p> <ol> <li>A general approach that allows for any sampling scheme, including deterministic,    uniform, permutations, and so on. This is implemented in    ShapleyValuation</li> <li>A default configuration for the Truncated Monte Carlo Shapley (TMCS) method,    described in Ghorbani and Zou (2019)<sup>1</sup>. This is basically a wrapper to the more    general class, but with a permutation sampler by default. Besides being convenient,    it allows deactivating importance sampling internally (see Sampling strategies for    semi-values).</li> </ol> <p>Computing values in PyDVL typically follows the following pattern: construct a ModelUtility, a sampler, and a stopping criterion, then pass them to the valuation and fit it.</p> General usage pattern <pre><code>from pydvl.valuation import (\n    ShapleyValuation,\n    ModelUtility,\n    SupervisedScorer,\n    PermutationSampler,\n    MaxSamples\n)\n\nmodel = SomeSKLearnModel()\nscorer = SupervisedScorer(\"accuracy\", test_data, default=0)\nutility = ModelUtility(model, scorer, ...)\nsampler = UniformSampler(seed=42)\nstopping = MaxSamples(5000)\nvaluation = ShapleyValuation(utility, sampler, is_done=stopping)\nwith parallel_config(n_jobs=16):\n    valuation.fit(training_data)\nresult = valuation.values()\n</code></pre>"},{"location":"api/pydvl/valuation/methods/shapley/#pydvl.valuation.methods.shapley--choosing-samplers","title":"Choosing samplers","text":"<p>Different choices of sampler yield different qualities of approximation, see Sampling strategies for semi-values for a discussion of the internals.</p> <p>The most basic one is DeterministicUniformSampler, which iterates over all possible subsets of the training set. This is the most accurate, but also the most computationally expensive method (with complexity \\(O(2^n)\\)), so it is never used in practice.</p> <p>However, the most common one is PermutationSampler, which samples random permutations of the training set. Despite the apparent greater complexity of \\(O(n!)\\), the method is much faster to converge in practice, especially when using truncation policies to early-stop the processing of each permutation. As mentioned above, the default configuration of TMCS is available via TMCShapleyValuation.</p> Manually instantiating TMCS <p>Alternatively to using TMCShapleyValuation, in order to compute Shapley values as described in Ghorbani and Zou (2019)<sup>1</sup>, use this configuration:</p> <pre><code>truncation = RelativeTruncation(rtol=0.05)\nsampler = PermutationSampler(truncation=truncation, seed=seed)\nstopping = HistoryDeviation(n_steps=100, rtol=0.05)\nvaluation = ShapleyValuation(utility, sampler, stopping, skip_converged, progress)\n</code></pre> <p>Other samplers introduce different importance sampling schemes for the computation of Shapley values, like the Owen samplers,<sup>2</sup> or the Maximum-Sample-Reuse sampler,<sup>3</sup> these can be both beneficial and detrimental, but the usage pattern remains the same.</p> <p>Choosing stopping criteria</p> <p>As mentioned, computing Shapley values can be computationally expensive, especially for large datasets. Some samplers yield better convergence, but not in all cases. Proper choice of a stopping criterion is crucial to obtain useful results, while avoiding unnecessary computation.</p> <p>Bogus configurations</p> <p>While it is possible to mix-and-match different components of the valuation method, it is not always advisable, and it can sometimes be incorrect. For example, using a deterministic sampler with a count-based stopping criterion is likely to yield poor results. More importantly, not all samplers, nor sampler configurations, are compatible with Shapley value computation. For instance using NoIndexIteration with a PowersetSampler will not work since the evaluation strategy expects samples consisting of an index and a subset of its complement in the whole index set.</p>"},{"location":"api/pydvl/valuation/methods/shapley/#pydvl.valuation.methods.shapley--references","title":"References","text":"<ol> <li> <p>Ghorbani, A., &amp; Zou, J. Y. (2019). Data Shapley:   Equitable Valuation of Data for Machine   Learning. In Proceedings of   the 36th International Conference on Machine Learning, PMLR pp. 2242--2251.\u00a0\u21a9\u21a9</p> </li> <li> <p>Okhrati, Ramin, and Aldo Lipani. A   Multilinear Sampling Algorithm to Estimate Shapley   Values. In 2020 25th   International Conference on Pattern Recognition (ICPR), 7992\u201399. IEEE, 2021.\u00a0\u21a9</p> </li> <li> <p> Wang, Jiachen T., and Ruoxi Jia. Data Banzhaf: A   Robust Data Valuation Framework for Machine   Learning. In Proceedings of The   26th International Conference on Artificial Intelligence and Statistics,   6388\u20136421. PMLR, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/methods/shapley/#pydvl.valuation.methods.shapley.ShapleyValuation","title":"ShapleyValuation","text":"<pre><code>ShapleyValuation(\n    utility: UtilityBase,\n    sampler: IndexSampler,\n    is_done: StoppingCriterion,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n)\n</code></pre> <p>               Bases: <code>SemivalueValuation</code></p> <p>Computes Shapley values with any sampler.</p> <p>Use this class to test different sampling schemes. For a default configuration, use TMCShapleyValuation.</p> <p>For an introduction to the algorithm, see the main documentation.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Object to compute utilities.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>sampler</code> <p>Sampling scheme to use.</p> <p> TYPE: <code>IndexSampler</code> </p> <code>is_done</code> <p>Stopping criterion to use.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>skip_converged</code> <p>Whether to skip converged indices, as determined by the stopping criterion's <code>converged</code> array.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>show_warnings</code> <p>Whether to show warnings.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <p>Whether to show a progress bar. If a dictionary, it is passed to <code>tqdm</code> as keyword arguments, and the progress bar is displayed.</p> <p> TYPE: <code>dict[str, Any] | bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/valuation/methods/semivalue.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    sampler: IndexSampler,\n    is_done: StoppingCriterion,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n):\n    super().__init__()\n    self.utility = utility\n    self.sampler = sampler\n    self.is_done = is_done\n    self.skip_converged = skip_converged\n    if skip_converged:  # test whether the sampler supports skipping indices:\n        self.sampler.skip_indices = np.array([], dtype=np.int_)\n    self.show_warnings = show_warnings\n    self.tqdm_args: dict[str, Any] = {\"desc\": str(self)}\n    # HACK: parse additional args for the progress bar if any (we probably want\n    #  something better)\n    if isinstance(progress, bool):\n        self.tqdm_args.update({\"disable\": not progress})\n    elif isinstance(progress, dict):\n        self.tqdm_args.update(progress)\n    else:\n        raise TypeError(f\"Invalid type for progress: {type(progress)}\")\n</code></pre>"},{"location":"api/pydvl/valuation/methods/shapley/#pydvl.valuation.methods.shapley.ShapleyValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/shapley/#pydvl.valuation.methods.shapley.StratifiedShapleyValuation","title":"StratifiedShapleyValuation","text":"<pre><code>StratifiedShapleyValuation(\n    utility: UtilityBase,\n    is_done: StoppingCriterion,\n    batch_size: int = 1,\n    seed: Seed | None = None,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n)\n</code></pre> <p>               Bases: <code>ShapleyValuation</code></p> <p>Computes Shapley values using a uniform stratified sampler.</p> <p>Uses a StratifiedSampler with uniform probability for the sample sizes. Under this sampling scheme, the expected marginal utility coincides with the Shapley value. See the documentation for details.</p> <p>When to use this class</p> <p>This class is for illustrative purposes only. In general, permutation based sampling exhibits better convergence. See e.g. TMCShapleyValuation.</p> <p>If you need different size strategies, or wish to clip subset sizes outside a given range, instantiate ShapleyValuation directly with a StratifiedSampler.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Object to compute utilities.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>is_done</code> <p>Stopping criterion to use.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>batch_size</code> <p>The number of samples to generate per batch. Batches are processed together by each subprocess when working in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Random seed for the sampler.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>skip_converged</code> <p>Whether to skip converged indices. Convergence is determined by the stopping criterion's <code>converged</code> array.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>show_warnings</code> <p>Whether to show warnings when the stopping criterion is not met.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <p>Whether to show a progress bar. If a dictionary, it is passed to <code>tqdm</code> as keyword arguments, and the progress bar is displayed.</p> <p> TYPE: <code>dict[str, Any] | bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/valuation/methods/shapley.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    is_done: StoppingCriterion,\n    batch_size: int = 1,\n    seed: Seed | None = None,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n):\n    sampler = StratifiedSampler(\n        sample_sizes=ConstantSampleSize(),\n        sample_sizes_iteration=RandomSizeIteration,\n        index_iteration=RandomIndexIteration,\n        batch_size=batch_size,\n        seed=seed,\n    )\n    super().__init__(\n        utility, sampler, is_done, skip_converged, show_warnings, progress\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/methods/shapley/#pydvl.valuation.methods.shapley.StratifiedShapleyValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/methods/shapley/#pydvl.valuation.methods.shapley.TMCShapleyValuation","title":"TMCShapleyValuation","text":"<pre><code>TMCShapleyValuation(\n    utility: UtilityBase,\n    truncation: TruncationPolicy | None = None,\n    is_done: StoppingCriterion | None = None,\n    seed: Seed | None = None,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n)\n</code></pre> <p>               Bases: <code>ShapleyValuation</code></p> <p>Computes Shapley values using the Truncated Monte Carlo method.</p> <p>This class provides defaults similar to those in the experiments by Ghorbani and Zou (2019)<sup>1</sup>.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Object to compute utilities.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>truncation</code> <p>Truncation policy to use. Defaults to RelativeTruncation with a relative tolerance of 0.01 and a burn-in fraction of 0.4.</p> <p> TYPE: <code>TruncationPolicy | None</code> DEFAULT: <code>None</code> </p> <code>is_done</code> <p>Stopping criterion to use. Defaults to HistoryDeviation with a relative tolerance of 0.05 and a window of 100 samples.</p> <p> TYPE: <code>StoppingCriterion | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Random seed for the sampler.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>skip_converged</code> <p>Whether to skip converged indices. Convergence is determined by the stopping criterion's <code>converged</code> array.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>show_warnings</code> <p>Whether to show warnings when the stopping criterion is not met.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>progress</code> <p>Whether to show a progress bar. If a dictionary, it is passed to <code>tqdm</code> as keyword arguments, and the progress bar is displayed.</p> <p> TYPE: <code>dict[str, Any] | bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/valuation/methods/shapley.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    truncation: TruncationPolicy | None = None,\n    is_done: StoppingCriterion | None = None,\n    seed: Seed | None = None,\n    skip_converged: bool = False,\n    show_warnings: bool = True,\n    progress: dict[str, Any] | bool = False,\n):\n    if truncation is None:\n        truncation = RelativeTruncation(rtol=0.01, burn_in_fraction=0.4)\n    if is_done is None:\n        is_done = HistoryDeviation(n_steps=100, rtol=0.05)\n    sampler = PermutationSampler(truncation=truncation, seed=seed)\n    super().__init__(\n        utility, sampler, is_done, skip_converged, show_warnings, progress\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/methods/shapley/#pydvl.valuation.methods.shapley.TMCShapleyValuation.values","title":"values","text":"<pre><code>values(sort: bool = False) -&gt; ValuationResult\n</code></pre> <p>Returns a copy of the valuation result.</p> <p>The valuation must have been run with <code>fit()</code> before calling this method.</p> PARAMETER DESCRIPTION <code>sort</code> <p>Whether to sort the valuation result by value before returning it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     The result of the valuation.</p> Source code in <code>src/pydvl/valuation/base.py</code> <pre><code>def values(self, sort: bool = False) -&gt; ValuationResult:\n    \"\"\"Returns a copy of the valuation result.\n\n    The valuation must have been run with `fit()` before calling this method.\n\n    Args:\n        sort: Whether to sort the valuation result by value before returning it.\n    Returns:\n        The result of the valuation.\n    \"\"\"\n    if not self.is_fitted:\n        raise NotFittedException(type(self))\n    assert self.result is not None\n\n    from copy import copy\n\n    r = copy(self.result)\n    if sort:\n        r.sort()\n    return r\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/","title":"Samplers","text":""},{"location":"api/pydvl/valuation/samplers/#pydvl.valuation.samplers","title":"pydvl.valuation.samplers","text":"<p>Samplers iterate over subsets of indices.</p> <p>The classes in this module are used to iterate over sets of indices, as required for the computation of marginal utilities for semi-values and other marginal-utility based methods, in particular all game-theoretic methods. Because of the intertwining of these algorithms with the sampler employed, there are several strategies to choose when deploying, or extending each.</p>"},{"location":"api/pydvl/valuation/samplers/#pydvl.valuation.samplers--a-users-guide","title":"A user's guide","text":"<ul> <li>Construct a sampler by instantiating one of the classes in this module. Refer to   the documentation of each class for details.</li> <li>Pass the constructed sampler to the method. Not all combinations of sampler and   valuation method are meaningful.</li> <li>When using finite samplers, use the NoStopping   criterion and pass it the sampler to keep track of progress.</li> </ul>"},{"location":"api/pydvl/valuation/samplers/#pydvl.valuation.samplers--a-high-level-overview","title":"A high-level overview","text":"<p>Subclasses of IndexSampler are iterators over batches of Samples. Each sample is typically, but not necessarily, of the form \\((i, S)\\), where \\(i\\) is an index of interest, and \\(S \\subseteq N \\setminus \\{i\\}\\) is a subset of the complement of \\(i\\) over the index set \\(N.\\)</p> <p>Samplers reside in the main process. Their samples are sent to the workers by the <code>fit()</code> method of the valuation class, to be processed by a so-called EvaluationStrategy's <code>process()</code> method. These strategies return ValueUpdate objects, which are then aggregated into the final result by the main process.</p>"},{"location":"api/pydvl/valuation/samplers/#pydvl.valuation.samplers--sampler-weights","title":"Sampler weights","text":"<p>Because the samplers are used in a Monte Carlo setting, they can be weighted to perform importance sampling. To this end, classes inheriting from IndexSampler implement the log_weight() method, which returns the (logarithm of) the probability of sampling a given subset. This is used to correct the mean of the Monte Carlo samples, so that it converges to the desired expression. For an explanation of the interactions between sampler weights, semi-value coefficients and importance sampling, see Sampling strategies for semi-values.</p>"},{"location":"api/pydvl/valuation/samplers/#pydvl.valuation.samplers--sampler-evaluation","title":"Sampler evaluation","text":"<p>Different samplers require different strategies for processing samples, i.e. for evaluating the utility of the subsets. For instance, permutation samplers generate full permutations of the index set, and rely on a special evaluation loop that allows semi-value calculations to reuse computations, by iterating through each permutation sequentially.</p> <p>This behaviour is encoded in the EvaluationStrategy class, which the evaluation method retrieves through make_strategy().</p> Usage pattern in valuation method <p>The basic pattern is the following (see below for info on the <code>updater</code>): <pre><code>    def fit(self, data: Dataset):\n\n        ...\n\n        strategy = self.sampler.make_strategy(self.utility, self.log_coefficient)\n        processor = delayed(strategy.process)\n        updater = self.sampler.result_updater(self.result)\n\n        delayed_batches = Parallel()(\n            processor(batch=list(batch), is_interrupted=flag) for batch in self.sampler\n        )\n        for batch in delayed_batches:\n            for evaluation in batch:\n                self.result = updater(evaluation)\n            ...\n</code></pre></p>"},{"location":"api/pydvl/valuation/samplers/#pydvl.valuation.samplers--updating-the-result","title":"Updating the result","text":"<p>Yet another behaviour that depends on the sampling scheme is the way that results are updated. For instance, the MSRSampler requires tracking updates to two sequences of samples which are then merged in a specific way. This strategy is declared by the sampler through the factory method result_updater(), which returns a callable that updates the result with a single evaluation.</p>"},{"location":"api/pydvl/valuation/samplers/#pydvl.valuation.samplers--creating-custom-samplers","title":"Creating custom samplers","text":"<p>To create a custom sampler, subclass either PowersetSampler or PermutationSamplerBase, or implement the IndexSampler interface directly.</p> <p>There are three main methods to implement (and others that can be overridden):</p> <ul> <li>generate(), which yields   samples of the form \\((i, S)\\). These will be batched together by <code>__iter__</code> for   parallel processing. Note that, if the index set has size \\(N\\), for   PermutationSampler, a   batch size of \\(B\\) implies \\(O(B*N)\\) evaluations of the utility in one process, since   single permutations are always processed in one go.</li> <li> <p>log_weight() to provide a   factor by which to multiply Monte Carlo samples in stochastic methods, so that the   mean converges to the desired expression. This will be the logarithm of the   probability of sampling a given subset. For an explanation of the interactions between   sampler weights, semi-value coefficients and importance sampling, see   Sampling strategies for semi-values.</p> Disabling importance sampling <p>If you want to disable importance sampling, you can override the property log_coefficient() and return <code>None</code>. This will make the evaluation strategy ignore the sampler weights and the Monte Carlo sums converge to the expectation of the marginal utilities wrt. the sampling distribution, with no change.</p> <ul> <li>sample_limit() to return   the maximum number of samples that can be generated from a set of indices. Infinte   samplers should return <code>None</code>.</li> <li>make_strategy() to create   an evaluation strategy that processes the samples. This is typically a subclass of   EvaluationStrategy that computes   utilities and weights them with coefficients and sampler weights.   One can also use any of the predefined strategies, like the successive marginal   evaluations of   PowersetEvaluationStrategy   or the successive evaluations of   PermutationEvaluationStrategy.</li> </ul> </li> </ul> <p>Finally, if the sampler requires a dedicated result updater, you must override result_updater() to return a callable that updates a ValuationResult with one evaluation ValueUpdate. This is used e.g. for the MSRSampler which uses two running means for positive and negative updates.</p> <p>Changed in version 0.10.0</p> <p>All the samplers in this module have been changed to work with the new evaluation strategies.</p>"},{"location":"api/pydvl/valuation/samplers/#pydvl.valuation.samplers--references","title":"References","text":"<ol> <li> <p>Mitchell, Rory, Joshua Cooper, Eibe   Frank, and Geoffrey Holmes. Sampling Permutations for Shapley Value   Estimation. Journal of Machine   Learning Research 23, no. 43 (2022): 1\u201346.\u00a0\u21a9</p> </li> <li> <p>Watson, Lauren, Zeno Kujawa, Rayna Andreeva,   Hao-Tsung Yang, Tariq Elahi, and Rik Sarkar. Accelerated Shapley Value   Approximation for Data Evaluation.   arXiv, 9 November 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/samplers/base/","title":"Base","text":""},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base","title":"pydvl.valuation.samplers.base","text":"<p>Base classes for samplers and evaluation strategies.</p> <p>Read pydvl.valuation.samplers for an architectural overview of the samplers and their evaluation strategies.</p> <p>For an explanation of the interactions between sampler weights, semi-value coefficients and importance sampling, read [[semi-values-sampling]].</p>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.EvaluationStrategy","title":"EvaluationStrategy","text":"<pre><code>EvaluationStrategy(\n    utility: UtilityBase, log_coefficient: SemivalueCoefficient | None\n)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[SamplerT, ValueUpdateT]</code></p> <p>An evaluation strategy for samplers.</p> <p>An evaluation strategy is used to process the sample batches generated by a sampler and compute value updates. It's the main loop of the workers.</p> <p>Different sampling schemes require different strategies for the evaluation of the utilities. This class defines the common interface.</p> <p>For instance PermutationEvaluationStrategy evaluates the samples from PermutationSampler in sequence to save computation, and MSREvaluationStrategy keeps track of update signs for the two sums required by the MSR method.</p> Usage pattern in valuation methods <pre><code>def fit(self, data: Dataset):\n    self.utility = self.utility.with_dataset(data)\n    strategy = self.sampler.make_strategy(self.utility, self.log_coefficient)\n    delayed_batches = Parallel()(\n        delayed(strategy.process)(batch=list(batch), is_interrupted=flag)\n        for batch in self.sampler\n    )\n    for batch in delayed_batches:\n        for evaluation in batch:\n            self.result.update(evaluation.idx, evaluation.update)\n        if self.is_done(self.result):\n            flag.set()\n            break\n</code></pre> PARAMETER DESCRIPTION <code>utility</code> <p>Required to set up some strategies and to process the samples. Since this contains the training data, it is expensive to pickle and send to workers.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>log_coefficient</code> <p>An additional coefficient to multiply marginals with. This depends on the valuation method, hence the delayed setup. If <code>None</code>, the default is to apply no correction. This makes the sampling probabilities of the sampler the effective valuation coefficient in the limit.</p> <p> TYPE: <code>SemivalueCoefficient | None</code> </p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    log_coefficient: SemivalueCoefficient | None,\n):\n    self.utility = utility\n    # Used by the decorator suppress_warnings:\n    self.show_warnings = getattr(utility, \"show_warnings\", False)\n    self.n_indices = (\n        len(utility.training_data) if utility.training_data is not None else 0\n    )\n    if log_coefficient is not None:\n        self.valuation_coefficient = log_coefficient\n    else:\n        # Allow method implementations to disable importance sampling by setting\n        # the log_coefficient to None.\n        # Note that being in logspace, we are adding and subtracting 0s, so that\n        # this is not a problem.\n        self.valuation_coefficient = lambda n, subset_len: 0.0\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.EvaluationStrategy.process","title":"process  <code>abstractmethod</code>","text":"<pre><code>process(\n    batch: SampleBatch, is_interrupted: NullaryPredicate\n) -&gt; list[ValueUpdateT]\n</code></pre> <p>Processes batches of samples using the evaluator, with the strategy required for the sampler.</p> <p>Warning</p> <p>This method is intended to be used by the evaluator to process the samples in one batch, which means it might be sent to another process. Be careful with the objects you use here, as they will be pickled and sent over the wire.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of samples to process.</p> <p> TYPE: <code>SampleBatch</code> </p> <code>is_interrupted</code> <p>A predicate that returns True if the processing should be interrupted.</p> <p> TYPE: <code>NullaryPredicate</code> </p> YIELDS DESCRIPTION <code>list[ValueUpdateT]</code> <p>Updates to values as tuples (idx, update)</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>@abstractmethod\ndef process(\n    self, batch: SampleBatch, is_interrupted: NullaryPredicate\n) -&gt; list[ValueUpdateT]:\n    \"\"\"Processes batches of samples using the evaluator, with the strategy\n    required for the sampler.\n\n    !!! Warning\n        This method is intended to be used by the evaluator to process the samples\n        in one batch, which means it might be sent to another process. Be careful\n        with the objects you use here, as they will be pickled and sent over the\n        wire.\n\n    Args:\n        batch: A batch of samples to process.\n        is_interrupted: A predicate that returns True if the processing should be\n            interrupted.\n\n    Yields:\n        Updates to values as tuples (idx, update)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler","title":"IndexSampler","text":"<pre><code>IndexSampler(batch_size: int = 1)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[SampleT, ValueUpdateT]</code></p> <p>Samplers are custom iterables over batches of subsets of indices.</p> <p>Calling generate_batches(indices) on a sampler returns a generator over batches of Samples. Each batch is a list of samples, and each <code>Sample</code> is a tuple of the form \\((i, S)\\), where \\(i\\) is an index of interest, and \\(S \\subset I \\setminus \\{i\\}\\) is a subset of the complement of \\(i\\) in \\(I\\).</p> <p>Warning</p> <p>Samplers are not iterators themselves, so that each call to <code>generate_batches()</code> e.g. in a new for loop creates a new iterator.</p> <p>Subclassing IndexSampler</p> <p>Derived samplers must implement several methods, most importantly log_weight() and generate(). See the module's documentation for more details.</p>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler--interrupting-samplers","title":"Interrupting samplers","text":"<p>Calling interrupt() on a sampler will stop the batched generator after the current batch has been yielded.</p> PARAMETER DESCRIPTION <code>batch_size</code> <p>The number of samples to generate per batch. Batches are processed by EvaluationStrategy so that individual valuations in batch are guaranteed to be received in the right sequence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Example <pre><code>&gt;&gt;&gt;from pydvl.valuation.samplers import DeterministicUniformSampler\n&gt;&gt;&gt;import numpy as np\n&gt;&gt;&gt;sampler = DeterministicUniformSampler()\n&gt;&gt;&gt;for idx, s in sampler.generate_batches(np.arange(2)):\n&gt;&gt;&gt;    print(s, end=\"\")\n[][2,][][1,]\n</code></pre> PARAMETER DESCRIPTION <code>batch_size</code> <p>The number of samples to generate per batch. Batches are processed by the EvaluationStrategy</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __init__(self, batch_size: int = 1):\n    \"\"\"\n    Args:\n        batch_size: The number of samples to generate per batch. Batches are\n            processed by the\n            [EvaluationStrategy][pydvl.valuation.samplers.base.EvaluationStrategy]\n    \"\"\"\n    self._batch_size = batch_size\n    self._n_samples = 0\n    self._interrupted = False\n    self._skip_indices = np.empty(0, dtype=bool)\n    self._len: int | None = None\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices: IndexSetT\n</code></pre> <p>Indices being skipped in the sampler. The exact behaviour will be sampler-dependent, so that setting this property is disabled by default.</p>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(indices: IndexSetT) -&gt; SampleGenerator\n</code></pre> <p>Generates single samples.</p> <p><code>IndexSampler.generate_batches()</code> will batch these samples according to the batch size set upon construction.</p> PARAMETER DESCRIPTION <code>indices</code> <p> TYPE: <code>IndexSetT</code> </p> YIELDS DESCRIPTION <code>SampleGenerator</code> <p>A tuple (idx, subset) for each sample.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>@abstractmethod\ndef generate(self, indices: IndexSetT) -&gt; SampleGenerator:\n    \"\"\"Generates single samples.\n\n    `IndexSampler.generate_batches()` will batch these samples according to the\n    batch size set upon construction.\n\n    Args:\n        indices:\n\n    Yields:\n        A tuple (idx, subset) for each sample.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler.log_weight","title":"log_weight  <code>abstractmethod</code>","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Log probability of sampling a set S.</p> <p>We assume that every sampler allows computing \\(p(S)\\) as a function of the size of the index set \\(n\\) and the size \\(k\\) of the subset being sampled:</p> \\[p(S) = p(S | k) p(k),\\] <p>For details on weighting, importance sampling and usage with semi-values, see [[semi-values-sampling]].</p> <p>Log-space computation</p> <p>Because the weight is a probability that can be arbitrarily small, we compute it in log-space for numerical stability.</p> PARAMETER DESCRIPTION <code>n</code> <p>The size of the index set.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>The size of the subset being sampled</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The natural logarithm of the probability of sampling a set of the given size, when the index set has size <code>n</code>.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>@abstractmethod\ndef log_weight(self, n: int, subset_len: int) -&gt; float:\n    r\"\"\"Log probability of sampling a set S.\n\n    We assume that every sampler allows computing $p(S)$ as a function of the size\n    of the index set $n$ and the size $k$ of the subset being sampled:\n\n    $$p(S) = p(S | k) p(k),$$\n\n    For details on weighting, importance sampling and usage with semi-values, see\n    [[semi-values-sampling]].\n\n    !!! Info \"Log-space computation\"\n        Because the weight is a probability that can be arbitrarily small, we\n        compute it in log-space for numerical stability.\n\n    Args:\n        n: The size of the index set.\n        subset_len: The size of the subset being sampled\n\n    Returns:\n        The natural logarithm of the probability of sampling a set of the given\n            size, when the index set has size `n`.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler.make_strategy","title":"make_strategy  <code>abstractmethod</code>","text":"<pre><code>make_strategy(\n    utility: UtilityBase, log_coefficient: SemivalueCoefficient | None\n) -&gt; EvaluationStrategy\n</code></pre> <p>Returns the strategy for this sampler.</p> <p>The evaluation strategy is used to process the samples generated by the sampler and compute value updates. It's the main loop of the workers.</p> PARAMETER DESCRIPTION <code>utility</code> <p>The utility to use for the evaluation strategy.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>log_coefficient</code> <p>An additional coefficient to multiply marginals with. This depends on the valuation method, hence the delayed setup. If <code>None</code>, the default is to apply no correction. This makes the sampling probabilities of the sampler the effective valuation coefficient in the limit.</p> <p> TYPE: <code>SemivalueCoefficient | None</code> </p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>@abstractmethod\ndef make_strategy(\n    self,\n    utility: UtilityBase,\n    log_coefficient: SemivalueCoefficient | None,\n) -&gt; EvaluationStrategy:\n    \"\"\"Returns the strategy for this sampler.\n\n    The evaluation strategy is used to process the samples generated by the sampler\n    and compute value updates. It's the main loop of the workers.\n\n    Args:\n        utility: The utility to use for the evaluation strategy.\n        log_coefficient: An additional coefficient to multiply marginals with. This\n            depends on the valuation method, hence the delayed setup. If `None`,\n            the default is **to apply no correction**. This makes the sampling\n            probabilities of the sampler the effective valuation coefficient in the\n            limit.\n    \"\"\"\n    ...  # return SomeEvaluationStrategy(self)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/base/#pydvl.valuation.samplers.base.IndexSampler.sample_limit","title":"sample_limit  <code>abstractmethod</code>","text":"<pre><code>sample_limit(indices: IndexSetT) -&gt; int | None\n</code></pre> <p>Number of samples that can be generated from the indices.</p> PARAMETER DESCRIPTION <code>indices</code> <p>The indices used in the sampler.</p> <p> TYPE: <code>IndexSetT</code> </p> RETURNS DESCRIPTION <code>int | None</code> <p>The maximum number of samples that will be generated, or  <code>None</code> if the number of samples is infinite.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>@abstractmethod\ndef sample_limit(self, indices: IndexSetT) -&gt; int | None:\n    \"\"\"Number of samples that can be generated from the indices.\n\n    Args:\n        indices: The indices used in the sampler.\n\n    Returns:\n        The maximum number of samples that will be generated, or  `None` if the\n            number of samples is infinite.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/classwise/","title":"Classwise","text":""},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise","title":"pydvl.valuation.samplers.classwise","text":"<p>Class-wise sampler for the class-wise Shapley valuation method.</p> <p>The class-wise Shapley method, introduced by Schoch et al., 2022<sup>1</sup>, uses a so-called set-conditional marginal Shapley value that requires selectively sampling subsets of data points with the same or a different class from that of the data point of interest.</p> <p>This sampling scheme is divided into an outer and an inner sampler.</p> <ul> <li>The outer sampler is any subclass of   PowersetSampler that generates   subsets within the complement set of the data point of interest, and with a different   label (so-called \"out-of-class\" samples, denoted by \\(S_{-y_i}\\) in this documentation).</li> <li>The inner sampler is any subclass of   IndexSampler, typically (and in the   paper) a   PermutationSampler. It   returns so-called \"in-class\" samples (denoted by \\(S_{y_i}\\) in this documentation) from   the set \\(N_{y_i}\\), i.e., the set of all indices with the same label as the data point   of interest.</li> </ul> <p>Info</p> <p>For more information on the class-wise Shapley method, as well as a summary of the reproduction results by Semmler and de Benito Delgado (2024)<sup>2</sup> see the main documentation for the method.</p>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise--references","title":"References","text":"<ol> <li> <p>Schoch, Stephanie, Haifeng Xu, and Yangfeng Ji. CS-Shapley: Class-wise Shapley Values for Data Valuation in Classification. In Proc. of the Thirty-Sixth Conference on Neural Information Processing Systems (NeurIPS). New Orleans, Louisiana, USA, 2022.\u00a0\u21a9</p> </li> <li> <p>Semmler, Markus, and Miguel de Benito Delgado. [Re] Classwise-Shapley Values for Data Valuation. Transactions on Machine Learning Research, July 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.ClasswiseSampler","title":"ClasswiseSampler","text":"<pre><code>ClasswiseSampler(\n    in_class: IndexSampler,\n    out_of_class: PowersetSampler,\n    *,\n    min_elements_per_label: int = 1,\n    batch_size: int = 1\n)\n</code></pre> <p>               Bases: <code>IndexSampler[ClasswiseSample, ValueUpdate]</code></p> <p>A sampler that samples elements from a dataset in two steps, based on the labels.</p> <p>It proceeds by sampling out-of-class indices (training points with a different label to the point of interest), and in-class indices (training points with the same label as the point of interest).</p> <p>Used by the class-wise Shapley valuation method.</p> PARAMETER DESCRIPTION <code>in_class</code> <p>Sampling scheme for elements of a given label.</p> <p> TYPE: <code>IndexSampler</code> </p> <code>out_of_class</code> <p>Sampling scheme for elements of different labels.</p> <p> TYPE: <code>PowersetSampler</code> </p> <code>min_elements_per_label</code> <p>Minimum number of elements per label to sample from the complement set, i.e., out of class elements.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>src/pydvl/valuation/samplers/classwise.py</code> <pre><code>def __init__(\n    self,\n    in_class: IndexSampler,\n    out_of_class: PowersetSampler,\n    *,\n    min_elements_per_label: int = 1,\n    batch_size: int = 1,\n):\n    super().__init__(batch_size=batch_size)\n    self.in_class = in_class\n    self.out_of_class = out_of_class\n    self.min_elements_per_label = min_elements_per_label\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.ClasswiseSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices: IndexSetT\n</code></pre> <p>Indices being skipped in the sampler. The exact behaviour will be sampler-dependent, so that setting this property is disabled by default.</p>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.ClasswiseSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.ClasswiseSampler.generate","title":"generate","text":"<pre><code>generate(indices: IndexSetT) -&gt; SampleGenerator\n</code></pre> <p>This is not needed because this sampler is used by calling the <code>from_data</code> method instead of the <code>generate_batches</code> method.</p> Source code in <code>src/pydvl/valuation/samplers/classwise.py</code> <pre><code>def generate(self, indices: IndexSetT) -&gt; SampleGenerator:\n    \"\"\"This is not needed because this sampler is used by calling the `from_data`\n    method instead of the `generate_batches` method.\"\"\"\n    raise AttributeError(\"Cannot sample from indices directly.\")\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.ClasswiseSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.ClasswiseSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt() -&gt; None\n</code></pre> <p>Interrupts the current sampler as well as the passed in samplers</p> Source code in <code>src/pydvl/valuation/samplers/classwise.py</code> <pre><code>def interrupt(self) -&gt; None:\n    \"\"\"Interrupts the current sampler as well as the passed in samplers\"\"\"\n    super().interrupt()\n    self.in_class.interrupt()\n    self.out_of_class.interrupt()\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.ClasswiseSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>CW-Shapley uses the evaluation strategy from the in-class sampler, so this method should never be called.</p> Source code in <code>src/pydvl/valuation/samplers/classwise.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    \"\"\"CW-Shapley uses the evaluation strategy from the in-class sampler, so this\n    method should never be called.\"\"\"\n    raise AttributeError(\"The weight should come from the in-class sampler\")\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.ClasswiseSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.ClasswiseSampler.sample_limit","title":"sample_limit","text":"<pre><code>sample_limit(indices: IndexSetT) -&gt; int | None\n</code></pre> <p>The sample limit cannot be computed without accessing the label information and using that to compute the sample limits of the in-class and out-of-class samplers first.</p> Source code in <code>src/pydvl/valuation/samplers/classwise.py</code> <pre><code>def sample_limit(self, indices: IndexSetT) -&gt; int | None:\n    \"\"\"The sample limit cannot be computed without accessing the label information\n    and using that to compute the sample limits of the in-class and out-of-class\n    samplers first.\"\"\"\n    raise AttributeError(\n        \"The sample limit cannot be computed without the label information.\"\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.get_unique_labels","title":"get_unique_labels","text":"<pre><code>get_unique_labels(array: NDArray) -&gt; NDArray\n</code></pre> <p>Returns unique labels in a categorical dataset.</p> PARAMETER DESCRIPTION <code>array</code> <p>The input array to find unique labels from. It should be of    categorical types such as Object, String, Unicode, Unsigned    integer, Signed integer, or Boolean.</p> <p> TYPE: <code>NDArray</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>An array of unique labels.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the input array is not of a categorical type.</p> Source code in <code>src/pydvl/valuation/samplers/classwise.py</code> <pre><code>def get_unique_labels(array: NDArray) -&gt; NDArray:\n    \"\"\"Returns unique labels in a categorical dataset.\n\n    Args:\n        array: The input array to find unique labels from. It should be of\n               categorical types such as Object, String, Unicode, Unsigned\n               integer, Signed integer, or Boolean.\n\n    Returns:\n        An array of unique labels.\n\n    Raises:\n        ValueError: If the input array is not of a categorical type.\n    \"\"\"\n    # Object, String, Unicode, Unsigned integer, Signed integer, boolean\n    if array.dtype.kind in \"OSUiub\":\n        return cast(NDArray, np.unique(array))\n    raise ValueError(\n        f\"Input array has an unsupported data type for categorical labels: {array.dtype}. \"\n        \"Expected types: Object, String, Unicode, Unsigned integer, Signed integer, or Boolean.\"\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/classwise/#pydvl.valuation.samplers.classwise.roundrobin","title":"roundrobin","text":"<pre><code>roundrobin(\n    batch_generators: Mapping[U, Iterable[V]]\n) -&gt; Generator[tuple[U, V], None, None]\n</code></pre> <p>Take samples from batch generators in order until all of them are exhausted.</p> <p>This was heavily inspired by the roundrobin recipe in the official Python documentation for the itertools package.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pydvl.valuation.samplers.classwise import roundrobin\n&gt;&gt;&gt; list(roundrobin({\"A\": \"123\"}, {\"B\": \"456\"}))\n[(\"A\", \"1\"), (\"B\", \"4\"), (\"A\", \"2\"), (\"B\", \"5\"), (\"A\", \"3\"), (\"B\", \"6\")]\n</code></pre> PARAMETER DESCRIPTION <code>batch_generators</code> <p>dictionary mapping labels to batch generators.</p> <p> TYPE: <code>Mapping[U, Iterable[V]]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>Combined generators</p> Source code in <code>src/pydvl/valuation/samplers/classwise.py</code> <pre><code>def roundrobin(\n    batch_generators: Mapping[U, Iterable[V]],\n) -&gt; Generator[tuple[U, V], None, None]:\n    \"\"\"Take samples from batch generators in order until all of them are exhausted.\n\n    This was heavily inspired by the roundrobin recipe\n    in the official Python documentation for the itertools package.\n\n    Examples:\n        &gt;&gt;&gt; from pydvl.valuation.samplers.classwise import roundrobin\n        &gt;&gt;&gt; list(roundrobin({\"A\": \"123\"}, {\"B\": \"456\"}))\n        [(\"A\", \"1\"), (\"B\", \"4\"), (\"A\", \"2\"), (\"B\", \"5\"), (\"A\", \"3\"), (\"B\", \"6\")]\n\n    Args:\n        batch_generators: dictionary mapping labels to batch generators.\n\n    Returns:\n        Combined generators\n    \"\"\"\n    n_active = len(batch_generators)\n    remaining_generators = cycle(\n        (label, iter(it).__next__) for label, it in batch_generators.items()\n    )\n    while n_active:\n        try:\n            for label, next_generator in remaining_generators:\n                yield label, next_generator()\n        except StopIteration:\n            # Remove the iterator we just exhausted from the cycle.\n            n_active -= 1\n            remaining_generators = cycle(islice(remaining_generators, n_active))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/","title":"Msr","text":""},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr","title":"pydvl.valuation.samplers.msr","text":"<p>This module implements Maximum Sample Re-use (MSR) sampling for valuation, as described in Wang and Jia (2023)<sup>1</sup>, where it was introduced specifically for Data Banhzaf.</p> <p>When used with this method, sample complexity is reduced by a factor of \\(O(n)\\).</p> <p>Warning</p> <p>MSR can be very unstable when used with valuation algorithms other than Data Banzhaf. This is because of the instabilities introduced by the correction coefficients. For more, see Appendix C.1 of Wang and Jia (2023)<sup>1</sup>.</p> <p>The idea behind MSR is to update all indices in the dataset with every evaluation of the utility function on a sample. Updates are divided into positive, if the index is in the sample, and negative, if it is not. The two running means are later combined into a final result.</p> <p>Note that this requires defining a special evaluation strategy and result updater, as returned by the make_strategy() and result_updater() methods, respectively.</p> <p>For more on the general architecture of samplers see pydvl.valuation.samplers.</p>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr--references","title":"References","text":"<ol> <li> <p>Wang, J.T. and Jia, R., 2023. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning. In: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, pp. 6388-6421.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSREvaluationStrategy","title":"MSREvaluationStrategy","text":"<pre><code>MSREvaluationStrategy(\n    utility: UtilityBase, log_coefficient: SemivalueCoefficient | None\n)\n</code></pre> <p>               Bases: <code>EvaluationStrategy[MSRSampler, MSRValueUpdate]</code></p> <p>Evaluation strategy for Maximum Sample Re-use (MSR) valuation in log space.</p> <p>The MSR evaluation strategy makes one utility evaluation per sample but generates <code>n_indices</code> many updates from it. The updates will be used to update two running means that will later be combined into a final value. We use the field <code>ValueUpdate.in_sample</code> field to inform MSRResultUpdater of which of the two running means must be updated.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    log_coefficient: SemivalueCoefficient | None,\n):\n    self.utility = utility\n    # Used by the decorator suppress_warnings:\n    self.show_warnings = getattr(utility, \"show_warnings\", False)\n    self.n_indices = (\n        len(utility.training_data) if utility.training_data is not None else 0\n    )\n    if log_coefficient is not None:\n        self.valuation_coefficient = log_coefficient\n    else:\n        # Allow method implementations to disable importance sampling by setting\n        # the log_coefficient to None.\n        # Note that being in logspace, we are adding and subtracting 0s, so that\n        # this is not a problem.\n        self.valuation_coefficient = lambda n, subset_len: 0.0\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRResultUpdater","title":"MSRResultUpdater","text":"<pre><code>MSRResultUpdater(result: ValuationResult)\n</code></pre> <p>               Bases: <code>ResultUpdater[MSRValueUpdate]</code></p> <p>Update running means for MSR valuation (in log-space).</p> <p>This class is used to update two running means for positive and negative updates separately. The two running means are later combined into a final result.</p> <p>Since values computed with MSR are not a mean over marginals, both the variances of the marginals and the update counts are ill-defined. We use the following conventions:</p> <ol> <li> <p>The counts are defined as the minimum of the two counts. This definition enables us to ensure a minimal number of updates for both running means via stopping criteria and correctly detects that no actual update has taken place if one of the counts is zero.</p> </li> <li> <p>We reverse engineer the variances so that they yield correct standard errors given our convention for the counts and the normal calculation of standard errors in the valuation result.</p> </li> </ol> <p>Note that we cannot use the normal addition or subtraction defined by the ValuationResult because it is weighted with counts. If we were to simply subtract the negative result from the positive we would get wrong variance estimates, misleading update counts and even wrong values if no further precaution is taken.</p> Source code in <code>src/pydvl/valuation/samplers/msr.py</code> <pre><code>def __init__(self, result: ValuationResult):\n    super().__init__(result)\n    self.in_sample = ValuationResult.zeros(\n        algorithm=result.algorithm, indices=result.indices, data_names=result.names\n    )\n    self.out_of_sample = ValuationResult.zeros(\n        algorithm=result.algorithm, indices=result.indices, data_names=result.names\n    )\n\n    self.in_sample_updater = LogResultUpdater[MSRValueUpdate](self.in_sample)\n    self.out_of_sample_updater = LogResultUpdater[MSRValueUpdate](\n        self.out_of_sample\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRResultUpdater.combine_results","title":"combine_results","text":"<pre><code>combine_results() -&gt; ValuationResult\n</code></pre> <p>Combine the positive and negative running means into a final result. Returns:     The combined valuation result.</p> Verify that the two running means are statistically independent (which is <p>assumed in the aggregation of variances).</p> Source code in <code>src/pydvl/valuation/samplers/msr.py</code> <pre><code>def combine_results(self) -&gt; ValuationResult:\n    \"\"\"Combine the positive and negative running means into a final result.\n    Returns:\n        The combined valuation result.\n\n    TODO: Verify that the two running means are statistically independent (which is\n        assumed in the aggregation of variances).\n    \"\"\"\n    # define counts as minimum of the two counts (see docstring)\n    counts = np.minimum(self.in_sample.counts, self.out_of_sample.counts)\n\n    values = self.in_sample.values - self.out_of_sample.values\n    values[counts == 0] = np.nan\n\n    # define variances that yield correct standard errors (see docstring)\n    pos_var = self.in_sample.variances / np.clip(self.in_sample.counts, 1, np.inf)\n    neg_var = self.out_of_sample.variances / np.clip(\n        self.out_of_sample.counts, 1, np.inf\n    )\n    variances = np.where(counts != 0, (pos_var + neg_var) * counts, np.inf)\n\n    self.result = ValuationResult(\n        values=values,\n        variances=variances,\n        counts=counts,\n        indices=self.result.indices,\n        data_names=self.result.names,\n        algorithm=self.result.algorithm,\n    )\n\n    return self.result\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRSampler","title":"MSRSampler","text":"<pre><code>MSRSampler(batch_size: int = 1, seed: Seed | None = None)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>IndexSampler[Sample, MSRValueUpdate]</code></p> <p>Sampler for unweighted Maximum Sample Re-use (MSR) valuation.</p> <p>The sampling is similar to a UniformSampler but without an outer index. However,the MSR sampler uses a special evaluation strategy and result updater, as returned by the make_strategy() and result_updater() methods, respectively.</p> <p>Two running means are updated separately for positive and negative updates. The two running means are later combined into a final result.</p> PARAMETER DESCRIPTION <code>batch_size</code> <p>Number of samples to generate in each batch.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Seed for the random number generator.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/samplers/msr.py</code> <pre><code>def __init__(self, batch_size: int = 1, seed: Seed | None = None):\n    super().__init__(batch_size=batch_size, seed=seed)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices: IndexSetT\n</code></pre> <p>Indices being skipped in the sampler. The exact behaviour will be sampler-dependent, so that setting this property is disabled by default.</p>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Probability of sampling a set under MSR.</p> <p>In the MSR scheme, the sampling is done from the full power set \\(2^N\\) (each set \\(S \\subseteq N\\) with probability \\(1 / 2^n\\)), and then for each data point \\(i\\) one partitions the sample into:</p> <pre><code>* $\\mathcal{S}_{\\ni i} = \\{S \\in \\mathcal{S}: i \\in S\\},$ and\n* $\\mathcal{S}_{\\nni i} = \\{S \\in \\mathcal{S}: i \\nin S\\}.$.\n</code></pre> <p>When we condition on the event \\(i \\in S\\), the remaining part \\(S_{-i}\\) is uniformly distributed over \\(2^{N_{-i}}\\). In other words, the act of partitioning recovers the uniform distribution on \\(2^{N_{-i}}\\) \"for free\" because</p> \\[P (S_{-i} = T \\mid i \\in S) = \\frac{1}{2^{n - 1}},\\] <p>for every \\(T \\subseteq N_{-i}\\).</p> PARAMETER DESCRIPTION <code>n</code> <p>Size of the index set.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>Size of the subset.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The logarithm of the probability of having sampled a set of size <code>subset_len</code>.</p> Source code in <code>src/pydvl/valuation/samplers/msr.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    r\"\"\"Probability of sampling a set under MSR.\n\n    In the **MSR scheme**, the sampling is done from the full power set $2^N$ (each\n    set $S \\subseteq N$ with probability $1 / 2^n$), and then for each data point\n    $i$ one partitions the sample into:\n\n        * $\\mathcal{S}_{\\ni i} = \\{S \\in \\mathcal{S}: i \\in S\\},$ and\n        * $\\mathcal{S}_{\\nni i} = \\{S \\in \\mathcal{S}: i \\nin S\\}.$.\n\n    When we condition on the event $i \\in S$, the remaining part $S_{-i}$ is\n    uniformly distributed over $2^{N_{-i}}$. In other words, the act of\n    partitioning recovers the uniform distribution on $2^{N_{-i}}$ \"for free\"\n    because\n\n    $$P (S_{-i} = T \\mid i \\in S) = \\frac{1}{2^{n - 1}},$$\n\n    for every $T \\subseteq N_{-i}$.\n\n    Args:\n        n: Size of the index set.\n        subset_len: Size of the subset.\n\n    Returns:\n        The logarithm of the probability of having sampled a set of size\n            `subset_len`.\n    \"\"\"\n    return float(-(n - 1) * np.log(2)) if n &gt; 0 else 0.0\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRSampler.make_strategy","title":"make_strategy","text":"<pre><code>make_strategy(\n    utility: UtilityBase, coefficient: SemivalueCoefficient | None = None\n) -&gt; MSREvaluationStrategy\n</code></pre> <p>Returns the strategy for this sampler.</p> PARAMETER DESCRIPTION <code>utility</code> <p>Utility function to evaluate.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>coefficient</code> <p>Coefficient function for the utility function.</p> <p> TYPE: <code>SemivalueCoefficient | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/samplers/msr.py</code> <pre><code>def make_strategy(\n    self,\n    utility: UtilityBase,\n    coefficient: SemivalueCoefficient | None = None,\n) -&gt; MSREvaluationStrategy:\n    \"\"\"Returns the strategy for this sampler.\n\n    Args:\n        utility: Utility function to evaluate.\n        coefficient: Coefficient function for the utility function.\n    \"\"\"\n    assert coefficient is not None\n    return MSREvaluationStrategy(utility, coefficient)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater\n</code></pre> <p>Returns a callable that updates a valuation result with an MSR value update.</p> <p>MSR updates two running means for positive and negative updates separately. The two running means are later combined into a final result.</p> PARAMETER DESCRIPTION <code>result</code> <p>The valuation result to update with each call of the returned callable.</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the valuation result with very         MSRValueUpdate.</p> Source code in <code>src/pydvl/valuation/samplers/msr.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater:\n    \"\"\"Returns a callable that updates a valuation result with an MSR value update.\n\n    MSR updates two running means for positive and negative updates separately. The\n    two running means are later combined into a final result.\n\n    Args:\n        result: The valuation result to update with each call of the returned\n            callable.\n    Returns:\n        A callable object that updates the valuation result with very\n            [MSRValueUpdate][pydvl.valuation.samplers.msr.MSRValueUpdate].\n    \"\"\"\n    return MSRResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/msr/#pydvl.valuation.samplers.msr.MSRValueUpdate","title":"MSRValueUpdate  <code>dataclass</code>","text":"<pre><code>MSRValueUpdate(idx: IndexT, log_update: float, sign: int, in_sample: bool)\n</code></pre> <p>               Bases: <code>ValueUpdate</code></p> <p>Update for Maximum Sample Re-use (MSR) valuation (in log space).</p> ATTRIBUTE DESCRIPTION <code>in_sample</code> <p>Whether the index to be updated was in the sample.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>src/pydvl/valuation/samplers/msr.py</code> <pre><code>def __init__(self, idx: IndexT, log_update: float, sign: int, in_sample: bool):\n    object.__setattr__(self, \"idx\", idx)\n    object.__setattr__(self, \"log_update\", log_update)\n    object.__setattr__(self, \"sign\", sign)\n    object.__setattr__(self, \"in_sample\", in_sample)\n    object.__setattr__(self, \"update\", np.exp(log_update) * sign)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/","title":"Owen","text":""},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen","title":"pydvl.valuation.samplers.owen","text":"<p>Samplers for Owen Shapley values.</p> <p>The Owen Shapley value is a sampling-based method to estimate Shapley values. It samples probability values between 0 and 1 and then draws subsets of the complement of the current index where each element is sampled with the given probability.</p>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen--possible-configurations","title":"Possible configurations","text":"<p>The basic sampler is OwenSampler. It can be configured with a deterministic grid of probability values or a uniform distribution between 0 and 1. The former follows the idea of the original paper (Okhrati and Lipani, 2021)<sup>1</sup>.</p> <p>This strategy for the sampling of probability values can be specified with the GridOwenStrategy or UniformOwenStrategy classes.</p> <p>In addition, and as is the case for all PowerSetSamplers, one can configure the way the sampler iterates over indices to be updated. This can be done with the IndexIteration strategy.</p> <p>When using infinite index iteration, the sampler can be used with a stopping criterion to estimate Shapley values. This follows more closely the typical usage pattern in PyDVL than the original sampling method described in Okhrati and Lipani (2021)<sup>1</sup>.</p>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen--antithetic-owen-sampling","title":"Antithetic Owen sampling","text":"<p>We also provide an AntitheticOwenSampler, which draws probability values \\(q\\) between 0 and 0.5 again, either deterministically over a discrete grid or at uniformly at random, and then generates two samples for each index, one with the probability \\(q\\) and another with probability \\(1-q\\).</p> <p>It can be configured in the same manner as the regular Owen sampler.</p> The four main configurations <pre><code>standard_owen = OwenSampler(\n    outer_sampling_strategy=GridOwenStrategy(n_samples_outer=100),\n    n_samples_inner=2,\n    index_iteration=FiniteSequentialIndexIteration,\n    )\nantithetic_owen = AntitheticOwenSampler(\n    outer_sampling_strategy=GridOwenStrategy(n_samples_outer=100),\n    n_samples_inner=2,\n    index_iteration=FiniteSequentialIndexIteration,\n    )\ninfinite_owen = OwenSampler(\n    outer_sampling_strategy=UniformOwenStrategy(seed=42),\n    n_samples_inner=2,\n    index_iteration=RandomIndexIteration,\n    seed=42\n)\ninfinite_antithetic_owen = AntitheticOwenSampler(\n    outer_sampling_strategy=UniformOwenStrategy(seed=42),\n    n_samples_inner=2,\n    index_iteration=RandomIndexIteration,\n    seed=42\n)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen--references","title":"References","text":"<ol> <li> <p>Okhrati, R., Lipani, A., 2021. A Multilinear Sampling Algorithm to Estimate Shapley Values. In: 2020 25th International Conference on Pattern Recognition (ICPR), pp. 7992\u20137999. IEEE.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.AntitheticOwenSampler","title":"AntitheticOwenSampler","text":"<pre><code>AntitheticOwenSampler(\n    outer_sampling_strategy: OwenStrategy,\n    n_samples_inner: int = 2,\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = FiniteSequentialIndexIteration,\n    seed: Seed | None = None,\n)\n</code></pre> <p>               Bases: <code>OwenSampler</code></p> <p>A sampler for antithetic Owen shapley values.</p> <p>For each sample obtained with the method of OwenSampler, a second sample is generated by taking the complement of the first sample.</p> <p>For the same number of total samples, the antithetic Owen sampler yields usually more precise estimates of shapley values than the regular Owen sampler.</p> Source code in <code>src/pydvl/valuation/samplers/owen.py</code> <pre><code>def __init__(\n    self,\n    outer_sampling_strategy: OwenStrategy,\n    n_samples_inner: int = 2,\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = FiniteSequentialIndexIteration,\n    seed: Seed | None = None,\n):\n    super().__init__(\n        outer_sampling_strategy=outer_sampling_strategy,\n        n_samples_inner=n_samples_inner,\n        batch_size=batch_size,\n        index_iteration=index_iteration,\n        seed=seed,\n    )\n    self.q_stop = 0.5\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.AntitheticOwenSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices\n</code></pre> <p>Set of indices to skip in the outer loop.</p>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.AntitheticOwenSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.AntitheticOwenSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.AntitheticOwenSampler.index_iterable","title":"index_iterable","text":"<pre><code>index_iterable(indices: IndexSetT) -&gt; Generator[IndexT | None, None, None]\n</code></pre> <p>Iterates over indices with the method specified at construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def index_iterable(\n    self, indices: IndexSetT\n) -&gt; Generator[IndexT | None, None, None]:\n    \"\"\"Iterates over indices with the method specified at construction.\"\"\"\n    try:\n        iterable = self._index_iterator_cls(indices, seed=self._rng)  # type: ignore\n    except (AttributeError, TypeError):\n        iterable = self._index_iterator_cls(indices)\n    for idx in iterable:\n        if idx not in self.skip_indices:\n            yield idx\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.AntitheticOwenSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.AntitheticOwenSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>For each \\(q_j, j \\in \\{1, ..., N\\}\\) in the outer probabilities, the probability of drawing a subset \\(S_k\\) of size \\(k\\) is:</p> \\[ P (| S_{q_j} | = k) = \\binom{n}{k} \\  q_j^k  (1 - q_j)^{n - k}.\\] <p>So, if each \\(q_j\\) is chosen with equal weight (or more generally with probability \\(p_j\\)),then by total probability, the overall probability of obtaining a subset of size \\(k\\) is a mixture of the binomials: $$ P (| S | = k) = \\sum_{j = 1}^N p_j \\ \\binom{n}{k} \\ q_j^k  (1 - q_j)^{n - k}. $$</p> <p>In our case \\(p_j = 1/N\\), so that \\(P(|S|=k) = \\frac{1}{N} \\sum_{j=1}^N P (| S_{q_j} | = k)\\). For large enough \\(N\\) this is</p> \\[ P(|S|=k) \\approx \\binom{n}{k} \\int_0^1 q^k (1 - q)^{n - k} \\, dq = \\frac{1}{ n+1}, \\] <p>where we computed the integral using the beta function and its expression as products of gamma functions.</p> <p>Now, given the symmetry wrt. the indices in the sampling procedure, any given set \\(S\\) of size \\(k\\) is equally likely to be drawn. So the probability of a set being of size \\(k\\) must be equally divided by the number of sets of that size, and the weight of a set of size \\(k\\) is:</p> \\[ P(S) = \\frac{1}{n+1} \\binom{n}{|S|}^{-1}. \\] PARAMETER DESCRIPTION <code>n</code> <p>Size of the index set.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>Size of the subset.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     The logarithm of the weight of a subset of size <code>subset_len</code>.</p> Source code in <code>src/pydvl/valuation/samplers/owen.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    r\"\"\"For each $q_j, j \\in \\{1, ..., N\\}$ in the outer probabilities, the\n    probability of drawing a subset $S_k$ of size $k$ is:\n\n    $$ P (| S_{q_j} | = k) = \\binom{n}{k} \\  q_j^k  (1 - q_j)^{n - k}.$$\n\n    So, if each $q_j$ is chosen with equal weight (or more generally with\n    probability $p_j$),then by total probability, the overall probability of\n    obtaining a subset of size $k$ is a mixture of the binomials:\n    $$\n    P (| S | = k) = \\sum_{j = 1}^N p_j \\ \\binom{n}{k} \\ q_j^k  (1 - q_j)^{n - k}.\n    $$\n\n    In our case $p_j = 1/N$, so that $P(|S|=k) = \\frac{1}{N} \\sum_{j=1}^N P (|\n    S_{q_j} | = k)$. For large enough $N$ this is\n\n    $$\n    P(|S|=k) \\approx \\binom{n}{k} \\int_0^1 q^k (1 - q)^{n - k} \\, dq = \\frac{1}{\n    n+1},\n    $$\n\n    where we computed the integral using the beta function and its expression as\n    products of gamma functions.\n\n    Now, given the symmetry wrt. the indices in the sampling procedure, any given\n    set $S$ of size $k$ is equally likely to be drawn. So the probability of a set\n    being of size $k$ must be equally divided by the number of sets of that size,\n    and the weight of a set of size $k$ is:\n\n    $$ P(S) = \\frac{1}{n+1} \\binom{n}{|S|}^{-1}. $$\n\n    Args:\n        n: Size of the index set.\n        subset_len: Size of the subset.\n    Returns:\n        The logarithm of the weight of a subset of size `subset_len`.\n    \"\"\"\n    m = self.complement_size(n)\n    return float(-logcomb(m, subset_len) - np.log(m + 1))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.AntitheticOwenSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.GridOwenStrategy","title":"GridOwenStrategy","text":"<pre><code>GridOwenStrategy(n_samples_outer: int)\n</code></pre> <p>               Bases: <code>OwenStrategy</code></p> <p>A strategy for OwenSampler to sample  probability values on a linear grid.</p> PARAMETER DESCRIPTION <code>n_samples_outer</code> <p>The number of probability values \\(q\\) used for the outer loop. These will be linearly spaced between 0 and \\(q_     ext{stop}\\).</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/valuation/samplers/owen.py</code> <pre><code>def __init__(self, n_samples_outer: int):\n    super().__init__(n_samples_outer=n_samples_outer)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.OwenSampler","title":"OwenSampler","text":"<pre><code>OwenSampler(\n    outer_sampling_strategy: OwenStrategy,\n    n_samples_inner: int = 2,\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = FiniteSequentialIndexIteration,\n    seed: Seed | None = None,\n)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>PowersetSampler</code></p> <p>A sampler for semi-values using the Owen method.</p> <p>For each index \\(i\\) we sample <code>n_samples_outer</code> probability values \\(q_j\\) between 0 and 1 and then, for each \\(j\\) we draw <code>n_samples_inner</code> subsets of the complement of the current index where each element is sampled probability \\(q_j\\).</p> <p>The distribution for the outer sampling can be either uniform or deterministic. The default is deterministic on a grid, which is the original method described in Okhrati and Lipani (2021)<sup>1</sup>. This can be achieved by using the GridOwenStrategy strategy.</p> <p>Alternatively, the distribution can be uniform between 0 and 1. This can be achieved by using the UniformOwenStrategy strategy.</p> <p>By combining a UniformOwenStrategy with an infinite IndexIteration strategy, this sampler can be used with a stopping criterion to estimate semi-values. This follows more closely the typical usage pattern in PyDVL than the original sampling method described in Okhrati and Lipani (2021)<sup>1</sup>.</p> Example usage <pre><code>sampler = OwenSampler(\n    outer_sampling_strategy=GridOwenStrategy(n_samples_outer=200),\n    n_samples_inner=8,\n    index_iteration=FiniteSequentialIndexIteration,\n)\n</code></pre> PARAMETER DESCRIPTION <code>n_samples_inner</code> <p>The number of samples drawn for each probability. In the original paper this was fixed to 2 for all experiments.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>batch_size</code> <p>The batch size of the sampler.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>index_iteration</code> <p>The index iteration strategy, sequential or random, finite or infinite.</p> <p> TYPE: <code>Type[IndexIteration]</code> DEFAULT: <code>FiniteSequentialIndexIteration</code> </p> <code>seed</code> <p>The seed for the random number generator.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/samplers/owen.py</code> <pre><code>def __init__(\n    self,\n    outer_sampling_strategy: OwenStrategy,\n    n_samples_inner: int = 2,\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = FiniteSequentialIndexIteration,\n    seed: Seed | None = None,\n):\n    super().__init__(\n        batch_size=batch_size, index_iteration=index_iteration, seed=seed\n    )\n    self.n_samples_inner = n_samples_inner\n    self.sampling_probabilities = outer_sampling_strategy\n    self.q_stop = 1.0\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.OwenSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices\n</code></pre> <p>Set of indices to skip in the outer loop.</p>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.OwenSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.OwenSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.OwenSampler.index_iterable","title":"index_iterable","text":"<pre><code>index_iterable(indices: IndexSetT) -&gt; Generator[IndexT | None, None, None]\n</code></pre> <p>Iterates over indices with the method specified at construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def index_iterable(\n    self, indices: IndexSetT\n) -&gt; Generator[IndexT | None, None, None]:\n    \"\"\"Iterates over indices with the method specified at construction.\"\"\"\n    try:\n        iterable = self._index_iterator_cls(indices, seed=self._rng)  # type: ignore\n    except (AttributeError, TypeError):\n        iterable = self._index_iterator_cls(indices)\n    for idx in iterable:\n        if idx not in self.skip_indices:\n            yield idx\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.OwenSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.OwenSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>For each \\(q_j, j \\in \\{1, ..., N\\}\\) in the outer probabilities, the probability of drawing a subset \\(S_k\\) of size \\(k\\) is:</p> \\[ P (| S_{q_j} | = k) = \\binom{n}{k} \\  q_j^k  (1 - q_j)^{n - k}.\\] <p>So, if each \\(q_j\\) is chosen with equal weight (or more generally with probability \\(p_j\\)),then by total probability, the overall probability of obtaining a subset of size \\(k\\) is a mixture of the binomials: $$ P (| S | = k) = \\sum_{j = 1}^N p_j \\ \\binom{n}{k} \\ q_j^k  (1 - q_j)^{n - k}. $$</p> <p>In our case \\(p_j = 1/N\\), so that \\(P(|S|=k) = \\frac{1}{N} \\sum_{j=1}^N P (| S_{q_j} | = k)\\). For large enough \\(N\\) this is</p> \\[ P(|S|=k) \\approx \\binom{n}{k} \\int_0^1 q^k (1 - q)^{n - k} \\, dq = \\frac{1}{ n+1}, \\] <p>where we computed the integral using the beta function and its expression as products of gamma functions.</p> <p>Now, given the symmetry wrt. the indices in the sampling procedure, any given set \\(S\\) of size \\(k\\) is equally likely to be drawn. So the probability of a set being of size \\(k\\) must be equally divided by the number of sets of that size, and the weight of a set of size \\(k\\) is:</p> \\[ P(S) = \\frac{1}{n+1} \\binom{n}{|S|}^{-1}. \\] PARAMETER DESCRIPTION <code>n</code> <p>Size of the index set.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>Size of the subset.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     The logarithm of the weight of a subset of size <code>subset_len</code>.</p> Source code in <code>src/pydvl/valuation/samplers/owen.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    r\"\"\"For each $q_j, j \\in \\{1, ..., N\\}$ in the outer probabilities, the\n    probability of drawing a subset $S_k$ of size $k$ is:\n\n    $$ P (| S_{q_j} | = k) = \\binom{n}{k} \\  q_j^k  (1 - q_j)^{n - k}.$$\n\n    So, if each $q_j$ is chosen with equal weight (or more generally with\n    probability $p_j$),then by total probability, the overall probability of\n    obtaining a subset of size $k$ is a mixture of the binomials:\n    $$\n    P (| S | = k) = \\sum_{j = 1}^N p_j \\ \\binom{n}{k} \\ q_j^k  (1 - q_j)^{n - k}.\n    $$\n\n    In our case $p_j = 1/N$, so that $P(|S|=k) = \\frac{1}{N} \\sum_{j=1}^N P (|\n    S_{q_j} | = k)$. For large enough $N$ this is\n\n    $$\n    P(|S|=k) \\approx \\binom{n}{k} \\int_0^1 q^k (1 - q)^{n - k} \\, dq = \\frac{1}{\n    n+1},\n    $$\n\n    where we computed the integral using the beta function and its expression as\n    products of gamma functions.\n\n    Now, given the symmetry wrt. the indices in the sampling procedure, any given\n    set $S$ of size $k$ is equally likely to be drawn. So the probability of a set\n    being of size $k$ must be equally divided by the number of sets of that size,\n    and the weight of a set of size $k$ is:\n\n    $$ P(S) = \\frac{1}{n+1} \\binom{n}{|S|}^{-1}. $$\n\n    Args:\n        n: Size of the index set.\n        subset_len: Size of the subset.\n    Returns:\n        The logarithm of the weight of a subset of size `subset_len`.\n    \"\"\"\n    m = self.complement_size(n)\n    return float(-logcomb(m, subset_len) - np.log(m + 1))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.OwenSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.OwenSampler.sample_limit","title":"sample_limit","text":"<pre><code>sample_limit(indices: IndexSetT) -&gt; int | None\n</code></pre> <p>The number of samples that will be generated by the sampler.</p> PARAMETER DESCRIPTION <code>indices</code> <p> TYPE: <code>IndexSetT</code> </p> RETURNS DESCRIPTION <code>int | None</code> <p>0 if there are no indices, <code>None</code> if there's no limit and the number of</p> <code>int | None</code> <p>samples otherwise.</p> Source code in <code>src/pydvl/valuation/samplers/owen.py</code> <pre><code>def sample_limit(self, indices: IndexSetT) -&gt; int | None:\n    \"\"\"The number of samples that will be generated by the sampler.\n\n    Args:\n        indices:\n\n    Returns:\n        0 if there are no indices, `None` if there's no limit and the number of\n        samples otherwise.\n    \"\"\"\n    if len(indices) == 0:\n        return 0\n    if not self._index_iterator_cls.is_finite():\n        return None\n\n    return (\n        cast(int, self._index_iterator_cls.length(len(indices)))\n        * self.sampling_probabilities.n_samples_outer\n        * self.n_samples_inner\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.OwenStrategy","title":"OwenStrategy","text":"<pre><code>OwenStrategy(n_samples_outer: int)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for strategies for the Owen sampler to sample probability values.</p> Source code in <code>src/pydvl/valuation/samplers/owen.py</code> <pre><code>def __init__(self, n_samples_outer: int):\n    self.n_samples_outer = n_samples_outer\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/owen/#pydvl.valuation.samplers.owen.UniformOwenStrategy","title":"UniformOwenStrategy","text":"<pre><code>UniformOwenStrategy(n_samples_outer: int, seed: Seed | None = None)\n</code></pre> <p>               Bases: <code>OwenStrategy</code></p> <p>A strategy for OwenSampler to sample probability values uniformly between 0 and \\(q_    ext{stop}\\).</p> PARAMETER DESCRIPTION <code>n_samples_outer</code> <p>The number of probability values \\(q\\) used for the outer loop. Since samples are taken anew for each index, a high number will delay updating new indices and has no effect on the final accuracy if using an infinite index iteration. In general, it only makes sense to change this number if using a finite index iteration.</p> <p> TYPE: <code>int</code> </p> <code>seed</code> <p>The seed for the random number generator.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/samplers/owen.py</code> <pre><code>def __init__(self, n_samples_outer: int, seed: Seed | None = None):\n    super().__init__(n_samples_outer=n_samples_outer)\n    self.rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/","title":"Permutation","text":""},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation","title":"pydvl.valuation.samplers.permutation","text":"<p>Permutation samplers draw permutations \\(\\sigma^N\\) from the index set \\(N\\) uniformly at random and iterate through it returning the sets:</p> \\[S_1 = \\{\\sigma^N_1\\}, S_2 = \\{\\sigma^N_1, \\sigma^N_2\\}, S_3 = \\{\\sigma^N_1,\\sigma^N_2, \\sigma^N_3\\}, ...\\]"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation--the-probability-of-sampling-a-set","title":"The probability of sampling a set","text":"<p>Read on below for the actual formula</p> <p>This section describes the general case. However, because of how the samplers are used to compute marginals, the effective sampling probabilities are slightly different.</p> <p>Let \\(k\\) be the size of a sampled set \\(S\\) and \\(n=|N|\\) the size of the index set. By the product rule:</p> \\[p(S) = p(S \\mid k) \\ p(k).\\] <p>Now, for a uniformly chosen permutation, every subset of size \\(k\\) appears as the prefix (the first \\(k\\) elements) with probability</p> \\[ p (S \\mid k) = \\frac{k! (n - k) !}{n!} = \\binom{n}{k}^{- 1}, \\] <p>and, because we iterate from left to right, each size \\(k\\) is sampled with probability \\(p(k) = 1/n.\\) Hence:</p> \\[ p(S) = \\frac{1}{n} \\binom{n}{k}^{- 1}. \\] <p>The same argument applies to the DeterministicPermutationSampler and PermutationSampler, but also to AntitheticPermutationSampler. For the latter, note that the sampling process is symmetric.</p>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation--the-case-of-a-fixed-index-when-computing-marginals","title":"The case of a fixed index when computing marginals","text":"<p>However, when computing marginal utilities, which is what the samplers are used for, the situation is slightly different since we update indices \\(i\\) sequentially. In Sampling strategies for semi-values a more convoluted argument is made, whereby one splits the permutation by the index \\(i\\). This ends up being similar to what we found above:</p> \\[ p(S) = \\frac{1}{n} \\binom{n-1}{k}^{- 1}, \\] <p>Therefore, log_weight() is valid, when computing marginal utilities.</p>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation--references","title":"References","text":"<ol> <li> <p>Mitchell, Rory, Joshua Cooper, Eibe   Frank, and Geoffrey Holmes. Sampling Permutations for Shapley Value   Estimation. Journal of Machine   Learning Research 23, no. 43 (2022): 1\u201346.\u00a0\u21a9</p> </li> <li> <p>Watson, Lauren, Zeno Kujawa, Rayna Andreeva,   Hao-Tsung Yang, Tariq Elahi, and Rik Sarkar. Accelerated Shapley Value   Approximation for Data Evaluation.   arXiv, 9 November 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.AntitheticPermutationSampler","title":"AntitheticPermutationSampler","text":"<pre><code>AntitheticPermutationSampler(\n    truncation: TruncationPolicy | None = None,\n    seed: Seed | None = None,\n    batch_size: int = 1,\n)\n</code></pre> <p>               Bases: <code>PermutationSampler</code></p> <p>Samples permutations like PermutationSampler, but after each permutation, it returns the same permutation in reverse order.</p> <p>This sampler was suggested in (Mitchell et al. 2022)<sup>1</sup></p> <p>Batching</p> <p>Even though this sampler supports batching, it is not recommended to use it since the PermutationEvaluationStrategy processes whole permutations in one go, effectively batching the computation of up to n-1 marginal utilities in one process.</p> PARAMETER DESCRIPTION <code>truncation</code> <p>A policy to stop the permutation early.</p> <p> TYPE: <code>TruncationPolicy | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Seed for the random number generator.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>The number of samples (full permutations) to generate at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <p>New in version 0.7.1</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def __init__(\n    self,\n    truncation: TruncationPolicy | None = None,\n    seed: Seed | None = None,\n    batch_size: int = 1,\n):\n    super().__init__(seed=seed, truncation=truncation, batch_size=batch_size)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.AntitheticPermutationSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.AntitheticPermutationSampler.complement_size","title":"complement_size","text":"<pre><code>complement_size(n: int) -&gt; int\n</code></pre> <p>Size of the complement of an index wrt. set size <code>n</code>.</p> <p>Required in certain coefficient computations. Even though we are sampling permutations, updates are always done per-index and the size of the complement is always \\(n-1\\).</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def complement_size(self, n: int) -&gt; int:\n    \"\"\"Size of the complement of an index wrt. set size `n`.\n\n    Required in certain coefficient computations. Even though we are sampling\n    permutations, updates are always done per-index and the size of the complement\n    is always $n-1$.\n    \"\"\"\n    return n - 1\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.AntitheticPermutationSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.AntitheticPermutationSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.AntitheticPermutationSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Log probability of sampling a set S from a set of size n-1.</p> <p>See the module's documentation for details.</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    r\"\"\"Log probability of sampling a set S from a set of size **n-1**.\n\n    See [the module's documentation][pydvl.valuation.samplers.permutation] for\n    details.\n    \"\"\"\n    if n &gt; 0:\n        return float(-np.log(n) - logcomb(n - 1, subset_len))\n    return -np.inf\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.AntitheticPermutationSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.DeterministicPermutationSampler","title":"DeterministicPermutationSampler","text":"<pre><code>DeterministicPermutationSampler(\n    *args,\n    truncation: TruncationPolicy | None = None,\n    batch_size: int = 1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>PermutationSamplerBase</code></p> <p>Samples all n! permutations of the indices deterministically.</p> <p>Batching</p> <p>Even though this sampler supports batching, it is not recommended to use it since the PermutationEvaluationStrategy processes whole permutations in one go, effectively batching the computation of up to n-1 marginal utilities in one process.</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    truncation: TruncationPolicy | None = None,\n    batch_size: int = 1,\n    **kwargs,\n):\n    super().__init__(batch_size=batch_size)\n    self.truncation = truncation or NoTruncation()\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.DeterministicPermutationSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices: IndexSetT\n</code></pre> <p>Indices being skipped in the sampler. The exact behaviour will be sampler-dependent, so that setting this property is disabled by default.</p>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.DeterministicPermutationSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.DeterministicPermutationSampler.complement_size","title":"complement_size","text":"<pre><code>complement_size(n: int) -&gt; int\n</code></pre> <p>Size of the complement of an index wrt. set size <code>n</code>.</p> <p>Required in certain coefficient computations. Even though we are sampling permutations, updates are always done per-index and the size of the complement is always \\(n-1\\).</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def complement_size(self, n: int) -&gt; int:\n    \"\"\"Size of the complement of an index wrt. set size `n`.\n\n    Required in certain coefficient computations. Even though we are sampling\n    permutations, updates are always done per-index and the size of the complement\n    is always $n-1$.\n    \"\"\"\n    return n - 1\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.DeterministicPermutationSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.DeterministicPermutationSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.DeterministicPermutationSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Log probability of sampling a set S from a set of size n-1.</p> <p>See the module's documentation for details.</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    r\"\"\"Log probability of sampling a set S from a set of size **n-1**.\n\n    See [the module's documentation][pydvl.valuation.samplers.permutation] for\n    details.\n    \"\"\"\n    if n &gt; 0:\n        return float(-np.log(n) - logcomb(n - 1, subset_len))\n    return -np.inf\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.DeterministicPermutationSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationEvaluationStrategy","title":"PermutationEvaluationStrategy","text":"<pre><code>PermutationEvaluationStrategy(\n    sampler: PermutationSamplerBase,\n    utility: UtilityBase,\n    coefficient: SemivalueCoefficient | None,\n)\n</code></pre> <p>               Bases: <code>EvaluationStrategy[PermutationSamplerBase, ValueUpdate]</code></p> <p>Computes marginal values for permutation sampling schemes.</p> <p>This strategy iterates over permutations from left to right, computing the marginal utility wrt. the previous one at each step to save computation.</p> <p>The TruncationPolicy of the sampler is copied and reset for each permutation in a batch.</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def __init__(\n    self,\n    sampler: PermutationSamplerBase,\n    utility: UtilityBase,\n    coefficient: SemivalueCoefficient | None,\n):\n    super().__init__(utility, coefficient)\n    self.truncation = copy(sampler.truncation)\n    self.truncation.reset(utility)  # Perform initial setup (e.g. total_utility)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSampler","title":"PermutationSampler","text":"<pre><code>PermutationSampler(\n    truncation: TruncationPolicy | None = None,\n    seed: Seed | None = None,\n    batch_size: int = 1,\n)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>PermutationSamplerBase</code></p> <p>Samples permutations of indices.</p> <p>Batching</p> <p>Even though this sampler supports batching, it is not recommended to use it since the PermutationEvaluationStrategy processes whole permutations in one go, effectively batching the computation of up to n-1 marginal utilities in one process.</p> PARAMETER DESCRIPTION <code>truncation</code> <p>A policy to stop the permutation early.</p> <p> TYPE: <code>TruncationPolicy | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Seed for the random number generator.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>The number of samples (full permutations) to generate at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def __init__(\n    self,\n    truncation: TruncationPolicy | None = None,\n    seed: Seed | None = None,\n    batch_size: int = 1,\n):\n    super().__init__(seed=seed, truncation=truncation, batch_size=batch_size)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSampler.complement_size","title":"complement_size","text":"<pre><code>complement_size(n: int) -&gt; int\n</code></pre> <p>Size of the complement of an index wrt. set size <code>n</code>.</p> <p>Required in certain coefficient computations. Even though we are sampling permutations, updates are always done per-index and the size of the complement is always \\(n-1\\).</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def complement_size(self, n: int) -&gt; int:\n    \"\"\"Size of the complement of an index wrt. set size `n`.\n\n    Required in certain coefficient computations. Even though we are sampling\n    permutations, updates are always done per-index and the size of the complement\n    is always $n-1$.\n    \"\"\"\n    return n - 1\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSampler.generate","title":"generate","text":"<pre><code>generate(indices: IndexSetT) -&gt; SampleGenerator\n</code></pre> <p>Generates the permutation samples.</p> PARAMETER DESCRIPTION <code>indices</code> <p>The indices to sample from. If empty, no samples are generated. If skip_indices is set, these indices are removed from the set before generating the permutation.</p> <p> TYPE: <code>IndexSetT</code> </p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def generate(self, indices: IndexSetT) -&gt; SampleGenerator:\n    \"\"\"Generates the permutation samples.\n\n    Args:\n        indices: The indices to sample from. If empty, no samples are generated. If\n            [skip_indices][pydvl.valuation.samplers.base.IndexSampler.skip_indices]\n            is set, these indices are removed from the set before generating the\n            permutation.\n    \"\"\"\n    if len(indices) == 0:\n        return\n    while True:\n        _indices = np.setdiff1d(indices, self.skip_indices)\n        yield Sample(None, self._rng.permutation(_indices))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Log probability of sampling a set S from a set of size n-1.</p> <p>See the module's documentation for details.</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    r\"\"\"Log probability of sampling a set S from a set of size **n-1**.\n\n    See [the module's documentation][pydvl.valuation.samplers.permutation] for\n    details.\n    \"\"\"\n    if n &gt; 0:\n        return float(-np.log(n) - logcomb(n - 1, subset_len))\n    return -np.inf\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSamplerBase","title":"PermutationSamplerBase","text":"<pre><code>PermutationSamplerBase(\n    *args,\n    truncation: TruncationPolicy | None = None,\n    batch_size: int = 1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>IndexSampler</code>, <code>ABC</code></p> <p>Base class for permutation samplers.</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    truncation: TruncationPolicy | None = None,\n    batch_size: int = 1,\n    **kwargs,\n):\n    super().__init__(batch_size=batch_size)\n    self.truncation = truncation or NoTruncation()\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSamplerBase.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices: IndexSetT\n</code></pre> <p>Indices being skipped in the sampler. The exact behaviour will be sampler-dependent, so that setting this property is disabled by default.</p>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSamplerBase.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSamplerBase.complement_size","title":"complement_size","text":"<pre><code>complement_size(n: int) -&gt; int\n</code></pre> <p>Size of the complement of an index wrt. set size <code>n</code>.</p> <p>Required in certain coefficient computations. Even though we are sampling permutations, updates are always done per-index and the size of the complement is always \\(n-1\\).</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def complement_size(self, n: int) -&gt; int:\n    \"\"\"Size of the complement of an index wrt. set size `n`.\n\n    Required in certain coefficient computations. Even though we are sampling\n    permutations, updates are always done per-index and the size of the complement\n    is always $n-1$.\n    \"\"\"\n    return n - 1\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSamplerBase.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(indices: IndexSetT) -&gt; SampleGenerator\n</code></pre> <p>Generates single samples.</p> <p><code>IndexSampler.generate_batches()</code> will batch these samples according to the batch size set upon construction.</p> PARAMETER DESCRIPTION <code>indices</code> <p> TYPE: <code>IndexSetT</code> </p> YIELDS DESCRIPTION <code>SampleGenerator</code> <p>A tuple (idx, subset) for each sample.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>@abstractmethod\ndef generate(self, indices: IndexSetT) -&gt; SampleGenerator:\n    \"\"\"Generates single samples.\n\n    `IndexSampler.generate_batches()` will batch these samples according to the\n    batch size set upon construction.\n\n    Args:\n        indices:\n\n    Yields:\n        A tuple (idx, subset) for each sample.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSamplerBase.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSamplerBase.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSamplerBase.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Log probability of sampling a set S from a set of size n-1.</p> <p>See the module's documentation for details.</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    r\"\"\"Log probability of sampling a set S from a set of size **n-1**.\n\n    See [the module's documentation][pydvl.valuation.samplers.permutation] for\n    details.\n    \"\"\"\n    if n &gt; 0:\n        return float(-np.log(n) - logcomb(n - 1, subset_len))\n    return -np.inf\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSamplerBase.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.PermutationSamplerBase.sample_limit","title":"sample_limit  <code>abstractmethod</code>","text":"<pre><code>sample_limit(indices: IndexSetT) -&gt; int | None\n</code></pre> <p>Number of samples that can be generated from the indices.</p> PARAMETER DESCRIPTION <code>indices</code> <p>The indices used in the sampler.</p> <p> TYPE: <code>IndexSetT</code> </p> RETURNS DESCRIPTION <code>int | None</code> <p>The maximum number of samples that will be generated, or  <code>None</code> if the number of samples is infinite.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>@abstractmethod\ndef sample_limit(self, indices: IndexSetT) -&gt; int | None:\n    \"\"\"Number of samples that can be generated from the indices.\n\n    Args:\n        indices: The indices used in the sampler.\n\n    Returns:\n        The maximum number of samples that will be generated, or  `None` if the\n            number of samples is infinite.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.TruncationPolicy","title":"TruncationPolicy","text":"<pre><code>TruncationPolicy()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A policy for deciding whether to stop computation of a batch of samples</p> <p>Statistics are kept on the total number of calls and truncations as <code>n_calls</code> and <code>n_truncations</code> respectively.</p> ATTRIBUTE DESCRIPTION <code>n_calls</code> <p>Number of calls to the policy.</p> <p> TYPE: <code>int</code> </p> <code>n_truncations</code> <p>Number of truncations made by the policy.</p> <p> TYPE: <code>int</code> </p> <p>Todo</p> <p>Because the policy objects are copied to the workers, the statistics are not accessible from the coordinating process. We need to add methods for this.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.n_calls: int = 0\n    self.n_truncations: int = 0\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.TruncationPolicy.__call__","title":"__call__","text":"<pre><code>__call__(idx: IndexT, score: float, batch_size: int) -&gt; bool\n</code></pre> <p>Check whether the computation should be interrupted.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Position in the batch currently being computed.</p> <p> TYPE: <code>IndexT</code> </p> <code>score</code> <p>Last utility computed.</p> <p> TYPE: <code>float</code> </p> <code>batch_size</code> <p>Size of the batch being computed.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the computation should be interrupted.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>def __call__(self, idx: IndexT, score: float, batch_size: int) -&gt; bool:\n    \"\"\"Check whether the computation should be interrupted.\n\n    Args:\n        idx: Position in the batch currently being computed.\n        score: Last utility computed.\n        batch_size: Size of the batch being computed.\n\n    Returns:\n        `True` if the computation should be interrupted.\n    \"\"\"\n\n    ret = self._check(idx, score, batch_size)\n    self.n_calls += 1\n    self.n_truncations += 1 if ret else 0\n    return ret\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.TruncationPolicy._check","title":"_check  <code>abstractmethod</code>","text":"<pre><code>_check(idx: IndexT, score: float, batch_size: int) -&gt; bool\n</code></pre> <p>Implement the policy.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>@abstractmethod\ndef _check(self, idx: IndexT, score: float, batch_size: int) -&gt; bool:\n    \"\"\"Implement the policy.\"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/permutation/#pydvl.valuation.samplers.permutation.TruncationPolicy.reset","title":"reset  <code>abstractmethod</code>","text":"<pre><code>reset(utility: UtilityBase)\n</code></pre> <p>(Re)set the policy to a state ready for a new permutation.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>@abstractmethod\ndef reset(self, utility: UtilityBase):\n    \"\"\"(Re)set the policy to a state ready for a new permutation.\"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/","title":"Powerset","text":""},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset","title":"pydvl.valuation.samplers.powerset","text":"<p>This module provides the base implementation for powerset samplers.</p> <p>These samplers operate in two loops:</p> <ol> <li>Outer iteration over all indices. This is configurable with subclasses of    IndexIteration. At each step we    fix an index \\(i \\in N\\).</li> <li>Inner iteration over subsets of \\(N_{-i}\\). This step can return one or more subsets,    sampled in different ways: uniformly, with varying probabilities, in tuples of    complementary sets, etc.</li> </ol>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset--index-iteration","title":"Index iteration","text":"<p>The type of iteration over indices \\(i\\) and their complements is configured upon construction of the sampler with the classes SequentialIndexIteration, RandomIndexIteration, or their finite counterparts, when each index must be visited just once (albeit possibly generating many samples per index).</p> <p>However, some valuation schemes require iteration over subsets of the whole set (as opposed to iterating over complements of individual indices). For this purpose, one can use NoIndexIteration or its finite counterpart.</p> <p>See also</p> <p>General information on semi-values and, an explanation of importance sampling in the context of semi-values.</p>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset--references","title":"References","text":"<ol> <li> <p>Mitchell, Rory, Joshua Cooper, Eibe   Frank, and Geoffrey Holmes. Sampling Permutations for Shapley Value   Estimation. Journal of Machine   Learning Research 23, no. 43 (2022): 1\u201346.\u00a0\u21a9</p> </li> <li> <p>Maleki, Sasan, Long Tran-Thanh, Greg Hines,   Talal Rahwan, and Alex Rogers. Bounding the Estimation Error of Sampling-Based   Shapley Value Approximation. arXiv:1306.4265   [Cs], 12 February 2014.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.AntitheticSampler","title":"AntitheticSampler","text":"<pre><code>AntitheticSampler(*args, seed: Seed | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>PowersetSampler</code></p> <p>A sampler that draws samples uniformly, followed by their complements.</p> <p>Works as UniformSampler, but for every tuple \\((i,S)\\), it subsequently returns \\((i,S^c)\\), where \\(S^c\\) is the complement of the set \\(S\\) in the set of indices, excluding \\(i\\).</p> <p>By symmetry, the probability of sampling a set \\(S\\) is the same as the probability of sampling its complement \\(S^c\\), so that \\(p(S)\\) in <code>log_weight</code> is the same as in the PowersetSampler class.</p> Source code in <code>src/pydvl/valuation/samplers/utils.py</code> <pre><code>def __init__(self, *args, seed: Seed | None = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.AntitheticSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices\n</code></pre> <p>Set of indices to skip in the outer loop.</p>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.AntitheticSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.AntitheticSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.AntitheticSampler.index_iterable","title":"index_iterable","text":"<pre><code>index_iterable(indices: IndexSetT) -&gt; Generator[IndexT | None, None, None]\n</code></pre> <p>Iterates over indices with the method specified at construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def index_iterable(\n    self, indices: IndexSetT\n) -&gt; Generator[IndexT | None, None, None]:\n    \"\"\"Iterates over indices with the method specified at construction.\"\"\"\n    try:\n        iterable = self._index_iterator_cls(indices, seed=self._rng)  # type: ignore\n    except (AttributeError, TypeError):\n        iterable = self._index_iterator_cls(indices)\n    for idx in iterable:\n        if idx not in self.skip_indices:\n            yield idx\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.AntitheticSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.AntitheticSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Probability of sampling a set S as a function of total number of indices and  set size.</p> <p>The uniform distribution over the powerset of a set with \\(n\\) elements has mass \\(1/2^{n}\\) over each subset.</p> PARAMETER DESCRIPTION <code>n</code> <p>The size of the index set. Note that the actual size of the set being sampled will often be n-1, as one index might be removed from the set. See IndexIteration for more.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>The size of the subset being sampled</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The natural logarithm of the probability of sampling a set of the given size, when the index set has size <code>n</code>, under the IndexIteration given upon construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    \"\"\"Probability of sampling a set S as a function of total number of indices and\n     set size.\n\n    The uniform distribution over the powerset of a set with $n$ elements has mass\n    $1/2^{n}$ over each subset.\n\n    Args:\n        n: The size of the index set. Note that the actual size of the set being\n            sampled will often be n-1, as one index might be removed from the set.\n            See [IndexIteration][pydvl.valuation.samplers.powerset.IndexIteration]\n            for more.\n        subset_len: The size of the subset being sampled\n\n    Returns:\n        The natural logarithm of the probability of sampling a set of the given\n            size, when the index set has size `n`, under the\n            [IndexIteration][pydvl.valuation.samplers.powerset.IndexIteration] given\n            upon construction.\n\n    \"\"\"\n    m = self.complement_size(n)\n    return float(-m * np.log(2))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.AntitheticSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.DeterministicUniformSampler","title":"DeterministicUniformSampler","text":"<pre><code>DeterministicUniformSampler(\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = FiniteSequentialIndexIteration,\n)\n</code></pre> <p>               Bases: <code>PowersetSampler</code></p> <p>An iterator to perform uniform deterministic sampling of subsets.</p> <p>For every index \\(i\\), each subset of the complement <code>indices - {i}</code> is returned.</p> PARAMETER DESCRIPTION <code>batch_size</code> <p>The number of samples to generate per batch. Batches are processed together by each subprocess when working in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>index_iteration</code> <p>the strategy to use for iterating over indices to update. This iteration can be either finite or infinite.</p> <p> TYPE: <code>Type[IndexIteration]</code> DEFAULT: <code>FiniteSequentialIndexIteration</code> </p> Example <p>The code:</p> <pre><code>from pydvl.valuation.samplers import DeterministicUniformSampler\nimport numpy as np\nsampler = DeterministicUniformSampler()\nfor idx, s in sampler.generate_batches(np.arange(2)):\n    print(f\"{idx} - {s}\", end=\", \")\n</code></pre> <p>Should produce the output:</p> <pre><code>1 - [], 1 - [2], 2 - [], 2 - [1],\n</code></pre> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = FiniteSequentialIndexIteration,\n):\n    super().__init__(batch_size=batch_size, index_iteration=index_iteration)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.DeterministicUniformSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices\n</code></pre> <p>Set of indices to skip in the outer loop.</p>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.DeterministicUniformSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.DeterministicUniformSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.DeterministicUniformSampler.index_iterable","title":"index_iterable","text":"<pre><code>index_iterable(indices: IndexSetT) -&gt; Generator[IndexT | None, None, None]\n</code></pre> <p>Iterates over indices with the method specified at construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def index_iterable(\n    self, indices: IndexSetT\n) -&gt; Generator[IndexT | None, None, None]:\n    \"\"\"Iterates over indices with the method specified at construction.\"\"\"\n    try:\n        iterable = self._index_iterator_cls(indices, seed=self._rng)  # type: ignore\n    except (AttributeError, TypeError):\n        iterable = self._index_iterator_cls(indices)\n    for idx in iterable:\n        if idx not in self.skip_indices:\n            yield idx\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.DeterministicUniformSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.DeterministicUniformSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Probability of sampling a set S as a function of total number of indices and  set size.</p> <p>The uniform distribution over the powerset of a set with \\(n\\) elements has mass \\(1/2^{n}\\) over each subset.</p> PARAMETER DESCRIPTION <code>n</code> <p>The size of the index set. Note that the actual size of the set being sampled will often be n-1, as one index might be removed from the set. See IndexIteration for more.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>The size of the subset being sampled</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The natural logarithm of the probability of sampling a set of the given size, when the index set has size <code>n</code>, under the IndexIteration given upon construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    \"\"\"Probability of sampling a set S as a function of total number of indices and\n     set size.\n\n    The uniform distribution over the powerset of a set with $n$ elements has mass\n    $1/2^{n}$ over each subset.\n\n    Args:\n        n: The size of the index set. Note that the actual size of the set being\n            sampled will often be n-1, as one index might be removed from the set.\n            See [IndexIteration][pydvl.valuation.samplers.powerset.IndexIteration]\n            for more.\n        subset_len: The size of the subset being sampled\n\n    Returns:\n        The natural logarithm of the probability of sampling a set of the given\n            size, when the index set has size `n`, under the\n            [IndexIteration][pydvl.valuation.samplers.powerset.IndexIteration] given\n            upon construction.\n\n    \"\"\"\n    m = self.complement_size(n)\n    return float(-m * np.log(2))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.DeterministicUniformSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.FiniteIterationMixin","title":"FiniteIterationMixin","text":"<p>Careful with MRO when using this and subclassing!</p>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.FiniteNoIndexIteration","title":"FiniteNoIndexIteration","text":"<pre><code>FiniteNoIndexIteration(indices: IndexSetT)\n</code></pre> <p>               Bases: <code>FiniteIterationMixin</code>, <code>NoIndexIteration</code></p> <p>A finite iteration over no indices.</p> <p>The iterator will yield <code>None</code> once and then stop.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(self, indices: IndexSetT):\n    self._indices = indices\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.FiniteNoIndexIteration.length","title":"length  <code>staticmethod</code>","text":"<pre><code>length(n_indices: int) -&gt; int | None\n</code></pre> <p>Returns 1, as the iteration yields exactly one item (<code>None</code>)</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>@staticmethod\ndef length(n_indices: int) -&gt; int | None:\n    \"\"\"Returns 1, as the iteration yields exactly one item (`None`)\"\"\"\n    return 1\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.FiniteRandomIndexIteration","title":"FiniteRandomIndexIteration","text":"<pre><code>FiniteRandomIndexIteration(indices: NDArray[IndexT], seed: Seed)\n</code></pre> <p>               Bases: <code>FiniteIterationMixin</code>, <code>RandomIndexIteration</code></p> <p>Samples indices at random, once</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(self, indices: NDArray[IndexT], seed: Seed):\n    super().__init__(indices, seed=seed)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.FiniteSequentialIndexIteration","title":"FiniteSequentialIndexIteration","text":"<pre><code>FiniteSequentialIndexIteration(indices: IndexSetT)\n</code></pre> <p>               Bases: <code>FiniteIterationMixin</code>, <code>SequentialIndexIteration</code></p> <p>Samples indices sequentially, once.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(self, indices: IndexSetT):\n    self._indices = indices\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.IndexIteration","title":"IndexIteration","text":"<pre><code>IndexIteration(indices: IndexSetT)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>An index iteration defines the way in which the outer loop over indices in a PowersetSampler is done.</p> <p>Iterations can be finite, infinite, at random, etc. Subclasses must implement certain methods to inform the samplers of the size of the complement, the number of iterations, etc.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(self, indices: IndexSetT):\n    self._indices = indices\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.IndexIteration.complement_size","title":"complement_size  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>complement_size(n: int) -&gt; int\n</code></pre> <p>Returns the size of complements of sets of size n, with respect to the indices returned by the iteration.</p> <p>If the iteration returns single indices, then this is n-1, if it returns no indices, then it is n. If it returned tuples, then n-2, etc. Args:     n: The size of the index set.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef complement_size(n: int) -&gt; int:\n    \"\"\"Returns the size of complements of sets of size n, with respect to the\n    indices returned by the iteration.\n\n    If the iteration returns single indices, then this is n-1, if it returns no\n    indices, then it is n. If it returned tuples, then n-2, etc.\n    Args:\n        n: The size of the index set.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.IndexIteration.length","title":"length  <code>abstractmethod</code> <code>staticmethod</code>","text":"<pre><code>length(n_indices: int) -&gt; int | None\n</code></pre> <p>Returns the length of the iteration over the index set</p> PARAMETER DESCRIPTION <code>n_indices</code> <p>The size of the index set.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>int | None</code> <p>The length of the iteration. It can be: - a non-negative integer, if the iteration is finite - <code>None</code> if the iteration never ends.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef length(n_indices: int) -&gt; int | None:\n    \"\"\"Returns the length of the iteration over the index set\n\n    Args:\n        n_indices: The size of the index set.\n\n    Returns:\n        The length of the iteration. It can be:\n            - a non-negative integer, if the iteration is finite\n            - `None` if the iteration never ends.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.InfiniteIterationMixin","title":"InfiniteIterationMixin","text":"<p>Careful with MRO when using this and subclassing!</p>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.LOOEvaluationStrategy","title":"LOOEvaluationStrategy","text":"<pre><code>LOOEvaluationStrategy(\n    utility: UtilityBase, coefficient: SemivalueCoefficient | None\n)\n</code></pre> <p>               Bases: <code>PowersetEvaluationStrategy[LOOSampler]</code></p> <p>Computes marginal values for LOO.</p> <p>Upon construction, the total utility is computed once. Then, the utility for every sample processed in [process()] is subtracted from it and returned as value update.</p> PARAMETER DESCRIPTION <code>utility</code> <p>The utility function to use.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>coefficient</code> <p>The coefficient to use. If <code>None</code>, the correction of importance sampling is disabled.</p> <p> TYPE: <code>SemivalueCoefficient | None</code> </p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    coefficient: SemivalueCoefficient | None,\n):\n    super().__init__(utility, coefficient)\n    assert utility.training_data is not None\n    self.total_utility = utility(Sample(None, utility.training_data.indices))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.LOOSampler","title":"LOOSampler","text":"<pre><code>LOOSampler(\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = FiniteSequentialIndexIteration,\n    seed: Seed | None = None,\n)\n</code></pre> <p>               Bases: <code>PowersetSampler</code></p> <p>Leave-One-Out sampler.</p> <p>In this special case of a powerset sampler, for every index \\(i\\) in the set \\(S\\), the sample \\((i, S_{-i})\\) is returned.</p> PARAMETER DESCRIPTION <code>batch_size</code> <p>The number of samples to generate per batch. Batches are processed together by each subprocess when working in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>index_iteration</code> <p>the strategy to use for iterating over indices to update. By default, a finite sequential index iteration is used, which is what LOOValuation expects.</p> <p> TYPE: <code>Type[IndexIteration]</code> DEFAULT: <code>FiniteSequentialIndexIteration</code> </p> <code>seed</code> <p>The seed for the random number generator used in case the index iteration is random.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <p>New in version 0.10.0</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = FiniteSequentialIndexIteration,\n    seed: Seed | None = None,\n):\n    super().__init__(batch_size, index_iteration)\n    if not self._index_iterator_cls.is_proper():\n        raise ValueError(\"LOO samplers require a proper index iteration strategy\")\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.LOOSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices\n</code></pre> <p>Set of indices to skip in the outer loop.</p>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.LOOSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.LOOSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.LOOSampler.index_iterable","title":"index_iterable","text":"<pre><code>index_iterable(indices: IndexSetT) -&gt; Generator[IndexT | None, None, None]\n</code></pre> <p>Iterates over indices with the method specified at construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def index_iterable(\n    self, indices: IndexSetT\n) -&gt; Generator[IndexT | None, None, None]:\n    \"\"\"Iterates over indices with the method specified at construction.\"\"\"\n    try:\n        iterable = self._index_iterator_cls(indices, seed=self._rng)  # type: ignore\n    except (AttributeError, TypeError):\n        iterable = self._index_iterator_cls(indices)\n    for idx in iterable:\n        if idx not in self.skip_indices:\n            yield idx\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.LOOSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.LOOSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>This sampler returns only sets of size n-1. There are n such sets, so the probability of drawing one is 1/n, or 0 if subset_len != n-1.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    \"\"\"This sampler returns only sets of size n-1. There are n such sets, so the\n    probability of drawing one is 1/n, or 0 if subset_len != n-1.\"\"\"\n    return float(-np.log(n) if subset_len == n - 1 else -np.inf)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.LOOSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.NoIndexIteration","title":"NoIndexIteration","text":"<pre><code>NoIndexIteration(indices: IndexSetT)\n</code></pre> <p>               Bases: <code>InfiniteIterationMixin</code>, <code>IndexIteration</code></p> <p>An infinite iteration over no indices.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(self, indices: IndexSetT):\n    self._indices = indices\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetEvaluationStrategy","title":"PowersetEvaluationStrategy","text":"<pre><code>PowersetEvaluationStrategy(\n    utility: UtilityBase, log_coefficient: SemivalueCoefficient | None\n)\n</code></pre> <p>               Bases: <code>Generic[PowersetSamplerT]</code>, <code>EvaluationStrategy[PowersetSamplerT, ValueUpdate]</code></p> <p>The standard strategy for evaluating the utility of subsets of a set.</p> <p>This strategy computes the marginal value of each subset of the complement of an index in the training set. The marginal value is the difference between the utility of the subset and the utility of the subset with the index added back in.</p> <p>It is the standard strategy for the direct implementation of semi-values, when sampling is done over the powerset of the complement of an index.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    log_coefficient: SemivalueCoefficient | None,\n):\n    self.utility = utility\n    # Used by the decorator suppress_warnings:\n    self.show_warnings = getattr(utility, \"show_warnings\", False)\n    self.n_indices = (\n        len(utility.training_data) if utility.training_data is not None else 0\n    )\n    if log_coefficient is not None:\n        self.valuation_coefficient = log_coefficient\n    else:\n        # Allow method implementations to disable importance sampling by setting\n        # the log_coefficient to None.\n        # Note that being in logspace, we are adding and subtracting 0s, so that\n        # this is not a problem.\n        self.valuation_coefficient = lambda n, subset_len: 0.0\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetSampler","title":"PowersetSampler","text":"<pre><code>PowersetSampler(\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = SequentialIndexIteration,\n)\n</code></pre> <p>               Bases: <code>IndexSampler</code>, <code>ABC</code></p> <p>An abstract class for samplers which iterate over the powerset of the complement of an index in the training set.</p> <p>This is done in two nested loops, where the outer loop iterates over the set of indices, and the inner loop iterates over subsets of the complement of the current index. The outer iteration can be either sequential or at random. Args:     batch_size: The number of samples to generate per batch. Batches are         processed together by <code>process()</code> in the evaluation strategy         PowersetEvaluationStrategy.     index_iteration: the strategy to use for iterating over indices to update.</p> PARAMETER DESCRIPTION <code>batch_size</code> <p>The number of samples to generate per batch. Batches are processed together by EvaluationStrategy.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>index_iteration</code> <p>the strategy to use for iterating over indices to update</p> <p> TYPE: <code>Type[IndexIteration]</code> DEFAULT: <code>SequentialIndexIteration</code> </p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = SequentialIndexIteration,\n):\n    \"\"\"\n    Args:\n        batch_size: The number of samples to generate per batch. Batches are\n            processed together by\n            [EvaluationStrategy][pydvl.valuation.samplers.base.EvaluationStrategy].\n        index_iteration: the strategy to use for iterating over indices to update\n    \"\"\"\n    super().__init__(batch_size)\n    self._index_iterator_cls = index_iteration\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices\n</code></pre> <p>Set of indices to skip in the outer loop.</p>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetSampler.generate","title":"generate  <code>abstractmethod</code>","text":"<pre><code>generate(indices: IndexSetT) -&gt; SampleGenerator\n</code></pre> <p>Generates samples over the powerset of <code>indices</code></p> <p>Each <code>PowersetSampler</code> defines its own way to generate the subsets by implementing this method. The outer loop is handled by the index_iterable() method. Batching is handled by the generate_batches() method.</p> PARAMETER DESCRIPTION <code>indices</code> <p>The set from which to generate samples.</p> <p> TYPE: <code>IndexSetT</code> </p> <p>Returns:     A generator that yields samples over the powerset of <code>indices</code>.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>@abstractmethod\ndef generate(self, indices: IndexSetT) -&gt; SampleGenerator:\n    \"\"\"Generates samples over the powerset of `indices`\n\n    Each `PowersetSampler` defines its own way to generate the subsets by\n    implementing this method. The outer loop is handled by the\n    [index_iterable()][pydvl.valuation.samplers.powerset.PowersetSampler.index_iterable]\n    method. Batching is handled by the\n    [generate_batches()][pydvl.valuation.samplers.base.IndexSampler.generate_batches]\n    method.\n\n    Args:\n        indices: The set from which to generate samples.\n    Returns:\n        A generator that yields samples over the powerset of `indices`.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetSampler.index_iterable","title":"index_iterable","text":"<pre><code>index_iterable(indices: IndexSetT) -&gt; Generator[IndexT | None, None, None]\n</code></pre> <p>Iterates over indices with the method specified at construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def index_iterable(\n    self, indices: IndexSetT\n) -&gt; Generator[IndexT | None, None, None]:\n    \"\"\"Iterates over indices with the method specified at construction.\"\"\"\n    try:\n        iterable = self._index_iterator_cls(indices, seed=self._rng)  # type: ignore\n    except (AttributeError, TypeError):\n        iterable = self._index_iterator_cls(indices)\n    for idx in iterable:\n        if idx not in self.skip_indices:\n            yield idx\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Probability of sampling a set S as a function of total number of indices and  set size.</p> <p>The uniform distribution over the powerset of a set with \\(n\\) elements has mass \\(1/2^{n}\\) over each subset.</p> PARAMETER DESCRIPTION <code>n</code> <p>The size of the index set. Note that the actual size of the set being sampled will often be n-1, as one index might be removed from the set. See IndexIteration for more.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>The size of the subset being sampled</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The natural logarithm of the probability of sampling a set of the given size, when the index set has size <code>n</code>, under the IndexIteration given upon construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    \"\"\"Probability of sampling a set S as a function of total number of indices and\n     set size.\n\n    The uniform distribution over the powerset of a set with $n$ elements has mass\n    $1/2^{n}$ over each subset.\n\n    Args:\n        n: The size of the index set. Note that the actual size of the set being\n            sampled will often be n-1, as one index might be removed from the set.\n            See [IndexIteration][pydvl.valuation.samplers.powerset.IndexIteration]\n            for more.\n        subset_len: The size of the subset being sampled\n\n    Returns:\n        The natural logarithm of the probability of sampling a set of the given\n            size, when the index set has size `n`, under the\n            [IndexIteration][pydvl.valuation.samplers.powerset.IndexIteration] given\n            upon construction.\n\n    \"\"\"\n    m = self.complement_size(n)\n    return float(-m * np.log(2))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.PowersetSampler.sample_limit","title":"sample_limit  <code>abstractmethod</code>","text":"<pre><code>sample_limit(indices: IndexSetT) -&gt; int | None\n</code></pre> <p>Returns the number of samples that can be generated from the index set.</p> <p>This will depend, among other things, on the type of IndexIteration.</p> PARAMETER DESCRIPTION <code>indices</code> <p>The set of indices to sample from.</p> <p> TYPE: <code>IndexSetT</code> </p> <p>Returns:     The number of samples that can be generated from the index set.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>@abstractmethod\ndef sample_limit(self, indices: IndexSetT) -&gt; int | None:\n    \"\"\"Returns the number of samples that can be generated from the index set.\n\n    This will depend, among other things, on the type of\n    [IndexIteration][pydvl.valuation.samplers.powerset.IndexIteration].\n\n    Args:\n        indices: The set of indices to sample from.\n    Returns:\n        The number of samples that can be generated from the index set.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.RandomIndexIteration","title":"RandomIndexIteration","text":"<pre><code>RandomIndexIteration(indices: NDArray[IndexT], seed: Seed)\n</code></pre> <p>               Bases: <code>InfiniteIterationMixin</code>, <code>StochasticSamplerMixin</code>, <code>IndexIteration</code></p> <p>Samples indices at random, indefinitely</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(self, indices: NDArray[IndexT], seed: Seed):\n    super().__init__(indices, seed=seed)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.SequentialIndexIteration","title":"SequentialIndexIteration","text":"<pre><code>SequentialIndexIteration(indices: IndexSetT)\n</code></pre> <p>               Bases: <code>InfiniteIterationMixin</code>, <code>IndexIteration</code></p> <p>Samples indices sequentially, indefinitely.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(self, indices: IndexSetT):\n    self._indices = indices\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.UniformSampler","title":"UniformSampler","text":"<pre><code>UniformSampler(\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = SequentialIndexIteration,\n    seed: Seed | None = None,\n)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>PowersetSampler</code></p> <p>Draws random samples uniformly from the powerset of the index set.</p> <p>Iterating over every index \\(i\\), either in sequence or at random depending on the value of <code>index_iteration</code>, one subset of the complement <code>indices - {i}</code> is sampled with equal probability \\(2^{n-1}\\).</p> PARAMETER DESCRIPTION <code>batch_size</code> <p>The number of samples to generate per batch. Batches are processed together by each subprocess when working in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>index_iteration</code> <p>the strategy to use for iterating over indices to update.</p> <p> TYPE: <code>Type[IndexIteration]</code> DEFAULT: <code>SequentialIndexIteration</code> </p> <code>seed</code> <p>The seed for the random number generator.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Example <p>The code <pre><code>for idx, s in UniformSampler(np.arange(3)):\n   print(f\"{idx} - {s}\", end=\", \")\n</code></pre> Produces the output: <pre><code>0 - [1 4], 1 - [2 3], 2 - [0 1 3], 3 - [], 4 - [2], 0 - [1 3 4], 1 - [0 2]\n(...)\n</code></pre></p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = SequentialIndexIteration,\n    seed: Seed | None = None,\n):\n    super().__init__(\n        batch_size=batch_size, index_iteration=index_iteration, seed=seed\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.UniformSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices\n</code></pre> <p>Set of indices to skip in the outer loop.</p>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.UniformSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.UniformSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.UniformSampler.index_iterable","title":"index_iterable","text":"<pre><code>index_iterable(indices: IndexSetT) -&gt; Generator[IndexT | None, None, None]\n</code></pre> <p>Iterates over indices with the method specified at construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def index_iterable(\n    self, indices: IndexSetT\n) -&gt; Generator[IndexT | None, None, None]:\n    \"\"\"Iterates over indices with the method specified at construction.\"\"\"\n    try:\n        iterable = self._index_iterator_cls(indices, seed=self._rng)  # type: ignore\n    except (AttributeError, TypeError):\n        iterable = self._index_iterator_cls(indices)\n    for idx in iterable:\n        if idx not in self.skip_indices:\n            yield idx\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.UniformSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.UniformSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Probability of sampling a set S as a function of total number of indices and  set size.</p> <p>The uniform distribution over the powerset of a set with \\(n\\) elements has mass \\(1/2^{n}\\) over each subset.</p> PARAMETER DESCRIPTION <code>n</code> <p>The size of the index set. Note that the actual size of the set being sampled will often be n-1, as one index might be removed from the set. See IndexIteration for more.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>The size of the subset being sampled</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The natural logarithm of the probability of sampling a set of the given size, when the index set has size <code>n</code>, under the IndexIteration given upon construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    \"\"\"Probability of sampling a set S as a function of total number of indices and\n     set size.\n\n    The uniform distribution over the powerset of a set with $n$ elements has mass\n    $1/2^{n}$ over each subset.\n\n    Args:\n        n: The size of the index set. Note that the actual size of the set being\n            sampled will often be n-1, as one index might be removed from the set.\n            See [IndexIteration][pydvl.valuation.samplers.powerset.IndexIteration]\n            for more.\n        subset_len: The size of the subset being sampled\n\n    Returns:\n        The natural logarithm of the probability of sampling a set of the given\n            size, when the index set has size `n`, under the\n            [IndexIteration][pydvl.valuation.samplers.powerset.IndexIteration] given\n            upon construction.\n\n    \"\"\"\n    m = self.complement_size(n)\n    return float(-m * np.log(2))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/powerset/#pydvl.valuation.samplers.powerset.UniformSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/","title":"Stratified","text":""},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified","title":"pydvl.valuation.samplers.stratified","text":"<p>This module implements stratified samplers.</p> <p>Stratified samplers change the subset sampling distribution to be a function of set size with the goal of reducing the variance of the Monte Carlo estimate of the marginal utility. They key assumption / heuristic is that the utility's variance is a function of the training set size.</p> <p>Stratified sampling was introduced at least as early as Maleki et al. (2014)<sup>3</sup>. Later on, Wu et al. 2023<sup>2</sup>, extended these heuristics and proposed some for ML tasks, which they called VRDS (see below). Watson et al. (2023)<sup>1</sup> used error estimates for certain model classes to propose a different heuristic. See below and \\(\\delta\\)-Shapley.</p> <p>All stratified samplers in pyDVL are implemented by configuring (or subclassing) the classes StratifiedSampler and StratifiedPermutationSampler.</p> <p>In the simplest case, StratifiedSampler employs some strategy with a fixed number of samples \\(m_k\\) for each set size \\(k \\in [0, n],\\) where \\(n\\) is the total number of indices in the index set \\(N.\\) It iterates through all indices exactly (e.g. exactly once, if using FiniteSequentialIndexIteration) and for each index \\(i \\in N\\), iterates through all set sizes \\(k\\), then samples exactly \\(m_k\\) subsets \\(S \\subset N_{-i}\\) of size \\(k\\). The iteration over set sizes is configured with SampleSizeIteration.</p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified--choosing-set-size-heuristics","title":"Choosing set size heuristics","text":"<p>Optimal sampling (leading to minimal variance estimators) involves a dynamic choice of the number \\(m_k\\) of samples at size \\(k\\) based on the variance of the Monte Carlo integrand, but Wu et al. (2023)<sup>2</sup> show that there exist methods applicable to semi-values which precompute these sizes while still providing reasonable performance.</p> The number of sets of size \\(k\\) <p>Recall that uniform sampling from the powerset \\(2^{N_{-i}}\\) produces a binomial distribution of set sizes: the number of sets of size \\(k\\) is \\(m_k = \\binom{n-1}{k},\\) which is the (inverse of the) Shapley coefficient. Therefore, setting for instance \\(m_k = C\\) for some constant will drastically reduce the number of sets of size \\(\\sim n/2\\) while increasing the number of sets of size 1 or \\(n-1.\\) This will then have stark implications on the Monte Carlo estimate of semi-values, depending on how the marginal utility (i.e. the training of the model) is affected by the size of the training set.</p> <p>This heuristic is configured with the argument <code>sample_size</code> of StratifiedSampler, which is an instance of SampleSizeStrategy.</p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified--variance-reduced-stratified-sampler-vrds","title":"Variance Reduced Stratified Sampler (VRDS)","text":"<p>It is known (Wu et al. (2023), Theorem 4.2)<sup>2</sup> that a minimum variance estimator of Shapley values samples \\(m_k\\) sets of size \\(k\\) based on the variance of the marginal utility at that set size. However, this quantity is unknown in practice, so the authors propose a simple deterministic heuristic, which in particular does not depend on run-time variance estimates, as an adaptive method might do. Section 4 of Wu et al. (2023)<sup>2</sup> shows a good default choice is based on the harmonic function of the set size \\(k\\).</p> <p>This sampler is available through VRDSSampler.</p> Constructing a VRDS <p>It is possible to \"manually\" replicate VRDSSampler with:</p> <pre><code>n_samples_per_index = 1000  # Total number of samples is: n_indices times this\nsampler = StratifiedSampler(\n    sample_sizes=HarmonicSampleSize(n_samples=1000),\n    sample_sizes_iteration=FiniteSequentialSizeIteration,\n    index_iteration=FiniteSequentialIndexIteration,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified--iterating-over-indices-and-its-effect-on-n_samples","title":"Iterating over indices and its effect on 'n_samples'","text":"<p>As any other sampler, StratifiedSampler can iterate over indices finitely or infinitely many times. It can also use NoIndexIteration to sample from the whole powerset. This is configured with the parameter <code>index_iteration</code>.</p> <p>In the case of finite iterations, the sampler must distribute a finite total number of samples among all indices. This is done by the SampleSizeStrategy, which therefore requires an argument <code>n_samples</code> to be set to the number of samples per index.</p> <p>Warning</p> <p>On the other hand, if the sampler iterates over the indices indefinitely, <code>n_indices</code> can be set, but only relative frequencies will matter. As we see next, there is another component that will affect how the sampler behaves.</p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified--iterating-over-set-sizes","title":"Iterating over set sizes","text":"<p>Additionally, StratifiedSampler must iterate over sample sizes \\(k \\in [0, n]\\), and this can be done in multiple ways, configured via subclasses of SampleSizeIteration.</p> <ul> <li>FiniteSequentialSizeIteration   will generate exactly \\(m_k\\) samples for each \\(k\\) before moving to the next \\(k.\\) This   implies that <code>n_samples</code> must be large enough for the computed \\(m_k\\) to be valid.   Alternatively, and preferably, some strategies allow <code>n_samples = None</code> to signal them   to compute the total number of samples.</li> <li>RandomSizeIteration will   sample a set size \\(k\\) according to the distribution of sizes given by the strategy.   When using this in conjunction with an infinite index iteration for the sampler,   <code>n_samples</code> can be safely left to its default <code>None</code> since \\(m_k\\) will be interpreted   as a probability.</li> <li>RoundRobinSizeIteration will   iterate over set sizes \\(k\\) and generate one sample each time, until reaching \\(m_k.\\)</li> </ul>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified--other-known-strategies","title":"Other known strategies","text":"<p>All components described above can be mixed in most ways, but some specific configurations besides VRDS appear in the literature as follows:</p> <ul> <li>Sample sizes given by stability bounds related to the algorithm, to ensure good   approximation of per-set-size marginal utilities. This sampling method was introduced   by Watson et al. (2023)<sup>1</sup> for the computation of Shapley values as   \\(\\delta\\)-Shapley.</li> </ul> <p>DeltaShapleyNSGDSampleSize   implements the choice of \\(m_k\\) corresponding to non-convex losses minimized with SGD.   Alas, it requires many parameters to be set which are hard to estimate in practice. An   alternative is to use   PowerLawSampleSize (see   below) with an exponent of -2, which is the order of \\(m_k\\) in \\(\\delta\\)-Shapley.</p> <pre><code>??? Example \"Constructing an alternative sampler for DeltaShapley\"\n    ```python\n    config = DeltaShapleyNCSGDConfig(...)  # See the docs / paper\n    sampler = StratifiedSampler(\n        sample_sizes=DeltaShapleyNSGDSampleSize(config, lower_bound, upper_bound),\n        sample_sizes_iteration=FiniteSequentialSizeIteration,\n        index_iteration=SequentialIndexIteration,\n        )\n    ```\n!!! Note \"Other bounds\"\n    Implementing the other bounds from the paper is just a matter of subclassing\n    [SampleSizeStrategy][pydvl.valuation.samplers.stratified.SampleSizeStrategy] and\n    implementing the  `fun` method, as done in\n    [DeltaShapleyNCSGDSampleSize][pydvl.valuation.samplers.stratified.DeltaShapleyNCSGDSampleSize].\n</code></pre> <ul> <li> <p>Sample sizes decreasing with a power law. Use   PowerLawSampleSize for the   strategy. This was also proposed in Wu et al. (2023)<sup>2</sup>. Empirically they found   an exponent between -1 and -0.5 to perform well.</p> Power law heuristic <pre><code>sampler = StratifiedSampler(\n      sample_sizes=PowerLawSampleSize(exponent=-0.5),\n      sample_sizes_iteration=RandomSizeIteration,\n      index_iteration=RandomIndexIteration,\n      )\n</code></pre> </li> <li> <p>Group Testing Sample Size. This heuristic is used for the stratified sampling   required by   GroupTestingShapleyValuation.</p> </li> </ul>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified--references","title":"References","text":"<ol> <li> <p>Watson, Lauren, Zeno Kujawa, Rayna Andreeva,   Hao-Tsung Yang, Tariq Elahi, and Rik Sarkar. Accelerated Shapley Value   Approximation for Data Evaluation.   arXiv, 9 November 2023.\u00a0\u21a9\u21a9</p> </li> <li> <p>Wu, Mengmeng, Ruoxi Jia, Changle Lin, Wei Huang,   and Xiangyu Chang. Variance Reduced Shapley Value Estimation for Trustworthy Data   Valuation. Computers &amp; Operations   Research 159 (1 November 2023): 106305.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Maleki, Sasan, Long Tran-Thanh, Greg Hines,   Talal Rahwan, and Alex Rogers. Bounding the Estimation Error of Sampling-Based   Shapley Value Approximation. arXiv:1306.4265   [Cs], 12 February 2014.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.ConstantSampleSize","title":"ConstantSampleSize","text":"<pre><code>ConstantSampleSize(\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n)\n</code></pre> <p>               Bases: <code>SampleSizeStrategy</code></p> <p>Use a constant number of samples for each set size.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>Number of samples for the stratified sampler to generate. It can be <code>None</code>, 0 or a positive integer. See the documentation of  SampleSizeStrategy  for details.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>lower_bound</code> <p>Lower bound for the set sizes.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>upper_bound</code> <p>Upper bound for the set sizes.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self,\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n):\n    if n_samples is not None:\n        n_samples = validate_number(\"n_samples\", n_samples, int, lower=0)\n    if lower_bound is not None:\n        lower_bound = validate_number(\"lower_bound\", lower_bound, int, lower=0)\n    if upper_bound is not None:\n        upper_bound = validate_number(\n            \"upper_bound\", upper_bound, int, lower=lower_bound\n        )\n    self._n_samples_per_index = n_samples\n    self.lower_bound = lower_bound\n    self.upper_bound = upper_bound\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.ConstantSampleSize.effective_bounds","title":"effective_bounds","text":"<pre><code>effective_bounds(n: int) -&gt; tuple[int, int]\n</code></pre> <p>Returns the effective bounds for the sample sizes, given the number of indices <code>n</code> from which sets are sampled.</p> <p>Note</p> <p>The number of indices <code>n</code> will typically be <code>complement_size(len(train))</code>, i.e. what we sometimes denote as <code>effective_n</code>.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of indices from which subsets are drawn.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     A tuple of [lower, upper] bounds for sample sizes (inclusive).</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def effective_bounds(self, n: int) -&gt; tuple[int, int]:\n    \"\"\"Returns the effective bounds for the sample sizes, given the number of\n    indices `n` from which sets are sampled.\n\n    !!! note\n        The number of indices `n` will typically be `complement_size(len(train))`,\n        i.e. what we sometimes denote as `effective_n`.\n\n    Args:\n        n: The number of indices from which subsets are drawn.\n    Returns:\n        A tuple of [lower, upper] bounds for sample sizes (inclusive).\n    \"\"\"\n    lower = 0 if self.lower_bound is None else self.lower_bound\n    upper = n if self.upper_bound is None else self.upper_bound\n    lower = min(lower, n)\n    upper = min(upper, n)\n\n    return lower, upper\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.ConstantSampleSize.n_samples_per_index","title":"n_samples_per_index","text":"<pre><code>n_samples_per_index(n_indices: int) -&gt; int | None\n</code></pre> <p>Returns the total number of samples to take for the given number of indices, or <code>None</code> if no limit was set and samples are generated with probabilities.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def n_samples_per_index(self, n_indices: int) -&gt; int | None:\n    \"\"\"Returns the total number of samples to take for the given number of indices,\n    or `None` if no limit was set and samples are generated with probabilities.\"\"\"\n    if self._n_samples_per_index is None:\n        return None\n    sizes = self.sample_sizes(n_indices, probs=False)\n    return int(np.sum(sizes).item())\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.ConstantSampleSize.sample_sizes","title":"sample_sizes  <code>cached</code>","text":"<pre><code>sample_sizes(\n    n_indices: int, probs: bool = True\n) -&gt; NDArray[int64] | NDArray[float64]\n</code></pre> <p>Precomputes the number of samples to take for each set size, from 0 up to <code>n_indices</code> inclusive.</p> <p>If <code>probs</code> is <code>True</code>, the result is a vector of floats, where each element is the probability of sampling a set of size \\(k.\\) This is useful e.g. for RandomSizeIteration where one needs frequencies. In this case <code>self.n_samples_per_index</code> can be <code>None</code>.</p> <p>If <code>probs</code> is <code>False</code>, the result is a vector of integers, where each element \\(k\\) is the number of samples to take for set size \\(k.\\) The sum of all elements will depend on the value of <code>n_samples</code> upon construction. It will be</p> <ul> <li>equal to <code>n_samples</code> if <code>n_samples &gt; 0</code>,</li> <li>the sum of the values of <code>fun</code> for all set sizes if <code>n_samples == 0</code>,</li> <li>the sum after a normalization such that the smallest non-zero value is 1, if   <code>n_samples == None</code>.</li> </ul> <p>This somewhat complex behavior is necessary to ensure that the total number of samples is respected when provided either globally upon construction, or implicitly by the sum of the values of <code>fun</code> when the strategy computes absolute numbers, but also when the strategy has a <code>fun</code> that returns frequencies. The special handling of the case <code>n_samples == None</code> is used implicitly by StratifiedPermutationSampler (yeah, it's not pretty).</p> <p>When <code>probs</code> is <code>False</code>, this method corrects rounding errors taking into account the fractional parts so that the total number of samples is respected, while allocating remainders in a way that follows the relative sizes of the fractional parts.</p> PARAMETER DESCRIPTION <code>n_indices</code> <p>number of indices in the index set from which to sample. This is typically <code>len(dataset) - 1</code> with the usual index iterations.</p> <p> TYPE: <code>int</code> </p> <code>probs</code> <p>Whether to perform the remainder distribution. If <code>True</code>, sampling probabilities are returned. If <code>False</code>, then <code>n_samples_per_index</code> is used to compute the actual number of samples and the values are rounded down to the nearest integer, and the remainder is distributed to maintain the relative frequencies.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Returns:     The exact (integer) number of samples to take for each set size, if     <code>probs</code> is <code>False</code>. Otherwise, the fractional number of samples.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>@cache\ndef sample_sizes(\n    self, n_indices: int, probs: bool = True\n) -&gt; NDArray[np.int64] | NDArray[np.float64]:\n    \"\"\"Precomputes the number of samples to take for each set size, from 0 up to\n    `n_indices` inclusive.\n\n    If `probs` is `True`, the result is a vector of floats, where each element\n    is the probability of sampling a set of size $k.$ This is useful e.g. for\n    [RandomSizeIteration][pydvl.valuation.samplers.stratified.RandomSizeIteration]\n    where one needs frequencies. In this case `self.n_samples_per_index` can be\n    `None`.\n\n    If `probs` is `False`, the result is a vector of integers, where each element\n    $k$ is the number of samples to take for set size $k.$ The sum of all elements\n    will depend on the value of `n_samples` upon construction. It will be\n\n    * equal to `n_samples` if `n_samples &gt; 0`,\n    * the sum of the values of `fun` for all set sizes if `n_samples == 0`,\n    * the sum after a normalization such that the smallest non-zero value is 1, if\n      `n_samples == None`.\n\n    This somewhat complex behavior is necessary to ensure that the total number of\n    samples is respected when provided either globally upon construction, or\n    implicitly by the sum of the values of `fun` when the strategy computes absolute\n    numbers, but also when the strategy has a `fun` that returns frequencies. The\n    special handling of the case `n_samples == None` is used implicitly by\n    [StratifiedPermutationSampler][pydvl.valuation.samplers.stratified.StratifiedPermutationSampler]\n    (yeah, it's not pretty).\n\n    When `probs` is `False`, this method corrects rounding errors taking into\n    account the fractional parts so that the total number of samples is respected,\n    while allocating remainders in a way that follows the relative sizes of the\n    fractional parts.\n\n    Args:\n        n_indices: number of indices in the index set from which to sample. This is\n            typically `len(dataset) - 1` with the usual index iterations.\n        probs: Whether to perform the remainder distribution. If `True`, sampling\n            probabilities are returned. If `False`, then `n_samples_per_index` is\n            used to compute the actual number of samples and the values are rounded\n            down to the nearest integer, and the remainder is distributed to\n            maintain the relative frequencies.\n    Returns:\n        The exact (integer) number of samples to take for each set size, if\n        `probs` is `False`. Otherwise, the fractional number of samples.\n    \"\"\"\n\n    # m_k = m * f(k) / sum_j f(j)\n    values = np.zeros(n_indices + 1, dtype=float)\n    s = 0.0\n    lb, ub = self.effective_bounds(n_indices)\n\n    for k in range(lb, ub + 1):\n        val = self.fun(n_indices, k)\n        values[k] = val\n        s += val\n\n    assert n_indices == 0 or s &gt; 0, \"Sum of sample sizes must be positive\"\n    values /= s\n\n    if probs:\n        values.setflags(write=False)  # avoid accidental modification of cache\n        return values  # m_k / m\n\n    if self._n_samples_per_index is None:\n        normalization = min(values[values &gt; 0])\n        n_samples = np.sum(values / normalization).astype(int)  # min m_k = 1\n    elif self._n_samples_per_index == 0:\n        n_samples = np.ceil(s).astype(int)  # sum m_k = sum f_k\n    else:\n        n_samples = self._n_samples_per_index  # sum m_k = n_samples\n\n    values *= n_samples\n\n    # Round down and distribute remainder by adjusting the largest fractional parts\n    # A naive implementation with e.g.\n    #\n    # m_k = [max(1, int(round(m * f(k)/sum(f(j) for j in range(n)), 0)))\n    #         for k in range(n)]\n    #\n    # would not respect the total number of samples, and would not distribute\n    # remainders correctly\n    int_values: NDArray[np.int64] = np.floor(values).astype(np.int64)\n    remainder = n_samples - np.sum(int_values)\n    fractional_parts = values - int_values\n    fractional_parts_indices = np.argsort(-fractional_parts, kind=\"stable\")[\n        :remainder\n    ]\n    int_values[fractional_parts_indices] += 1\n    int_values.setflags(write=False)  # avoid accidental modification of cache\n    return int_values\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.DeltaShapleyNCSGDConfig","title":"DeltaShapleyNCSGDConfig  <code>dataclass</code>","text":"<pre><code>DeltaShapleyNCSGDConfig(\n    max_loss: float,\n    lipschitz_loss: float,\n    lipschitz_grad: float,\n    lr_factor: float,\n    n_sgd_iter: int,\n    n_val: int,\n    n_train: int,\n    eps: float = 0.01,\n    delta: float = 0.05,\n    version: Literal[\"theorem7\", \"code\"] = \"theorem7\",\n)\n</code></pre> <p>Configuration for Delta-Shapley non-convex SGD sampling.</p> <p>See Watson et al. (2023)<sup>1</sup> for details. Given that it can be difficult to estimate these constants, an alternative which has a similar decay rate of \\(O(1/k^2)\\) is to use a PowerLawSampleSize strategy.</p> PARAMETER DESCRIPTION <code>max_loss</code> <p>Maximum of the loss.</p> <p> TYPE: <code>float</code> </p> <code>lipschitz_loss</code> <p>Lipschitz constant of the loss</p> <p> TYPE: <code>float</code> </p> <code>lipschitz_grad</code> <p>Lipschitz constant of the gradient of the loss</p> <p> TYPE: <code>float</code> </p> <code>lr_factor</code> <p>Learning rate factor c, assuming it has the form \\(\u0007lpha_t = c/t.\\)</p> <p> TYPE: <code>float</code> </p> <code>n_sgd_iter</code> <p>Number of SGD iterations.</p> <p> TYPE: <code>int</code> </p> <code>n_val</code> <p>Number of test samples.</p> <p> TYPE: <code>int</code> </p> <code>n_train</code> <p>Number of training samples.</p> <p> TYPE: <code>int</code> </p> <code>eps</code> <p>Epsilon value in the epsilon-delta guarantee, i.e. the distance to the true value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>delta</code> <p>Delta value in the epsilon-delta guarantee, i.e. the probability of failure.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>version</code> <p>Version of the bound to use: either the one from the paper or the one in the code.</p> <p> TYPE: <code>Literal['theorem7', 'code']</code> DEFAULT: <code>'theorem7'</code> </p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.DeltaShapleyNCSGDSampleSize","title":"DeltaShapleyNCSGDSampleSize","text":"<pre><code>DeltaShapleyNCSGDSampleSize(\n    config: DeltaShapleyNCSGDConfig,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n)\n</code></pre> <p>               Bases: <code>SampleSizeStrategy</code></p> <p>Heuristic choice of samples per set size for \\(\\delta\\)-Shapley.</p> <p>This implements the non-convex SGD bound from Watson et al. (2023)<sup>1</sup>.</p> <p>Note</p> <p>This strategy does not accept an <code>n_samples</code> parameter because it provides the absolute number of samples to take for each set size based on the constants provided in the configuration.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration for the sample bound.</p> <p> TYPE: <code>DeltaShapleyNCSGDConfig</code> </p> <code>lower_bound</code> <p>Lower bound for the set sizes. If the set size is smaller than this, the probability of sampling is 0. If <code>None</code>, the lower bound is set to 0.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>upper_bound</code> <p>Upper bound for the set size. If the set size is larger than this, the probability of sampling is 0. If <code>None</code>, the upper bound is set to the number of indices.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self,\n    config: DeltaShapleyNCSGDConfig,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n):\n    super().__init__(n_samples=0, lower_bound=lower_bound, upper_bound=upper_bound)\n    self.config = config\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.DeltaShapleyNCSGDSampleSize.effective_bounds","title":"effective_bounds","text":"<pre><code>effective_bounds(n: int) -&gt; tuple[int, int]\n</code></pre> <p>Returns the effective bounds for the sample sizes, given the number of indices <code>n</code> from which sets are sampled.</p> <p>Note</p> <p>The number of indices <code>n</code> will typically be <code>complement_size(len(train))</code>, i.e. what we sometimes denote as <code>effective_n</code>.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of indices from which subsets are drawn.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     A tuple of [lower, upper] bounds for sample sizes (inclusive).</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def effective_bounds(self, n: int) -&gt; tuple[int, int]:\n    \"\"\"Returns the effective bounds for the sample sizes, given the number of\n    indices `n` from which sets are sampled.\n\n    !!! note\n        The number of indices `n` will typically be `complement_size(len(train))`,\n        i.e. what we sometimes denote as `effective_n`.\n\n    Args:\n        n: The number of indices from which subsets are drawn.\n    Returns:\n        A tuple of [lower, upper] bounds for sample sizes (inclusive).\n    \"\"\"\n    lower = 0 if self.lower_bound is None else self.lower_bound\n    upper = n if self.upper_bound is None else self.upper_bound\n    lower = min(lower, n)\n    upper = min(upper, n)\n\n    return lower, upper\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.DeltaShapleyNCSGDSampleSize.fun","title":"fun","text":"<pre><code>fun(n_indices: int, subset_size: int) -&gt; int\n</code></pre> <p>Computes the number of samples for the non-convex SGD bound.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def fun(self, n_indices: int, subset_size: int) -&gt; int:\n    \"\"\"Computes the number of samples for the non-convex SGD bound.\"\"\"\n    q = self.config.lipschitz_grad * self.config.lr_factor\n    H_1 = self.config.max_loss ** (q / (q + 1))\n    H_2 = (2 * self.config.lr_factor * (self.config.lipschitz_loss**2)) ** (\n        1 / (q + 1)\n    )\n    H_3 = self.config.n_sgd_iter ** (q / (q + 1))\n    H_4 = (1 + (1 / q)) / (max(subset_size - 1, 1))\n    H = H_1 * H_2 * H_3 * H_4\n\n    if self.config.version == \"code\":\n        return int(\n            np.ceil(\n                2\n                * np.log((2 * n_indices) / self.config.delta)\n                * (\n                    (\n                        (H**2 / self.config.n_val)\n                        + 2 * self.config.max_loss * self.config.eps / 3\n                    )\n                    / self.config.eps**2\n                )\n            )\n        )\n    elif self.config.version == \"theorem7\":\n        return int(\n            np.ceil(\n                2\n                * np.log((2 * n_indices) / self.config.delta)\n                * (\n                    (\n                        2 * H**2\n                        + 2 * self.config.max_loss * H\n                        + 4 * self.config.max_loss * self.config.eps / 3\n                    )\n                    / self.config.eps**2\n                )\n            )\n        )\n    else:\n        raise ValueError(f\"Unknown version: {self.config.version}\")\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.DeltaShapleyNCSGDSampleSize.n_samples_per_index","title":"n_samples_per_index","text":"<pre><code>n_samples_per_index(n_indices: int) -&gt; int | None\n</code></pre> <p>Returns the total number of samples to take for the given number of indices, or <code>None</code> if no limit was set and samples are generated with probabilities.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def n_samples_per_index(self, n_indices: int) -&gt; int | None:\n    \"\"\"Returns the total number of samples to take for the given number of indices,\n    or `None` if no limit was set and samples are generated with probabilities.\"\"\"\n    if self._n_samples_per_index is None:\n        return None\n    sizes = self.sample_sizes(n_indices, probs=False)\n    return int(np.sum(sizes).item())\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.DeltaShapleyNCSGDSampleSize.sample_sizes","title":"sample_sizes  <code>cached</code>","text":"<pre><code>sample_sizes(\n    n_indices: int, probs: bool = True\n) -&gt; NDArray[int64] | NDArray[float64]\n</code></pre> <p>Precomputes the number of samples to take for each set size, from 0 up to <code>n_indices</code> inclusive.</p> <p>If <code>probs</code> is <code>True</code>, the result is a vector of floats, where each element is the probability of sampling a set of size \\(k.\\) This is useful e.g. for RandomSizeIteration where one needs frequencies. In this case <code>self.n_samples_per_index</code> can be <code>None</code>.</p> <p>If <code>probs</code> is <code>False</code>, the result is a vector of integers, where each element \\(k\\) is the number of samples to take for set size \\(k.\\) The sum of all elements will depend on the value of <code>n_samples</code> upon construction. It will be</p> <ul> <li>equal to <code>n_samples</code> if <code>n_samples &gt; 0</code>,</li> <li>the sum of the values of <code>fun</code> for all set sizes if <code>n_samples == 0</code>,</li> <li>the sum after a normalization such that the smallest non-zero value is 1, if   <code>n_samples == None</code>.</li> </ul> <p>This somewhat complex behavior is necessary to ensure that the total number of samples is respected when provided either globally upon construction, or implicitly by the sum of the values of <code>fun</code> when the strategy computes absolute numbers, but also when the strategy has a <code>fun</code> that returns frequencies. The special handling of the case <code>n_samples == None</code> is used implicitly by StratifiedPermutationSampler (yeah, it's not pretty).</p> <p>When <code>probs</code> is <code>False</code>, this method corrects rounding errors taking into account the fractional parts so that the total number of samples is respected, while allocating remainders in a way that follows the relative sizes of the fractional parts.</p> PARAMETER DESCRIPTION <code>n_indices</code> <p>number of indices in the index set from which to sample. This is typically <code>len(dataset) - 1</code> with the usual index iterations.</p> <p> TYPE: <code>int</code> </p> <code>probs</code> <p>Whether to perform the remainder distribution. If <code>True</code>, sampling probabilities are returned. If <code>False</code>, then <code>n_samples_per_index</code> is used to compute the actual number of samples and the values are rounded down to the nearest integer, and the remainder is distributed to maintain the relative frequencies.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Returns:     The exact (integer) number of samples to take for each set size, if     <code>probs</code> is <code>False</code>. Otherwise, the fractional number of samples.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>@cache\ndef sample_sizes(\n    self, n_indices: int, probs: bool = True\n) -&gt; NDArray[np.int64] | NDArray[np.float64]:\n    \"\"\"Precomputes the number of samples to take for each set size, from 0 up to\n    `n_indices` inclusive.\n\n    If `probs` is `True`, the result is a vector of floats, where each element\n    is the probability of sampling a set of size $k.$ This is useful e.g. for\n    [RandomSizeIteration][pydvl.valuation.samplers.stratified.RandomSizeIteration]\n    where one needs frequencies. In this case `self.n_samples_per_index` can be\n    `None`.\n\n    If `probs` is `False`, the result is a vector of integers, where each element\n    $k$ is the number of samples to take for set size $k.$ The sum of all elements\n    will depend on the value of `n_samples` upon construction. It will be\n\n    * equal to `n_samples` if `n_samples &gt; 0`,\n    * the sum of the values of `fun` for all set sizes if `n_samples == 0`,\n    * the sum after a normalization such that the smallest non-zero value is 1, if\n      `n_samples == None`.\n\n    This somewhat complex behavior is necessary to ensure that the total number of\n    samples is respected when provided either globally upon construction, or\n    implicitly by the sum of the values of `fun` when the strategy computes absolute\n    numbers, but also when the strategy has a `fun` that returns frequencies. The\n    special handling of the case `n_samples == None` is used implicitly by\n    [StratifiedPermutationSampler][pydvl.valuation.samplers.stratified.StratifiedPermutationSampler]\n    (yeah, it's not pretty).\n\n    When `probs` is `False`, this method corrects rounding errors taking into\n    account the fractional parts so that the total number of samples is respected,\n    while allocating remainders in a way that follows the relative sizes of the\n    fractional parts.\n\n    Args:\n        n_indices: number of indices in the index set from which to sample. This is\n            typically `len(dataset) - 1` with the usual index iterations.\n        probs: Whether to perform the remainder distribution. If `True`, sampling\n            probabilities are returned. If `False`, then `n_samples_per_index` is\n            used to compute the actual number of samples and the values are rounded\n            down to the nearest integer, and the remainder is distributed to\n            maintain the relative frequencies.\n    Returns:\n        The exact (integer) number of samples to take for each set size, if\n        `probs` is `False`. Otherwise, the fractional number of samples.\n    \"\"\"\n\n    # m_k = m * f(k) / sum_j f(j)\n    values = np.zeros(n_indices + 1, dtype=float)\n    s = 0.0\n    lb, ub = self.effective_bounds(n_indices)\n\n    for k in range(lb, ub + 1):\n        val = self.fun(n_indices, k)\n        values[k] = val\n        s += val\n\n    assert n_indices == 0 or s &gt; 0, \"Sum of sample sizes must be positive\"\n    values /= s\n\n    if probs:\n        values.setflags(write=False)  # avoid accidental modification of cache\n        return values  # m_k / m\n\n    if self._n_samples_per_index is None:\n        normalization = min(values[values &gt; 0])\n        n_samples = np.sum(values / normalization).astype(int)  # min m_k = 1\n    elif self._n_samples_per_index == 0:\n        n_samples = np.ceil(s).astype(int)  # sum m_k = sum f_k\n    else:\n        n_samples = self._n_samples_per_index  # sum m_k = n_samples\n\n    values *= n_samples\n\n    # Round down and distribute remainder by adjusting the largest fractional parts\n    # A naive implementation with e.g.\n    #\n    # m_k = [max(1, int(round(m * f(k)/sum(f(j) for j in range(n)), 0)))\n    #         for k in range(n)]\n    #\n    # would not respect the total number of samples, and would not distribute\n    # remainders correctly\n    int_values: NDArray[np.int64] = np.floor(values).astype(np.int64)\n    remainder = n_samples - np.sum(int_values)\n    fractional_parts = values - int_values\n    fractional_parts_indices = np.argsort(-fractional_parts, kind=\"stable\")[\n        :remainder\n    ]\n    int_values[fractional_parts_indices] += 1\n    int_values.setflags(write=False)  # avoid accidental modification of cache\n    return int_values\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.FiniteSequentialSizeIteration","title":"FiniteSequentialSizeIteration","text":"<pre><code>FiniteSequentialSizeIteration(strategy: SampleSizeStrategy, n_indices: int)\n</code></pre> <p>               Bases: <code>SampleSizeIteration</code></p> <p>Generates exactly \\(m_k\\) samples for each set size \\(k\\) before moving to the next.</p> <p>Only for deterministic sample sizes</p> <p>This iteration is only valid for deterministic sample sizes. In particular, <code>n_samples</code> must be set to the total number of samples.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(self, strategy: SampleSizeStrategy, n_indices: int):\n    self.strategy = strategy\n    self.n_indices = n_indices\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.GroupTestingSampleSize","title":"GroupTestingSampleSize","text":"<pre><code>GroupTestingSampleSize(\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n)\n</code></pre> <p>               Bases: <code>SampleSizeStrategy</code></p> <p>Heuristic choice of samples per set size used for Group Testing.</p> <p>GroupTestingShapleyValuation uses this strategy for the stratified sampling of samples with which to construct the linear problem it requires.</p> <p>This heuristic sets the number of sets at size \\(k\\) to be</p> \\[m_k = m \\frac{f(k)}{\\sum_{j=0}^{n-1} f(j)},\\] <p>for a total number of samples \\(m\\) and:</p> \\[ f(k) = \\frac{1}{k} + \\frac{1}{n-k}, \\text{for} k \\in \\{1, n-1\\}. \\] <p>For GT Shapley, \\(m=1\\) and \\(m_k\\) is interpreted as a probability of sampling size \\(k.\\)</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self,\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n):\n    if n_samples is not None:\n        n_samples = validate_number(\"n_samples\", n_samples, int, lower=0)\n    if lower_bound is not None:\n        lower_bound = validate_number(\"lower_bound\", lower_bound, int, lower=0)\n    if upper_bound is not None:\n        upper_bound = validate_number(\n            \"upper_bound\", upper_bound, int, lower=lower_bound\n        )\n    self._n_samples_per_index = n_samples\n    self.lower_bound = lower_bound\n    self.upper_bound = upper_bound\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.GroupTestingSampleSize.effective_bounds","title":"effective_bounds","text":"<pre><code>effective_bounds(n: int) -&gt; tuple[int, int]\n</code></pre> <p>Returns the effective bounds for the sample sizes, given the number of indices <code>n</code> from which sets are sampled.</p> <p>Note</p> <p>The number of indices <code>n</code> will typically be <code>complement_size(len(train))</code>, i.e. what we sometimes denote as <code>effective_n</code>.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of indices from which subsets are drawn.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     A tuple of [lower, upper] bounds for sample sizes (inclusive).</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def effective_bounds(self, n: int) -&gt; tuple[int, int]:\n    \"\"\"Returns the effective bounds for the sample sizes, given the number of\n    indices `n` from which sets are sampled.\n\n    !!! note\n        The number of indices `n` will typically be `complement_size(len(train))`,\n        i.e. what we sometimes denote as `effective_n`.\n\n    Args:\n        n: The number of indices from which subsets are drawn.\n    Returns:\n        A tuple of [lower, upper] bounds for sample sizes (inclusive).\n    \"\"\"\n    lower = 0 if self.lower_bound is None else self.lower_bound\n    upper = n if self.upper_bound is None else self.upper_bound\n    lower = min(lower, n)\n    upper = min(upper, n)\n\n    return lower, upper\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.GroupTestingSampleSize.n_samples_per_index","title":"n_samples_per_index","text":"<pre><code>n_samples_per_index(n_indices: int) -&gt; int | None\n</code></pre> <p>Returns the total number of samples to take for the given number of indices, or <code>None</code> if no limit was set and samples are generated with probabilities.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def n_samples_per_index(self, n_indices: int) -&gt; int | None:\n    \"\"\"Returns the total number of samples to take for the given number of indices,\n    or `None` if no limit was set and samples are generated with probabilities.\"\"\"\n    if self._n_samples_per_index is None:\n        return None\n    sizes = self.sample_sizes(n_indices, probs=False)\n    return int(np.sum(sizes).item())\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.GroupTestingSampleSize.sample_sizes","title":"sample_sizes  <code>cached</code>","text":"<pre><code>sample_sizes(\n    n_indices: int, probs: bool = True\n) -&gt; NDArray[int64] | NDArray[float64]\n</code></pre> <p>Precomputes the number of samples to take for each set size, from 0 up to <code>n_indices</code> inclusive.</p> <p>If <code>probs</code> is <code>True</code>, the result is a vector of floats, where each element is the probability of sampling a set of size \\(k.\\) This is useful e.g. for RandomSizeIteration where one needs frequencies. In this case <code>self.n_samples_per_index</code> can be <code>None</code>.</p> <p>If <code>probs</code> is <code>False</code>, the result is a vector of integers, where each element \\(k\\) is the number of samples to take for set size \\(k.\\) The sum of all elements will depend on the value of <code>n_samples</code> upon construction. It will be</p> <ul> <li>equal to <code>n_samples</code> if <code>n_samples &gt; 0</code>,</li> <li>the sum of the values of <code>fun</code> for all set sizes if <code>n_samples == 0</code>,</li> <li>the sum after a normalization such that the smallest non-zero value is 1, if   <code>n_samples == None</code>.</li> </ul> <p>This somewhat complex behavior is necessary to ensure that the total number of samples is respected when provided either globally upon construction, or implicitly by the sum of the values of <code>fun</code> when the strategy computes absolute numbers, but also when the strategy has a <code>fun</code> that returns frequencies. The special handling of the case <code>n_samples == None</code> is used implicitly by StratifiedPermutationSampler (yeah, it's not pretty).</p> <p>When <code>probs</code> is <code>False</code>, this method corrects rounding errors taking into account the fractional parts so that the total number of samples is respected, while allocating remainders in a way that follows the relative sizes of the fractional parts.</p> PARAMETER DESCRIPTION <code>n_indices</code> <p>number of indices in the index set from which to sample. This is typically <code>len(dataset) - 1</code> with the usual index iterations.</p> <p> TYPE: <code>int</code> </p> <code>probs</code> <p>Whether to perform the remainder distribution. If <code>True</code>, sampling probabilities are returned. If <code>False</code>, then <code>n_samples_per_index</code> is used to compute the actual number of samples and the values are rounded down to the nearest integer, and the remainder is distributed to maintain the relative frequencies.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Returns:     The exact (integer) number of samples to take for each set size, if     <code>probs</code> is <code>False</code>. Otherwise, the fractional number of samples.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>@cache\ndef sample_sizes(\n    self, n_indices: int, probs: bool = True\n) -&gt; NDArray[np.int64] | NDArray[np.float64]:\n    \"\"\"Precomputes the number of samples to take for each set size, from 0 up to\n    `n_indices` inclusive.\n\n    If `probs` is `True`, the result is a vector of floats, where each element\n    is the probability of sampling a set of size $k.$ This is useful e.g. for\n    [RandomSizeIteration][pydvl.valuation.samplers.stratified.RandomSizeIteration]\n    where one needs frequencies. In this case `self.n_samples_per_index` can be\n    `None`.\n\n    If `probs` is `False`, the result is a vector of integers, where each element\n    $k$ is the number of samples to take for set size $k.$ The sum of all elements\n    will depend on the value of `n_samples` upon construction. It will be\n\n    * equal to `n_samples` if `n_samples &gt; 0`,\n    * the sum of the values of `fun` for all set sizes if `n_samples == 0`,\n    * the sum after a normalization such that the smallest non-zero value is 1, if\n      `n_samples == None`.\n\n    This somewhat complex behavior is necessary to ensure that the total number of\n    samples is respected when provided either globally upon construction, or\n    implicitly by the sum of the values of `fun` when the strategy computes absolute\n    numbers, but also when the strategy has a `fun` that returns frequencies. The\n    special handling of the case `n_samples == None` is used implicitly by\n    [StratifiedPermutationSampler][pydvl.valuation.samplers.stratified.StratifiedPermutationSampler]\n    (yeah, it's not pretty).\n\n    When `probs` is `False`, this method corrects rounding errors taking into\n    account the fractional parts so that the total number of samples is respected,\n    while allocating remainders in a way that follows the relative sizes of the\n    fractional parts.\n\n    Args:\n        n_indices: number of indices in the index set from which to sample. This is\n            typically `len(dataset) - 1` with the usual index iterations.\n        probs: Whether to perform the remainder distribution. If `True`, sampling\n            probabilities are returned. If `False`, then `n_samples_per_index` is\n            used to compute the actual number of samples and the values are rounded\n            down to the nearest integer, and the remainder is distributed to\n            maintain the relative frequencies.\n    Returns:\n        The exact (integer) number of samples to take for each set size, if\n        `probs` is `False`. Otherwise, the fractional number of samples.\n    \"\"\"\n\n    # m_k = m * f(k) / sum_j f(j)\n    values = np.zeros(n_indices + 1, dtype=float)\n    s = 0.0\n    lb, ub = self.effective_bounds(n_indices)\n\n    for k in range(lb, ub + 1):\n        val = self.fun(n_indices, k)\n        values[k] = val\n        s += val\n\n    assert n_indices == 0 or s &gt; 0, \"Sum of sample sizes must be positive\"\n    values /= s\n\n    if probs:\n        values.setflags(write=False)  # avoid accidental modification of cache\n        return values  # m_k / m\n\n    if self._n_samples_per_index is None:\n        normalization = min(values[values &gt; 0])\n        n_samples = np.sum(values / normalization).astype(int)  # min m_k = 1\n    elif self._n_samples_per_index == 0:\n        n_samples = np.ceil(s).astype(int)  # sum m_k = sum f_k\n    else:\n        n_samples = self._n_samples_per_index  # sum m_k = n_samples\n\n    values *= n_samples\n\n    # Round down and distribute remainder by adjusting the largest fractional parts\n    # A naive implementation with e.g.\n    #\n    # m_k = [max(1, int(round(m * f(k)/sum(f(j) for j in range(n)), 0)))\n    #         for k in range(n)]\n    #\n    # would not respect the total number of samples, and would not distribute\n    # remainders correctly\n    int_values: NDArray[np.int64] = np.floor(values).astype(np.int64)\n    remainder = n_samples - np.sum(int_values)\n    fractional_parts = values - int_values\n    fractional_parts_indices = np.argsort(-fractional_parts, kind=\"stable\")[\n        :remainder\n    ]\n    int_values[fractional_parts_indices] += 1\n    int_values.setflags(write=False)  # avoid accidental modification of cache\n    return int_values\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.HarmonicSampleSize","title":"HarmonicSampleSize","text":"<pre><code>HarmonicSampleSize(\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n)\n</code></pre> <p>               Bases: <code>SampleSizeStrategy</code></p> <p>Heuristic choice of samples per set size for VRDS.</p> <p>Sets the number of sets at size \\(k\\) to be</p> \\[m_k = m \\frac{f(k)}{\\sum_{j=0}^{n-1} f(j)},\\] <p>for a total number of samples \\(m\\) and:</p> \\[f(k) = \\frac{1}{1+k}.\\] Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self,\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n):\n    if n_samples is not None:\n        n_samples = validate_number(\"n_samples\", n_samples, int, lower=0)\n    if lower_bound is not None:\n        lower_bound = validate_number(\"lower_bound\", lower_bound, int, lower=0)\n    if upper_bound is not None:\n        upper_bound = validate_number(\n            \"upper_bound\", upper_bound, int, lower=lower_bound\n        )\n    self._n_samples_per_index = n_samples\n    self.lower_bound = lower_bound\n    self.upper_bound = upper_bound\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.HarmonicSampleSize.effective_bounds","title":"effective_bounds","text":"<pre><code>effective_bounds(n: int) -&gt; tuple[int, int]\n</code></pre> <p>Returns the effective bounds for the sample sizes, given the number of indices <code>n</code> from which sets are sampled.</p> <p>Note</p> <p>The number of indices <code>n</code> will typically be <code>complement_size(len(train))</code>, i.e. what we sometimes denote as <code>effective_n</code>.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of indices from which subsets are drawn.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     A tuple of [lower, upper] bounds for sample sizes (inclusive).</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def effective_bounds(self, n: int) -&gt; tuple[int, int]:\n    \"\"\"Returns the effective bounds for the sample sizes, given the number of\n    indices `n` from which sets are sampled.\n\n    !!! note\n        The number of indices `n` will typically be `complement_size(len(train))`,\n        i.e. what we sometimes denote as `effective_n`.\n\n    Args:\n        n: The number of indices from which subsets are drawn.\n    Returns:\n        A tuple of [lower, upper] bounds for sample sizes (inclusive).\n    \"\"\"\n    lower = 0 if self.lower_bound is None else self.lower_bound\n    upper = n if self.upper_bound is None else self.upper_bound\n    lower = min(lower, n)\n    upper = min(upper, n)\n\n    return lower, upper\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.HarmonicSampleSize.n_samples_per_index","title":"n_samples_per_index","text":"<pre><code>n_samples_per_index(n_indices: int) -&gt; int | None\n</code></pre> <p>Returns the total number of samples to take for the given number of indices, or <code>None</code> if no limit was set and samples are generated with probabilities.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def n_samples_per_index(self, n_indices: int) -&gt; int | None:\n    \"\"\"Returns the total number of samples to take for the given number of indices,\n    or `None` if no limit was set and samples are generated with probabilities.\"\"\"\n    if self._n_samples_per_index is None:\n        return None\n    sizes = self.sample_sizes(n_indices, probs=False)\n    return int(np.sum(sizes).item())\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.HarmonicSampleSize.sample_sizes","title":"sample_sizes  <code>cached</code>","text":"<pre><code>sample_sizes(\n    n_indices: int, probs: bool = True\n) -&gt; NDArray[int64] | NDArray[float64]\n</code></pre> <p>Precomputes the number of samples to take for each set size, from 0 up to <code>n_indices</code> inclusive.</p> <p>If <code>probs</code> is <code>True</code>, the result is a vector of floats, where each element is the probability of sampling a set of size \\(k.\\) This is useful e.g. for RandomSizeIteration where one needs frequencies. In this case <code>self.n_samples_per_index</code> can be <code>None</code>.</p> <p>If <code>probs</code> is <code>False</code>, the result is a vector of integers, where each element \\(k\\) is the number of samples to take for set size \\(k.\\) The sum of all elements will depend on the value of <code>n_samples</code> upon construction. It will be</p> <ul> <li>equal to <code>n_samples</code> if <code>n_samples &gt; 0</code>,</li> <li>the sum of the values of <code>fun</code> for all set sizes if <code>n_samples == 0</code>,</li> <li>the sum after a normalization such that the smallest non-zero value is 1, if   <code>n_samples == None</code>.</li> </ul> <p>This somewhat complex behavior is necessary to ensure that the total number of samples is respected when provided either globally upon construction, or implicitly by the sum of the values of <code>fun</code> when the strategy computes absolute numbers, but also when the strategy has a <code>fun</code> that returns frequencies. The special handling of the case <code>n_samples == None</code> is used implicitly by StratifiedPermutationSampler (yeah, it's not pretty).</p> <p>When <code>probs</code> is <code>False</code>, this method corrects rounding errors taking into account the fractional parts so that the total number of samples is respected, while allocating remainders in a way that follows the relative sizes of the fractional parts.</p> PARAMETER DESCRIPTION <code>n_indices</code> <p>number of indices in the index set from which to sample. This is typically <code>len(dataset) - 1</code> with the usual index iterations.</p> <p> TYPE: <code>int</code> </p> <code>probs</code> <p>Whether to perform the remainder distribution. If <code>True</code>, sampling probabilities are returned. If <code>False</code>, then <code>n_samples_per_index</code> is used to compute the actual number of samples and the values are rounded down to the nearest integer, and the remainder is distributed to maintain the relative frequencies.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Returns:     The exact (integer) number of samples to take for each set size, if     <code>probs</code> is <code>False</code>. Otherwise, the fractional number of samples.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>@cache\ndef sample_sizes(\n    self, n_indices: int, probs: bool = True\n) -&gt; NDArray[np.int64] | NDArray[np.float64]:\n    \"\"\"Precomputes the number of samples to take for each set size, from 0 up to\n    `n_indices` inclusive.\n\n    If `probs` is `True`, the result is a vector of floats, where each element\n    is the probability of sampling a set of size $k.$ This is useful e.g. for\n    [RandomSizeIteration][pydvl.valuation.samplers.stratified.RandomSizeIteration]\n    where one needs frequencies. In this case `self.n_samples_per_index` can be\n    `None`.\n\n    If `probs` is `False`, the result is a vector of integers, where each element\n    $k$ is the number of samples to take for set size $k.$ The sum of all elements\n    will depend on the value of `n_samples` upon construction. It will be\n\n    * equal to `n_samples` if `n_samples &gt; 0`,\n    * the sum of the values of `fun` for all set sizes if `n_samples == 0`,\n    * the sum after a normalization such that the smallest non-zero value is 1, if\n      `n_samples == None`.\n\n    This somewhat complex behavior is necessary to ensure that the total number of\n    samples is respected when provided either globally upon construction, or\n    implicitly by the sum of the values of `fun` when the strategy computes absolute\n    numbers, but also when the strategy has a `fun` that returns frequencies. The\n    special handling of the case `n_samples == None` is used implicitly by\n    [StratifiedPermutationSampler][pydvl.valuation.samplers.stratified.StratifiedPermutationSampler]\n    (yeah, it's not pretty).\n\n    When `probs` is `False`, this method corrects rounding errors taking into\n    account the fractional parts so that the total number of samples is respected,\n    while allocating remainders in a way that follows the relative sizes of the\n    fractional parts.\n\n    Args:\n        n_indices: number of indices in the index set from which to sample. This is\n            typically `len(dataset) - 1` with the usual index iterations.\n        probs: Whether to perform the remainder distribution. If `True`, sampling\n            probabilities are returned. If `False`, then `n_samples_per_index` is\n            used to compute the actual number of samples and the values are rounded\n            down to the nearest integer, and the remainder is distributed to\n            maintain the relative frequencies.\n    Returns:\n        The exact (integer) number of samples to take for each set size, if\n        `probs` is `False`. Otherwise, the fractional number of samples.\n    \"\"\"\n\n    # m_k = m * f(k) / sum_j f(j)\n    values = np.zeros(n_indices + 1, dtype=float)\n    s = 0.0\n    lb, ub = self.effective_bounds(n_indices)\n\n    for k in range(lb, ub + 1):\n        val = self.fun(n_indices, k)\n        values[k] = val\n        s += val\n\n    assert n_indices == 0 or s &gt; 0, \"Sum of sample sizes must be positive\"\n    values /= s\n\n    if probs:\n        values.setflags(write=False)  # avoid accidental modification of cache\n        return values  # m_k / m\n\n    if self._n_samples_per_index is None:\n        normalization = min(values[values &gt; 0])\n        n_samples = np.sum(values / normalization).astype(int)  # min m_k = 1\n    elif self._n_samples_per_index == 0:\n        n_samples = np.ceil(s).astype(int)  # sum m_k = sum f_k\n    else:\n        n_samples = self._n_samples_per_index  # sum m_k = n_samples\n\n    values *= n_samples\n\n    # Round down and distribute remainder by adjusting the largest fractional parts\n    # A naive implementation with e.g.\n    #\n    # m_k = [max(1, int(round(m * f(k)/sum(f(j) for j in range(n)), 0)))\n    #         for k in range(n)]\n    #\n    # would not respect the total number of samples, and would not distribute\n    # remainders correctly\n    int_values: NDArray[np.int64] = np.floor(values).astype(np.int64)\n    remainder = n_samples - np.sum(int_values)\n    fractional_parts = values - int_values\n    fractional_parts_indices = np.argsort(-fractional_parts, kind=\"stable\")[\n        :remainder\n    ]\n    int_values[fractional_parts_indices] += 1\n    int_values.setflags(write=False)  # avoid accidental modification of cache\n    return int_values\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.LinearSampleSize","title":"LinearSampleSize","text":"<pre><code>LinearSampleSize(\n    scale: float,\n    offset: int = 0,\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n)\n</code></pre> <p>               Bases: <code>SampleSizeStrategy</code></p> <p>Use a linear function of the set size to determine the number of samples to take.</p> <p>This is mostly intended for testing purposes, as it is not a very useful heuristic.</p> PARAMETER DESCRIPTION <code>scale</code> <p>Slope of the linear function.</p> <p> TYPE: <code>float</code> </p> <code>offset</code> <p>Offset of the linear function.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>n_samples</code> <p>Number of samples for the stratified sampler to generate. It can be <code>None</code>, 0 or a positive integer. See the documentation of  SampleSizeStrategy  for details.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>lower_bound</code> <p>Lower bound for the set sizes.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>upper_bound</code> <p>Upper bound for the set sizes.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self,\n    scale: float,\n    offset: int = 0,\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n):\n    super().__init__(\n        n_samples=n_samples, lower_bound=lower_bound, upper_bound=upper_bound\n    )\n    self.scale = validate_number(\"scale\", scale, float, lower=0.0)\n    self.offset = validate_number(\"offset\", offset, int, lower=0)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.LinearSampleSize.effective_bounds","title":"effective_bounds","text":"<pre><code>effective_bounds(n: int) -&gt; tuple[int, int]\n</code></pre> <p>Returns the effective bounds for the sample sizes, given the number of indices <code>n</code> from which sets are sampled.</p> <p>Note</p> <p>The number of indices <code>n</code> will typically be <code>complement_size(len(train))</code>, i.e. what we sometimes denote as <code>effective_n</code>.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of indices from which subsets are drawn.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     A tuple of [lower, upper] bounds for sample sizes (inclusive).</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def effective_bounds(self, n: int) -&gt; tuple[int, int]:\n    \"\"\"Returns the effective bounds for the sample sizes, given the number of\n    indices `n` from which sets are sampled.\n\n    !!! note\n        The number of indices `n` will typically be `complement_size(len(train))`,\n        i.e. what we sometimes denote as `effective_n`.\n\n    Args:\n        n: The number of indices from which subsets are drawn.\n    Returns:\n        A tuple of [lower, upper] bounds for sample sizes (inclusive).\n    \"\"\"\n    lower = 0 if self.lower_bound is None else self.lower_bound\n    upper = n if self.upper_bound is None else self.upper_bound\n    lower = min(lower, n)\n    upper = min(upper, n)\n\n    return lower, upper\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.LinearSampleSize.n_samples_per_index","title":"n_samples_per_index","text":"<pre><code>n_samples_per_index(n_indices: int) -&gt; int | None\n</code></pre> <p>Returns the total number of samples to take for the given number of indices, or <code>None</code> if no limit was set and samples are generated with probabilities.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def n_samples_per_index(self, n_indices: int) -&gt; int | None:\n    \"\"\"Returns the total number of samples to take for the given number of indices,\n    or `None` if no limit was set and samples are generated with probabilities.\"\"\"\n    if self._n_samples_per_index is None:\n        return None\n    sizes = self.sample_sizes(n_indices, probs=False)\n    return int(np.sum(sizes).item())\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.LinearSampleSize.sample_sizes","title":"sample_sizes  <code>cached</code>","text":"<pre><code>sample_sizes(\n    n_indices: int, probs: bool = True\n) -&gt; NDArray[int64] | NDArray[float64]\n</code></pre> <p>Precomputes the number of samples to take for each set size, from 0 up to <code>n_indices</code> inclusive.</p> <p>If <code>probs</code> is <code>True</code>, the result is a vector of floats, where each element is the probability of sampling a set of size \\(k.\\) This is useful e.g. for RandomSizeIteration where one needs frequencies. In this case <code>self.n_samples_per_index</code> can be <code>None</code>.</p> <p>If <code>probs</code> is <code>False</code>, the result is a vector of integers, where each element \\(k\\) is the number of samples to take for set size \\(k.\\) The sum of all elements will depend on the value of <code>n_samples</code> upon construction. It will be</p> <ul> <li>equal to <code>n_samples</code> if <code>n_samples &gt; 0</code>,</li> <li>the sum of the values of <code>fun</code> for all set sizes if <code>n_samples == 0</code>,</li> <li>the sum after a normalization such that the smallest non-zero value is 1, if   <code>n_samples == None</code>.</li> </ul> <p>This somewhat complex behavior is necessary to ensure that the total number of samples is respected when provided either globally upon construction, or implicitly by the sum of the values of <code>fun</code> when the strategy computes absolute numbers, but also when the strategy has a <code>fun</code> that returns frequencies. The special handling of the case <code>n_samples == None</code> is used implicitly by StratifiedPermutationSampler (yeah, it's not pretty).</p> <p>When <code>probs</code> is <code>False</code>, this method corrects rounding errors taking into account the fractional parts so that the total number of samples is respected, while allocating remainders in a way that follows the relative sizes of the fractional parts.</p> PARAMETER DESCRIPTION <code>n_indices</code> <p>number of indices in the index set from which to sample. This is typically <code>len(dataset) - 1</code> with the usual index iterations.</p> <p> TYPE: <code>int</code> </p> <code>probs</code> <p>Whether to perform the remainder distribution. If <code>True</code>, sampling probabilities are returned. If <code>False</code>, then <code>n_samples_per_index</code> is used to compute the actual number of samples and the values are rounded down to the nearest integer, and the remainder is distributed to maintain the relative frequencies.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Returns:     The exact (integer) number of samples to take for each set size, if     <code>probs</code> is <code>False</code>. Otherwise, the fractional number of samples.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>@cache\ndef sample_sizes(\n    self, n_indices: int, probs: bool = True\n) -&gt; NDArray[np.int64] | NDArray[np.float64]:\n    \"\"\"Precomputes the number of samples to take for each set size, from 0 up to\n    `n_indices` inclusive.\n\n    If `probs` is `True`, the result is a vector of floats, where each element\n    is the probability of sampling a set of size $k.$ This is useful e.g. for\n    [RandomSizeIteration][pydvl.valuation.samplers.stratified.RandomSizeIteration]\n    where one needs frequencies. In this case `self.n_samples_per_index` can be\n    `None`.\n\n    If `probs` is `False`, the result is a vector of integers, where each element\n    $k$ is the number of samples to take for set size $k.$ The sum of all elements\n    will depend on the value of `n_samples` upon construction. It will be\n\n    * equal to `n_samples` if `n_samples &gt; 0`,\n    * the sum of the values of `fun` for all set sizes if `n_samples == 0`,\n    * the sum after a normalization such that the smallest non-zero value is 1, if\n      `n_samples == None`.\n\n    This somewhat complex behavior is necessary to ensure that the total number of\n    samples is respected when provided either globally upon construction, or\n    implicitly by the sum of the values of `fun` when the strategy computes absolute\n    numbers, but also when the strategy has a `fun` that returns frequencies. The\n    special handling of the case `n_samples == None` is used implicitly by\n    [StratifiedPermutationSampler][pydvl.valuation.samplers.stratified.StratifiedPermutationSampler]\n    (yeah, it's not pretty).\n\n    When `probs` is `False`, this method corrects rounding errors taking into\n    account the fractional parts so that the total number of samples is respected,\n    while allocating remainders in a way that follows the relative sizes of the\n    fractional parts.\n\n    Args:\n        n_indices: number of indices in the index set from which to sample. This is\n            typically `len(dataset) - 1` with the usual index iterations.\n        probs: Whether to perform the remainder distribution. If `True`, sampling\n            probabilities are returned. If `False`, then `n_samples_per_index` is\n            used to compute the actual number of samples and the values are rounded\n            down to the nearest integer, and the remainder is distributed to\n            maintain the relative frequencies.\n    Returns:\n        The exact (integer) number of samples to take for each set size, if\n        `probs` is `False`. Otherwise, the fractional number of samples.\n    \"\"\"\n\n    # m_k = m * f(k) / sum_j f(j)\n    values = np.zeros(n_indices + 1, dtype=float)\n    s = 0.0\n    lb, ub = self.effective_bounds(n_indices)\n\n    for k in range(lb, ub + 1):\n        val = self.fun(n_indices, k)\n        values[k] = val\n        s += val\n\n    assert n_indices == 0 or s &gt; 0, \"Sum of sample sizes must be positive\"\n    values /= s\n\n    if probs:\n        values.setflags(write=False)  # avoid accidental modification of cache\n        return values  # m_k / m\n\n    if self._n_samples_per_index is None:\n        normalization = min(values[values &gt; 0])\n        n_samples = np.sum(values / normalization).astype(int)  # min m_k = 1\n    elif self._n_samples_per_index == 0:\n        n_samples = np.ceil(s).astype(int)  # sum m_k = sum f_k\n    else:\n        n_samples = self._n_samples_per_index  # sum m_k = n_samples\n\n    values *= n_samples\n\n    # Round down and distribute remainder by adjusting the largest fractional parts\n    # A naive implementation with e.g.\n    #\n    # m_k = [max(1, int(round(m * f(k)/sum(f(j) for j in range(n)), 0)))\n    #         for k in range(n)]\n    #\n    # would not respect the total number of samples, and would not distribute\n    # remainders correctly\n    int_values: NDArray[np.int64] = np.floor(values).astype(np.int64)\n    remainder = n_samples - np.sum(int_values)\n    fractional_parts = values - int_values\n    fractional_parts_indices = np.argsort(-fractional_parts, kind=\"stable\")[\n        :remainder\n    ]\n    int_values[fractional_parts_indices] += 1\n    int_values.setflags(write=False)  # avoid accidental modification of cache\n    return int_values\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.PowerLawSampleSize","title":"PowerLawSampleSize","text":"<pre><code>PowerLawSampleSize(\n    exponent: float,\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n)\n</code></pre> <p>               Bases: <code>SampleSizeStrategy</code></p> <p>Heuristic choice of samples per set size for VRDS.</p> <p>Sets the number of sets at size \\(k\\) to be</p> \\[m_k = m \\frac{f(k)}{\\sum_{j=0}^{n-1} f(j)},\\] <p>for a total number of samples \\(m\\) and:</p> \\[f(k) = (1+k)^a, \\] <p>and some exponent \\(a.\\) With \\(a=-1\\) one recovers the HarmonicSampleSize heuristic. With \\(a=-2\\) one has the same asymptotic behaviour as the \\(\\delta\\)-Shapley strategies.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>Total number of samples to generate per index.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>exponent</code> <p>The exponent to use. Recommended values are between -1 and -0.5.</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self,\n    exponent: float,\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n):\n    super().__init__(n_samples, lower_bound, upper_bound)\n    self.exponent = exponent\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.PowerLawSampleSize.effective_bounds","title":"effective_bounds","text":"<pre><code>effective_bounds(n: int) -&gt; tuple[int, int]\n</code></pre> <p>Returns the effective bounds for the sample sizes, given the number of indices <code>n</code> from which sets are sampled.</p> <p>Note</p> <p>The number of indices <code>n</code> will typically be <code>complement_size(len(train))</code>, i.e. what we sometimes denote as <code>effective_n</code>.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of indices from which subsets are drawn.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     A tuple of [lower, upper] bounds for sample sizes (inclusive).</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def effective_bounds(self, n: int) -&gt; tuple[int, int]:\n    \"\"\"Returns the effective bounds for the sample sizes, given the number of\n    indices `n` from which sets are sampled.\n\n    !!! note\n        The number of indices `n` will typically be `complement_size(len(train))`,\n        i.e. what we sometimes denote as `effective_n`.\n\n    Args:\n        n: The number of indices from which subsets are drawn.\n    Returns:\n        A tuple of [lower, upper] bounds for sample sizes (inclusive).\n    \"\"\"\n    lower = 0 if self.lower_bound is None else self.lower_bound\n    upper = n if self.upper_bound is None else self.upper_bound\n    lower = min(lower, n)\n    upper = min(upper, n)\n\n    return lower, upper\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.PowerLawSampleSize.n_samples_per_index","title":"n_samples_per_index","text":"<pre><code>n_samples_per_index(n_indices: int) -&gt; int | None\n</code></pre> <p>Returns the total number of samples to take for the given number of indices, or <code>None</code> if no limit was set and samples are generated with probabilities.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def n_samples_per_index(self, n_indices: int) -&gt; int | None:\n    \"\"\"Returns the total number of samples to take for the given number of indices,\n    or `None` if no limit was set and samples are generated with probabilities.\"\"\"\n    if self._n_samples_per_index is None:\n        return None\n    sizes = self.sample_sizes(n_indices, probs=False)\n    return int(np.sum(sizes).item())\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.PowerLawSampleSize.sample_sizes","title":"sample_sizes  <code>cached</code>","text":"<pre><code>sample_sizes(\n    n_indices: int, probs: bool = True\n) -&gt; NDArray[int64] | NDArray[float64]\n</code></pre> <p>Precomputes the number of samples to take for each set size, from 0 up to <code>n_indices</code> inclusive.</p> <p>If <code>probs</code> is <code>True</code>, the result is a vector of floats, where each element is the probability of sampling a set of size \\(k.\\) This is useful e.g. for RandomSizeIteration where one needs frequencies. In this case <code>self.n_samples_per_index</code> can be <code>None</code>.</p> <p>If <code>probs</code> is <code>False</code>, the result is a vector of integers, where each element \\(k\\) is the number of samples to take for set size \\(k.\\) The sum of all elements will depend on the value of <code>n_samples</code> upon construction. It will be</p> <ul> <li>equal to <code>n_samples</code> if <code>n_samples &gt; 0</code>,</li> <li>the sum of the values of <code>fun</code> for all set sizes if <code>n_samples == 0</code>,</li> <li>the sum after a normalization such that the smallest non-zero value is 1, if   <code>n_samples == None</code>.</li> </ul> <p>This somewhat complex behavior is necessary to ensure that the total number of samples is respected when provided either globally upon construction, or implicitly by the sum of the values of <code>fun</code> when the strategy computes absolute numbers, but also when the strategy has a <code>fun</code> that returns frequencies. The special handling of the case <code>n_samples == None</code> is used implicitly by StratifiedPermutationSampler (yeah, it's not pretty).</p> <p>When <code>probs</code> is <code>False</code>, this method corrects rounding errors taking into account the fractional parts so that the total number of samples is respected, while allocating remainders in a way that follows the relative sizes of the fractional parts.</p> PARAMETER DESCRIPTION <code>n_indices</code> <p>number of indices in the index set from which to sample. This is typically <code>len(dataset) - 1</code> with the usual index iterations.</p> <p> TYPE: <code>int</code> </p> <code>probs</code> <p>Whether to perform the remainder distribution. If <code>True</code>, sampling probabilities are returned. If <code>False</code>, then <code>n_samples_per_index</code> is used to compute the actual number of samples and the values are rounded down to the nearest integer, and the remainder is distributed to maintain the relative frequencies.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Returns:     The exact (integer) number of samples to take for each set size, if     <code>probs</code> is <code>False</code>. Otherwise, the fractional number of samples.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>@cache\ndef sample_sizes(\n    self, n_indices: int, probs: bool = True\n) -&gt; NDArray[np.int64] | NDArray[np.float64]:\n    \"\"\"Precomputes the number of samples to take for each set size, from 0 up to\n    `n_indices` inclusive.\n\n    If `probs` is `True`, the result is a vector of floats, where each element\n    is the probability of sampling a set of size $k.$ This is useful e.g. for\n    [RandomSizeIteration][pydvl.valuation.samplers.stratified.RandomSizeIteration]\n    where one needs frequencies. In this case `self.n_samples_per_index` can be\n    `None`.\n\n    If `probs` is `False`, the result is a vector of integers, where each element\n    $k$ is the number of samples to take for set size $k.$ The sum of all elements\n    will depend on the value of `n_samples` upon construction. It will be\n\n    * equal to `n_samples` if `n_samples &gt; 0`,\n    * the sum of the values of `fun` for all set sizes if `n_samples == 0`,\n    * the sum after a normalization such that the smallest non-zero value is 1, if\n      `n_samples == None`.\n\n    This somewhat complex behavior is necessary to ensure that the total number of\n    samples is respected when provided either globally upon construction, or\n    implicitly by the sum of the values of `fun` when the strategy computes absolute\n    numbers, but also when the strategy has a `fun` that returns frequencies. The\n    special handling of the case `n_samples == None` is used implicitly by\n    [StratifiedPermutationSampler][pydvl.valuation.samplers.stratified.StratifiedPermutationSampler]\n    (yeah, it's not pretty).\n\n    When `probs` is `False`, this method corrects rounding errors taking into\n    account the fractional parts so that the total number of samples is respected,\n    while allocating remainders in a way that follows the relative sizes of the\n    fractional parts.\n\n    Args:\n        n_indices: number of indices in the index set from which to sample. This is\n            typically `len(dataset) - 1` with the usual index iterations.\n        probs: Whether to perform the remainder distribution. If `True`, sampling\n            probabilities are returned. If `False`, then `n_samples_per_index` is\n            used to compute the actual number of samples and the values are rounded\n            down to the nearest integer, and the remainder is distributed to\n            maintain the relative frequencies.\n    Returns:\n        The exact (integer) number of samples to take for each set size, if\n        `probs` is `False`. Otherwise, the fractional number of samples.\n    \"\"\"\n\n    # m_k = m * f(k) / sum_j f(j)\n    values = np.zeros(n_indices + 1, dtype=float)\n    s = 0.0\n    lb, ub = self.effective_bounds(n_indices)\n\n    for k in range(lb, ub + 1):\n        val = self.fun(n_indices, k)\n        values[k] = val\n        s += val\n\n    assert n_indices == 0 or s &gt; 0, \"Sum of sample sizes must be positive\"\n    values /= s\n\n    if probs:\n        values.setflags(write=False)  # avoid accidental modification of cache\n        return values  # m_k / m\n\n    if self._n_samples_per_index is None:\n        normalization = min(values[values &gt; 0])\n        n_samples = np.sum(values / normalization).astype(int)  # min m_k = 1\n    elif self._n_samples_per_index == 0:\n        n_samples = np.ceil(s).astype(int)  # sum m_k = sum f_k\n    else:\n        n_samples = self._n_samples_per_index  # sum m_k = n_samples\n\n    values *= n_samples\n\n    # Round down and distribute remainder by adjusting the largest fractional parts\n    # A naive implementation with e.g.\n    #\n    # m_k = [max(1, int(round(m * f(k)/sum(f(j) for j in range(n)), 0)))\n    #         for k in range(n)]\n    #\n    # would not respect the total number of samples, and would not distribute\n    # remainders correctly\n    int_values: NDArray[np.int64] = np.floor(values).astype(np.int64)\n    remainder = n_samples - np.sum(int_values)\n    fractional_parts = values - int_values\n    fractional_parts_indices = np.argsort(-fractional_parts, kind=\"stable\")[\n        :remainder\n    ]\n    int_values[fractional_parts_indices] += 1\n    int_values.setflags(write=False)  # avoid accidental modification of cache\n    return int_values\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.RandomSizeIteration","title":"RandomSizeIteration","text":"<pre><code>RandomSizeIteration(\n    strategy: SampleSizeStrategy, n_indices: int, seed: Seed | None = None\n)\n</code></pre> <p>               Bases: <code>SampleSizeIteration</code></p> <p>Draws a set size \\(k\\) following the distribution of sizes given by the strategy.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self, strategy: SampleSizeStrategy, n_indices: int, seed: Seed | None = None\n):\n    super().__init__(strategy, n_indices)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.RoundRobinSizeIteration","title":"RoundRobinSizeIteration","text":"<pre><code>RoundRobinSizeIteration(strategy: SampleSizeStrategy, n_indices: int)\n</code></pre> <p>               Bases: <code>SampleSizeIteration</code></p> <p>Generates one sample for each set size \\(k\\) before moving to the next.</p> <p>This continues yielding until every size \\(k\\) has been emitted exactly \\(m_k\\) times. For example, if <code>strategy.sample_sizes() == [2, 3, 1]</code> then we want the sequence: (0,1), (1,1), (2,1), (0,1), (1,1), (1,1)</p> <p>Only for deterministic sample sizes</p> <p>This iteration is only valid for deterministic sample sizes. In particular, <code>n_samples</code> must be set to the total number of samples.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(self, strategy: SampleSizeStrategy, n_indices: int):\n    self.strategy = strategy\n    self.n_indices = n_indices\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.SampleSizeIteration","title":"SampleSizeIteration","text":"<pre><code>SampleSizeIteration(strategy: SampleSizeStrategy, n_indices: int)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Given a strategy and the number of indices, yield tuples (k, count) that the sampler loop will use. Args:     strategy: The strategy to use for computing the number of samples to take.     n_indices: The number of indices in the index set from which samples are taken.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(self, strategy: SampleSizeStrategy, n_indices: int):\n    self.strategy = strategy\n    self.n_indices = n_indices\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.SampleSizeStrategy","title":"SampleSizeStrategy","text":"<pre><code>SampleSizeStrategy(\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>An object to compute the number of samples to take for a given set size.</p> <p>To be used with StratifiedSampler.</p> <p>Following the structure proposed in Wu et al. (2023),<sup>2</sup> this sets the number of sets at size \\(k\\) to be:</p> \\[m(k) = m \\frac{f(k)}{\\sum_{j=0}^{n} f(j)},\\] <p>for some choice of \\(f.\\) Implementations of this base class must override the method <code>fun()</code> implementing \\(f\\). It is provided both the size \\(k\\) and the total number of indices \\(n\\) as arguments.</p> <p>The argument <code>n_samples</code> can be:</p> <ul> <li>Fixed to a positive integer. For powerset samplers, this is the number of samples   per index that will be generated, i.e. if the sampler iterates over each index   exactly once, e.g.   FiniteSequentialIndexIteration,   then the total number of samples will be <code>n_samples * n_indices</code>.</li> <li>Fixed to <code>0</code> to indicate the strategy produces an absolute number of samples,   this only used when subclassing and implementing a <code>fun</code> that does not return   frequencies.</li> <li><code>None</code> if only sampling probabilities are required, e.g. when using a stochastic   sampler and a   RandomSizeIteration.</li> </ul> PARAMETER DESCRIPTION <code>n_samples</code> <p>Number of samples for the stratified sampler to generate. It can be <code>None</code>, 0 or a positive integer. See the class documentation for details.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>lower_bound</code> <p>Lower bound for the set sizes. If the set size is smaller than this, the probability of sampling is 0. If <code>None</code>, the lower bound is set to 0.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>upper_bound</code> <p>Upper bound for the set size. If the set size is larger than this, the probability of sampling is 0. If <code>None</code>, the upper bound is set to the number of indices.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self,\n    n_samples: int | None = None,\n    lower_bound: int | None = None,\n    upper_bound: int | None = None,\n):\n    if n_samples is not None:\n        n_samples = validate_number(\"n_samples\", n_samples, int, lower=0)\n    if lower_bound is not None:\n        lower_bound = validate_number(\"lower_bound\", lower_bound, int, lower=0)\n    if upper_bound is not None:\n        upper_bound = validate_number(\n            \"upper_bound\", upper_bound, int, lower=lower_bound\n        )\n    self._n_samples_per_index = n_samples\n    self.lower_bound = lower_bound\n    self.upper_bound = upper_bound\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.SampleSizeStrategy.effective_bounds","title":"effective_bounds","text":"<pre><code>effective_bounds(n: int) -&gt; tuple[int, int]\n</code></pre> <p>Returns the effective bounds for the sample sizes, given the number of indices <code>n</code> from which sets are sampled.</p> <p>Note</p> <p>The number of indices <code>n</code> will typically be <code>complement_size(len(train))</code>, i.e. what we sometimes denote as <code>effective_n</code>.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of indices from which subsets are drawn.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     A tuple of [lower, upper] bounds for sample sizes (inclusive).</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def effective_bounds(self, n: int) -&gt; tuple[int, int]:\n    \"\"\"Returns the effective bounds for the sample sizes, given the number of\n    indices `n` from which sets are sampled.\n\n    !!! note\n        The number of indices `n` will typically be `complement_size(len(train))`,\n        i.e. what we sometimes denote as `effective_n`.\n\n    Args:\n        n: The number of indices from which subsets are drawn.\n    Returns:\n        A tuple of [lower, upper] bounds for sample sizes (inclusive).\n    \"\"\"\n    lower = 0 if self.lower_bound is None else self.lower_bound\n    upper = n if self.upper_bound is None else self.upper_bound\n    lower = min(lower, n)\n    upper = min(upper, n)\n\n    return lower, upper\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.SampleSizeStrategy.fun","title":"fun  <code>abstractmethod</code>","text":"<pre><code>fun(n_indices: int, subset_len: int) -&gt; float\n</code></pre> <p>The function \\(f\\) to use in the heuristic. Args:     n_indices: Size of the index set.     subset_len: Size of the subset.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>@abstractmethod\ndef fun(self, n_indices: int, subset_len: int) -&gt; float:\n    \"\"\"The function $f$ to use in the heuristic.\n    Args:\n        n_indices: Size of the index set.\n        subset_len: Size of the subset.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.SampleSizeStrategy.n_samples_per_index","title":"n_samples_per_index","text":"<pre><code>n_samples_per_index(n_indices: int) -&gt; int | None\n</code></pre> <p>Returns the total number of samples to take for the given number of indices, or <code>None</code> if no limit was set and samples are generated with probabilities.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def n_samples_per_index(self, n_indices: int) -&gt; int | None:\n    \"\"\"Returns the total number of samples to take for the given number of indices,\n    or `None` if no limit was set and samples are generated with probabilities.\"\"\"\n    if self._n_samples_per_index is None:\n        return None\n    sizes = self.sample_sizes(n_indices, probs=False)\n    return int(np.sum(sizes).item())\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.SampleSizeStrategy.sample_sizes","title":"sample_sizes  <code>cached</code>","text":"<pre><code>sample_sizes(\n    n_indices: int, probs: bool = True\n) -&gt; NDArray[int64] | NDArray[float64]\n</code></pre> <p>Precomputes the number of samples to take for each set size, from 0 up to <code>n_indices</code> inclusive.</p> <p>If <code>probs</code> is <code>True</code>, the result is a vector of floats, where each element is the probability of sampling a set of size \\(k.\\) This is useful e.g. for RandomSizeIteration where one needs frequencies. In this case <code>self.n_samples_per_index</code> can be <code>None</code>.</p> <p>If <code>probs</code> is <code>False</code>, the result is a vector of integers, where each element \\(k\\) is the number of samples to take for set size \\(k.\\) The sum of all elements will depend on the value of <code>n_samples</code> upon construction. It will be</p> <ul> <li>equal to <code>n_samples</code> if <code>n_samples &gt; 0</code>,</li> <li>the sum of the values of <code>fun</code> for all set sizes if <code>n_samples == 0</code>,</li> <li>the sum after a normalization such that the smallest non-zero value is 1, if   <code>n_samples == None</code>.</li> </ul> <p>This somewhat complex behavior is necessary to ensure that the total number of samples is respected when provided either globally upon construction, or implicitly by the sum of the values of <code>fun</code> when the strategy computes absolute numbers, but also when the strategy has a <code>fun</code> that returns frequencies. The special handling of the case <code>n_samples == None</code> is used implicitly by StratifiedPermutationSampler (yeah, it's not pretty).</p> <p>When <code>probs</code> is <code>False</code>, this method corrects rounding errors taking into account the fractional parts so that the total number of samples is respected, while allocating remainders in a way that follows the relative sizes of the fractional parts.</p> PARAMETER DESCRIPTION <code>n_indices</code> <p>number of indices in the index set from which to sample. This is typically <code>len(dataset) - 1</code> with the usual index iterations.</p> <p> TYPE: <code>int</code> </p> <code>probs</code> <p>Whether to perform the remainder distribution. If <code>True</code>, sampling probabilities are returned. If <code>False</code>, then <code>n_samples_per_index</code> is used to compute the actual number of samples and the values are rounded down to the nearest integer, and the remainder is distributed to maintain the relative frequencies.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Returns:     The exact (integer) number of samples to take for each set size, if     <code>probs</code> is <code>False</code>. Otherwise, the fractional number of samples.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>@cache\ndef sample_sizes(\n    self, n_indices: int, probs: bool = True\n) -&gt; NDArray[np.int64] | NDArray[np.float64]:\n    \"\"\"Precomputes the number of samples to take for each set size, from 0 up to\n    `n_indices` inclusive.\n\n    If `probs` is `True`, the result is a vector of floats, where each element\n    is the probability of sampling a set of size $k.$ This is useful e.g. for\n    [RandomSizeIteration][pydvl.valuation.samplers.stratified.RandomSizeIteration]\n    where one needs frequencies. In this case `self.n_samples_per_index` can be\n    `None`.\n\n    If `probs` is `False`, the result is a vector of integers, where each element\n    $k$ is the number of samples to take for set size $k.$ The sum of all elements\n    will depend on the value of `n_samples` upon construction. It will be\n\n    * equal to `n_samples` if `n_samples &gt; 0`,\n    * the sum of the values of `fun` for all set sizes if `n_samples == 0`,\n    * the sum after a normalization such that the smallest non-zero value is 1, if\n      `n_samples == None`.\n\n    This somewhat complex behavior is necessary to ensure that the total number of\n    samples is respected when provided either globally upon construction, or\n    implicitly by the sum of the values of `fun` when the strategy computes absolute\n    numbers, but also when the strategy has a `fun` that returns frequencies. The\n    special handling of the case `n_samples == None` is used implicitly by\n    [StratifiedPermutationSampler][pydvl.valuation.samplers.stratified.StratifiedPermutationSampler]\n    (yeah, it's not pretty).\n\n    When `probs` is `False`, this method corrects rounding errors taking into\n    account the fractional parts so that the total number of samples is respected,\n    while allocating remainders in a way that follows the relative sizes of the\n    fractional parts.\n\n    Args:\n        n_indices: number of indices in the index set from which to sample. This is\n            typically `len(dataset) - 1` with the usual index iterations.\n        probs: Whether to perform the remainder distribution. If `True`, sampling\n            probabilities are returned. If `False`, then `n_samples_per_index` is\n            used to compute the actual number of samples and the values are rounded\n            down to the nearest integer, and the remainder is distributed to\n            maintain the relative frequencies.\n    Returns:\n        The exact (integer) number of samples to take for each set size, if\n        `probs` is `False`. Otherwise, the fractional number of samples.\n    \"\"\"\n\n    # m_k = m * f(k) / sum_j f(j)\n    values = np.zeros(n_indices + 1, dtype=float)\n    s = 0.0\n    lb, ub = self.effective_bounds(n_indices)\n\n    for k in range(lb, ub + 1):\n        val = self.fun(n_indices, k)\n        values[k] = val\n        s += val\n\n    assert n_indices == 0 or s &gt; 0, \"Sum of sample sizes must be positive\"\n    values /= s\n\n    if probs:\n        values.setflags(write=False)  # avoid accidental modification of cache\n        return values  # m_k / m\n\n    if self._n_samples_per_index is None:\n        normalization = min(values[values &gt; 0])\n        n_samples = np.sum(values / normalization).astype(int)  # min m_k = 1\n    elif self._n_samples_per_index == 0:\n        n_samples = np.ceil(s).astype(int)  # sum m_k = sum f_k\n    else:\n        n_samples = self._n_samples_per_index  # sum m_k = n_samples\n\n    values *= n_samples\n\n    # Round down and distribute remainder by adjusting the largest fractional parts\n    # A naive implementation with e.g.\n    #\n    # m_k = [max(1, int(round(m * f(k)/sum(f(j) for j in range(n)), 0)))\n    #         for k in range(n)]\n    #\n    # would not respect the total number of samples, and would not distribute\n    # remainders correctly\n    int_values: NDArray[np.int64] = np.floor(values).astype(np.int64)\n    remainder = n_samples - np.sum(int_values)\n    fractional_parts = values - int_values\n    fractional_parts_indices = np.argsort(-fractional_parts, kind=\"stable\")[\n        :remainder\n    ]\n    int_values[fractional_parts_indices] += 1\n    int_values.setflags(write=False)  # avoid accidental modification of cache\n    return int_values\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutation","title":"StratifiedPermutation  <code>dataclass</code>","text":"<pre><code>StratifiedPermutation(\n    idx: IndexT | None,\n    subset: NDArray[IndexT],\n    lower_bound: int,\n    upper_bound: int,\n)\n</code></pre> <p>               Bases: <code>Sample</code></p> <p>A sample for the stratified permutation sampling strategy.</p> <p>This is a subclass of Sample which adds information about the set sizes to sample. It is used by StratifiedPermutationEvaluationStrategy to clip permutations to the required lengths.</p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutation.idx","title":"idx  <code>instance-attribute</code>","text":"<pre><code>idx: IndexT | None\n</code></pre> <p>Index of current sample</p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutation.lower_bound","title":"lower_bound  <code>instance-attribute</code>","text":"<pre><code>lower_bound: int\n</code></pre> <p>The lower bound for the set sizes.</p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutation.subset","title":"subset  <code>instance-attribute</code>","text":"<pre><code>subset: NDArray[IndexT]\n</code></pre> <p>Indices of current sample</p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutation.upper_bound","title":"upper_bound  <code>instance-attribute</code>","text":"<pre><code>upper_bound: int\n</code></pre> <p>The upper bound for the set sizes.</p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutation.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> <p>This type must be hashable for the utility caching to work. We use hashlib.sha256 which is about 4-5x faster than hash(), and returns the same value in all processes, as opposed to hash() which is salted in each process</p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def __hash__(self):\n    \"\"\"This type must be hashable for the utility caching to work.\n    We use hashlib.sha256 which is about 4-5x faster than hash(), and returns the\n    same value in all processes, as opposed to hash() which is salted in each\n    process\n    \"\"\"\n    sha256_hash = hashlib.sha256(self.subset.tobytes()).hexdigest()\n    return int(sha256_hash, base=16)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutation.with_idx","title":"with_idx","text":"<pre><code>with_idx(idx: IndexT) -&gt; Self\n</code></pre> <p>Return a copy of sample with idx changed.</p> <p>Returns the original sample if idx is the same.</p> PARAMETER DESCRIPTION <code>idx</code> <p>New value for idx.</p> <p> TYPE: <code>IndexT</code> </p> RETURNS DESCRIPTION <code>Sample</code> <p>A copy of the sample with idx changed.</p> <p> TYPE: <code>Self</code> </p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def with_idx(self, idx: IndexT) -&gt; Self:\n    \"\"\"Return a copy of sample with idx changed.\n\n    Returns the original sample if idx is the same.\n\n    Args:\n        idx: New value for idx.\n\n    Returns:\n        Sample: A copy of the sample with idx changed.\n    \"\"\"\n    if self.idx == idx:\n        return self\n\n    return replace(self, idx=idx)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutation.with_idx_in_subset","title":"with_idx_in_subset","text":"<pre><code>with_idx_in_subset() -&gt; Self\n</code></pre> <p>Return a copy of sample with idx added to the subset.</p> <p>Returns the original sample if idx was already part of the subset.</p> RETURNS DESCRIPTION <code>Sample</code> <p>A copy of the sample with idx added to the subset.</p> <p> TYPE: <code>Self</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If idx is None.</p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def with_idx_in_subset(self) -&gt; Self:\n    \"\"\"Return a copy of sample with idx added to the subset.\n\n    Returns the original sample if idx was already part of the subset.\n\n    Returns:\n        Sample: A copy of the sample with idx added to the subset.\n\n    Raises:\n        ValueError: If idx is None.\n    \"\"\"\n    if self.idx in self.subset:\n        return self\n\n    if self.idx is None:\n        raise ValueError(\"Cannot add idx to subset if idx is None.\")\n\n    new_subset = np.append(self.subset, self.idx)\n    return replace(self, subset=new_subset)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutation.with_subset","title":"with_subset","text":"<pre><code>with_subset(subset: NDArray[IndexT]) -&gt; Self\n</code></pre> <p>Return a copy of sample with subset changed.</p> <p>Returns the original sample if subset is the same.</p> PARAMETER DESCRIPTION <code>subset</code> <p>New value for subset.</p> <p> TYPE: <code>NDArray[IndexT]</code> </p> RETURNS DESCRIPTION <code>Sample</code> <p>A copy of the sample with subset changed.</p> <p> TYPE: <code>Self</code> </p> Source code in <code>src/pydvl/valuation/types.py</code> <pre><code>def with_subset(self, subset: NDArray[IndexT]) -&gt; Self:\n    \"\"\"Return a copy of sample with subset changed.\n\n    Returns the original sample if subset is the same.\n\n    Args:\n        subset: New value for subset.\n\n    Returns:\n        Sample: A copy of the sample with subset changed.\n    \"\"\"\n    if np.array_equal(self.subset, subset):\n        return self\n\n    return replace(self, subset=subset)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutationEvaluationStrategy","title":"StratifiedPermutationEvaluationStrategy","text":"<pre><code>StratifiedPermutationEvaluationStrategy(\n    sampler: PermutationSamplerBase,\n    utility: UtilityBase,\n    coefficient: SemivalueCoefficient | None,\n)\n</code></pre> <p>               Bases: <code>PermutationEvaluationStrategy</code></p> <p>Evaluation strategy for the StratifiedPermutationSampler.</p> <p>Experimental</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def __init__(\n    self,\n    sampler: PermutationSamplerBase,\n    utility: UtilityBase,\n    coefficient: SemivalueCoefficient | None,\n):\n    super().__init__(utility, coefficient)\n    self.truncation = copy(sampler.truncation)\n    self.truncation.reset(utility)  # Perform initial setup (e.g. total_utility)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutationSampler","title":"StratifiedPermutationSampler","text":"<pre><code>StratifiedPermutationSampler(\n    sample_sizes: SampleSizeStrategy,\n    truncation: TruncationPolicy | None = None,\n    seed: Seed | None = None,\n    batch_size: int = 1,\n)\n</code></pre> <p>               Bases: <code>PermutationSampler</code></p> <p>A stratified permutation sampler.</p> <p>Experimental</p> <p>This is just an approximation for now. The number of set sizes generated is only roughly equal to that specified by the SampleSizeStrategy. In particular, there is a single counter of sizes for all indices.</p> PARAMETER DESCRIPTION <code>sample_sizes</code> <p>An object which returns the number of samples to take for a given set size. If the strategy fixes a total number of samples for each set size, then this sampler will produce (approximately) that amount of samples for each index. If it does not, then the sampler will generate infinitely many samples, with set sizes following the distribution set by the strategy.</p> <p> TYPE: <code>SampleSizeStrategy</code> </p> <code>truncation</code> <p>A policy to stop the permutation early.</p> <p> TYPE: <code>TruncationPolicy | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Seed for the random number generator.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>The number of samples (full permutations) to generate at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self,\n    sample_sizes: SampleSizeStrategy,\n    truncation: TruncationPolicy | None = None,\n    seed: Seed | None = None,\n    batch_size: int = 1,\n):\n    super().__init__(truncation, seed, batch_size)\n    self.sample_sizes_strategy = sample_sizes\n    logger.warning(\n        \"StratifiedPermutationSampler is experimental and inexact. \"\n        \"Please use another sampler if you are benchmarking methods.\"\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutationSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutationSampler.complement_size","title":"complement_size","text":"<pre><code>complement_size(n: int) -&gt; int\n</code></pre> <p>Size of the complement of an index wrt. set size <code>n</code>.</p> <p>Required in certain coefficient computations. Even though we are sampling permutations, updates are always done per-index and the size of the complement is always \\(n-1\\).</p> Source code in <code>src/pydvl/valuation/samplers/permutation.py</code> <pre><code>def complement_size(self, n: int) -&gt; int:\n    \"\"\"Size of the complement of an index wrt. set size `n`.\n\n    Required in certain coefficient computations. Even though we are sampling\n    permutations, updates are always done per-index and the size of the complement\n    is always $n-1$.\n    \"\"\"\n    return n - 1\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutationSampler.generate","title":"generate","text":"<pre><code>generate(indices: IndexSetT) -&gt; SampleGenerator[StratifiedPermutation]\n</code></pre> <p>Generates the permutation samples.</p> <p>These samples include information as to what sample sizes can be taken from the permutation by the strategy.</p> <p>Info</p> <p>This generator ignores <code>skip_indices</code>.</p> PARAMETER DESCRIPTION <code>indices</code> <p>The indices to sample from. If empty, no samples are generated.</p> <p> TYPE: <code>IndexSetT</code> </p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def generate(self, indices: IndexSetT) -&gt; SampleGenerator[StratifiedPermutation]:\n    \"\"\"Generates the permutation samples.\n\n    These samples include information as to what sample sizes can be taken from the\n    permutation by the strategy.\n\n    !!! info\n        This generator ignores `skip_indices`.\n\n    Args:\n        indices: The indices to sample from. If empty, no samples are generated.\n    \"\"\"\n    n = len(indices)\n    if n == 0:\n        return\n\n    sizes = self.sample_sizes_strategy.sample_sizes(n, probs=False)\n    # FIXME: This is just an approximation. On expectation we should produce roughly\n    #   the correct number of sizes per index, but we should probably keep track\n    #   separately.\n    sizes = sizes * n  # Need a copy because the method is @cached\n\n    while True:\n        # Can't have skip indices: if the index set is smaller than the lower bound\n        # for the set sizes, the strategy's process() will always return empty\n        # evaluations and the method might never stop with criteria depending on the\n        # number of updates\n        # _indices = np.setdiff1d(indices, self.skip_indices)\n\n        positive = np.where(sizes &gt; 0)[0]\n        if len(positive) == 0:  # Restart\n            sizes = self.sample_sizes_strategy.sample_sizes(n, probs=False)\n            sizes = sizes * n\n            continue\n        lb, ub = int(positive[0]), int(positive[-1])\n        # FIXME: do we really want to restrict the strategies to be monotonic?\n        assert all(sizes[lb : ub + 1] &gt; 0), \"Sample size function must be monotonic\"\n        sizes = np.maximum(sizes - 1, 0)\n\n        yield StratifiedPermutation(\n            idx=None,\n            subset=self._rng.permutation(indices),\n            lower_bound=lb,\n            upper_bound=ub,\n        )\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutationSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutationSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutationSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>The probability of sampling a set of size <code>subset_len</code> from <code>n</code> indices.</p> <p>See StratifiedSampler.log_weight()</p> PARAMETER DESCRIPTION <code>n</code> <p>Size of the index set.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>Size of the subset.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The logarithm of the probability of having sampled a set of size <code>subset_len</code>.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    \"\"\"The probability of sampling a set of size `subset_len` from `n` indices.\n\n    See\n    [StratifiedSampler.log_weight()][pydvl.valuation.samplers.stratified.StratifiedSampler.log_weight]\n\n    Args:\n        n:  Size of the index set.\n        subset_len: Size of the subset.\n\n    Returns:\n        The logarithm of the probability of having sampled a set of size\n            `subset_len`.\n    \"\"\"\n    effective_n = self.complement_size(n)\n    p = self.sample_sizes_strategy.sample_sizes(effective_n, probs=True)\n    p_k = p[subset_len]\n    if p_k == 0:\n        return -np.inf\n\n    return float(np.log(p_k) - logcomb(effective_n, subset_len))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedPermutationSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedSampler","title":"StratifiedSampler","text":"<pre><code>StratifiedSampler(\n    sample_sizes: SampleSizeStrategy,\n    sample_sizes_iteration: Type[\n        SampleSizeIteration\n    ] = FiniteSequentialSizeIteration,\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = FiniteSequentialIndexIteration,\n    seed: Seed | None = None,\n)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>PowersetSampler</code></p> <p>A sampler stratified by coalition size with variable number of samples per set size.</p> PARAMETER DESCRIPTION <code>sample_sizes</code> <p>An object which returns the number of samples to take for a given set size. If <code>index_iteration</code> below is finite, then the sampler will generate exactly as many samples of each size as returned by this object. If the iteration is infinite, then the <code>sample_sizes</code> will be used as probabilities of sampling.</p> <p> TYPE: <code>SampleSizeStrategy</code> </p> <code>sample_sizes_iteration</code> <p>How to loop over sample sizes. The main modes are: * deterministically. For every k generate m_k samples before moving to k+1. * stochastically. Sample sizes k according to the distribution given by   <code>sample_sizes</code>. * round-robin. Iterate over k, and generate 1 sample each time, until   reaching m_k. But more can be created by subclassing SampleSizeIteration.</p> <p> TYPE: <code>Type[SampleSizeIteration]</code> DEFAULT: <code>FiniteSequentialSizeIteration</code> </p> <code>batch_size</code> <p>The number of samples to generate per batch. Batches are processed together by each subprocess when working in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>index_iteration</code> <p>the strategy to use for iterating over indices to update.</p> <p> TYPE: <code>Type[IndexIteration]</code> DEFAULT: <code>FiniteSequentialIndexIteration</code> </p> <code>seed</code> <p>The seed for the random number generator.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <p>New in version 0.10.0</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self,\n    sample_sizes: SampleSizeStrategy,\n    sample_sizes_iteration: Type[\n        SampleSizeIteration\n    ] = FiniteSequentialSizeIteration,\n    batch_size: int = 1,\n    index_iteration: Type[IndexIteration] = FiniteSequentialIndexIteration,\n    seed: Seed | None = None,\n):\n    super().__init__(\n        batch_size=batch_size, index_iteration=index_iteration, seed=seed\n    )\n    self.sample_sizes_strategy = sample_sizes\n    self.sample_sizes_iteration = maybe_add_argument(sample_sizes_iteration, \"seed\")\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices\n</code></pre> <p>Set of indices to skip in the outer loop.</p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedSampler.index_iterable","title":"index_iterable","text":"<pre><code>index_iterable(indices: IndexSetT) -&gt; Generator[IndexT | None, None, None]\n</code></pre> <p>Iterates over indices with the method specified at construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def index_iterable(\n    self, indices: IndexSetT\n) -&gt; Generator[IndexT | None, None, None]:\n    \"\"\"Iterates over indices with the method specified at construction.\"\"\"\n    try:\n        iterable = self._index_iterator_cls(indices, seed=self._rng)  # type: ignore\n    except (AttributeError, TypeError):\n        iterable = self._index_iterator_cls(indices)\n    for idx in iterable:\n        if idx not in self.skip_indices:\n            yield idx\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>The probability of sampling a set of size k is 1/(n choose k) times the probability of the set having size k, which is the number of samples for that size divided by the total number of samples for all sizes:</p> \\[P(S) = \\binom{n}{k}^{-1} \\ \\frac{m_k}{m},\\] <p>where \\(m_k\\) is the number of samples of size \\(k\\) and \\(m\\) is the total number of samples.</p> PARAMETER DESCRIPTION <code>n</code> <p>Size of the index set.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>Size of the subset.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     The logarithm of the probability of having sampled a set of size <code>subset_len</code>.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    r\"\"\"The probability of sampling a set of size k is 1/(n choose k) times the\n    probability of the set having size k, which is the number of samples for that\n    size divided by the total number of samples for all sizes:\n\n    $$P(S) = \\binom{n}{k}^{-1} \\ \\frac{m_k}{m},$$\n\n    where $m_k$ is the number of samples of size $k$ and $m$ is the total number\n    of samples.\n\n    Args:\n        n: Size of the index set.\n        subset_len: Size of the subset.\n    Returns:\n        The logarithm of the probability of having sampled a set of size `subset_len`.\n    \"\"\"\n\n    effective_n = self.complement_size(n)\n\n    # Note that we can simplify the quotient\n    # $$ \\frac{m_k}{m} =\n    #    \\frac{m \\frac{f (k)}{\\sum_j f (j)}}{m} = \\frac{f(k)}{\\sum_j f (j)} $$\n    # so that in the weight computation we can use the function $f$ directly from\n    # the strategy, or equivalently, call `sample_sizes(n, probs=True)`.\n    # This is useful for the stochastic iteration, where we are given sampling\n    # frequencies for each size instead of counts, and the total number of samples\n    # m is 1, so that quantization would yield a bunch of zeros.\n    p = self.sample_sizes_strategy.sample_sizes(effective_n, probs=True)\n    p_k = p[subset_len]  # also m_k / m\n    if p_k == 0:\n        return -np.inf\n\n    return float(np.log(p_k) - logcomb(effective_n, subset_len))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.StratifiedSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.VRDSSampler","title":"VRDSSampler","text":"<pre><code>VRDSSampler(\n    n_samples_per_index: int, batch_size: int = 1, seed: Seed | None = None\n)\n</code></pre> <p>               Bases: <code>StratifiedSampler</code></p> <p>A sampler stratified by coalition size with variable number of samples per set size.</p> <p>This sampler iterates once per index and generates a fixed mount of subsets of each size in its complement.</p> <p>This is a convenience subclass of StratifiedSampler which implements the VRDS heuristic from Wu et al. (2023)<sup>2</sup>.</p> <p>It is functionally equivalent to a StratifiedSampler with HarmonicSampleSize, FiniteSequentialSizeIteration, and FiniteSequentialIndexIteration.</p> PARAMETER DESCRIPTION <code>n_samples_per_index</code> <p>The number of samples to generate per index. To compute with the (\u03b5,\u03b4) bound from the paper, use min_samples(). The distribution per set size will follow a harmonic function, as defined in HarmonicSampleSize.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The number of samples to generate per batch. Batches are processed together by each subprocess when working in parallel.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>The seed for the random number generator.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def __init__(\n    self,\n    n_samples_per_index: int,\n    batch_size: int = 1,\n    seed: Seed | None = None,\n):\n    super().__init__(\n        sample_sizes=HarmonicSampleSize(n_samples=n_samples_per_index),\n        sample_sizes_iteration=FiniteSequentialSizeIteration,\n        batch_size=batch_size,\n        index_iteration=FiniteSequentialIndexIteration,\n        seed=seed,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.VRDSSampler.skip_indices","title":"skip_indices  <code>property</code> <code>writable</code>","text":"<pre><code>skip_indices\n</code></pre> <p>Set of indices to skip in the outer loop.</p>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.VRDSSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the length of the current sample generation in generate_batches.</p> RAISES DESCRIPTION <code>`TypeError`</code> <p>if the sampler is infinite or generate_batches has not been called yet.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the length of the current sample generation in generate_batches.\n\n    Raises:\n        `TypeError`: if the sampler is infinite or\n            [generate_batches][pydvl.valuation.samplers.IndexSampler.generate_batches]\n            has not been called yet.\n    \"\"\"\n    if self._len is None:\n        raise TypeError(f\"This {self.__class__.__name__} has no length\")\n    return self._len\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.VRDSSampler.generate_batches","title":"generate_batches","text":"<pre><code>generate_batches(indices: IndexSetT) -&gt; BatchGenerator\n</code></pre> <p>Batches the samples and yields them.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def generate_batches(self, indices: IndexSetT) -&gt; BatchGenerator:\n    \"\"\"Batches the samples and yields them.\"\"\"\n    self._len = self.sample_limit(indices)\n\n    # Create an empty generator if the indices are empty: `return` acts like a\n    # `break`, and produces an empty generator.\n    if len(indices) == 0:\n        return\n\n    self._interrupted = False\n    self._n_samples = 0\n    for batch in chunked(self.generate(indices), self.batch_size):\n        self._n_samples += len(batch)\n        yield batch\n        if self._interrupted:\n            break\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.VRDSSampler.index_iterable","title":"index_iterable","text":"<pre><code>index_iterable(indices: IndexSetT) -&gt; Generator[IndexT | None, None, None]\n</code></pre> <p>Iterates over indices with the method specified at construction.</p> Source code in <code>src/pydvl/valuation/samplers/powerset.py</code> <pre><code>def index_iterable(\n    self, indices: IndexSetT\n) -&gt; Generator[IndexT | None, None, None]:\n    \"\"\"Iterates over indices with the method specified at construction.\"\"\"\n    try:\n        iterable = self._index_iterator_cls(indices, seed=self._rng)  # type: ignore\n    except (AttributeError, TypeError):\n        iterable = self._index_iterator_cls(indices)\n    for idx in iterable:\n        if idx not in self.skip_indices:\n            yield idx\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.VRDSSampler.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Signals the sampler to stop generating samples after the current batch.</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def interrupt(self):\n    \"\"\"Signals the sampler to stop generating samples after the current batch.\"\"\"\n    self._interrupted = True\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.VRDSSampler.log_weight","title":"log_weight","text":"<pre><code>log_weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>The probability of sampling a set of size k is 1/(n choose k) times the probability of the set having size k, which is the number of samples for that size divided by the total number of samples for all sizes:</p> \\[P(S) = \\binom{n}{k}^{-1} \\ \\frac{m_k}{m},\\] <p>where \\(m_k\\) is the number of samples of size \\(k\\) and \\(m\\) is the total number of samples.</p> PARAMETER DESCRIPTION <code>n</code> <p>Size of the index set.</p> <p> TYPE: <code>int</code> </p> <code>subset_len</code> <p>Size of the subset.</p> <p> TYPE: <code>int</code> </p> <p>Returns:     The logarithm of the probability of having sampled a set of size <code>subset_len</code>.</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>def log_weight(self, n: int, subset_len: int) -&gt; float:\n    r\"\"\"The probability of sampling a set of size k is 1/(n choose k) times the\n    probability of the set having size k, which is the number of samples for that\n    size divided by the total number of samples for all sizes:\n\n    $$P(S) = \\binom{n}{k}^{-1} \\ \\frac{m_k}{m},$$\n\n    where $m_k$ is the number of samples of size $k$ and $m$ is the total number\n    of samples.\n\n    Args:\n        n: Size of the index set.\n        subset_len: Size of the subset.\n    Returns:\n        The logarithm of the probability of having sampled a set of size `subset_len`.\n    \"\"\"\n\n    effective_n = self.complement_size(n)\n\n    # Note that we can simplify the quotient\n    # $$ \\frac{m_k}{m} =\n    #    \\frac{m \\frac{f (k)}{\\sum_j f (j)}}{m} = \\frac{f(k)}{\\sum_j f (j)} $$\n    # so that in the weight computation we can use the function $f$ directly from\n    # the strategy, or equivalently, call `sample_sizes(n, probs=True)`.\n    # This is useful for the stochastic iteration, where we are given sampling\n    # frequencies for each size instead of counts, and the total number of samples\n    # m is 1, so that quantization would yield a bunch of zeros.\n    p = self.sample_sizes_strategy.sample_sizes(effective_n, probs=True)\n    p_k = p[subset_len]  # also m_k / m\n    if p_k == 0:\n        return -np.inf\n\n    return float(np.log(p_k) - logcomb(effective_n, subset_len))\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.VRDSSampler.min_samples","title":"min_samples  <code>staticmethod</code>","text":"<pre><code>min_samples(n_indices: int, eps: float = 0.01, delta: float = 0.05) -&gt; int\n</code></pre> <p>Computes the minimal amount of samples for an (\u03b5,\u03b4)-approximation of data Shapley.</p> <p>This is the bound shown in Theorem 4.3 of Wu et al. (2023)<sup>2</sup>.</p> PARAMETER DESCRIPTION <code>n_indices</code> <p>The number of indices in the index set.</p> <p> TYPE: <code>int</code> </p> <code>eps</code> <p>The epsilon value in the epsilon-delta guarantee, i.e. the distance to the true value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>delta</code> <p>The delta value in the epsilon-delta guarantee, i.e. the probability of failure.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <p>Returns:     \\((2 \\log(2/\\delta) / \\epsilon^2) \\log(n + 1)^2.\\)</p> Source code in <code>src/pydvl/valuation/samplers/stratified.py</code> <pre><code>@staticmethod\ndef min_samples(n_indices: int, eps: float = 0.01, delta: float = 0.05) -&gt; int:\n    r\"\"\"Computes the minimal amount of samples for an (\u03b5,\u03b4)-approximation of data\n    Shapley.\n\n    This is the bound shown in Theorem 4.3 of Wu et al. (2023)&lt;sup&gt;&lt;a\n    href=\"#wu_variance_2023\"&gt;2&lt;/a&gt;&lt;/sup&gt;.\n\n    Args:\n        n_indices: The number of indices in the index set.\n        eps: The epsilon value in the epsilon-delta guarantee, i.e. the distance to the\n            true value.\n        delta: The delta value in the epsilon-delta guarantee, i.e. the probability of\n            failure.\n    Returns:\n        $(2 \\log(2/\\delta) / \\epsilon^2) \\log(n + 1)^2.$\n    \"\"\"\n    m = 2 * (np.log(2) - np.log(delta)) / eps**2\n    m *= (np.log(n_indices) + 1) ** 2\n    return int(np.ceil(m).item())\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/stratified/#pydvl.valuation.samplers.stratified.VRDSSampler.result_updater","title":"result_updater","text":"<pre><code>result_updater(result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]\n</code></pre> <p>Returns an object that updates a valuation result with a value update.</p> <p>Because we use log-space computation for numerical stability, the default result updater keeps track of several quantities required to maintain accurate running 1st and 2nd moments.</p> PARAMETER DESCRIPTION <code>result</code> <p>The result to update</p> <p> TYPE: <code>ValuationResult</code> </p> <p>Returns:     A callable object that updates the result with a value update</p> Source code in <code>src/pydvl/valuation/samplers/base.py</code> <pre><code>def result_updater(self, result: ValuationResult) -&gt; ResultUpdater[ValueUpdateT]:\n    \"\"\"Returns an object that updates a valuation result with a value update.\n\n    Because we use log-space computation for numerical stability, the default result\n    updater keeps track of several quantities required to maintain accurate running\n    1st and 2nd moments.\n\n    Args:\n        result: The result to update\n    Returns:\n        A callable object that updates the result with a value update\n    \"\"\"\n    return LogResultUpdater(result)\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/truncation/","title":"Truncation","text":""},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation","title":"pydvl.valuation.samplers.truncation","text":"<p>Truncation policies for the interruption of batched computations.</p> <p>When estimating values with a PermutationSampler, it is possible to interrupt the marginal utility updates for one permutation based on different heuristics. A naive approach is to stop after a fixed number of updates, using FixedTruncation.</p> <p>However, a more successful one is to stop if the utility of the current batch of samples is close enough to the total utility of the dataset. This idea is implemented in RelativeTruncation, and was introduced as Truncated Montecarlo Shapley (TMCS) in Ghobani and Zou (2019)<sup>1</sup>. A slight variation is to use the standard deviation of the utilities to set the tolerance, which can be done with DeviationTruncation.</p> <p>Stopping too early</p> <p>Truncation policies can lead to underestimation of values if the utility function has high variance. To avoid this, one can set a burn-in period before checking the utility, or use a policy that is less sensitive to variance.</p>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation--references","title":"References","text":"<ol> <li> <p>Ghorbani, A., Zou, J., 2019. Data Shapley: Equitable Valuation of Data for Machine Learning. In: Proceedings of the 36th International Conference on Machine Learning, PMLR, pp. 2242\u20132251.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation.DeviationTruncation","title":"DeviationTruncation","text":"<pre><code>DeviationTruncation(sigmas: float, burn_in_fraction: float = 0.0)\n</code></pre> <p>               Bases: <code>TruncationPolicy</code></p> <p>Break a computation if the last computed utility is close to the total utility.</p> <p>This is essentially the same as RelativeTruncation, but with the tolerance determined by a multiple of the standard deviation of the utilities.</p> <p>Danger</p> <p>This policy can break early if the utility function has high variance. This can lead to gross underestimation of values. Use with caution.</p> <p>Warning</p> <p>Initialization and <code>reset()</code> of this policy imply the computation of the total utility for the dataset, which can be expensive!</p> PARAMETER DESCRIPTION <code>burn_in_fraction</code> <p>Fraction of samples within a permutation to wait until actually checking.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sigmas</code> <p>Number of standard deviations to use as a threshold.</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>def __init__(self, sigmas: float, burn_in_fraction: float = 0.0):\n    super().__init__()\n    assert 0 &lt;= burn_in_fraction &lt;= 1\n\n    self.burn_in_fraction = burn_in_fraction\n    self.total_utility = 0.0\n    self.count = 0  # within-permutation count\n    self.variance = 0.0\n    self.mean = 0.0\n    self.sigmas = sigmas\n    self._is_setup = False\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation.FixedTruncation","title":"FixedTruncation","text":"<pre><code>FixedTruncation(fraction: float)\n</code></pre> <p>               Bases: <code>TruncationPolicy</code></p> <p>Break a computation after a fixed number of updates.</p> <p>The experiments in Appendix B of (Ghorbani and Zou, 2019)<sup>1</sup> show that when the training set size is large enough, one can simply truncate the iteration over permutations after a fixed number of steps. This happens because beyond a certain number of samples in a training set, the model becomes insensitive to new ones. Alas, this strongly depends on the data distribution and the model and there is no automatic way of estimating this number.</p> PARAMETER DESCRIPTION <code>fraction</code> <p>Fraction of updates in a batch to compute before stopping (e.g. 0.5 to compute half of the marginals in a permutation).</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>def __init__(self, fraction: float):\n    super().__init__()\n    if fraction &lt;= 0 or fraction &gt; 1:\n        raise ValueError(\"fraction must be in (0, 1]\")\n    self.fraction = fraction\n    self.count = 0  # within-permutation count\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation.FixedTruncation.__call__","title":"__call__","text":"<pre><code>__call__(idx: IndexT, score: float, batch_size: int) -&gt; bool\n</code></pre> <p>Check whether the computation should be interrupted.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Position in the batch currently being computed.</p> <p> TYPE: <code>IndexT</code> </p> <code>score</code> <p>Last utility computed.</p> <p> TYPE: <code>float</code> </p> <code>batch_size</code> <p>Size of the batch being computed.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the computation should be interrupted.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>def __call__(self, idx: IndexT, score: float, batch_size: int) -&gt; bool:\n    \"\"\"Check whether the computation should be interrupted.\n\n    Args:\n        idx: Position in the batch currently being computed.\n        score: Last utility computed.\n        batch_size: Size of the batch being computed.\n\n    Returns:\n        `True` if the computation should be interrupted.\n    \"\"\"\n\n    ret = self._check(idx, score, batch_size)\n    self.n_calls += 1\n    self.n_truncations += 1 if ret else 0\n    return ret\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation.NoTruncation","title":"NoTruncation","text":"<pre><code>NoTruncation()\n</code></pre> <p>               Bases: <code>TruncationPolicy</code></p> <p>A policy which never interrupts the computation.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.n_calls: int = 0\n    self.n_truncations: int = 0\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation.NoTruncation.__call__","title":"__call__","text":"<pre><code>__call__(idx: IndexT, score: float, batch_size: int) -&gt; bool\n</code></pre> <p>Check whether the computation should be interrupted.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Position in the batch currently being computed.</p> <p> TYPE: <code>IndexT</code> </p> <code>score</code> <p>Last utility computed.</p> <p> TYPE: <code>float</code> </p> <code>batch_size</code> <p>Size of the batch being computed.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the computation should be interrupted.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>def __call__(self, idx: IndexT, score: float, batch_size: int) -&gt; bool:\n    \"\"\"Check whether the computation should be interrupted.\n\n    Args:\n        idx: Position in the batch currently being computed.\n        score: Last utility computed.\n        batch_size: Size of the batch being computed.\n\n    Returns:\n        `True` if the computation should be interrupted.\n    \"\"\"\n\n    ret = self._check(idx, score, batch_size)\n    self.n_calls += 1\n    self.n_truncations += 1 if ret else 0\n    return ret\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation.RelativeTruncation","title":"RelativeTruncation","text":"<pre><code>RelativeTruncation(rtol: float, burn_in_fraction: float = 0.0)\n</code></pre> <p>               Bases: <code>TruncationPolicy</code></p> <p>Break a computation if the utility is close enough to the total utility.</p> <p>This is called \"performance tolerance\" in (Ghorbani and Zou, 2019)<sup>1</sup>.</p> <p>Warning</p> <p>Initialization and <code>reset()</code> of this policy imply the computation of the total utility for the dataset, which can be expensive!</p> PARAMETER DESCRIPTION <code>rtol</code> <p>Relative tolerance. The permutation is broken if the last computed utility is within this tolerance of the total utility.</p> <p> TYPE: <code>float</code> </p> <code>burn_in_fraction</code> <p>Fraction of samples within a permutation to wait until actually checking.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>def __init__(self, rtol: float, burn_in_fraction: float = 0.0):\n    super().__init__()\n    assert 0 &lt;= burn_in_fraction &lt;= 1\n    self.burn_in_fraction = burn_in_fraction\n    self.rtol = rtol\n    self.total_utility = 0.0\n    self.count = 0  # within-permutation count\n    self._is_setup = False\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation.TruncationPolicy","title":"TruncationPolicy","text":"<pre><code>TruncationPolicy()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A policy for deciding whether to stop computation of a batch of samples</p> <p>Statistics are kept on the total number of calls and truncations as <code>n_calls</code> and <code>n_truncations</code> respectively.</p> ATTRIBUTE DESCRIPTION <code>n_calls</code> <p>Number of calls to the policy.</p> <p> TYPE: <code>int</code> </p> <code>n_truncations</code> <p>Number of truncations made by the policy.</p> <p> TYPE: <code>int</code> </p> <p>Todo</p> <p>Because the policy objects are copied to the workers, the statistics are not accessible from the coordinating process. We need to add methods for this.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.n_calls: int = 0\n    self.n_truncations: int = 0\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation.TruncationPolicy.__call__","title":"__call__","text":"<pre><code>__call__(idx: IndexT, score: float, batch_size: int) -&gt; bool\n</code></pre> <p>Check whether the computation should be interrupted.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Position in the batch currently being computed.</p> <p> TYPE: <code>IndexT</code> </p> <code>score</code> <p>Last utility computed.</p> <p> TYPE: <code>float</code> </p> <code>batch_size</code> <p>Size of the batch being computed.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the computation should be interrupted.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>def __call__(self, idx: IndexT, score: float, batch_size: int) -&gt; bool:\n    \"\"\"Check whether the computation should be interrupted.\n\n    Args:\n        idx: Position in the batch currently being computed.\n        score: Last utility computed.\n        batch_size: Size of the batch being computed.\n\n    Returns:\n        `True` if the computation should be interrupted.\n    \"\"\"\n\n    ret = self._check(idx, score, batch_size)\n    self.n_calls += 1\n    self.n_truncations += 1 if ret else 0\n    return ret\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation.TruncationPolicy._check","title":"_check  <code>abstractmethod</code>","text":"<pre><code>_check(idx: IndexT, score: float, batch_size: int) -&gt; bool\n</code></pre> <p>Implement the policy.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>@abstractmethod\ndef _check(self, idx: IndexT, score: float, batch_size: int) -&gt; bool:\n    \"\"\"Implement the policy.\"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/truncation/#pydvl.valuation.samplers.truncation.TruncationPolicy.reset","title":"reset  <code>abstractmethod</code>","text":"<pre><code>reset(utility: UtilityBase)\n</code></pre> <p>(Re)set the policy to a state ready for a new permutation.</p> Source code in <code>src/pydvl/valuation/samplers/truncation.py</code> <pre><code>@abstractmethod\ndef reset(self, utility: UtilityBase):\n    \"\"\"(Re)set the policy to a state ready for a new permutation.\"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/samplers/utils/","title":"Utils","text":""},{"location":"api/pydvl/valuation/samplers/utils/#pydvl.valuation.samplers.utils","title":"pydvl.valuation.samplers.utils","text":"<p>This module contains mixin classes.</p> <p>Currently only one for samplers which use a random number generator.</p>"},{"location":"api/pydvl/valuation/samplers/utils/#pydvl.valuation.samplers.utils.StochasticSamplerMixin","title":"StochasticSamplerMixin","text":"<pre><code>StochasticSamplerMixin(*args, seed: Seed | None = None, **kwargs)\n</code></pre> <p>Mixin class for samplers which use a random number generator. Args:     seed: Seed for the random number generator. Passed to         numpy.random.default_rng.</p> Source code in <code>src/pydvl/valuation/samplers/utils.py</code> <pre><code>def __init__(self, *args, seed: Seed | None = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"api/pydvl/valuation/scorers/","title":"Scorers","text":""},{"location":"api/pydvl/valuation/scorers/#pydvl.valuation.scorers","title":"pydvl.valuation.scorers","text":"<p>Scorers are a fundamental building block of many data valuation methods. They are typically used by ModelUtility and its subclasses to evaluate the quality of a model when trained on subsets of the training data.</p> <p>Scorers evaluate trained models in user-defined ways, and provide additional information about themselves, like their range and default value, which can be used by some data valuation methods (e.g. Group Testing Shapley) to estimate the number of samples required for a certain quality of approximation.</p>"},{"location":"api/pydvl/valuation/scorers/base/","title":"Base","text":""},{"location":"api/pydvl/valuation/scorers/base/#pydvl.valuation.scorers.base","title":"pydvl.valuation.scorers.base","text":"<p>This module implements the base class for all scorers used by valuation methods.</p>"},{"location":"api/pydvl/valuation/scorers/base/#pydvl.valuation.scorers.base.Scorer","title":"Scorer","text":"<p>               Bases: <code>ABC</code></p> <p>A scoring callable that takes a model and returns a scalar.</p> <p>Added in version 0.10.0</p> <p>ABC added</p>"},{"location":"api/pydvl/valuation/scorers/classwise/","title":"Classwise","text":""},{"location":"api/pydvl/valuation/scorers/classwise/#pydvl.valuation.scorers.classwise","title":"pydvl.valuation.scorers.classwise","text":"<p>This module contains the implementation of scorer class for Class-wise Shapley values.</p> <p>Its value is computed from an in-class and an out-of-class \"inner score\" (Schoch et al., 2022)<sup>1</sup>. Let \\(S\\) be the training set and \\(D\\) be the valuation set. For each label \\(c\\), \\(D\\) is factorized into two disjoint sets: \\(D_c\\) for in-class instances and \\(D_{-c}\\) for out-of-class instances. The score combines an in-class metric of performance, adjusted by a discounted out-of-class metric. These inner scores must be provided upon construction or default to accuracy. They are combined into:</p> \\[ u(S_{y_i}) = f(a_S(D_{y_i}))\\ g(a_S(D_{-y_i})), \\] <p>where \\(f\\) and \\(g\\) are continuous, monotonic functions. For a detailed explanation, refer to section four of (Schoch et al., 2022)<sup>1 </sup>.</p>"},{"location":"api/pydvl/valuation/scorers/classwise/#pydvl.valuation.scorers.classwise.ClasswiseSupervisedScorer","title":"ClasswiseSupervisedScorer","text":"<pre><code>ClasswiseSupervisedScorer(\n    scoring: (\n        str | SupervisedScorerCallable[SupervisedModelT] | SupervisedModelT\n    ),\n    test_data: Dataset,\n    default: float = 0.0,\n    range: tuple[float, float] = (0, 1),\n    in_class_discount_fn: Callable[[float], float] = lambda x: x,\n    out_of_class_discount_fn: Callable[[float], float] = exp,\n    rescale_scores: bool = True,\n    name: str | None = None,\n)\n</code></pre> <p>               Bases: <code>SupervisedScorer[SupervisedModelT]</code></p> <p>A Scorer designed for evaluation in classification problems.</p> <p>The final score is the combination of the in-class and out-of-class scores, which are e.g. the accuracy of the trained model over the instances of the test set with the same, and different, labels, respectively. See the module's documentation for more on this.</p> <p>These two scores are computed with an \"inner\" scoring function, which must be provided upon construction.</p> <p>Multi-class support</p> <p>The inner score must support multiple class labels if you intend to apply them to a multi-class problem. For instance, 'accuracy' supports multiple classes, but <code>f1</code> does not. For a two-class classification problem, using <code>f1_weighted</code> is essentially equivalent to using <code>accuracy</code>.</p> PARAMETER DESCRIPTION <code>scoring</code> <p>Name of the scoring function or a callable that can be passed to SupervisedScorer.</p> <p> TYPE: <code>str | SupervisedScorerCallable[SupervisedModelT] | SupervisedModelT</code> </p> <code>default</code> <p>Score to use when a model fails to provide a number, e.g. when too little was used to train it, or errors arise.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>range</code> <p>Numerical range of the score function. Some Monte Carlo methods can use this to estimate the number of samples required for a certain quality of approximation. If not provided, it can be read from the <code>scoring</code> object if it provides it, for instance if it was constructed with compose_score.</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(0, 1)</code> </p> <code>in_class_discount_fn</code> <p>Continuous, monotonic increasing function used to discount the in-class score.</p> <p> TYPE: <code>Callable[[float], float]</code> DEFAULT: <code>lambda x: x</code> </p> <code>out_of_class_discount_fn</code> <p>Continuous, monotonic increasing function used to discount the out-of-class score.</p> <p> TYPE: <code>Callable[[float], float]</code> DEFAULT: <code>exp</code> </p> <code>rescale_scores</code> <p>If set to <code>True</code>, the scores will be denormalized. This is particularly useful when the inner score function \\(a_S\\) is calculated by an estimator of the form $\frac{1}{N} \\sum_i x_i$.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>name</code> <p>Name of the scorer. If not provided, the name of the inner scoring function will be prefixed by <code>classwise</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <p>New in version 0.7.1</p> Source code in <code>src/pydvl/valuation/scorers/classwise.py</code> <pre><code>def __init__(\n    self,\n    scoring: str | SupervisedScorerCallable[SupervisedModelT] | SupervisedModelT,\n    test_data: Dataset,\n    default: float = 0.0,\n    range: tuple[float, float] = (0, 1),\n    in_class_discount_fn: Callable[[float], float] = lambda x: x,\n    out_of_class_discount_fn: Callable[[float], float] = np.exp,\n    rescale_scores: bool = True,\n    name: str | None = None,\n):\n    disc_score_in_class = in_class_discount_fn(range[1])\n    disc_score_out_of_class = out_of_class_discount_fn(range[1])\n    transformed_range = (0, disc_score_in_class * disc_score_out_of_class)\n    super().__init__(\n        scoring=scoring,\n        test_data=test_data,\n        range=transformed_range,\n        default=default,\n        name=name or f\"classwise {str(scoring)}\",\n    )\n    self._in_class_discount_fn = in_class_discount_fn\n    self._out_of_class_discount_fn = out_of_class_discount_fn\n    self.label: int | None = None\n    self.num_classes = len(np.unique(self.test_data.data().y))\n    self.rescale_scores = rescale_scores\n</code></pre>"},{"location":"api/pydvl/valuation/scorers/classwise/#pydvl.valuation.scorers.classwise.ClasswiseSupervisedScorer.compute_in_and_out_of_class_scores","title":"compute_in_and_out_of_class_scores","text":"<pre><code>compute_in_and_out_of_class_scores(\n    model: SupervisedModelT, rescale_scores: bool = True\n) -&gt; tuple[float, float]\n</code></pre> <p>Computes in-class and out-of-class scores using the provided inner scoring function. The result is:</p> \\[ a_S(D=\\{(x_1, y_1), \\dots, (x_K, y_K)\\}) = \\frac{1}{N} \\sum_k s(y(x_k), y_k). \\] <p>In this context, for label \\(c\\) calculations are executed twice: once for \\(D_c\\) and once for \\(D_{-c}\\) to determine the in-class and out-of-class scores, respectively. By default, the raw scores are multiplied by \\(\\frac{|D_c|}{|D|}\\) and \\(\\frac{|D_{-c}|}{|D|}\\), respectively. This is done to ensure that both scores are of the same order of magnitude. This normalization is particularly useful when the inner score function \\(a_S\\) is calculated by an estimator of the form \\(\\frac{1}{N} \\sum_i x_i\\), e.g. the accuracy.</p> PARAMETER DESCRIPTION <code>model</code> <p>Model used for computing the score on the validation set.</p> <p> TYPE: <code>SupervisedModelT</code> </p> <code>rescale_scores</code> <p>If set to <code>True</code>, the scores will be denormalized. This is particularly useful when the inner score function \\(a_S\\) is calculated by an estimator of the form \\(\\frac{1}{N} \\sum_i x_i\\).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>tuple[float, float]</code> <p>Tuple containing the in-class and out-of-class scores.</p> Source code in <code>src/pydvl/valuation/scorers/classwise.py</code> <pre><code>def compute_in_and_out_of_class_scores(\n    self, model: SupervisedModelT, rescale_scores: bool = True\n) -&gt; tuple[float, float]:\n    r\"\"\"Computes in-class and out-of-class scores using the provided inner scoring\n    function. The result is:\n\n    $$\n    a_S(D=\\{(x_1, y_1), \\dots, (x_K, y_K)\\}) = \\frac{1}{N} \\sum_k s(y(x_k), y_k).\n    $$\n\n    In this context, for label $c$ calculations are executed twice: once for $D_c$\n    and once for $D_{-c}$ to determine the in-class and out-of-class scores,\n    respectively. By default, the raw scores are multiplied by $\\frac{|D_c|}{|D|}$\n    and $\\frac{|D_{-c}|}{|D|}$, respectively. This is done to ensure that both\n    scores are of the same order of magnitude. This normalization is particularly\n    useful when the inner score function $a_S$ is calculated by an estimator of the\n    form $\\frac{1}{N} \\sum_i x_i$, e.g. the accuracy.\n\n    Args:\n        model: Model used for computing the score on the validation set.\n        rescale_scores: If set to `True`, the scores will be denormalized. This is\n            particularly useful when the inner score function $a_S$ is calculated by\n            an estimator of the form $\\frac{1}{N} \\sum_i x_i$.\n\n    Returns:\n        Tuple containing the in-class and out-of-class scores.\n    \"\"\"\n    if self.label is None:\n        raise ValueError(\n            \"The scorer's label attribute should be set before calling it\"\n        )\n\n    scorer = self._scorer\n    label_set_match = self.test_data.data().y == self.label\n    label_set = np.where(label_set_match)[0]\n\n    if len(label_set) == 0:\n        return 0, 1 / max(1, self.num_classes - 1)\n\n    complement_label_set = np.where(~label_set_match)[0]\n    in_class_score = scorer(model, *self.test_data.data(label_set))\n    out_of_class_score = scorer(model, *self.test_data.data(complement_label_set))\n\n    if rescale_scores:\n        # TODO: This can lead to NaN values\n        #       We should clearly indicate this to users\n        _, y_test = self.test_data.data()\n        n_in_class = np.count_nonzero(y_test == self.label)\n        n_out_of_class = len(y_test) - n_in_class\n        in_class_score *= n_in_class / (n_in_class + n_out_of_class)\n        out_of_class_score *= n_out_of_class / (n_in_class + n_out_of_class)\n\n    return in_class_score, out_of_class_score\n</code></pre>"},{"location":"api/pydvl/valuation/scorers/supervised/","title":"Supervised","text":""},{"location":"api/pydvl/valuation/scorers/supervised/#pydvl.valuation.scorers.supervised","title":"pydvl.valuation.scorers.supervised","text":"<p>This module provides a SupervisedScorer class that wraps scoring functions for supervised problems with additional information.</p> <p>Supervised scorers can be constructed in the same way as in scikit-learn: either from known strings or from a callable. Greater values must be better. If they are not, a negated version can be used, see scikit-learn's make_scorer().</p> <p>SupervisedScorer holds the test data used to evaluate the model.</p> <p>Named scorer</p> <p>It is possible to use all named scorers from scikit-learn.</p> <pre><code>from pydvl.valuation import Dataset, SupervisedScorer\n\ntrain, test = Dataset.from_arrays(X, y, train_size=0.7)\nmodel = SomeSKLearnModel()\nscorer = SupervisedScorer(\"accuracy\", test, default=0, range=(0, 1))\n</code></pre> <p>Model scorer</p> <p>It is also possible to use the <code>score()</code> function from the model if it defines one:</p> <pre><code>from pydvl.valuation import Dataset, SupervisedScorer\n\ntrain, test = Dataset.from_arrays(X, y, train_size=0.7)\nmodel = SomeSKLearnModel()\nscorer = SupervisedScorer(model, test, default=0, range=(-np.inf, 1))\n</code></pre>"},{"location":"api/pydvl/valuation/scorers/supervised/#pydvl.valuation.scorers.supervised.SupervisedScorer","title":"SupervisedScorer","text":"<pre><code>SupervisedScorer(\n    scoring: (\n        str | SupervisedScorerCallable[SupervisedModelT] | SupervisedModelT\n    ),\n    test_data: Dataset,\n    default: float,\n    range: tuple[float, float] = (-inf, inf),\n    name: str | None = None,\n)\n</code></pre> <p>               Bases: <code>Generic[SupervisedModelT]</code>, <code>Scorer</code></p> <p>A scoring callable that takes a model, data, and labels and returns a scalar.</p> PARAMETER DESCRIPTION <code>scoring</code> <p>Either a string or callable that can be passed to get_scorer.</p> <p> TYPE: <code>str | SupervisedScorerCallable[SupervisedModelT] | SupervisedModelT</code> </p> <code>test_data</code> <p>Dataset where the score will be evaluated.</p> <p> TYPE: <code>Dataset</code> </p> <code>default</code> <p>score to be used when a model cannot be fit, e.g. when too little data is passed, or errors arise.</p> <p> TYPE: <code>float</code> </p> <code>range</code> <p>numerical range of the score function. Some Monte Carlo methods can use this to estimate the number of samples required for a certain quality of approximation. If not provided, it can be read from the <code>scoring</code> object if it provides it, for instance if it was constructed with compose_score().</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>(-inf, inf)</code> </p> <code>name</code> <p>The name of the scorer. If not provided, the name of the function passed will be used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <p>New in version 0.5.0</p> <p>Changed in version 0.10.0</p> <p>This is now <code>SupervisedScorer</code> and holds the test data used to evaluate the model.</p> Source code in <code>src/pydvl/valuation/scorers/supervised.py</code> <pre><code>def __init__(\n    self,\n    scoring: str | SupervisedScorerCallable[SupervisedModelT] | SupervisedModelT,\n    test_data: Dataset,\n    default: float,\n    range: tuple[float, float] = (-np.inf, np.inf),\n    name: str | None = None,\n):\n    super().__init__()\n    if isinstance(scoring, SupervisedModel):\n        from sklearn.metrics import check_scoring\n\n        self._scorer = check_scoring(scoring)\n        if name is None:\n            name = f\"Default scorer for {scoring.__class__.__name__}\"\n    elif isinstance(scoring, str):\n        self._scorer = get_scorer(scoring)\n        if name is None:\n            name = scoring\n    else:\n        self._scorer = scoring\n        if name is None:\n            name = getattr(scoring, \"__name__\", \"scorer\")\n    self.test_data = test_data\n    self.default = default\n    # TODO: auto-fill from known scorers ?\n    self.range = range\n    self.name = name\n</code></pre>"},{"location":"api/pydvl/valuation/scorers/supervised/#pydvl.valuation.scorers.supervised.SupervisedScorerCallable","title":"SupervisedScorerCallable","text":"<p>               Bases: <code>Protocol[SupervisedModelT]</code></p> <p>Signature for a supervised scorer</p>"},{"location":"api/pydvl/valuation/scorers/utils/","title":"Utils","text":""},{"location":"api/pydvl/valuation/scorers/utils/#pydvl.valuation.scorers.utils","title":"pydvl.valuation.scorers.utils","text":"<p>Utilities for composing scoring functions.</p>"},{"location":"api/pydvl/valuation/scorers/utils/#pydvl.valuation.scorers.utils.compose_score","title":"compose_score","text":"<pre><code>compose_score(\n    scorer: SupervisedScorer,\n    transformation: Callable[[float], float],\n    name: str,\n) -&gt; SupervisedScorer\n</code></pre> <p>Composes a scoring function with an arbitrary scalar transformation.</p> <p>Useful to squash unbounded scores into ranges manageable by data valuation methods.</p> Example <pre><code>sigmoid = lambda x: 1/(1+np.exp(-x))\ncompose_score(Scorer(\"r2\"), sigmoid, range=(0,1), name=\"squashed r2\")\n</code></pre> PARAMETER DESCRIPTION <code>scorer</code> <p>The object to be composed.</p> <p> TYPE: <code>SupervisedScorer</code> </p> <code>transformation</code> <p>A scalar transformation</p> <p> TYPE: <code>Callable[[float], float]</code> </p> <code>name</code> <p>A string representation for the composition, for <code>str()</code>.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>SupervisedScorer</code> <p>The composite SupervisedScorer.</p> Source code in <code>src/pydvl/valuation/scorers/utils.py</code> <pre><code>def compose_score(\n    scorer: SupervisedScorer,\n    transformation: Callable[[float], float],\n    name: str,\n) -&gt; SupervisedScorer:\n    \"\"\"Composes a scoring function with an arbitrary scalar transformation.\n\n    Useful to squash unbounded scores into ranges manageable by data valuation\n    methods.\n\n    ??? Example\n        ```python\n        sigmoid = lambda x: 1/(1+np.exp(-x))\n        compose_score(Scorer(\"r2\"), sigmoid, range=(0,1), name=\"squashed r2\")\n        ```\n\n    Args:\n        scorer: The object to be composed.\n        transformation: A scalar transformation\n        name: A string representation for the composition, for `str()`.\n\n    Returns:\n        The composite [SupervisedScorer][pydvl.valuation.scorers.SupervisedScorer].\n    \"\"\"\n\n    class CompositeSupervisedScorer(SupervisedScorer[SupervisedModelT]):\n        def __call__(self, model: SupervisedModelT) -&gt; float:\n            raw = super().__call__(model)\n            return transformation(raw)\n\n    new_scorer = CompositeSupervisedScorer(\n        scoring=scorer._scorer,\n        test_data=scorer.test_data,\n        default=transformation(scorer.default),\n        range=(\n            transformation(scorer.range[0]),\n            transformation(scorer.range[1]),\n        ),\n        name=name,\n    )\n    return new_scorer\n</code></pre>"},{"location":"api/pydvl/valuation/utility/","title":"Utility","text":""},{"location":"api/pydvl/valuation/utility/#pydvl.valuation.utility","title":"pydvl.valuation.utility","text":"<p>This module contains classes to manage and learn utility functions for the computation of values.</p> <p>Utilities evaluate functions over subsets of the training set (\"samples\"). As such, they are assumed to be invariant under permutations of the training data. The base class for all utilities is UtilityBase.</p>"},{"location":"api/pydvl/valuation/utility/#pydvl.valuation.utility--utility-for-model-based-methods","title":"Utility for model-based methods","text":"<p>ModelUtility holds information about model and scoring function (the latter being what one usually understands under utility in the general definition of Shapley value). Model-based evaluation methods define the utility as a retraining of the model on a subset of the data, which is then scored. Please see the documentation on Computing Data Values for more information.</p>"},{"location":"api/pydvl/valuation/utility/#pydvl.valuation.utility--utility-learning","title":"Utility learning","text":"<p>DataUtilityLearning adds support for learning the utility to avoid repeated re-training of the model to compute the score. Several methods exist to learn the utility function.</p>"},{"location":"api/pydvl/valuation/utility/#pydvl.valuation.utility--caching","title":"Caching","text":"<p>Utilities can be automatically cached across machines when the cache is so configured and enabled upon construction.</p>"},{"location":"api/pydvl/valuation/utility/base/","title":"Base","text":""},{"location":"api/pydvl/valuation/utility/base/#pydvl.valuation.utility.base","title":"pydvl.valuation.utility.base","text":"<p>This module defines the base class for all utilities.</p>"},{"location":"api/pydvl/valuation/utility/base/#pydvl.valuation.utility.base.UtilityBase","title":"UtilityBase","text":"<p>               Bases: <code>Generic[SampleT]</code>, <code>ABC</code></p> <p>Base class for all utilities.</p> <p>A utility is a scalar-valued set function which will be evaluated over subsets of the training set.</p>"},{"location":"api/pydvl/valuation/utility/base/#pydvl.valuation.utility.base.UtilityBase.training_data","title":"training_data  <code>property</code>","text":"<pre><code>training_data: Dataset | None\n</code></pre> <p>Retrieves the training data used by this utility.</p> <p>This property is read-only. In order to set it, use with_dataset().</p>"},{"location":"api/pydvl/valuation/utility/base/#pydvl.valuation.utility.base.UtilityBase.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(sample: SampleT | None) -&gt; float\n</code></pre> <p>Note</p> <p>Calls with empty samples or None must always return the same valid value, e.g. 0, or whatever makes sense for the utility. Some samplers (e.g. permutations) depend on this.</p> PARAMETER DESCRIPTION <code>sample</code> <p> TYPE: <code>SampleT | None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The evaluation of the utility for the sample</p> Source code in <code>src/pydvl/valuation/utility/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, sample: SampleT | None) -&gt; float:\n    \"\"\"\n    !!! Note\n        Calls with empty samples or None must always return the same valid value,\n        e.g. 0, or whatever makes sense for the utility. Some samplers (e.g.\n        permutations) depend on this.\n\n    Args:\n        sample:\n\n    Returns:\n        The evaluation of the utility for the sample\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/pydvl/valuation/utility/base/#pydvl.valuation.utility.base.UtilityBase.with_dataset","title":"with_dataset","text":"<pre><code>with_dataset(data: Dataset, copy: bool = True) -&gt; Self\n</code></pre> <p>Returns the utility, or a copy of it, with the given dataset. Args:     data: The dataset to use for utility fitting (training data)     copy: Whether to copy the utility object or not. Valuation methods should         always make copies to avoid unexpected side effects. Returns:     The utility object.</p> Source code in <code>src/pydvl/valuation/utility/base.py</code> <pre><code>def with_dataset(self, data: Dataset, copy: bool = True) -&gt; Self:\n    \"\"\"Returns the utility, or a copy of it, with the given dataset.\n    Args:\n        data: The dataset to use for utility fitting (training data)\n        copy: Whether to copy the utility object or not. Valuation methods should\n            always make copies to avoid unexpected side effects.\n    Returns:\n        The utility object.\n    \"\"\"\n    utility = cp.copy(self) if copy else self\n    utility._training_data = data\n    return utility\n</code></pre>"},{"location":"api/pydvl/valuation/utility/classwise/","title":"Classwise","text":""},{"location":"api/pydvl/valuation/utility/classwise/#pydvl.valuation.utility.classwise","title":"pydvl.valuation.utility.classwise","text":"<p>This module defines the utility used by class-wise Shapley valuation methods.</p> <p>See the documentation for more information.</p>"},{"location":"api/pydvl/valuation/utility/classwise/#pydvl.valuation.utility.classwise.ClasswiseModelUtility","title":"ClasswiseModelUtility","text":"<pre><code>ClasswiseModelUtility(\n    model: SupervisedModel,\n    scorer: ClasswiseSupervisedScorer,\n    *,\n    catch_errors: bool = False,\n    show_warnings: bool = False,\n    cache_backend: CacheBackend | None = None,\n    cached_func_options: CachedFuncConfig | None = None,\n    clone_before_fit: bool = True\n)\n</code></pre> <p>               Bases: <code>ModelUtility[ClasswiseSample, SupervisedModel]</code></p> <p>ModelUtility class that is specific to class-wise shapley valuation.</p> <p>It expects a class-wise scorer and a classification task.</p> PARAMETER DESCRIPTION <code>model</code> <p>Any supervised model. Typical choices can be found in the sci-kit learn documentation.</p> <p> TYPE: <code>SupervisedModel</code> </p> <code>scorer</code> <p>A class-wise scoring object.</p> <p> TYPE: <code>ClasswiseSupervisedScorer</code> </p> <code>catch_errors</code> <p>set to <code>True</code> to catch the errors when <code>fit()</code> fails. This could happen in several steps of the pipeline, e.g. when too little training data is passed, which happens often during Shapley value calculations. When this happens, the scorer's default value is returned as a score and computation continues.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>show_warnings</code> <p>Set to <code>False</code> to suppress warnings thrown by <code>fit()</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>cache_backend</code> <p>Optional instance of CacheBackend used to wrap the _utility method of the Utility instance. By default, this is set to None and that means that the utility evaluations will not be cached.</p> <p> TYPE: <code>CacheBackend | None</code> DEFAULT: <code>None</code> </p> <code>cached_func_options</code> <p>Optional configuration object for cached utility evaluation.</p> <p> TYPE: <code>CachedFuncConfig | None</code> DEFAULT: <code>None</code> </p> <code>clone_before_fit</code> <p>If <code>True</code>, the model will be cloned before calling <code>fit()</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/utility/classwise.py</code> <pre><code>def __init__(\n    self,\n    model: SupervisedModel,\n    scorer: ClasswiseSupervisedScorer,\n    *,\n    catch_errors: bool = False,\n    show_warnings: bool = False,\n    cache_backend: CacheBackend | None = None,\n    cached_func_options: CachedFuncConfig | None = None,\n    clone_before_fit: bool = True,\n):\n    super().__init__(\n        model,\n        scorer,\n        catch_errors=catch_errors,\n        show_warnings=show_warnings,\n        cache_backend=cache_backend,\n        cached_func_options=cached_func_options,\n        clone_before_fit=clone_before_fit,\n    )\n    if not isinstance(self.scorer, ClasswiseSupervisedScorer):\n        raise ValueError(\"Scorer must be an instance of ClasswiseSupervisedScorer\")\n    self.scorer: ClasswiseSupervisedScorer\n</code></pre>"},{"location":"api/pydvl/valuation/utility/classwise/#pydvl.valuation.utility.classwise.ClasswiseModelUtility.cache_stats","title":"cache_stats  <code>property</code>","text":"<pre><code>cache_stats: CacheStats | None\n</code></pre> <p>Cache statistics are gathered when cache is enabled. See CacheStats for all fields returned.</p>"},{"location":"api/pydvl/valuation/utility/classwise/#pydvl.valuation.utility.classwise.ClasswiseModelUtility.training_data","title":"training_data  <code>property</code>","text":"<pre><code>training_data: Dataset | None\n</code></pre> <p>Retrieves the training data used by this utility.</p> <p>This property is read-only. In order to set it, use with_dataset().</p>"},{"location":"api/pydvl/valuation/utility/classwise/#pydvl.valuation.utility.classwise.ClasswiseModelUtility.__call__","title":"__call__","text":"<pre><code>__call__(sample: SampleT | None) -&gt; float\n</code></pre> PARAMETER DESCRIPTION <code>sample</code> <p>contains a subset of valid indices for the <code>x_train</code> attribute of Dataset.</p> <p> TYPE: <code>SampleT | None</code> </p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>def __call__(self, sample: SampleT | None) -&gt; float:\n    \"\"\"\n    Args:\n        sample: contains a subset of valid indices for the\n            `x_train` attribute of [Dataset][pydvl.utils.dataset.Dataset].\n    \"\"\"\n    if sample is None or len(sample.subset) == 0:\n        return self.scorer.default\n\n    return cast(float, self._utility_wrapper(sample))\n</code></pre>"},{"location":"api/pydvl/valuation/utility/classwise/#pydvl.valuation.utility.classwise.ClasswiseModelUtility._compute_score","title":"_compute_score","text":"<pre><code>_compute_score(model: ModelT) -&gt; float\n</code></pre> <p>Computes the score of a fitted model.</p> PARAMETER DESCRIPTION <code>model</code> <p>fitted model</p> <p> TYPE: <code>ModelT</code> </p> <p>Returns:     Computed score or the scorer's default value in case of an error     or a NaN value.</p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>def _compute_score(self, model: ModelT) -&gt; float:\n    \"\"\"Computes the score of a fitted model.\n\n    Args:\n        model: fitted model\n    Returns:\n        Computed score or the scorer's default value in case of an error\n        or a NaN value.\n    \"\"\"\n    try:\n        score = float(self.scorer(model))\n        # Some scorers raise exceptions if they return NaNs, some might not\n        if np.isnan(score):\n            warnings.warn(\"Scorer returned NaN\", RuntimeWarning)\n            return self.scorer.default\n    except Exception as e:\n        if self.catch_errors:\n            warnings.warn(str(e), RuntimeWarning)\n            return self.scorer.default\n        raise\n    return score\n</code></pre>"},{"location":"api/pydvl/valuation/utility/classwise/#pydvl.valuation.utility.classwise.ClasswiseModelUtility._maybe_clone_model","title":"_maybe_clone_model  <code>staticmethod</code>","text":"<pre><code>_maybe_clone_model(model: ModelT, do_clone: bool) -&gt; ModelT\n</code></pre> <p>Clones the passed model to avoid the possibility of reusing a fitted estimator.</p> PARAMETER DESCRIPTION <code>model</code> <p>Any supervised model. Typical choices can be found on this page</p> <p> TYPE: <code>ModelT</code> </p> <code>do_clone</code> <p>Whether to clone the model or not.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>@staticmethod\ndef _maybe_clone_model(model: ModelT, do_clone: bool) -&gt; ModelT:\n    \"\"\"Clones the passed model to avoid the possibility of reusing a fitted\n    estimator.\n\n    Args:\n        model: Any supervised model. Typical choices can be found\n            on [this page](https://scikit-learn.org/stable/supervised_learning.html)\n        do_clone: Whether to clone the model or not.\n    \"\"\"\n    if not do_clone:\n        return model\n    try:\n        model = clone(model)\n    except TypeError:\n        # This happens if the passed model is not an sklearn model\n        # In this case, we just make a deepcopy of the model.\n        model = clone(model, safe=False)\n    return cast(ModelT, model)\n</code></pre>"},{"location":"api/pydvl/valuation/utility/classwise/#pydvl.valuation.utility.classwise.ClasswiseModelUtility.sample_to_data","title":"sample_to_data","text":"<pre><code>sample_to_data(sample: SampleT) -&gt; tuple\n</code></pre> <p>Returns the raw data corresponding to a sample.</p> <p>Subclasses can override this e.g. to do reshaping of tensors. Be careful not to rely on <code>self.training_data</code> not changing between calls to this method. For manipulations to it, use the <code>with_dataset()</code> method.</p> PARAMETER DESCRIPTION <code>sample</code> <p>contains a subset of valid indices for the <code>x_train</code> attribute of Dataset.</p> <p> TYPE: <code>SampleT</code> </p> <p>Returns:     Tuple of the training data and labels corresponding to the sample indices.</p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>def sample_to_data(self, sample: SampleT) -&gt; tuple:\n    \"\"\"Returns the raw data corresponding to a sample.\n\n    Subclasses can override this e.g. to do reshaping of tensors. Be careful not to\n    rely on `self.training_data` not changing between calls to this method. For\n    manipulations to it, use the `with_dataset()` method.\n\n    Args:\n        sample: contains a subset of valid indices for the\n            `x_train` attribute of [Dataset][pydvl.utils.dataset.Dataset].\n    Returns:\n        Tuple of the training data and labels corresponding to the sample indices.\n    \"\"\"\n    if self.training_data is None:\n        raise ValueError(\"No training data provided\")\n\n    x_train, y_train = self.training_data.data(sample.subset)\n    return x_train, y_train\n</code></pre>"},{"location":"api/pydvl/valuation/utility/classwise/#pydvl.valuation.utility.classwise.ClasswiseModelUtility.with_dataset","title":"with_dataset","text":"<pre><code>with_dataset(data: Dataset, copy: bool = True) -&gt; Self\n</code></pre> <p>Returns the utility, or a copy of it, with the given dataset. Args:     data: The dataset to use for utility fitting (training data)     copy: Whether to copy the utility object or not. Valuation methods should         always make copies to avoid unexpected side effects. Returns:     The utility object.</p> Source code in <code>src/pydvl/valuation/utility/base.py</code> <pre><code>def with_dataset(self, data: Dataset, copy: bool = True) -&gt; Self:\n    \"\"\"Returns the utility, or a copy of it, with the given dataset.\n    Args:\n        data: The dataset to use for utility fitting (training data)\n        copy: Whether to copy the utility object or not. Valuation methods should\n            always make copies to avoid unexpected side effects.\n    Returns:\n        The utility object.\n    \"\"\"\n    utility = cp.copy(self) if copy else self\n    utility._training_data = data\n    return utility\n</code></pre>"},{"location":"api/pydvl/valuation/utility/deepset/","title":"Deepset","text":""},{"location":"api/pydvl/valuation/utility/deepset/#pydvl.valuation.utility.deepset","title":"pydvl.valuation.utility.deepset","text":"<p>This module provides an implementation of DeepSet, from Zaheer et al. (2017).<sup>1</sup></p> <p>DeepSet uses a simple permutation-invariant architecture to learn embeddings for sets of points. Please see the documentation or the paper for details.</p>"},{"location":"api/pydvl/valuation/utility/deepset/#pydvl.valuation.utility.deepset--references","title":"References","text":"<ol> <li> <p>Zaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh,   Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep   Sets.   In Advances in Neural Information Processing Systems, Vol. 30. Curran Associates,   Inc., 2017.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/utility/deepset/#pydvl.valuation.utility.deepset.DeepSet","title":"DeepSet","text":"<pre><code>DeepSet(\n    input_dim: int,\n    phi_hidden_dim: int,\n    phi_output_dim: int,\n    rho_hidden_dim: int,\n    use_embedding: bool = False,\n    num_embeddings: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Simple implementation of DeepSets<sup>12</sup>. to learn utility functions.</p> <p>This is a feed forward neural network with two hidden layers and a bottleneck operation (sum) that makes it permutation invariant.</p> PARAMETER DESCRIPTION <code>input_dim</code> <p>Dimensions of each instance in the set, or dimension of the embedding if using one.</p> <p> TYPE: <code>int</code> </p> <code>phi_hidden_dim</code> <p>Number of hidden units in the phi network.</p> <p> TYPE: <code>int</code> </p> <code>phi_output_dim</code> <p>Output dimension of the phi network.</p> <p> TYPE: <code>int</code> </p> <code>rho_hidden_dim</code> <p>Number of hidden units in the rho network.</p> <p> TYPE: <code>int</code> </p> <code>use_embedding</code> <p>If <code>True</code>, use an embedding layer to learn representations for x_i.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_embeddings</code> <p>Number of unique x_i values (only needed if <code>use_embedding</code> is <code>True</code>).</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/valuation/utility/deepset.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    phi_hidden_dim: int,\n    phi_output_dim: int,\n    rho_hidden_dim: int,\n    use_embedding: bool = False,\n    num_embeddings: int | None = None,\n):\n    super(DeepSet, self).__init__()\n\n    self.use_embedding = use_embedding\n    if use_embedding:\n        if num_embeddings is None or input_dim is None:\n            raise ValueError(\n                \"num_embeddings and input_dim must be provided when using embedding\"\n            )\n        self.embedding = nn.Embedding(num_embeddings, input_dim)\n\n    # The phi network processes each element in the set individually.\n    self.phi = nn.Sequential(\n        nn.Linear(input_dim, phi_hidden_dim),\n        nn.ReLU(),\n        nn.Linear(phi_hidden_dim, phi_output_dim),\n        nn.ReLU(),\n    )\n    # The rho network processes the aggregated (summed) representation.\n    self.rho = nn.Sequential(\n        nn.Linear(phi_output_dim, rho_hidden_dim),\n        nn.ReLU(),\n        nn.Linear(rho_hidden_dim, 1),\n    )\n\n    self.reset_parameters()\n</code></pre>"},{"location":"api/pydvl/valuation/utility/deepset/#pydvl.valuation.utility.deepset.DeepSet.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> PARAMETER DESCRIPTION <code>x</code> <p>If using embedding, x should be of shape (batch_size, set_size) with integer ids. Otherwise, x is of shape (batch_size, set_size, input_dim) with feature vectors.</p> <p> TYPE: <code>Tensor</code> </p> <p>Returns:     Output tensor of shape (batch_size, 1), the predicted y for each set.</p> Source code in <code>src/pydvl/valuation/utility/deepset.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Args:\n        x: If using embedding, x should be of shape (batch_size, set_size) with\n            integer ids. Otherwise, x is of shape (batch_size, set_size, input_dim)\n            with feature vectors.\n    Returns:\n        Output tensor of shape (batch_size, 1), the predicted y for each set.\n    \"\"\"\n    if self.use_embedding:\n        x = self.embedding(x)  # shape (batch_size, set_size, embed_dim)\n\n    phi_x = self.phi(x)  # shape: (batch_size, set_size, phi_output_dim)\n    aggregated = torch.sum(phi_x, dim=1)  # shape: (batch_size, phi_output_dim)\n    out = self.rho(aggregated)  # shape: (batch_size, 1)\n    return cast(Tensor, out)\n</code></pre>"},{"location":"api/pydvl/valuation/utility/deepset/#pydvl.valuation.utility.deepset.DeepSetUtilityModel","title":"DeepSetUtilityModel","text":"<pre><code>DeepSetUtilityModel(\n    data: Dataset,\n    phi_hidden_dim: int,\n    phi_output_dim: int,\n    rho_hidden_dim: int,\n    lr: float = 0.001,\n    lr_step_size: int = 10,\n    lr_gamma: float = 0.1,\n    batch_size: int = 64,\n    num_epochs: int = 20,\n    device: str = \"cpu\",\n    dtype: dtype = float32,\n    progress: dict[str, Any] | bool = False,\n)\n</code></pre> <p>               Bases: <code>UtilityModel</code></p> <p>A utility model that uses a simple DeepSet architecture to learn utility functions.</p> PARAMETER DESCRIPTION <code>data</code> <p>The pydvl dataset from which the samples are drawn.</p> <p> TYPE: <code>Dataset</code> </p> <code>phi_hidden_dim</code> <p>Number of hidden units in the phi network.</p> <p> TYPE: <code>int</code> </p> <code>phi_output_dim</code> <p>Output dimension of the phi network.</p> <p> TYPE: <code>int</code> </p> <code>rho_hidden_dim</code> <p>Number of hidden units in the rho network.</p> <p> TYPE: <code>int</code> </p> <code>lr</code> <p>Learning rate for the optimizer.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>lr_step_size</code> <p>Step size for the learning rate scheduler.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>lr_gamma</code> <p>Multiplicative factor for the learning rate scheduler.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>batch_size</code> <p>Batch size for training.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>num_epochs</code> <p>Number of epochs for training.</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>device</code> <p>Device to use for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cpu'</code> </p> <code>dtype</code> <p>Data type to use for training.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>float32</code> </p> <code>progress</code> <p>Whether to display a progress bar during training. If a dictionary is provided, it is passed to <code>tqdm</code> as keyword arguments.</p> <p> TYPE: <code>dict[str, Any] | bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/pydvl/valuation/utility/deepset.py</code> <pre><code>def __init__(\n    self,\n    data: Dataset,\n    phi_hidden_dim: int,\n    phi_output_dim: int,\n    rho_hidden_dim: int,\n    lr: float = 0.001,\n    lr_step_size: int = 10,\n    lr_gamma: float = 0.1,\n    batch_size: int = 64,\n    num_epochs: int = 20,\n    device: str = \"cpu\",\n    dtype: torch.dtype = torch.float32,\n    progress: dict[str, Any] | bool = False,\n):\n    super().__init__()\n    self.lr = lr\n    self.lr_step_size = lr_step_size\n    self.lr_gamma = lr_gamma\n    self.batch_size = batch_size\n    self.num_epochs = num_epochs\n    self.device = device\n    self.data = data\n    self.predictor = DeepSet(\n        input_dim=self.data.n_features,\n        phi_hidden_dim=phi_hidden_dim,\n        phi_output_dim=phi_output_dim,\n        rho_hidden_dim=rho_hidden_dim,\n    ).to(device=device, dtype=dtype)\n    self.is_trained = False\n\n    self.tqdm_args: dict[str, Any] = {\n        \"desc\": f\"{self.__class__.__name__}, training\",\n        \"unit\": \"epoch\",\n    }\n    # HACK: parse additional args for the progress bar if any (we probably want\n    #  something better)\n    if isinstance(progress, bool):\n        self.tqdm_args.update({\"disable\": not progress})\n    elif isinstance(progress, dict):\n        self.tqdm_args.update(progress)\n    else:\n        raise TypeError(f\"Invalid type for progress: {type(progress)}\")\n</code></pre>"},{"location":"api/pydvl/valuation/utility/deepset/#pydvl.valuation.utility.deepset.DeepSetUtilityModel.fit","title":"fit","text":"<pre><code>fit(samples: dict[Sample, float]) -&gt; Self\n</code></pre> PARAMETER DESCRIPTION <code>samples</code> <p>A collection of utility samples</p> <p> TYPE: <code>dict[Sample, float]</code> </p> <p>Returns:</p> Source code in <code>src/pydvl/valuation/utility/deepset.py</code> <pre><code>def fit(self, samples: dict[Sample, float]) -&gt; Self:\n    \"\"\"\n\n    Args:\n        samples: A collection of utility samples\n\n    Returns:\n\n    \"\"\"\n    if self.is_trained:\n        self.predictor.reset_parameters()\n        self.is_trained = False\n\n    dataset = SetDatasetRaw(samples, self.data)\n\n    loss_fn = nn.MSELoss()\n\n    optimizer = torch.optim.Adam(self.predictor.parameters(), lr=self.lr)\n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer, step_size=self.lr_step_size, gamma=self.lr_gamma\n    )\n    dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n    pbar = trange(self.num_epochs, **self.tqdm_args)\n    for _ in pbar:\n        epoch_loss = 0.0\n        for set_tensor, target in dataloader:\n            set_tensor = set_tensor.to(self.device, non_blocking=True)\n            target = target.to(self.device, non_blocking=True)\n            optimizer.zero_grad()\n            output = self.predictor(set_tensor)\n            loss = loss_fn(output, target)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        scheduler.step()\n        pbar.set_postfix_str(f\"Loss: {epoch_loss / len(dataloader):.4f}\")\n\n    self.is_trained = True\n    return self\n</code></pre>"},{"location":"api/pydvl/valuation/utility/deepset/#pydvl.valuation.utility.deepset.DeepSetUtilityModel.predict","title":"predict","text":"<pre><code>predict(samples: Collection[Sample]) -&gt; NDArray\n</code></pre> PARAMETER DESCRIPTION <code>samples</code> <p>A collection of samples to predict their utility values.</p> <p> TYPE: <code>Collection[Sample]</code> </p> RETURNS DESCRIPTION <code>NDArray</code> <p>An array of values of dimension (len(samples), 1) with the predicted utility</p> Source code in <code>src/pydvl/valuation/utility/deepset.py</code> <pre><code>def predict(self, samples: Collection[Sample]) -&gt; NDArray:\n    \"\"\"\n\n    Args:\n        samples: A collection of samples to predict their utility values.\n\n    Returns:\n        An array of values of dimension (len(samples), 1) with the predicted utility\n    \"\"\"\n    if not samples:\n        raise ValueError(\"The samples collection is empty.\")\n    max_set_size = max(len(s.subset) for s in samples)\n    # grab device and dtype\n    template = next(self.predictor.parameters())\n    set_tensor = template.new_zeros(\n        (len(samples), max_set_size, self.data.n_features)\n    )\n    for i, sample in enumerate(samples):\n        for j, idx in enumerate(sample.subset):\n            set_tensor[i, j] = torch.tensor(\n                self.data.data().x[idx],\n                device=template.device,\n                dtype=template.dtype,\n            )\n    with torch.no_grad():\n        prediction = self.predictor(set_tensor)\n\n    return cast(NDArray, prediction.cpu().numpy())\n</code></pre>"},{"location":"api/pydvl/valuation/utility/deepset/#pydvl.valuation.utility.deepset.SetDatasetRaw","title":"SetDatasetRaw","text":"<pre><code>SetDatasetRaw(\n    samples: dict[Sample, float],\n    training_data: Dataset,\n    dtype: dtype = float32,\n    device: device | str = \"cpu\",\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataloader compatible dataset for DeepSet.</p> PARAMETER DESCRIPTION <code>samples</code> <p>Mapping from samples to target y.</p> <p> TYPE: <code>dict[Sample, float]</code> </p> <code>training_data</code> <p>the Dataset from which the samples are drawn.</p> <p> TYPE: <code>Dataset</code> </p> Source code in <code>src/pydvl/valuation/utility/deepset.py</code> <pre><code>def __init__(\n    self,\n    samples: dict[Sample, float],\n    training_data: Dataset,\n    dtype: torch.dtype = torch.float32,\n    device: torch.device | str = \"cpu\",\n):\n    \"\"\"\n    Args:\n        samples: Mapping from samples to target y.\n        training_data: the [Dataset][pydvl.valuation.dataset.Dataset] from which the\n            samples are drawn.\n\n    \"\"\"\n    self.dtype = dtype\n    self.device = device\n    self.samples = list(samples.items())\n    self.data = training_data\n    self.max_set_size = max(len(s.subset) for s, _ in self.samples)  # For padding\n</code></pre>"},{"location":"api/pydvl/valuation/utility/deepset/#pydvl.valuation.utility.deepset.SetDatasetRaw.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int)\n</code></pre> <p>Builds the tensor for the set with index <code>idx</code></p> Source code in <code>src/pydvl/valuation/utility/deepset.py</code> <pre><code>def __getitem__(self, idx: int):\n    \"\"\"Builds the tensor for the set with index `idx`\"\"\"\n    sample, y = self.samples[idx]\n    set_tensor = torch.zeros(\n        self.max_set_size,\n        self.data.n_features,\n        dtype=self.dtype,\n        device=self.device,\n    )\n    for i, idx in enumerate(sample.subset):\n        set_tensor[i] = set_tensor.new_tensor(self.data.data().x[idx])\n    return set_tensor, set_tensor.new_tensor([y])\n</code></pre>"},{"location":"api/pydvl/valuation/utility/knn/","title":"Knn","text":""},{"location":"api/pydvl/valuation/utility/knn/#pydvl.valuation.utility.knn","title":"pydvl.valuation.utility.knn","text":"<p>This module implements the utility function used in KNN-Shapley, as introduced by Jia et al. (2019)<sup>1</sup>.</p> <p>Uses of this utility</p> <p>Although this class can be used in conjunction with any semi-value method and sampler, when computing Shapley values, it is recommended to use the dedicated valuation class KNNShapleyValuation, because it implements a more efficient algorithm for computing Shapley values which runs in \\(O(n \\log n)\\) time for each test point.</p> <p>KNN-Shapley</p> <p>See the documentation for an introduction to the method and our implementation.</p> <p>The utility implemented by the class KNNClassifierUtility is defined as:</p> \\[ u (S) := \\frac{1}{n_{\\text{test}}}  \\sum_{j = 1}^{n_{\\text{test}}}    \\frac{1}{K}  \\sum_{k = 1}^{| \\alpha^{(j)} | \\}}    \\mathbb{1} \\{ y_{\\alpha^{(j)}_k (S)} = y^{\\text{test}}_j \\}, \\] <p>where \\(\\alpha^{(j)} (S)\\) is the intersection of the \\(K\\)-nearest neighbors of the test point \\(x^{\\text{test}}_j\\) across the whole training set, and the sample \\(S\\). In particular, \\(\\alpha^{(j)}_k (S)\\) is the index of the training point in \\(S\\) which is ranked \\(k\\)-th closest to test point \\(x^{\\text{test}}_j.\\)</p>"},{"location":"api/pydvl/valuation/utility/knn/#pydvl.valuation.utility.knn--references","title":"References","text":"<ol> <li> <p>Jia, R. et al., 2019. Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms. In: Proceedings of the VLDB Endowment, Vol. 12, No. 11, pp. 1610\u20131623.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/utility/knn/#pydvl.valuation.utility.knn.KNNClassifierUtility","title":"KNNClassifierUtility","text":"<pre><code>KNNClassifierUtility(\n    model: KNeighborsClassifier,\n    test_data: Dataset,\n    *,\n    catch_errors: bool = False,\n    show_warnings: bool = False,\n    cache_backend: CacheBackend | None = None,\n    cached_func_options: CachedFuncConfig | None = None,\n    clone_before_fit: bool = True\n)\n</code></pre> <p>               Bases: <code>ModelUtility[Sample, KNeighborsClassifier]</code></p> <p>Utility object for KNN Classifiers.</p> <p>The utility function is the model's predicted probability for the true class.</p> <p>Uses of this utility</p> <p>Although this class can be used in conjunction with any semi-value method and sampler, when computing Shapley values, it is recommended to use the dedicated class KNNShapleyValuation, because it implements a more efficient algorithm for computing Shapley values which runs in O(n log n) time for each test point.</p> PARAMETER DESCRIPTION <code>model</code> <p>A KNN classifier model.</p> <p> TYPE: <code>KNeighborsClassifier</code> </p> <code>test_data</code> <p>The test data to evaluate the model on.</p> <p> TYPE: <code>Dataset</code> </p> <code>catch_errors</code> <p>set to <code>True</code> to catch the errors when <code>fit()</code> fails. This could happen in several steps of the pipeline, e.g. when too little training data is passed, which happens often during Shapley value calculations. When this happens, the scorer's default value is returned as a score and computation continues.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>show_warnings</code> <p>Set to <code>False</code> to suppress warnings thrown by <code>fit()</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>cache_backend</code> <p>Optional instance of [CacheBackend][ pydvl.utils.caching.base.CacheBackend] used to wrap the _utility method of the Utility instance. By default, this is set to None and that means that the utility evaluations will not be cached.</p> <p> TYPE: <code>CacheBackend | None</code> DEFAULT: <code>None</code> </p> <code>cached_func_options</code> <p>Optional configuration object for cached utility evaluation.</p> <p> TYPE: <code>CachedFuncConfig | None</code> DEFAULT: <code>None</code> </p> <code>clone_before_fit</code> <p>If <code>True</code>, the model will be cloned before calling <code>fit()</code> in utility evaluations.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/utility/knn.py</code> <pre><code>def __init__(\n    self,\n    model: KNeighborsClassifier,\n    test_data: Dataset,\n    *,\n    catch_errors: bool = False,\n    show_warnings: bool = False,\n    cache_backend: CacheBackend | None = None,\n    cached_func_options: CachedFuncConfig | None = None,\n    clone_before_fit: bool = True,\n):\n    self.test_data = test_data\n    self.sorted_neighbors: NDArray[np.int_] | None = None\n    dummy_scorer = _DummyScorer()\n\n    super().__init__(\n        model=model,\n        scorer=dummy_scorer,  # not applicable\n        catch_errors=catch_errors,\n        show_warnings=show_warnings,\n        cache_backend=cache_backend,\n        cached_func_options=cached_func_options,\n        clone_before_fit=clone_before_fit,\n    )\n</code></pre>"},{"location":"api/pydvl/valuation/utility/knn/#pydvl.valuation.utility.knn.KNNClassifierUtility.cache_stats","title":"cache_stats  <code>property</code>","text":"<pre><code>cache_stats: CacheStats | None\n</code></pre> <p>Cache statistics are gathered when cache is enabled. See CacheStats for all fields returned.</p>"},{"location":"api/pydvl/valuation/utility/knn/#pydvl.valuation.utility.knn.KNNClassifierUtility.training_data","title":"training_data  <code>property</code>","text":"<pre><code>training_data: Dataset | None\n</code></pre> <p>Retrieves the training data used by this utility.</p> <p>This property is read-only. In order to set it, use with_dataset().</p>"},{"location":"api/pydvl/valuation/utility/knn/#pydvl.valuation.utility.knn.KNNClassifierUtility.__call__","title":"__call__","text":"<pre><code>__call__(sample: SampleT | None) -&gt; float\n</code></pre> PARAMETER DESCRIPTION <code>sample</code> <p>contains a subset of valid indices for the <code>x_train</code> attribute of Dataset.</p> <p> TYPE: <code>SampleT | None</code> </p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>def __call__(self, sample: SampleT | None) -&gt; float:\n    \"\"\"\n    Args:\n        sample: contains a subset of valid indices for the\n            `x_train` attribute of [Dataset][pydvl.utils.dataset.Dataset].\n    \"\"\"\n    if sample is None or len(sample.subset) == 0:\n        return self.scorer.default\n\n    return cast(float, self._utility_wrapper(sample))\n</code></pre>"},{"location":"api/pydvl/valuation/utility/knn/#pydvl.valuation.utility.knn.KNNClassifierUtility._maybe_clone_model","title":"_maybe_clone_model  <code>staticmethod</code>","text":"<pre><code>_maybe_clone_model(model: ModelT, do_clone: bool) -&gt; ModelT\n</code></pre> <p>Clones the passed model to avoid the possibility of reusing a fitted estimator.</p> PARAMETER DESCRIPTION <code>model</code> <p>Any supervised model. Typical choices can be found on this page</p> <p> TYPE: <code>ModelT</code> </p> <code>do_clone</code> <p>Whether to clone the model or not.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>@staticmethod\ndef _maybe_clone_model(model: ModelT, do_clone: bool) -&gt; ModelT:\n    \"\"\"Clones the passed model to avoid the possibility of reusing a fitted\n    estimator.\n\n    Args:\n        model: Any supervised model. Typical choices can be found\n            on [this page](https://scikit-learn.org/stable/supervised_learning.html)\n        do_clone: Whether to clone the model or not.\n    \"\"\"\n    if not do_clone:\n        return model\n    try:\n        model = clone(model)\n    except TypeError:\n        # This happens if the passed model is not an sklearn model\n        # In this case, we just make a deepcopy of the model.\n        model = clone(model, safe=False)\n    return cast(ModelT, model)\n</code></pre>"},{"location":"api/pydvl/valuation/utility/knn/#pydvl.valuation.utility.knn.KNNClassifierUtility._utility","title":"_utility","text":"<pre><code>_utility(sample: SampleT) -&gt; float\n</code></pre> PARAMETER DESCRIPTION <code>sample</code> <p>contains a subset of valid indices for the <code>x</code> attribute of Dataset.</p> <p> TYPE: <code>SampleT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>0 if no indices are passed, otherwise the KNN utility for the sample.</p> Source code in <code>src/pydvl/valuation/utility/knn.py</code> <pre><code>def _utility(self, sample: SampleT) -&gt; float:\n    \"\"\"\n\n    Args:\n        sample: contains a subset of valid indices for the\n            `x` attribute of [Dataset][pydvl.valuation.dataset.Dataset].\n\n    Returns:\n        0 if no indices are passed, otherwise the KNN utility for the sample.\n    \"\"\"\n    if self.training_data is None:\n        raise ValueError(\"No training data provided\")\n\n    check_is_fitted(\n        self.model,\n        msg=\"The KNN model has to be fitted before calling the utility.\",\n    )\n\n    _, y_train = self.training_data.data()\n    x_test, y_test = self.test_data.data()\n    n_neighbors = self.model.get_params()[\"n_neighbors\"]\n\n    if self.sorted_neighbors is None:\n        self.sorted_neighbors = self.model.kneighbors(x_test, return_distance=False)\n\n    # Labels of the (restricted) nearest neighbors to each test point\n    nns_labels = np.full((len(x_test), n_neighbors), None)\n    for i, neighbors in enumerate(self.sorted_neighbors):\n        restricted_ns = neighbors[np.isin(neighbors, sample.subset)]\n        nns_labels[i][: len(restricted_ns)] = y_train[restricted_ns]\n    # Likelihood of the correct labels\n    probs = np.asarray(nns_labels == y_test[:, None]).sum(axis=1) / n_neighbors\n    return float(probs.mean())\n</code></pre>"},{"location":"api/pydvl/valuation/utility/knn/#pydvl.valuation.utility.knn.KNNClassifierUtility.sample_to_data","title":"sample_to_data","text":"<pre><code>sample_to_data(sample: SampleT) -&gt; tuple\n</code></pre> <p>Returns the raw data corresponding to a sample.</p> <p>Subclasses can override this e.g. to do reshaping of tensors. Be careful not to rely on <code>self.training_data</code> not changing between calls to this method. For manipulations to it, use the <code>with_dataset()</code> method.</p> PARAMETER DESCRIPTION <code>sample</code> <p>contains a subset of valid indices for the <code>x_train</code> attribute of Dataset.</p> <p> TYPE: <code>SampleT</code> </p> <p>Returns:     Tuple of the training data and labels corresponding to the sample indices.</p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>def sample_to_data(self, sample: SampleT) -&gt; tuple:\n    \"\"\"Returns the raw data corresponding to a sample.\n\n    Subclasses can override this e.g. to do reshaping of tensors. Be careful not to\n    rely on `self.training_data` not changing between calls to this method. For\n    manipulations to it, use the `with_dataset()` method.\n\n    Args:\n        sample: contains a subset of valid indices for the\n            `x_train` attribute of [Dataset][pydvl.utils.dataset.Dataset].\n    Returns:\n        Tuple of the training data and labels corresponding to the sample indices.\n    \"\"\"\n    if self.training_data is None:\n        raise ValueError(\"No training data provided\")\n\n    x_train, y_train = self.training_data.data(sample.subset)\n    return x_train, y_train\n</code></pre>"},{"location":"api/pydvl/valuation/utility/knn/#pydvl.valuation.utility.knn.KNNClassifierUtility.with_dataset","title":"with_dataset","text":"<pre><code>with_dataset(data: Dataset, copy: bool = True) -&gt; Self\n</code></pre> <p>Return the utility, or a copy of it, with the given dataset and the model fitted on it.</p> PARAMETER DESCRIPTION <code>data</code> <p>The dataset to use.</p> <p> TYPE: <code>Dataset</code> </p> <code>copy</code> <p>Whether to copy the utility object or not. Additionally, if <code>True</code> then the model is also cloned. If <code>False</code>, the model is only cloned if <code>clone_before_fit</code> is <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Returns:     The utility object.</p> Source code in <code>src/pydvl/valuation/utility/knn.py</code> <pre><code>def with_dataset(self, data: Dataset, copy: bool = True) -&gt; Self:\n    \"\"\"Return the utility, or a copy of it, with the given dataset and the model\n    fitted on it.\n\n    Args:\n        data: The dataset to use.\n        copy: Whether to copy the utility object or not. Additionally, if `True`\n            then the model is also cloned. If `False`, the model is only cloned if\n            `clone_before_fit` is `True`.\n    Returns:\n        The utility object.\n    \"\"\"\n    utility: Self = super().with_dataset(data, copy)\n    if copy or self.clone_before_fit:\n        utility.model = self._maybe_clone_model(self.model, do_clone=True)\n    utility.model.fit(*data.data())\n    return utility\n</code></pre>"},{"location":"api/pydvl/valuation/utility/learning/","title":"Learning","text":""},{"location":"api/pydvl/valuation/utility/learning/#pydvl.valuation.utility.learning","title":"pydvl.valuation.utility.learning","text":"<p>This module implements Data Utility Learning (Wang et al., 2022).<sup>1</sup></p> <p>DUL uses an ML model to learn the utility function. Essentially, it learns to predict the performance of a model when trained on a given set of indices from the dataset. The cost of training this model is quickly amortized by avoiding costly re-evaluations of the original utility.</p> <p>Usage is through the [DataUtilityLearning] class, which wraps any utility function and a UtilityModel to learn it. The wrapper collects utility samples until a given budget is reached, and then fits the model. After that, it forwards any queries for utility values to this learned model to predict the utility of new samples at constant, and low, cost.</p> <p>See the documentation for more information.</p> <p>Todo</p> <p>DUL does not support parallel training of the model yet. This is a limitation of the current architecture. Additionally, batching of utility evaluations should be added to really profit from neural network architectures.</p>"},{"location":"api/pydvl/valuation/utility/learning/#pydvl.valuation.utility.learning--references","title":"References","text":"<ol> <li> <p>Wang, T., Yang, Y. and Jia, R., 2021. Improving cooperative game theory-based data valuation via data utility learning. arXiv preprint arXiv:2107.06336.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pydvl/valuation/utility/learning/#pydvl.valuation.utility.learning.DataUtilityLearning","title":"DataUtilityLearning","text":"<pre><code>DataUtilityLearning(\n    utility: UtilityBase,\n    training_budget: int,\n    model: UtilityModel,\n    show_warnings: bool = True,\n)\n</code></pre> <p>               Bases: <code>UtilityBase[SampleT]</code></p> <p>This object wraps any class derived from UtilityBase and delegates calls to it, up until a given budget (number of iterations). Every tuple of input and output (a so-called utility sample) is stored. Once the budget is exhausted, <code>DataUtilityLearning</code> fits the given model to the utility samples. Subsequent calls will use the learned model to predict the utility instead of delegating.</p> PARAMETER DESCRIPTION <code>utility</code> <p>The utility to learn. Typically, this will be a ModelUtility object encapsulating a machine learning model which requires fitting on each evaluation of the utility.</p> <p> TYPE: <code>UtilityBase</code> </p> <code>training_budget</code> <p>Number of utility samples to collect before fitting the given model.</p> <p> TYPE: <code>int</code> </p> <code>model</code> <p>A wrapper for a supervised model that can be trained on a collection of utility samples.</p> <p> TYPE: <code>UtilityModel</code> </p> Source code in <code>src/pydvl/valuation/utility/learning.py</code> <pre><code>def __init__(\n    self,\n    utility: UtilityBase,\n    training_budget: int,\n    model: UtilityModel,\n    show_warnings: bool = True,\n) -&gt; None:\n    self.utility = utility\n    self.training_budget = training_budget\n    self.model = model\n    self.n_predictions = 0\n    self.show_warnings = show_warnings\n    self._is_fitted = False\n    self._utility_samples: dict[Sample, float] = {}\n</code></pre>"},{"location":"api/pydvl/valuation/utility/learning/#pydvl.valuation.utility.learning.DataUtilityLearning.training_data","title":"training_data  <code>property</code>","text":"<pre><code>training_data: Dataset | None\n</code></pre> <p>Retrieves the training data used by this utility.</p> <p>This property is read-only. In order to set it, use with_dataset().</p>"},{"location":"api/pydvl/valuation/utility/learning/#pydvl.valuation.utility.learning.DataUtilityLearning.with_dataset","title":"with_dataset","text":"<pre><code>with_dataset(data: Dataset, copy: bool = True) -&gt; Self\n</code></pre> <p>Returns the utility, or a copy of it, with the given dataset. Args:     data: The dataset to use for utility fitting (training data)     copy: Whether to copy the utility object or not. Valuation methods should         always make copies to avoid unexpected side effects. Returns:     The utility object.</p> Source code in <code>src/pydvl/valuation/utility/base.py</code> <pre><code>def with_dataset(self, data: Dataset, copy: bool = True) -&gt; Self:\n    \"\"\"Returns the utility, or a copy of it, with the given dataset.\n    Args:\n        data: The dataset to use for utility fitting (training data)\n        copy: Whether to copy the utility object or not. Valuation methods should\n            always make copies to avoid unexpected side effects.\n    Returns:\n        The utility object.\n    \"\"\"\n    utility = cp.copy(self) if copy else self\n    utility._training_data = data\n    return utility\n</code></pre>"},{"location":"api/pydvl/valuation/utility/learning/#pydvl.valuation.utility.learning.IndicatorUtilityModel","title":"IndicatorUtilityModel","text":"<pre><code>IndicatorUtilityModel(predictor: SupervisedModel, n_data: int)\n</code></pre> <p>               Bases: <code>UtilityModel</code></p> <p>A simple wrapper for arbitrary predictors.</p> <p>Uses 1-hot encoding of the indices as input for the model, as done in Wang et al., (2022)<sup>1</sup>.</p> <p>This encoding can be fed to any regressor. See the documentation for details.</p> PARAMETER DESCRIPTION <code>predictor</code> <p>A supervised model that implements the <code>fit</code> and <code>predict</code> methods. This model will be trained on the encoded utility samples gathered by the DataUtilityLearning object.</p> <p> TYPE: <code>SupervisedModel</code> </p> <code>n_data</code> <p>Number of indices in the dataset. This is used to create the input matrix for the model.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/valuation/utility/learning.py</code> <pre><code>def __init__(self, predictor: SupervisedModel, n_data: int):\n    self.n_data = n_data\n    self.predictor = predictor\n</code></pre>"},{"location":"api/pydvl/valuation/utility/learning/#pydvl.valuation.utility.learning.UtilityModel","title":"UtilityModel","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for utility models.</p> <p>A utility model predicts the value of a utility function given a sample. The model is trained on a collection of samples and their respective utility values. These tuples are called Utility Samples.</p> <p>Utility models:</p> <ul> <li>are fitted on dictionaries of Sample -&gt; utility value</li> <li>predict: Collection[samples] -&gt; NDArray[utility values]</li> </ul>"},{"location":"api/pydvl/valuation/utility/modelutility/","title":"Modelutility","text":""},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility","title":"pydvl.valuation.utility.modelutility","text":"<p>This module implements a utility function for supervised models.</p> <p>ModelUtility holds a model and a scorer. Each call to the utility will fit the model on a subset of the training data and evaluate the scorer on the test data. It is used by all the valuation methods in pydvl.valuation.</p> <p>This class is geared towards sci-kit-learn models, but can be used with any object that implements the BaseModel protocol, i.e. that has a <code>fit()</code> method.</p> <p>Errors are hidden by default</p> <p>During semi-value computations, the utility can be evaluated on subsets that break the fitting process. For instance, a classifier might require at least two classes to fit, but the utility is sometimes evaluated on subsets with only one class. This will raise an error with most classifiers. To avoid this, we set by default <code>catch_errors=True</code> upon instantiation, which will catch the error and return the scorer's default value instead. While we show a warning to signal that something went wrong, this suppression can lead to unexpected results, so it is important to be aware of this setting and to set it to <code>False</code> when testing, or if you are sure that the utility will not be evaluated on problematic subsets.</p>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility--examples","title":"Examples","text":"Standard usage <p>The utility takes a model and a scorer and is passed to the valuation method. Here's the basic usage:</p> <pre><code>from joblib import parallel_config\nfrom pydvl.valuation import (\n    Dataset, MinUpdates, ModelUtility, SupervisedScorer, TMCShapleyValuation\n)\n\ntrain, test = Dataset.from_arrays(X, y, ...)\nmodel = SomeModel()  # Implementing the basic scikit-learn interface\nscorer =  SupervisedScorer(\"r2\", test, default=0.0, range=(-np.inf, 1.0))\nutility = ModelUtility(model, scorer, catch_errors=True, show_warnings=True)\nvaluation = TMCShapleyValuation(utility, is_done=MinUpdates(1000))\nwith parallel_config(n_jobs=-1):\n    valuation.fit(train)\n</code></pre> Directly calling the utility <p>The following code instantiates a utility object and calls it directly. The underlying logistic regression model will be trained on the indices passed as argument, and evaluated on the test data.</p> <pre><code>from pydvl.valuation.utility import ModelUtility\nfrom pydvl.valuation.dataset import Dataset\nfrom pydvl.valuation.scorers import SupervisedScorer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.datasets import load_iris\n\ntrain, test = Dataset.from_sklearn(load_iris(), random_state=16)\nscorer =  SupervisedScorer(\"accuracy\", test, default=0.0, range=(0.0, 1.0))\nu = ModelUtility(LogisticRegression(random_state=16), scorer, catch_errors=True)\nu(Sample(None, subset=train.indices))\n</code></pre> Enabling the cache <p>In this example an in-memory cache is used. Note that caching is only useful under certain conditions, and does not really speed typical Monte Carlo approximations. See [the introduction][#getting-started-cache] and the module documentation for more.</p> <pre><code>(...)  # Imports as above\ncache_backend = InMemoryCacheBackend()  # See other backends in the caching module\nu = ModelUtility(\n        model=LogisticRegression(random_state=16),\n        scorer=SupervisedScorer(\"accuracy\", test, default=0.0, range=(0.0, 1.0)),\n        cache_backend=cache_backend,\n        catch_errors=True\n    )\nu(Sample(None, subset=train.indices))\nu(Sample(None, subset=train.indices))  # The second call does not retrain the model\n</code></pre>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility--data-type-of-the-underlying-data-arrays","title":"Data type of the underlying data arrays","text":"<p>In principle, very few to no assumptions are made about the data type. As long as it is contained in a Dataset object, it should work. If your data needs special handling before being fed to the model from the <code>Dataset</code>, you can override the sample_to_data() method. Be sure not to rely on the data being static for this. If you need to transform it before fitting, then override with_dataset().</p> <p>Caveats with parallel computation</p> <p>When running in parallel, the utility is copied to each worker, which implies copying the dataset as well, which can obviously be very expensive. In order to alleviate the problem, one can memmap the data to disk. Alas, automatic memmapping by joblib does not work for nested structures like Dataset objects, nor for pytorch tensors. For now, it should be possible to use memmap manually but it hasn't been tested.</p> <p>If you are working on a cluster, the data will be copied to each worker. In this case, subclassing of <code>Dataset</code> and <code>Utility</code> will be necessary to minimize copying, and the solution will depend on your storage solution. Feel free to open an issue if you need help with this.</p>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility.ModelUtility","title":"ModelUtility","text":"<pre><code>ModelUtility(\n    model: ModelT,\n    scorer: Scorer,\n    *,\n    catch_errors: bool = True,\n    show_warnings: bool = True,\n    cache_backend: CacheBackend | None = None,\n    cached_func_options: CachedFuncConfig | None = None,\n    clone_before_fit: bool = True\n)\n</code></pre> <p>               Bases: <code>UtilityBase[SampleT]</code>, <code>Generic[SampleT, ModelT]</code></p> <p>Convenience wrapper with configurable memoization of the utility.</p> <p>An instance of <code>ModelUtility</code> holds the tuple of model, and scoring function which determines the value of data points. This is used for the computation of all game-theoretic values like Shapley values and the Least Core.</p> <p><code>ModelUtility</code> expects the model to fulfill at least the BaseModel interface, i.e. to have a <code>fit()</code> method</p> <p>When calling the utility, the model will be cloned if it is a Scikit-Learn model, otherwise a copy is created using copy.deepcopy</p> <p>Since evaluating the scoring function requires retraining the model and that can be time-consuming, this class wraps it and caches the results of each execution. Caching is available both locally and across nodes, but must always be enabled for your project first, because most stochastic methods do not benefit much from it. See the documentation and the module documentation.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>The supervised model.</p> <p> TYPE: <code>ModelT</code> </p> <code>scorer</code> <p>A scoring function. If None, the <code>score()</code> method of the model will be used. See score for ways to create and compose scorers, in particular how to set default values and ranges.</p> <p> TYPE: <code>Scorer</code> </p> PARAMETER DESCRIPTION <code>model</code> <p>Any supervised model. Typical choices can be found in the sci-kit learn documentation.</p> <p> TYPE: <code>ModelT</code> </p> <code>scorer</code> <p>A scoring object. If None, the <code>score()</code> method of the model will be used. See scorers for ways to create and compose scorers, in particular how to set default values and ranges. For convenience, a string can be passed, which will be used to construct a SupervisedScorer.</p> <p> TYPE: <code>Scorer</code> </p> <code>catch_errors</code> <p>set to <code>True</code> to catch the errors when <code>fit()</code> fails. This could happen in several steps of the pipeline, e.g. when too little training data is passed, which happens often during Shapley value calculations. When this happens, the scorer's default value is returned as a score and computation continues.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_warnings</code> <p>Set to <code>False</code> to suppress warnings thrown by <code>fit()</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>cache_backend</code> <p>Optional instance of CacheBackend used to memoize results to avoid duplicate computation. Note however, that for most stochastic methods, cache hits are rare, making the memory expense of caching not worth it (YMMV).</p> <p> TYPE: <code>CacheBackend | None</code> DEFAULT: <code>None</code> </p> <code>cached_func_options</code> <p>Optional configuration object for cached utility evaluation.</p> <p> TYPE: <code>CachedFuncConfig | None</code> DEFAULT: <code>None</code> </p> <code>clone_before_fit</code> <p>If <code>True</code>, the model will be cloned before calling <code>fit()</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>def __init__(\n    self,\n    model: ModelT,\n    scorer: Scorer,\n    *,\n    catch_errors: bool = True,\n    show_warnings: bool = True,\n    cache_backend: CacheBackend | None = None,\n    cached_func_options: CachedFuncConfig | None = None,\n    clone_before_fit: bool = True,\n):\n    self.clone_before_fit = clone_before_fit\n    self.model = self._maybe_clone_model(model, clone_before_fit)\n    self.scorer = scorer\n    self.catch_errors = catch_errors\n    self.show_warnings = show_warnings\n    self.cache = cache_backend\n    if cached_func_options is None:\n        cached_func_options = CachedFuncConfig()\n    # TODO: Find a better way to do this.\n    if cached_func_options.hash_prefix is None:\n        # FIX: This does not handle reusing the same across runs.\n        cached_func_options.hash_prefix = str(hash((model, scorer)))\n    self.cached_func_options = cached_func_options\n    self._initialize_utility_wrapper()\n</code></pre>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility.ModelUtility.cache_stats","title":"cache_stats  <code>property</code>","text":"<pre><code>cache_stats: CacheStats | None\n</code></pre> <p>Cache statistics are gathered when cache is enabled. See CacheStats for all fields returned.</p>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility.ModelUtility.training_data","title":"training_data  <code>property</code>","text":"<pre><code>training_data: Dataset | None\n</code></pre> <p>Retrieves the training data used by this utility.</p> <p>This property is read-only. In order to set it, use with_dataset().</p>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility.ModelUtility.__call__","title":"__call__","text":"<pre><code>__call__(sample: SampleT | None) -&gt; float\n</code></pre> PARAMETER DESCRIPTION <code>sample</code> <p>contains a subset of valid indices for the <code>x_train</code> attribute of Dataset.</p> <p> TYPE: <code>SampleT | None</code> </p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>def __call__(self, sample: SampleT | None) -&gt; float:\n    \"\"\"\n    Args:\n        sample: contains a subset of valid indices for the\n            `x_train` attribute of [Dataset][pydvl.utils.dataset.Dataset].\n    \"\"\"\n    if sample is None or len(sample.subset) == 0:\n        return self.scorer.default\n\n    return cast(float, self._utility_wrapper(sample))\n</code></pre>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility.ModelUtility._compute_score","title":"_compute_score","text":"<pre><code>_compute_score(model: ModelT) -&gt; float\n</code></pre> <p>Computes the score of a fitted model.</p> PARAMETER DESCRIPTION <code>model</code> <p>fitted model</p> <p> TYPE: <code>ModelT</code> </p> <p>Returns:     Computed score or the scorer's default value in case of an error     or a NaN value.</p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>def _compute_score(self, model: ModelT) -&gt; float:\n    \"\"\"Computes the score of a fitted model.\n\n    Args:\n        model: fitted model\n    Returns:\n        Computed score or the scorer's default value in case of an error\n        or a NaN value.\n    \"\"\"\n    try:\n        score = float(self.scorer(model))\n        # Some scorers raise exceptions if they return NaNs, some might not\n        if np.isnan(score):\n            warnings.warn(\"Scorer returned NaN\", RuntimeWarning)\n            return self.scorer.default\n    except Exception as e:\n        if self.catch_errors:\n            warnings.warn(str(e), RuntimeWarning)\n            return self.scorer.default\n        raise\n    return score\n</code></pre>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility.ModelUtility._maybe_clone_model","title":"_maybe_clone_model  <code>staticmethod</code>","text":"<pre><code>_maybe_clone_model(model: ModelT, do_clone: bool) -&gt; ModelT\n</code></pre> <p>Clones the passed model to avoid the possibility of reusing a fitted estimator.</p> PARAMETER DESCRIPTION <code>model</code> <p>Any supervised model. Typical choices can be found on this page</p> <p> TYPE: <code>ModelT</code> </p> <code>do_clone</code> <p>Whether to clone the model or not.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>@staticmethod\ndef _maybe_clone_model(model: ModelT, do_clone: bool) -&gt; ModelT:\n    \"\"\"Clones the passed model to avoid the possibility of reusing a fitted\n    estimator.\n\n    Args:\n        model: Any supervised model. Typical choices can be found\n            on [this page](https://scikit-learn.org/stable/supervised_learning.html)\n        do_clone: Whether to clone the model or not.\n    \"\"\"\n    if not do_clone:\n        return model\n    try:\n        model = clone(model)\n    except TypeError:\n        # This happens if the passed model is not an sklearn model\n        # In this case, we just make a deepcopy of the model.\n        model = clone(model, safe=False)\n    return cast(ModelT, model)\n</code></pre>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility.ModelUtility._utility","title":"_utility","text":"<pre><code>_utility(sample: SampleT) -&gt; float\n</code></pre> <p>Clones the model, fits it on a subset of the training data and scores it on the test data.</p> PARAMETER DESCRIPTION <code>sample</code> <p>contains a subset of valid indices for the <code>x</code> attribute of Dataset.</p> <p> TYPE: <code>SampleT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>0 if no indices are passed, <code>scorer.default</code> if we fail to fit the model or the scorer returns numpy.nan. Otherwise, the score of the model.</p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>@suppress_warnings(flag=\"show_warnings\")\ndef _utility(self, sample: SampleT) -&gt; float:\n    \"\"\"Clones the model, fits it on a subset of the training data\n    and scores it on the test data.\n\n    Args:\n        sample: contains a subset of valid indices for the\n            `x` attribute of [Dataset][pydvl.valuation.dataset.Dataset].\n\n    Returns:\n        0 if no indices are passed, `scorer.default` if we fail to fit the\n            model or the scorer returns [numpy.nan][]. Otherwise, the score\n            of the model.\n    \"\"\"\n    x_train, y_train = self.sample_to_data(sample)\n\n    try:\n        model = self._maybe_clone_model(self.model, self.clone_before_fit)\n        model.fit(x_train, y_train)\n        score = self._compute_score(model)\n        return score\n    except Exception as e:\n        if self.catch_errors:\n            warnings.warn(str(e), RuntimeWarning)\n            return self.scorer.default\n        raise\n</code></pre>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility.ModelUtility.sample_to_data","title":"sample_to_data","text":"<pre><code>sample_to_data(sample: SampleT) -&gt; tuple\n</code></pre> <p>Returns the raw data corresponding to a sample.</p> <p>Subclasses can override this e.g. to do reshaping of tensors. Be careful not to rely on <code>self.training_data</code> not changing between calls to this method. For manipulations to it, use the <code>with_dataset()</code> method.</p> PARAMETER DESCRIPTION <code>sample</code> <p>contains a subset of valid indices for the <code>x_train</code> attribute of Dataset.</p> <p> TYPE: <code>SampleT</code> </p> <p>Returns:     Tuple of the training data and labels corresponding to the sample indices.</p> Source code in <code>src/pydvl/valuation/utility/modelutility.py</code> <pre><code>def sample_to_data(self, sample: SampleT) -&gt; tuple:\n    \"\"\"Returns the raw data corresponding to a sample.\n\n    Subclasses can override this e.g. to do reshaping of tensors. Be careful not to\n    rely on `self.training_data` not changing between calls to this method. For\n    manipulations to it, use the `with_dataset()` method.\n\n    Args:\n        sample: contains a subset of valid indices for the\n            `x_train` attribute of [Dataset][pydvl.utils.dataset.Dataset].\n    Returns:\n        Tuple of the training data and labels corresponding to the sample indices.\n    \"\"\"\n    if self.training_data is None:\n        raise ValueError(\"No training data provided\")\n\n    x_train, y_train = self.training_data.data(sample.subset)\n    return x_train, y_train\n</code></pre>"},{"location":"api/pydvl/valuation/utility/modelutility/#pydvl.valuation.utility.modelutility.ModelUtility.with_dataset","title":"with_dataset","text":"<pre><code>with_dataset(data: Dataset, copy: bool = True) -&gt; Self\n</code></pre> <p>Returns the utility, or a copy of it, with the given dataset. Args:     data: The dataset to use for utility fitting (training data)     copy: Whether to copy the utility object or not. Valuation methods should         always make copies to avoid unexpected side effects. Returns:     The utility object.</p> Source code in <code>src/pydvl/valuation/utility/base.py</code> <pre><code>def with_dataset(self, data: Dataset, copy: bool = True) -&gt; Self:\n    \"\"\"Returns the utility, or a copy of it, with the given dataset.\n    Args:\n        data: The dataset to use for utility fitting (training data)\n        copy: Whether to copy the utility object or not. Valuation methods should\n            always make copies to avoid unexpected side effects.\n    Returns:\n        The utility object.\n    \"\"\"\n    utility = cp.copy(self) if copy else self\n    utility._training_data = data\n    return utility\n</code></pre>"},{"location":"deprecated/","title":"New interface for data valuation","text":"<p>The module pydvl.value and its submodules have been deprecated in favor of the new interface pydvl.valuation. The new interface is more flexible and allows for more advanced data valuation techniques. The old interface will be removed in a future release.</p>"},{"location":"deprecated/pydvl/parallel/","title":"Parallelization","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0 in favor of    joblib's context manager joblib.parallel_config.</p>"},{"location":"deprecated/pydvl/parallel/#pydvl.parallel","title":"pydvl.parallel","text":"<p>This module provides a common interface to parallelization backends. The list of supported backends is here. Backends should be instantiated directly and passed to the respective valuation method.</p> <p>We use executors that implement the Executor interface to submit tasks in parallel. The basic high-level pattern is:</p> <pre><code>from pydvl.parallel import JoblibParallelBackend\n\nparallel_backend = JoblibParallelBackend()\nwith parallel_backend.executor(max_workers=2) as executor:\n    future = executor.submit(lambda x: x + 1, 1)\n    result = future.result()\nassert result == 2\n</code></pre> <p>Running a map-style job is also easy:</p> <pre><code>from pydvl.parallel import JoblibParallelBackend\n\nparallel_backend = JoblibParallelBackend()\nwith parallel_backend.executor(max_workers=2) as executor:\n    results = list(executor.map(lambda x: x + 1, range(5)))\nassert results == [1, 2, 3, 4, 5]\n</code></pre> <p>Passsing large objects</p> <p>When running tasks which accept heavy inputs, it is important to first use <code>put()</code> on the object and use the returned reference as argument to the callable within <code>submit()</code>. For example: <pre><code>u_ref = parallel_backend.put(u)\n...\nexecutor.submit(task, utility=u)\n</code></pre> Note that <code>task()</code> does not need to be changed in any way: the backend will <code>get()</code> the object and pass it to the function upon invocation.</p> <p>There is an alternative map-reduce implementation MapReduceJob which internally uses joblib's higher level API with <code>Parallel()</code> which then indirectly also supports the use of Dask and Ray.</p>"},{"location":"deprecated/pydvl/parallel/backend/","title":"Backend","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0 in favor of    joblib's context manager joblib.parallel_config.</p>"},{"location":"deprecated/pydvl/parallel/backend/#pydvl.parallel.backend","title":"pydvl.parallel.backend","text":""},{"location":"deprecated/pydvl/parallel/backend/#pydvl.parallel.backend.CancellationPolicy","title":"CancellationPolicy","text":"<p>               Bases: <code>Flag</code></p> <p>Policy to use when cancelling futures after exiting an Executor.</p> <p>Note</p> <p>Not all backends support all policies.</p> ATTRIBUTE DESCRIPTION <code>NONE</code> <p>Do not cancel any futures.</p> <p> </p> <code>PENDING</code> <p>Cancel all pending futures, but not running ones.</p> <p> </p> <code>RUNNING</code> <p>Cancel all running futures, but not pending ones.</p> <p> </p> <code>ALL</code> <p>Cancel all pending and running futures.</p> <p> </p>"},{"location":"deprecated/pydvl/parallel/backend/#pydvl.parallel.backend.ParallelBackend","title":"ParallelBackend","text":"<p>Abstract base class for all parallel backends.</p>"},{"location":"deprecated/pydvl/parallel/backend/#pydvl.parallel.backend.ParallelBackend.executor","title":"executor  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>executor(\n    max_workers: int | None = None,\n    *,\n    config: ParallelConfig | None = None,\n    cancel_futures: CancellationPolicy | bool = PENDING\n) -&gt; Executor\n</code></pre> <p>Returns a futures executor for the parallel backend.</p> Source code in <code>src/pydvl/parallel/backend.py</code> <pre><code>@classmethod\n@abstractmethod\ndef executor(\n    cls,\n    max_workers: int | None = None,\n    *,\n    config: ParallelConfig | None = None,\n    cancel_futures: CancellationPolicy | bool = CancellationPolicy.PENDING,\n) -&gt; Executor:\n    \"\"\"Returns a futures executor for the parallel backend.\"\"\"\n    ...\n</code></pre>"},{"location":"deprecated/pydvl/parallel/backend/#pydvl.parallel.backend._maybe_init_parallel_backend","title":"_maybe_init_parallel_backend","text":"<pre><code>_maybe_init_parallel_backend(\n    parallel_backend: ParallelBackend | None = None,\n    config: ParallelConfig | None = None,\n) -&gt; ParallelBackend\n</code></pre> <p>Helper function inside during the deprecation period of  and should be removed in v0.10.0</p> Source code in <code>src/pydvl/parallel/backend.py</code> <pre><code>def _maybe_init_parallel_backend(\n    parallel_backend: ParallelBackend | None = None,\n    config: ParallelConfig | None = None,\n) -&gt; ParallelBackend:\n    \"\"\"Helper function inside during the deprecation period of\n    [][pydvl.parallel.backend.init_parallel_backend] and should be removed in v0.10.0\n    \"\"\"\n    if parallel_backend is not None:\n        if config is not None:\n            warnings.warn(\n                \"You should not set both `config` and `parallel_backend`. The former will be ignored.\",\n                UserWarning,\n            )\n    else:\n        if config is not None:\n            parallel_backend = init_parallel_backend(config)\n        else:\n            from pydvl.parallel.backends import JoblibParallelBackend\n\n            parallel_backend = JoblibParallelBackend()\n    return parallel_backend\n</code></pre>"},{"location":"deprecated/pydvl/parallel/backend/#pydvl.parallel.backend.available_cpus","title":"available_cpus","text":"<pre><code>available_cpus() -&gt; int\n</code></pre> <p>Platform-independent count of available cores.</p> <p>FIXME: do we really need this or is <code>os.cpu_count</code> enough? Is this portable?</p> RETURNS DESCRIPTION <code>int</code> <p>Number of cores, or 1 if it is not possible to determine.</p> Source code in <code>src/pydvl/parallel/backend.py</code> <pre><code>def available_cpus() -&gt; int:\n    \"\"\"Platform-independent count of available cores.\n\n    FIXME: do we really need this or is `os.cpu_count` enough? Is this portable?\n\n    Returns:\n        Number of cores, or 1 if it is not possible to determine.\n    \"\"\"\n    from platform import system\n\n    if system() != \"Linux\":\n        return os.cpu_count() or 1\n    return len(os.sched_getaffinity(0))  # type: ignore\n</code></pre>"},{"location":"deprecated/pydvl/parallel/backend/#pydvl.parallel.backend.init_parallel_backend","title":"init_parallel_backend","text":"<pre><code>init_parallel_backend(\n    config: ParallelConfig | None = None, backend_name: str | None = None\n) -&gt; ParallelBackend\n</code></pre> <p>Initializes the parallel backend and returns an instance of it.</p> <p>The following example creates a parallel backend instance with the default configuration, which is a local joblib backend.</p> <p>If you don't pass any arguments, then by default it will instantiate the JoblibParallelBackend:</p> Example <pre><code>parallel_backend = init_parallel_backend()\n</code></pre> <p>To create a parallel backend instance with for example <code>ray</code> as a backend, you can pass the backend name as a string:.</p> Example <pre><code>parallel_backend = init_parallel_backend(backend_name=\"ray\")\n</code></pre> <p>The following is an example of the deprecated way for instantiating a parallel backend:</p> Example <pre><code>config = ParallelConfig()\nparallel_backend = init_parallel_backend(config)\n</code></pre> PARAMETER DESCRIPTION <code>backend_name</code> <p>Name of the backend to instantiate.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>ParallelConfig | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/parallel/backend.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef init_parallel_backend(\n    config: ParallelConfig | None = None, backend_name: str | None = None\n) -&gt; ParallelBackend:\n    \"\"\"Initializes the parallel backend and returns an instance of it.\n\n    The following example creates a parallel backend instance with the default\n    configuration, which is a local joblib backend.\n\n    If you don't pass any arguments, then by default it will instantiate\n    the JoblibParallelBackend:\n\n    ??? Example\n        ```python\n        parallel_backend = init_parallel_backend()\n        ```\n\n    To create a parallel backend instance with for example `ray` as a backend,\n    you can pass the backend name as a string:.\n\n    ??? Example\n        ```python\n        parallel_backend = init_parallel_backend(backend_name=\"ray\")\n        ```\n\n\n    The following is an example of the deprecated\n    way for instantiating a parallel backend:\n\n    ??? Example\n        ``` python\n        config = ParallelConfig()\n        parallel_backend = init_parallel_backend(config)\n        ```\n\n    Args:\n        backend_name: Name of the backend to instantiate.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n\n\n    \"\"\"\n    if backend_name is None:\n        if config is None:\n            backend_name = \"joblib\"\n        else:\n            backend_name = config.backend\n\n    try:\n        parallel_backend_cls = ParallelBackend.BACKENDS[backend_name]\n    except KeyError:\n        raise NotImplementedError(f\"Unexpected parallel backend {backend_name}\")\n    return parallel_backend_cls(config)  # type: ignore\n</code></pre>"},{"location":"deprecated/pydvl/parallel/config/","title":"Config","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0 in favor of    joblib's context manager joblib.parallel_config.</p>"},{"location":"deprecated/pydvl/parallel/config/#pydvl.parallel.config","title":"pydvl.parallel.config","text":""},{"location":"deprecated/pydvl/parallel/config/#pydvl.parallel.config.ParallelConfig","title":"ParallelConfig  <code>dataclass</code>","text":"<pre><code>ParallelConfig(\n    backend: Literal[\"joblib\", \"ray\"] = \"joblib\",\n    address: Optional[Union[str, Tuple[str, int]]] = None,\n    n_cpus_local: Optional[int] = None,\n    logging_level: Optional[int] = None,\n    wait_timeout: float = 1.0,\n)\n</code></pre> <p>Configuration for parallel computation backend.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Type of backend to use. Defaults to 'joblib'</p> <p> TYPE: <code>Literal['joblib', 'ray']</code> DEFAULT: <code>'joblib'</code> </p> <code>address</code> <p>(DEPRECATED) Address of existing remote or local cluster to use.</p> <p> TYPE: <code>Optional[Union[str, Tuple[str, int]]]</code> DEFAULT: <code>None</code> </p> <code>n_cpus_local</code> <p>(DEPRECATED) Number of CPUs to use when creating a local ray cluster. This has no effect when using an existing ray cluster.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>logging_level</code> <p>(DEPRECATED) Logging level for the parallel backend's worker.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>wait_timeout</code> <p>(DEPRECATED) Timeout in seconds for waiting on futures.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p>"},{"location":"deprecated/pydvl/parallel/map_reduce/","title":"Map reduce","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0 in favor of    joblib's context manager joblib.parallel_config.</p>"},{"location":"deprecated/pydvl/parallel/map_reduce/#pydvl.parallel.map_reduce","title":"pydvl.parallel.map_reduce","text":"<p>This module contains a wrapper around joblib's <code>Parallel()</code> class that makes it easy to run map-reduce jobs.</p> <p>Deprecation</p> <p>This interface might be deprecated or changed in a future release before 1.0</p>"},{"location":"deprecated/pydvl/parallel/map_reduce/#pydvl.parallel.map_reduce.MapReduceJob","title":"MapReduceJob","text":"<pre><code>MapReduceJob(\n    inputs: Union[Collection[T], T],\n    map_func: MapFunction[R],\n    reduce_func: ReduceFunction[R] = identity,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    *,\n    map_kwargs: Optional[Dict] = None,\n    reduce_kwargs: Optional[Dict] = None,\n    n_jobs: int = -1,\n    timeout: Optional[float] = None\n)\n</code></pre> <p>               Bases: <code>Generic[T, R]</code></p> <p>Takes an embarrassingly parallel fun and runs it in <code>n_jobs</code> parallel jobs, splitting the data evenly into a number of chunks equal to the number of jobs.</p> <p>Typing information for objects of this class requires the type of the inputs that are split for <code>map_func</code> and the type of its output.</p> PARAMETER DESCRIPTION <code>inputs</code> <p>The input that will be split and passed to <code>map_func</code>. if it's not a sequence object. It will be repeat <code>n_jobs</code> number of times.</p> <p> TYPE: <code>Union[Collection[T], T]</code> </p> <code>map_func</code> <p>Function that will be applied to the input chunks in each job.</p> <p> TYPE: <code>MapFunction[R]</code> </p> <code>reduce_func</code> <p>Function that will be applied to the results of <code>map_func</code> to reduce them.</p> <p> TYPE: <code>ReduceFunction[R]</code> DEFAULT: <code>identity</code> </p> <code>map_kwargs</code> <p>Keyword arguments that will be passed to <code>map_func</code> in each job. Alternatively, one can use functools.partial.</p> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>None</code> </p> <code>reduce_kwargs</code> <p>Keyword arguments that will be passed to <code>reduce_func</code> in each job. Alternatively, one can use functools.partial.</p> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>None</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to run. Does not accept 0</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> Example <p>A simple usage example with 2 jobs:</p> <pre><code>&gt;&gt;&gt; from pydvl.parallel import MapReduceJob\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; map_reduce_job: MapReduceJob[np.ndarray, np.ndarray] = MapReduceJob(\n...     np.arange(5),\n...     map_func=np.sum,\n...     reduce_func=np.sum,\n...     n_jobs=2,\n... )\n&gt;&gt;&gt; map_reduce_job()\n10\n</code></pre> <p>When passed a single object as input, it will be repeated for each job: <pre><code>&gt;&gt;&gt; from pydvl.parallel import MapReduceJob\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; map_reduce_job: MapReduceJob[int, np.ndarray] = MapReduceJob(\n...     5,\n...     map_func=lambda x: np.array([x]),\n...     reduce_func=np.sum,\n...     n_jobs=2,\n... )\n&gt;&gt;&gt; map_reduce_job()\n10\n</code></pre></p> Source code in <code>src/pydvl/parallel/map_reduce.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef __init__(\n    self,\n    inputs: Union[Collection[T], T],\n    map_func: MapFunction[R],\n    reduce_func: ReduceFunction[R] = identity,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    *,\n    map_kwargs: Optional[Dict] = None,\n    reduce_kwargs: Optional[Dict] = None,\n    n_jobs: int = -1,\n    timeout: Optional[float] = None,\n):\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n\n    self.parallel_backend = parallel_backend\n\n    self.timeout = timeout\n\n    self._n_jobs = -1\n    # This uses the setter defined below\n    self.n_jobs = n_jobs\n\n    self.inputs_ = inputs\n\n    self.map_kwargs = map_kwargs if map_kwargs is not None else dict()\n    self.reduce_kwargs = reduce_kwargs if reduce_kwargs is not None else dict()\n\n    self._map_func = reduce(maybe_add_argument, [\"job_id\", \"seed\"], map_func)\n    self._reduce_func = reduce_func\n</code></pre>"},{"location":"deprecated/pydvl/parallel/map_reduce/#pydvl.parallel.map_reduce.MapReduceJob.n_jobs","title":"n_jobs  <code>property</code> <code>writable</code>","text":"<pre><code>n_jobs: int\n</code></pre> <p>Effective number of jobs according to the used ParallelBackend instance.</p>"},{"location":"deprecated/pydvl/parallel/map_reduce/#pydvl.parallel.map_reduce.MapReduceJob.__call__","title":"__call__","text":"<pre><code>__call__(seed: Optional[Union[Seed, SeedSequence]] = None) -&gt; R\n</code></pre> <p>Runs the map-reduce job.</p> PARAMETER DESCRIPTION <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Union[Seed, SeedSequence]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>R</code> <p>The result of the reduce function.</p> Source code in <code>src/pydvl/parallel/map_reduce.py</code> <pre><code>def __call__(\n    self,\n    seed: Optional[Union[Seed, SeedSequence]] = None,\n) -&gt; R:\n    \"\"\"\n    Runs the map-reduce job.\n\n    Args:\n        seed: Either an instance of a numpy random number generator or a seed for\n            it.\n\n    Returns:\n         The result of the reduce function.\n    \"\"\"\n    seed_seq = ensure_seed_sequence(seed)\n\n    if hasattr(self.parallel_backend, \"_joblib_backend_name\"):\n        backend = getattr(self.parallel_backend, \"_joblib_backend_name\")\n    else:\n        warnings.warn(\n            \"Parallel backend \"\n            f\"{self.parallel_backend.__class__.__name__}. \"\n            \"should have a `_joblib_backend_name` attribute in order to work \"\n            \"property with MapReduceJob. \"\n            \"Defaulting to joblib loky backend\"\n        )\n        backend = \"loky\"\n\n    with Parallel(backend=backend, prefer=\"processes\") as parallel:\n        chunks = self._chunkify(self.inputs_, n_chunks=self.n_jobs)\n        map_results: List[R] = parallel(\n            delayed(self._map_func)(\n                next_chunk, job_id=j, seed=seed, **self.map_kwargs\n            )\n            for j, (next_chunk, seed) in enumerate(\n                zip(chunks, seed_seq.spawn(len(chunks)))\n            )\n        )\n\n    reduce_results: R = self._reduce_func(map_results, **self.reduce_kwargs)\n    return reduce_results\n</code></pre>"},{"location":"deprecated/pydvl/parallel/map_reduce/#pydvl.parallel.map_reduce.MapReduceJob._chunkify","title":"_chunkify","text":"<pre><code>_chunkify(\n    data: Union[NDArray, Collection[T], T], n_chunks: int\n) -&gt; List[Union[NDArray, Collection[T], T]]\n</code></pre> <p>If data is a Sequence, it splits it into Sequences of size <code>n_chunks</code> for each job that we call chunks. If instead data is an <code>ObjectRef</code> instance, then it yields it repeatedly <code>n_chunks</code> number of times.</p> Source code in <code>src/pydvl/parallel/map_reduce.py</code> <pre><code>def _chunkify(\n    self, data: Union[NDArray, Collection[T], T], n_chunks: int\n) -&gt; List[Union[NDArray, Collection[T], T]]:\n    \"\"\"If data is a Sequence, it splits it into Sequences of size `n_chunks` for each job that we call chunks.\n    If instead data is an `ObjectRef` instance, then it yields it repeatedly `n_chunks` number of times.\n    \"\"\"\n    if n_chunks &lt;= 0:\n        raise ValueError(\"Number of chunks should be greater than 0\")\n\n    if n_chunks == 1:\n        return [data]\n\n    try:\n        # This is used as a check to determine whether data is iterable or not\n        # if it's the former, then the value will be used to determine the chunk indices.\n        n = len(data)  # type: ignore\n    except TypeError:\n        return list(repeat(data, times=n_chunks))\n    else:\n        # This is very much inspired by numpy's array_split function\n        # The difference is that it only uses built-in functions\n        # and does not convert the input data to an array\n        chunk_size, remainder = divmod(n, n_chunks)\n        chunk_indices = tuple(\n            accumulate(\n                [0]\n                + remainder * [chunk_size + 1]\n                + (n_chunks - remainder) * [chunk_size]\n            )\n        )\n\n        chunks = []\n\n        for start_index, end_index in zip(chunk_indices[:-1], chunk_indices[1:]):\n            if start_index &gt;= end_index:\n                break\n            chunk = data[start_index:end_index]  # type: ignore\n            chunks.append(chunk)\n\n        return chunks\n</code></pre>"},{"location":"deprecated/pydvl/parallel/backends/","title":"Backends","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0 in favor of    joblib's context manager joblib.parallel_config.</p>"},{"location":"deprecated/pydvl/parallel/backends/#pydvl.parallel.backends","title":"pydvl.parallel.backends","text":""},{"location":"deprecated/pydvl/parallel/backends/joblib/","title":"Joblib","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0 in favor of    joblib's context manager joblib.parallel_config.</p>"},{"location":"deprecated/pydvl/parallel/backends/joblib/#pydvl.parallel.backends.joblib","title":"pydvl.parallel.backends.joblib","text":""},{"location":"deprecated/pydvl/parallel/backends/joblib/#pydvl.parallel.backends.joblib.JoblibParallelBackend","title":"JoblibParallelBackend","text":"<pre><code>JoblibParallelBackend(config: ParallelConfig | None = None)\n</code></pre> <p>               Bases: <code>ParallelBackend</code></p> <p>Class used to wrap joblib to make it transparent to algorithms.</p> <p>Example</p> <pre><code>from pydvl.parallel import JoblibParallelBackend\nparallel_backend = JoblibParallelBackend()\n</code></pre> Source code in <code>src/pydvl/parallel/backends/joblib.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": None},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef __init__(self, config: ParallelConfig | None = None) -&gt; None:\n    n_jobs: int | None = None\n    if config is not None:\n        n_jobs = config.n_cpus_local\n    self.config = {\n        \"n_jobs\": n_jobs,\n    }\n</code></pre>"},{"location":"deprecated/pydvl/parallel/backends/joblib/#pydvl.parallel.backends.joblib.JoblibParallelBackend._joblib_backend_name","title":"_joblib_backend_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_joblib_backend_name: str = 'loky'\n</code></pre> <p>Name of the backend to use for joblib inside MapReduceJob.</p>"},{"location":"deprecated/pydvl/parallel/backends/joblib/#pydvl.parallel.backends.joblib.JoblibParallelBackend.executor","title":"executor  <code>classmethod</code>","text":"<pre><code>executor(\n    max_workers: int | None = None,\n    *,\n    config: ParallelConfig | None = None,\n    cancel_futures: CancellationPolicy | bool = NONE\n) -&gt; Executor\n</code></pre> <p>Returns a futures executor for the parallel backend.</p> <p>Example</p> <pre><code>from pydvl.parallel import JoblibParallelBackend\nparallel_backend = JoblibParallelBackend()\nwith parallel_backend.executor() as executor:\n    executor.submit(...)\n</code></pre> PARAMETER DESCRIPTION <code>max_workers</code> <p>Maximum number of parallel workers.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>ParallelConfig | None</code> DEFAULT: <code>None</code> </p> <code>cancel_futures</code> <p>Policy to use when cancelling futures after exiting an Executor.</p> <p> TYPE: <code>CancellationPolicy | bool</code> DEFAULT: <code>NONE</code> </p> RETURNS DESCRIPTION <code>Executor</code> <p>Instance of _ReusablePoolExecutor</p> Source code in <code>src/pydvl/parallel/backends/joblib.py</code> <pre><code>@classmethod\ndef executor(\n    cls,\n    max_workers: int | None = None,\n    *,\n    config: ParallelConfig | None = None,\n    cancel_futures: CancellationPolicy | bool = CancellationPolicy.NONE,\n) -&gt; Executor:\n    \"\"\"Returns a futures executor for the parallel backend.\n\n    !!! Example\n        ``` python\n        from pydvl.parallel import JoblibParallelBackend\n        parallel_backend = JoblibParallelBackend()\n        with parallel_backend.executor() as executor:\n            executor.submit(...)\n        ```\n\n    Args:\n        max_workers: Maximum number of parallel workers.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        cancel_futures: Policy to use when cancelling futures\n            after exiting an Executor.\n\n    Returns:\n        Instance of\n            [_ReusablePoolExecutor](https://github.com/joblib/loky/blob/master/loky/reusable_executor.py)\n    \"\"\"\n    if config is not None:\n        warnings.warn(\n            \"The `JoblibParallelBackend` uses deprecated arguments: \"\n            \"`config`. They were deprecated since v0.9.0 \"\n            \"and will be removed in v0.10.0.\",\n            FutureWarning,\n        )\n\n    if cancel_futures not in (CancellationPolicy.NONE, False):\n        warnings.warn(\n            \"Cancellation of futures is not supported by the joblib backend\",\n        )\n    return cast(Executor, get_reusable_executor(max_workers=max_workers))\n</code></pre>"},{"location":"deprecated/pydvl/parallel/backends/joblib/#pydvl.parallel.backends.joblib.JoblibParallelBackend.wrap","title":"wrap","text":"<pre><code>wrap(fun: Callable, **kwargs) -&gt; Callable\n</code></pre> <p>Wraps a function as a joblib delayed.</p> PARAMETER DESCRIPTION <code>fun</code> <p>the function to wrap</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>The delayed function.</p> Source code in <code>src/pydvl/parallel/backends/joblib.py</code> <pre><code>def wrap(self, fun: Callable, **kwargs) -&gt; Callable:\n    \"\"\"Wraps a function as a joblib delayed.\n\n    Args:\n        fun: the function to wrap\n\n    Returns:\n        The delayed function.\n    \"\"\"\n    return delayed(fun)  # type: ignore\n</code></pre>"},{"location":"deprecated/pydvl/parallel/backends/ray/","title":"Ray","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0 in favor of    joblib's context manager joblib.parallel_config.</p>"},{"location":"deprecated/pydvl/parallel/backends/ray/#pydvl.parallel.backends.ray","title":"pydvl.parallel.backends.ray","text":""},{"location":"deprecated/pydvl/parallel/backends/ray/#pydvl.parallel.backends.ray.RayParallelBackend","title":"RayParallelBackend","text":"<pre><code>RayParallelBackend(config: ParallelConfig | None = None)\n</code></pre> <p>               Bases: <code>ParallelBackend</code></p> <p>Class used to wrap ray to make it transparent to algorithms.</p> <p>Example</p> <pre><code>import ray\nfrom pydvl.parallel import RayParallelBackend\nray.init()\nparallel_backend = RayParallelBackend()\n</code></pre> Source code in <code>src/pydvl/parallel/backends/ray.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": None},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef __init__(self, config: ParallelConfig | None = None) -&gt; None:\n    if not ray.is_initialized():\n        raise RuntimeError(\n            \"Starting from v0.9.0, ray is no longer automatically initialized. \"\n            \"Please use `ray.init()` with the desired configuration \"\n            \"before using this class.\"\n        )\n    # Register ray joblib backend\n    register_ray()\n</code></pre>"},{"location":"deprecated/pydvl/parallel/backends/ray/#pydvl.parallel.backends.ray.RayParallelBackend._joblib_backend_name","title":"_joblib_backend_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_joblib_backend_name: str = 'ray'\n</code></pre> <p>Name of the backend to use for joblib inside MapReduceJob.</p>"},{"location":"deprecated/pydvl/parallel/backends/ray/#pydvl.parallel.backends.ray.RayParallelBackend.executor","title":"executor  <code>classmethod</code>","text":"<pre><code>executor(\n    max_workers: int | None = None,\n    *,\n    config: ParallelConfig | None = None,\n    cancel_futures: CancellationPolicy | bool = PENDING\n) -&gt; Executor\n</code></pre> <p>Returns a futures executor for the parallel backend.</p> <p>Example</p> <pre><code>import ray\nfrom pydvl.parallel import RayParallelBackend\nray.init()\nparallel_backend = RayParallelBackend()\nwith parallel_backend.executor() as executor:\n    executor.submit(...)\n</code></pre> PARAMETER DESCRIPTION <code>max_workers</code> <p>Maximum number of parallel workers.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>ParallelConfig | None</code> DEFAULT: <code>None</code> </p> <code>cancel_futures</code> <p>Policy to use when cancelling futures after exiting an Executor.</p> <p> TYPE: <code>CancellationPolicy | bool</code> DEFAULT: <code>PENDING</code> </p> RETURNS DESCRIPTION <code>Executor</code> <p>Instance of RayExecutor.</p> Source code in <code>src/pydvl/parallel/backends/ray.py</code> <pre><code>@classmethod\ndef executor(\n    cls,\n    max_workers: int | None = None,\n    *,\n    config: ParallelConfig | None = None,\n    cancel_futures: CancellationPolicy | bool = CancellationPolicy.PENDING,\n) -&gt; Executor:\n    \"\"\"Returns a futures executor for the parallel backend.\n\n    !!! Example\n        ``` python\n        import ray\n        from pydvl.parallel import RayParallelBackend\n        ray.init()\n        parallel_backend = RayParallelBackend()\n        with parallel_backend.executor() as executor:\n            executor.submit(...)\n        ```\n\n    Args:\n        max_workers: Maximum number of parallel workers.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        cancel_futures: Policy to use when cancelling futures\n            after exiting an Executor.\n\n    Returns:\n        Instance of [RayExecutor][pydvl.parallel.futures.ray.RayExecutor].\n    \"\"\"\n    # Imported here to avoid circular import errors\n    from pydvl.parallel.futures.ray import RayExecutor\n\n    if config is not None:\n        warnings.warn(\n            \"The `RayParallelBackend` uses deprecated arguments: \"\n            \"`config`. They were deprecated since v0.9.0 \"\n            \"and will be removed in v0.10.0.\",\n            FutureWarning,\n        )\n\n    return RayExecutor(max_workers, cancel_futures=cancel_futures)  # type: ignore\n</code></pre>"},{"location":"deprecated/pydvl/parallel/backends/ray/#pydvl.parallel.backends.ray.RayParallelBackend.wrap","title":"wrap","text":"<pre><code>wrap(fun: Callable, **kwargs: dict[str, Any]) -&gt; Callable\n</code></pre> <p>Wraps a function as a ray remote.</p> PARAMETER DESCRIPTION <code>fun</code> <p>the function to wrap</p> <p> TYPE: <code>Callable</code> </p> <code>kwargs</code> <p>keyword arguments to pass to @ray.remote</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>The <code>.remote</code> method of the ray <code>RemoteFunction</code>.</p> Source code in <code>src/pydvl/parallel/backends/ray.py</code> <pre><code>def wrap(self, fun: Callable, **kwargs: dict[str, Any]) -&gt; Callable:\n    \"\"\"Wraps a function as a ray remote.\n\n    Args:\n        fun: the function to wrap\n        kwargs: keyword arguments to pass to @ray.remote\n\n    Returns:\n        The `.remote` method of the ray `RemoteFunction`.\n    \"\"\"\n    if len(kwargs) &gt; 0:\n        return ray.remote(**kwargs)(fun).remote  # type: ignore\n    return ray.remote(fun).remote  # type: ignore\n</code></pre>"},{"location":"deprecated/pydvl/parallel/futures/","title":"Futures","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0 in favor of    joblib's context manager joblib.parallel_config.</p>"},{"location":"deprecated/pydvl/parallel/futures/#pydvl.parallel.futures","title":"pydvl.parallel.futures","text":""},{"location":"deprecated/pydvl/parallel/futures/ray/","title":"Ray","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0 in favor of    joblib's context manager joblib.parallel_config.</p>"},{"location":"deprecated/pydvl/parallel/futures/ray/#pydvl.parallel.futures.ray","title":"pydvl.parallel.futures.ray","text":""},{"location":"deprecated/pydvl/parallel/futures/ray/#pydvl.parallel.futures.ray.RayExecutor","title":"RayExecutor","text":"<pre><code>RayExecutor(\n    max_workers: Optional[int] = None,\n    *,\n    config: Optional[ParallelConfig] = None,\n    cancel_futures: Union[CancellationPolicy, bool] = ALL\n)\n</code></pre> <p>               Bases: <code>Executor</code></p> <p>Asynchronous executor using Ray that implements the concurrent.futures API.</p> PARAMETER DESCRIPTION <code>max_workers</code> <p>Maximum number of concurrent tasks. Each task can request itself any number of vCPUs. You must ensure the product of this value and the n_cpus_per_job parameter passed to submit() does not exceed available cluster resources. If set to <code>None</code>, it will default to the total number of vCPUs in the ray cluster.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>cancel_futures</code> <p>Select which futures will be cancelled when exiting this context manager. <code>Pending</code> is the default, which will cancel all pending futures, but not running ones, as done by concurrent.futures.ProcessPoolExecutor. Additionally, <code>All</code> cancels all pending and running futures, and <code>None</code> doesn't cancel any. See CancellationPolicy</p> <p> TYPE: <code>Union[CancellationPolicy, bool]</code> DEFAULT: <code>ALL</code> </p> Source code in <code>src/pydvl/parallel/futures/ray.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": None},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef __init__(\n    self,\n    max_workers: Optional[int] = None,\n    *,\n    config: Optional[ParallelConfig] = None,\n    cancel_futures: Union[CancellationPolicy, bool] = CancellationPolicy.ALL,\n):\n    if max_workers is not None:\n        if max_workers &lt;= 0:\n            raise ValueError(\"max_workers must be greater than 0\")\n        max_workers = max_workers\n\n    if isinstance(cancel_futures, CancellationPolicy):\n        self._cancel_futures = cancel_futures\n    else:\n        self._cancel_futures = (\n            CancellationPolicy.PENDING\n            if cancel_futures\n            else CancellationPolicy.NONE\n        )\n\n    if not ray.is_initialized():\n        raise RuntimeError(\n            \"Starting from v0.9.0, ray is no longer automatically initialized. \"\n            \"Please use `ray.init()` with the desired configuration \"\n            \"before using this class.\"\n        )\n\n    self._max_workers = max_workers\n    if self._max_workers is None:\n        self._max_workers = int(ray._private.state.cluster_resources()[\"CPU\"])\n\n    self._shutdown = False\n    self._shutdown_lock = threading.Lock()\n    self._queue_lock = threading.Lock()\n    self._work_queue: \"queue.Queue[Optional[_WorkItem]]\" = queue.Queue(\n        maxsize=self._max_workers\n    )\n    self._pending_queue: \"queue.SimpleQueue[Optional[_WorkItem]]\" = (\n        queue.SimpleQueue()\n    )\n\n    # Work Item Manager Thread\n    self._work_item_manager_thread: Optional[_WorkItemManagerThread] = None\n</code></pre>"},{"location":"deprecated/pydvl/parallel/futures/ray/#pydvl.parallel.futures.ray.RayExecutor.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Exit the runtime context related to the RayExecutor object.</p> Source code in <code>src/pydvl/parallel/futures/ray.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Exit the runtime context related to the RayExecutor object.\"\"\"\n    self.shutdown()\n    return False\n</code></pre>"},{"location":"deprecated/pydvl/parallel/futures/ray/#pydvl.parallel.futures.ray.RayExecutor.shutdown","title":"shutdown","text":"<pre><code>shutdown(wait: bool = True, *, cancel_futures: Optional[bool] = None) -&gt; None\n</code></pre> <p>Clean up the resources associated with the Executor.</p> <p>This method tries to mimic the behaviour of Executor.shutdown while allowing one more value for <code>cancel_futures</code> which instructs it to use the CancellationPolicy defined upon construction.</p> PARAMETER DESCRIPTION <code>wait</code> <p>Whether to wait for pending futures to finish.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>cancel_futures</code> <p>Overrides the executor's default policy for cancelling futures on exit. If <code>True</code>, all pending futures are cancelled, and if <code>False</code>, no futures are cancelled. If <code>None</code> (default), the executor's policy set at initialization is used.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/parallel/futures/ray.py</code> <pre><code>def shutdown(\n    self, wait: bool = True, *, cancel_futures: Optional[bool] = None\n) -&gt; None:\n    \"\"\"Clean up the resources associated with the Executor.\n\n    This method tries to mimic the behaviour of\n    [Executor.shutdown][concurrent.futures.Executor.shutdown]\n    while allowing one more value for ``cancel_futures`` which instructs it\n    to use the [CancellationPolicy][pydvl.parallel.backend.CancellationPolicy]\n    defined upon construction.\n\n    Args:\n        wait: Whether to wait for pending futures to finish.\n        cancel_futures: Overrides the executor's default policy for\n            cancelling futures on exit. If ``True``, all pending futures are\n            cancelled, and if ``False``, no futures are cancelled. If ``None``\n            (default), the executor's policy set at initialization is used.\n    \"\"\"\n    logger.debug(\"executor shutting down\")\n    with self._shutdown_lock:\n        logger.debug(\"executor acquired shutdown lock\")\n        self._shutdown = True\n        self._cancel_futures = {\n            None: self._cancel_futures,\n            True: CancellationPolicy.PENDING,\n            False: CancellationPolicy.NONE,\n        }[cancel_futures]\n\n    if wait:\n        logger.debug(\"executor waiting for futures to finish\")\n        if self._work_item_manager_thread is not None:\n            # Putting None in the queue to signal\n            # to work item manager thread that we are shutting down\n            self._put_work_item_in_queue(None)\n            logger.debug(\n                \"executor waiting for work item manager thread to terminate\"\n            )\n            self._work_item_manager_thread.join()\n        # To reduce the risk of opening too many files, remove references to\n        # objects that use file descriptors.\n        self._work_item_manager_thread = None\n        del self._work_queue\n        del self._pending_queue\n</code></pre>"},{"location":"deprecated/pydvl/parallel/futures/ray/#pydvl.parallel.futures.ray.RayExecutor.submit","title":"submit","text":"<pre><code>submit(fn: Callable[..., T], /, *args: Any, **kwargs: Any) -&gt; Future[T]\n</code></pre> <p>Submits a callable to be executed with the given arguments.</p> <p>Schedules the callable to be executed as fn(*args, **kwargs) and returns a Future instance representing the execution of the callable.</p> PARAMETER DESCRIPTION <code>fn</code> <p>Callable.</p> <p> TYPE: <code>Callable[..., T]</code> </p> <code>args</code> <p>Positional arguments that will be passed to <code>fn</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>Keyword arguments that will be passed to <code>fn</code>. It can also optionally contain options for the ray remote function as a dictionary as the keyword argument <code>remote_function_options</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Returns:     A Future representing the given call.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If a task is submitted after the executor has been shut down.</p> Source code in <code>src/pydvl/parallel/futures/ray.py</code> <pre><code>def submit(self, fn: Callable[..., T], /, *args: Any, **kwargs: Any) -&gt; Future[T]:\n    r\"\"\"Submits a callable to be executed with the given arguments.\n\n    Schedules the callable to be executed as fn(\\*args, \\**kwargs)\n    and returns a Future instance representing the execution of the callable.\n\n    Args:\n        fn: Callable.\n        args: Positional arguments that will be passed to `fn`.\n        kwargs: Keyword arguments that will be passed to `fn`.\n            It can also optionally contain options for the ray remote function\n            as a dictionary as the keyword argument `remote_function_options`.\n    Returns:\n        A Future representing the given call.\n\n    Raises:\n        RuntimeError: If a task is submitted after the executor has been shut down.\n    \"\"\"\n    with self._shutdown_lock:\n        logger.debug(\"executor acquired shutdown lock\")\n        if self._shutdown:\n            raise RuntimeError(\"cannot schedule new futures after shutdown\")\n\n        logging.debug(\"Creating future and putting work item in work queue\")\n        future: \"Future[T]\" = Future()\n        remote_function_options = kwargs.pop(\"remote_function_options\", None)\n        w = _WorkItem(\n            future,\n            fn,\n            args,\n            kwargs,\n            remote_function_options=remote_function_options,\n        )\n        self._put_work_item_in_queue(w)\n        # We delay starting the thread until the first call to submit\n        self._start_work_item_manager_thread()\n        return future\n</code></pre>"},{"location":"deprecated/pydvl/parallel/futures/ray/#pydvl.parallel.futures.ray._WorkItem","title":"_WorkItem","text":"<pre><code>_WorkItem(\n    future: Future,\n    fn: Callable,\n    args: Any,\n    kwargs: Any,\n    *,\n    remote_function_options: Optional[dict] = None\n)\n</code></pre> <p>Inspired by code from: concurrent.futures</p> Source code in <code>src/pydvl/parallel/futures/ray.py</code> <pre><code>def __init__(\n    self,\n    future: Future,\n    fn: Callable,\n    args: Any,\n    kwargs: Any,\n    *,\n    remote_function_options: Optional[dict] = None,\n):\n    self.future = future\n    self.fn = fn\n    self.args = args\n    self.kwargs = kwargs\n    self.remote_function_options = remote_function_options or {\"num_cpus\": 1.0}\n</code></pre>"},{"location":"deprecated/pydvl/parallel/futures/ray/#pydvl.parallel.futures.ray._WorkItemManagerThread","title":"_WorkItemManagerThread","text":"<pre><code>_WorkItemManagerThread(executor: RayExecutor)\n</code></pre> <p>               Bases: <code>Thread</code></p> <p>Manages submitting the work items and throttling.</p> <p>It runs in a local thread. Args:     executor: An instance of RayExecutor that owns     this thread. A weakref will be owned by the manager as well as     references to internal objects used to introspect the state of     the executor.</p> Source code in <code>src/pydvl/parallel/futures/ray.py</code> <pre><code>def __init__(self, executor: RayExecutor):\n    self.executor_reference = ref(executor)\n    self.shutdown_lock: threading.Lock = executor._shutdown_lock\n    self.queue_lock: threading.Lock = executor._queue_lock\n    self.work_queue: \"queue.Queue[Optional[_WorkItem]]\" = executor._work_queue\n    self.pending_queue: \"queue.SimpleQueue[Optional[_WorkItem]]\" = (\n        executor._pending_queue\n    )\n    self.submitted_futures: \"WeakSet[Future]\" = WeakSet()\n    super().__init__()\n</code></pre>"},{"location":"deprecated/pydvl/value/","title":"Data Valuation","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/#pydvl.value","title":"pydvl.value","text":"<p>This module implements algorithms for the exact and approximate computation of values and semi-values.</p> <p>See Data valuation for an introduction to the concepts and methods implemented here.</p>"},{"location":"deprecated/pydvl/value/games/","title":"Games","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games","title":"pydvl.value.games","text":"<p>This module provides several predefined games and, depending on the game, the corresponding Shapley values, Least Core values or both of them, for benchmarking purposes.</p>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games--references","title":"References","text":"<ol> <li> <p>Castro, J., G\u00f3mez, D. and Tejada,   J., 2009. Polynomial calculation of the Shapley value based on   sampling.   Computers &amp; Operations Research, 36(5), pp.1726-1730.\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.AirportGame","title":"AirportGame","text":"<pre><code>AirportGame(n_players: int = 100)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>An airport game defined in (Castro et al., 2009)<sup>1</sup> Section 4.3</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> Source code in <code>src/pydvl/value/games.py</code> <pre><code>def __init__(self, n_players: int = 100) -&gt; None:\n    if n_players != 100:\n        raise ValueError(\n            f\"{self.__class__.__name__} only supports n_players=100 but got {n_players=}.\"\n        )\n    description = \"A dummy dataset for the airport game in Castro et al. 2009\"\n    super().__init__(n_players, score_range=(0, 100), description=description)\n    ranges = [\n        range(0, 8),\n        range(8, 20),\n        range(20, 26),\n        range(26, 40),\n        range(40, 48),\n        range(48, 57),\n        range(57, 70),\n        range(70, 80),\n        range(80, 90),\n        range(90, 100),\n    ]\n    exact = [\n        0.01,\n        0.020869565,\n        0.033369565,\n        0.046883079,\n        0.063549745,\n        0.082780515,\n        0.106036329,\n        0.139369662,\n        0.189369662,\n        0.289369662,\n    ]\n    c = list(range(1, 10))\n    score_table = np.zeros(100)\n    exact_values = np.zeros(100)\n\n    for r, v in zip(ranges, exact):\n        score_table[r] = c\n        exact_values[r] = v\n\n    self.exact_values = exact_values\n    self.score_table = score_table\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.AsymmetricVotingGame","title":"AsymmetricVotingGame","text":"<pre><code>AsymmetricVotingGame(n_players: int = 51)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>An asymmetric voting game defined in (Castro et al., 2009)<sup>1</sup> Section 4.2.</p> <p>For this game the player set is \\(N = \\{1,\\dots,51\\}\\) and the utility of a coalition is given by:</p> \\[{ v(S) = \\left\\{\\begin{array}{ll} 1, &amp; \\text{ if} \\quad \\sum\\limits_{i \\in S} w_i &gt; \\sum\\limits_{j \\in N}\\frac{w_j}{2} \\\\ 0, &amp; \\text{ otherwise} \\end{array}\\right. }\\] <p>where \\(w = [w_1,\\dots, w_{51}]\\) is a list of weights associated with each player.</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> DEFAULT: <code>51</code> </p> Source code in <code>src/pydvl/value/games.py</code> <pre><code>def __init__(self, n_players: int = 51) -&gt; None:\n    if n_players != 51:\n        raise ValueError(\n            f\"{self.__class__.__name__} only supports n_players=51 but got {n_players=}.\"\n        )\n    description = \"Dummy data for the asymmetric voting game in Castro et al. 2009\"\n    super().__init__(\n        n_players,\n        score_range=(0, 1),\n        description=description,\n    )\n\n    ranges = [\n        range(0, 1),\n        range(1, 2),\n        range(2, 3),\n        range(3, 5),\n        range(5, 6),\n        range(6, 7),\n        range(7, 9),\n        range(9, 10),\n        range(10, 12),\n        range(12, 15),\n        range(15, 16),\n        range(16, 20),\n        range(20, 24),\n        range(24, 26),\n        range(26, 30),\n        range(30, 34),\n        range(34, 35),\n        range(35, 44),\n        range(44, 51),\n    ]\n\n    ranges_weights = [\n        45,\n        41,\n        27,\n        26,\n        25,\n        21,\n        17,\n        14,\n        13,\n        12,\n        11,\n        10,\n        9,\n        8,\n        7,\n        6,\n        5,\n        4,\n        3,\n    ]\n    ranges_values = [\n        \"0.08831\",\n        \"0.07973\",\n        \"0.05096\",\n        \"0.04898\",\n        \"0.047\",\n        \"0.03917\",\n        \"0.03147\",\n        \"0.02577\",\n        \"0.02388\",\n        \"0.022\",\n        \"0.02013\",\n        \"0.01827\",\n        \"0.01641\",\n        \"0.01456\",\n        \"0.01272\",\n        \"0.01088\",\n        \"0.009053\",\n        \"0.00723\",\n        \"0.005412\",\n    ]\n\n    self.weight_table = np.zeros(self.n_players)\n    exact_values = np.zeros(self.n_players)\n    for r, w, v in zip(ranges, ranges_weights, ranges_values):\n        self.weight_table[r] = w\n        exact_values[r] = v\n\n    self.exact_values = exact_values\n    self.threshold = np.sum(self.weight_table) / 2\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.DummyGameDataset","title":"DummyGameDataset","text":"<pre><code>DummyGameDataset(n_players: int, description: Optional[str] = None)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dummy game dataset.</p> <p>Initializes a dummy game dataset with n_players and an optional description.</p> <p>This class is used internally inside the Game class.</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> </p> <code>description</code> <p>Optional description of the dataset.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/value/games.py</code> <pre><code>def __init__(self, n_players: int, description: Optional[str] = None) -&gt; None:\n    x = np.arange(0, n_players, 1).reshape(-1, 1)\n    nil = np.zeros_like(x)\n    super().__init__(\n        x,\n        nil.copy(),\n        nil.copy(),\n        nil.copy(),\n        feature_names=[\"x\"],\n        target_names=[\"y\"],\n        description=description,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.DummyGameDataset.data_names","title":"data_names  <code>property</code>","text":"<pre><code>data_names: NDArray[object_]\n</code></pre> <p>Names of each individual datapoint.</p> <p>Used for reporting Shapley values.</p>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.DummyGameDataset.dim","title":"dim  <code>property</code>","text":"<pre><code>dim: int\n</code></pre> <p>Returns the number of dimensions of a sample.</p>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.DummyGameDataset.indices","title":"indices  <code>property</code>","text":"<pre><code>indices: NDArray[int_]\n</code></pre> <p>Index of positions in data.x_train.</p> <p>Contiguous integers from 0 to len(Dataset).</p>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.DummyGameDataset.from_arrays","title":"from_arrays  <code>classmethod</code>","text":"<pre><code>from_arrays(\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any\n) -&gt; Dataset\n</code></pre> <p>Constructs a Dataset object from X and y numpy arrays  as returned by the <code>make_*</code> functions in sklearn generated datasets.</p> Example <pre><code>&gt;&gt;&gt; from pydvl.utils import Dataset\n&gt;&gt;&gt; from sklearn.datasets import make_regression\n&gt;&gt;&gt; X, y = make_regression()\n&gt;&gt;&gt; dataset = Dataset.from_arrays(X, y)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>numpy array of shape (n_samples, n_features)</p> <p> TYPE: <code>NDArray</code> </p> <code>y</code> <p>numpy array of shape (n_samples,)</p> <p> TYPE: <code>NDArray</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code></p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>random_state</code> <p>seed for train / test split</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stratify_by_target</code> <p>If <code>True</code>, data is split in a stratified fashion, using the y variable as labels. Read more in sklearn's user guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Dataset constructor. Use this to pass e.g. <code>feature_names</code> or <code>target_names</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>Object with the passed X and y arrays split across training and test sets.</p> <p>New in version 0.4.0</p> <p>Changed in version 0.6.0</p> <p>Added kwargs to pass to the Dataset constructor.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>@classmethod\ndef from_arrays(\n    cls,\n    X: NDArray,\n    y: NDArray,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any,\n) -&gt; \"Dataset\":\n    \"\"\"Constructs a [Dataset][pydvl.utils.Dataset] object from X and y numpy arrays  as\n    returned by the `make_*` functions in [sklearn generated datasets](https://scikit-learn.org/stable/datasets/sample_generators.html).\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from pydvl.utils import Dataset\n        &gt;&gt;&gt; from sklearn.datasets import make_regression\n        &gt;&gt;&gt; X, y = make_regression()\n        &gt;&gt;&gt; dataset = Dataset.from_arrays(X, y)\n        ```\n\n    Args:\n        X: numpy array of shape (n_samples, n_features)\n        y: numpy array of shape (n_samples,)\n        train_size: size of the training dataset. Used in `train_test_split`\n        random_state: seed for train / test split\n        stratify_by_target: If `True`, data is split in a stratified fashion,\n            using the y variable as labels. Read more in [sklearn's user\n            guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.utils.Dataset] constructor. Use this to pass e.g. `feature_names`\n            or `target_names`.\n\n    Returns:\n        Object with the passed X and y arrays split across training and test sets.\n\n    !!! tip \"New in version 0.4.0\"\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added kwargs to pass to the [Dataset][pydvl.utils.Dataset] constructor.\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(\n        X,\n        y,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=y if stratify_by_target else None,\n    )\n    return cls(x_train, y_train, x_test, y_test, **kwargs)\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.DummyGameDataset.from_sklearn","title":"from_sklearn  <code>classmethod</code>","text":"<pre><code>from_sklearn(\n    data: Bunch,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any\n) -&gt; Dataset\n</code></pre> <p>Constructs a Dataset object from a sklearn.utils.Bunch, as returned by the <code>load_*</code> functions in scikit-learn toy datasets.</p> Example <pre><code>&gt;&gt;&gt; from pydvl.utils import Dataset\n&gt;&gt;&gt; from sklearn.datasets import load_boston\n&gt;&gt;&gt; dataset = Dataset.from_sklearn(load_boston())\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>scikit-learn Bunch object. The following attributes are supported:</p> <ul> <li><code>data</code>: covariates.</li> <li><code>target</code>: target variables (labels).</li> <li><code>feature_names</code> (optional): the feature names.</li> <li><code>target_names</code> (optional): the target names.</li> <li><code>DESCR</code> (optional): a description.</li> </ul> <p> TYPE: <code>Bunch</code> </p> <code>train_size</code> <p>size of the training dataset. Used in <code>train_test_split</code></p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>random_state</code> <p>seed for train / test split</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>stratify_by_target</code> <p>If <code>True</code>, data is split in a stratified fashion, using the target variable as labels. Read more in scikit-learn's user guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>kwargs</code> <p>Additional keyword arguments to pass to the Dataset constructor. Use this to pass e.g. <code>is_multi_output</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>Object with the sklearn dataset</p> <p>Changed in version 0.6.0</p> <p>Added kwargs to pass to the Dataset constructor.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>@classmethod\ndef from_sklearn(\n    cls,\n    data: Bunch,\n    train_size: float = 0.8,\n    random_state: Optional[int] = None,\n    stratify_by_target: bool = False,\n    **kwargs: Any,\n) -&gt; \"Dataset\":\n    \"\"\"Constructs a [Dataset][pydvl.utils.Dataset] object from a\n    [sklearn.utils.Bunch][], as returned by the `load_*`\n    functions in [scikit-learn toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html).\n\n    ??? Example\n        ```pycon\n        &gt;&gt;&gt; from pydvl.utils import Dataset\n        &gt;&gt;&gt; from sklearn.datasets import load_boston\n        &gt;&gt;&gt; dataset = Dataset.from_sklearn(load_boston())\n        ```\n\n    Args:\n        data: scikit-learn Bunch object. The following attributes are supported:\n\n            - `data`: covariates.\n            - `target`: target variables (labels).\n            - `feature_names` (**optional**): the feature names.\n            - `target_names` (**optional**): the target names.\n            - `DESCR` (**optional**): a description.\n        train_size: size of the training dataset. Used in `train_test_split`\n        random_state: seed for train / test split\n        stratify_by_target: If `True`, data is split in a stratified\n            fashion, using the target variable as labels. Read more in\n            [scikit-learn's user guide](https://scikit-learn.org/stable/modules/cross_validation.html#stratification).\n        kwargs: Additional keyword arguments to pass to the\n            [Dataset][pydvl.utils.Dataset] constructor. Use this to pass e.g. `is_multi_output`.\n\n    Returns:\n        Object with the sklearn dataset\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added kwargs to pass to the [Dataset][pydvl.utils.Dataset] constructor.\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(\n        data.data,\n        data.target,\n        train_size=train_size,\n        random_state=random_state,\n        stratify=data.target if stratify_by_target else None,\n    )\n    return cls(\n        x_train,\n        y_train,\n        x_test,\n        y_test,\n        feature_names=data.get(\"feature_names\"),\n        target_names=data.get(\"target_names\"),\n        description=data.get(\"DESCR\"),\n        **kwargs,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.DummyGameDataset.get_test_data","title":"get_test_data","text":"<pre><code>get_test_data(\n    indices: Optional[Iterable[int]] = None,\n) -&gt; Tuple[NDArray, NDArray]\n</code></pre> <p>Returns the subsets of the train set instead of the test set.</p> PARAMETER DESCRIPTION <code>indices</code> <p>Indices into the training data.</p> <p> TYPE: <code>Optional[Iterable[int]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[NDArray, NDArray]</code> <p>Subset of the train data.</p> Source code in <code>src/pydvl/value/games.py</code> <pre><code>def get_test_data(\n    self, indices: Optional[Iterable[int]] = None\n) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"Returns the subsets of the train set instead of the test set.\n\n    Args:\n        indices: Indices into the training data.\n\n    Returns:\n        Subset of the train data.\n    \"\"\"\n    if indices is None:\n        return self.x_train, self.y_train\n    x = self.x_train[indices]\n    y = self.y_train[indices]\n    return x, y\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.DummyGameDataset.get_training_data","title":"get_training_data","text":"<pre><code>get_training_data(\n    indices: Optional[Iterable[int]] = None,\n) -&gt; Tuple[NDArray, NDArray]\n</code></pre> <p>Given a set of indices, returns the training data that refer to those indices.</p> <p>This is used mainly by Utility to retrieve subsets of the data from indices. It is typically not needed in algorithms.</p> PARAMETER DESCRIPTION <code>indices</code> <p>Optional indices that will be used to select points from the training data. If <code>None</code>, the entire training data will be returned.</p> <p> TYPE: <code>Optional[Iterable[int]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[NDArray, NDArray]</code> <p>If <code>indices</code> is not <code>None</code>, the selected x and y arrays from the training data. Otherwise, the entire dataset.</p> Source code in <code>src/pydvl/utils/dataset.py</code> <pre><code>def get_training_data(\n    self, indices: Optional[Iterable[int]] = None\n) -&gt; Tuple[NDArray, NDArray]:\n    \"\"\"Given a set of indices, returns the training data that refer to those\n    indices.\n\n    This is used mainly by [Utility][pydvl.utils.utility.Utility] to retrieve\n    subsets of the data from indices. It is typically **not needed in\n    algorithms**.\n\n    Args:\n        indices: Optional indices that will be used to select points from\n            the training data. If `None`, the entire training data will be\n            returned.\n\n    Returns:\n        If `indices` is not `None`, the selected x and y arrays from the\n            training data. Otherwise, the entire dataset.\n    \"\"\"\n    if indices is None:\n        return self.x_train, self.y_train\n    x = self.x_train[indices]\n    y = self.y_train[indices]\n    return x, y\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.DummyModel","title":"DummyModel","text":"<pre><code>DummyModel()\n</code></pre> <p>               Bases: <code>SupervisedModel</code></p> <p>Dummy model class.</p> <p>A dummy supervised model used for testing purposes only.</p> Source code in <code>src/pydvl/value/games.py</code> <pre><code>def __init__(self) -&gt; None:\n    pass\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.Game","title":"Game","text":"<pre><code>Game(\n    n_players: int,\n    score_range: Tuple[float, float] = (-inf, inf),\n    description: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for games</p> <p>Any Game subclass has to implement the abstract <code>_score</code> method to assign a score to each coalition/subset and at least one of <code>shapley_values</code>, <code>least_core_values</code>.</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> </p> <code>score_range</code> <p>Minimum and maximum values of the <code>_score</code> method.</p> <p> TYPE: <code>Tuple[float, float]</code> DEFAULT: <code>(-inf, inf)</code> </p> <code>description</code> <p>Optional string description of the dummy dataset that will be created.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> ATTRIBUTE DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> </p> <code>data</code> <p>Dummy dataset object.</p> <p> </p> <code>u</code> <p>Utility object with a dummy model and dataset.</p> <p> </p> Source code in <code>src/pydvl/value/games.py</code> <pre><code>def __init__(\n    self,\n    n_players: int,\n    score_range: Tuple[float, float] = (-np.inf, np.inf),\n    description: Optional[str] = None,\n):\n    self.n_players = n_players\n    self.data = DummyGameDataset(self.n_players, description)\n    self.u = Utility(\n        DummyModel(),\n        self.data,\n        scorer=Scorer(self._score, range=score_range),\n        catch_errors=False,\n        show_warnings=True,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.MinerGame","title":"MinerGame","text":"<pre><code>MinerGame(n_players: int)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>Consider a group of n miners, who have discovered large bars of gold.</p> <p>If two miners can carry one piece of gold, then the payoff of a coalition \\(S\\) is:</p> \\[{ v(S) = \\left\\{\\begin{array}{lll} \\mid S \\mid / 2, &amp; \\text{ if} &amp; \\mid S \\mid \\text{ is even} \\\\ ( \\mid S \\mid - 1)/2, &amp; \\text{ otherwise} \\end{array}\\right. }\\] <p>If there are more than two miners and there is an even number of miners, then the core consists of the single payoff where each miner gets 1/2.</p> <p>If there is an odd number of miners, then the core is empty.</p> <p>Taken from Wikipedia</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of miners that participate in the game.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/value/games.py</code> <pre><code>def __init__(self, n_players: int) -&gt; None:\n    if n_players &lt;= 2:\n        raise ValueError(f\"n_players, {n_players}, should be &gt; 2\")\n    description = \"Dummy data for Miner Game taken from https://en.wikipedia.org/wiki/Core_(game_theory)\"\n    super().__init__(\n        n_players,\n        score_range=(0, n_players // 2),\n        description=description,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.MinimumSpanningTreeGame","title":"MinimumSpanningTreeGame","text":"<pre><code>MinimumSpanningTreeGame(n_players: int = 100)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>A minimum spanning tree game defined in (Castro et al., 2009)<sup>1</sup>.</p> <p>Let \\(G = (N \\cup \\{0\\},E)\\) be a valued graph where \\(N = \\{1,\\dots,100\\}\\), and the cost associated to an edge \\((i, j)\\) is:</p> \\[{ c_{ij} = \\left\\{\\begin{array}{lll} 1, &amp; \\text{ if} &amp; i = j + 1 \\text{ or } i = j - 1 \\\\ &amp; &amp; \\text{ or } (i = 1 \\text{ and } j = 100) \\text{ or } (i = 100 \\text{ and } j = 1) \\\\ 101, &amp; \\text{ if} &amp; i = 0 \\text{ or } j = 0 \\\\ \\infty, &amp; \\text{ otherwise} \\end{array}\\right. }\\] <p>A minimum spanning tree game \\((N, c)\\) is a cost game, where for a given coalition \\(S \\subset N\\), \\(v(S)\\) is the sum of the edge cost of the minimum spanning tree, i.e. \\(v(S)\\) = Minimum Spanning Tree of the graph \\(G|_{S\\cup\\{0\\}}\\), which is the partial graph restricted to the players \\(S\\) and the source node \\(0\\).</p> PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> Source code in <code>src/pydvl/value/games.py</code> <pre><code>def __init__(self, n_players: int = 100) -&gt; None:\n    if n_players != 100:\n        raise ValueError(\n            f\"{self.__class__.__name__} only supports n_players=100 but got {n_players=}.\"\n        )\n    description = (\n        \"A dummy dataset for the minimum spanning tree game in Castro et al. 2009\"\n    )\n    super().__init__(n_players, score_range=(0, np.inf), description=description)\n\n    graph = np.zeros(shape=(self.n_players, self.n_players))\n\n    for i in range(self.n_players):\n        for j in range(self.n_players):\n            if (\n                i == j + 1\n                or i == j - 1\n                or (i == 1 and j == self.n_players - 1)\n                or (i == self.n_players - 1 and j == 1)\n            ):\n                graph[i, j] = 1\n            elif i == 0 or j == 0:\n                graph[i, j] = 0\n            else:\n                graph[i, j] = np.inf\n    assert np.all(graph == graph.T)\n\n    self.graph = graph\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.ShoesGame","title":"ShoesGame","text":"<pre><code>ShoesGame(left: int, right: int)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>A shoes game defined in (Castro et al., 2009)<sup>1</sup>.</p> <p>In this game, some players have a left shoe and others a right shoe. Single shoes have a worth of zero while pairs have a worth of 1.</p> <p>The payoff of a coalition \\(S\\) is:</p> \\[{ v(S) = \\min( \\mid S \\cap L \\mid, \\mid S \\cap R \\mid ) }\\] <p>Where \\(L\\), respectively \\(R\\), is the set of players with left shoes, respectively right shoes.</p> PARAMETER DESCRIPTION <code>left</code> <p>Number of players with a left shoe.</p> <p> TYPE: <code>int</code> </p> <code>right</code> <p>Number of players with a right shoe.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/value/games.py</code> <pre><code>def __init__(self, left: int, right: int) -&gt; None:\n    self.left = left\n    self.right = right\n    n_players = self.left + self.right\n    description = \"Dummy data for the shoe game in Castro et al. 2009\"\n    max_score = n_players // 2\n    super().__init__(n_players, score_range=(0, max_score), description=description)\n</code></pre>"},{"location":"deprecated/pydvl/value/games/#pydvl.value.games.SymmetricVotingGame","title":"SymmetricVotingGame","text":"<pre><code>SymmetricVotingGame(n_players: int)\n</code></pre> <p>               Bases: <code>Game</code></p> <p>Toy game that is used for testing and demonstration purposes.</p> <p>A symmetric voting game defined in (Castro et al., 2009)<sup>1</sup> Section 4.1</p> <p>For this game the utility of a coalition is 1 if its cardinality is greater than num_samples/2, or 0 otherwise.</p> \\[{ v(S) = \\left\\{\\begin{array}{ll} 1, &amp; \\text{ if} \\quad \\mid S \\mid &gt; \\frac{N}{2} \\\\ 0, &amp; \\text{ otherwise} \\end{array}\\right. }\\] PARAMETER DESCRIPTION <code>n_players</code> <p>Number of players that participate in the game.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/value/games.py</code> <pre><code>def __init__(self, n_players: int) -&gt; None:\n    if n_players % 2 != 0:\n        raise ValueError(\"n_players must be an even number.\")\n    description = \"Dummy data for the symmetric voting game in Castro et al. 2009\"\n    super().__init__(\n        n_players,\n        score_range=(0, 1),\n        description=description,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/result/","title":"Result","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result","title":"pydvl.value.result","text":"<p>This module collects types and methods for the inspection of the results of valuation algorithms.</p> <p>The most important class is ValuationResult, which provides access to raw values, as well as convenient behaviour as a <code>Sequence</code> with extended indexing and updating abilities, and conversion to pandas DataFrames.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result--operating-on-results","title":"Operating on results","text":"<p>Results can be added together with the standard <code>+</code> operator. Because values are typically running averages of iterative algorithms, addition behaves like a weighted average of the two results, with the weights being the number of updates in each result: adding two results is the same as generating one result with the mean of the values of the two results as values. The variances are updated accordingly. See ValuationResult for details.</p> <p>Results can also be sorted by value, variance or number of updates, see sort(). The arrays of ValuationResult.values, ValuationResult.variances, ValuationResult.counts, ValuationResult.indices, ValuationResult.names are sorted in the same way.</p> <p>Indexing and slicing of results is supported and ValueItem objects are returned. These objects can be compared with the usual operators, which take only the ValueItem.value into account.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result--creating-result-objects","title":"Creating result objects","text":"<p>The most commonly used factory method is ValuationResult.zeros(), which creates a result object with all values, variances and counts set to zero. ValuationResult.empty() creates an empty result object, which can be used as a starting point for adding results together. Empty results are discarded when added to other results. Finally, ValuationResult.from_random() samples random values uniformly.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult","title":"ValuationResult","text":"<pre><code>ValuationResult(\n    *,\n    values: NDArray[float64],\n    variances: Optional[NDArray[float64]] = None,\n    counts: Optional[NDArray[int_]] = None,\n    indices: Optional[NDArray[IndexT]] = None,\n    data_names: Optional[Sequence[NameT] | NDArray[NameT]] = None,\n    algorithm: str = \"\",\n    status: Status = Pending,\n    sort: bool = False,\n    **extra_values: Any\n)\n</code></pre> <p>               Bases: <code>Sequence</code>, <code>Iterable[ValueItem[IndexT, NameT]]</code>, <code>Generic[IndexT, NameT]</code></p> <p>Objects of this class hold the results of valuation algorithms.</p> <p>These include indices in the original Dataset, any data names (e.g. group names in GroupedDataset), the values themselves, and variance of the computation in the case of Monte Carlo methods. <code>ValuationResults</code> can be iterated over like any <code>Sequence</code>: <code>iter(valuation_result)</code> returns a generator of ValueItem in the order in which the object is sorted.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult--indexing","title":"Indexing","text":"<p>Indexing can be position-based, when accessing any of the attributes values, variances, counts and indices, as well as when iterating over the object, or using the item access operator, both getter and setter. The \"position\" is either the original sequence in which the data was passed to the constructor, or the sequence in which the object is sorted, see below.</p> <p>Alternatively, indexing can be data-based, i.e. using the indices in the original dataset. This is the case for the methods get() and update().</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult--sorting","title":"Sorting","text":"<p>Results can be sorted in-place with sort(), or alternatively using python's standard <code>sorted()</code> and <code>reversed()</code> Note that sorting values affects how iterators and the object itself as <code>Sequence</code> behave: <code>values[0]</code> returns a ValueItem with the highest or lowest ranking point if this object is sorted by descending or ascending value, respectively. If unsorted, <code>values[0]</code> returns the <code>ValueItem</code> at position 0, which has data index <code>indices[0]</code> in the Dataset.</p> <p>The same applies to direct indexing of the <code>ValuationResult</code>: the index is positional, according to the sorting. It does not refer to the \"data index\". To sort according to data index, use sort() with <code>key=\"index\"</code>.</p> <p>In order to access ValueItem objects by their data index, use get().</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult--operating-on-results","title":"Operating on results","text":"<p>Results can be added to each other with the <code>+</code> operator. Means and variances are correctly updated, using the <code>counts</code> attribute.</p> <p>Results can also be updated with new values using update(). Means and variances are updated accordingly using the Welford algorithm.</p> <p>Empty objects behave in a special way, see empty().</p> PARAMETER DESCRIPTION <code>values</code> <p>An array of values. If omitted, defaults to an empty array or to an array of zeros if <code>indices</code> are given.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>indices</code> <p>An optional array of indices in the original dataset. If omitted, defaults to <code>np.arange(len(values))</code>. Warning: It is common to pass the indices of a Dataset here. Attention must be paid in a parallel context to copy them to the local process. Just do <code>indices=np.copy(data.indices)</code>.</p> <p> TYPE: <code>Optional[NDArray[IndexT]]</code> DEFAULT: <code>None</code> </p> <code>variances</code> <p>An optional array of variances in the computation of each value.</p> <p> TYPE: <code>Optional[NDArray[float64]]</code> DEFAULT: <code>None</code> </p> <code>counts</code> <p>An optional array with the number of updates for each value. Defaults to an array of ones.</p> <p> TYPE: <code>Optional[NDArray[int_]]</code> DEFAULT: <code>None</code> </p> <code>data_names</code> <p>Names for the data points. Defaults to index numbers if not set.</p> <p> TYPE: <code>Optional[Sequence[NameT] | NDArray[NameT]]</code> DEFAULT: <code>None</code> </p> <code>algorithm</code> <p>The method used.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>status</code> <p>The end status of the algorithm.</p> <p> TYPE: <code>Status</code> DEFAULT: <code>Pending</code> </p> <code>sort</code> <p>Whether to sort the indices by ascending value. See above how this affects usage as an iterable or sequence.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>extra_values</code> <p>Any Additional values that can be passed as keyword arguments. This can contain, for example, the least core value.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If input arrays have mismatching lengths.</p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>def __init__(\n    self,\n    *,\n    values: NDArray[np.float64],\n    variances: Optional[NDArray[np.float64]] = None,\n    counts: Optional[NDArray[np.int_]] = None,\n    indices: Optional[NDArray[IndexT]] = None,\n    data_names: Optional[Sequence[NameT] | NDArray[NameT]] = None,\n    algorithm: str = \"\",\n    status: Status = Status.Pending,\n    sort: bool = False,\n    **extra_values: Any,\n):\n    if variances is not None and len(variances) != len(values):\n        raise ValueError(\"Lengths of values and variances do not match\")\n    if data_names is not None and len(data_names) != len(values):\n        raise ValueError(\"Lengths of values and data_names do not match\")\n    if indices is not None and len(indices) != len(values):\n        raise ValueError(\"Lengths of values and indices do not match\")\n\n    self._algorithm = algorithm\n    self._status = Status(status)  # Just in case we are given a string\n    self._values = values\n    self._variances = np.zeros_like(values) if variances is None else variances\n    self._counts = np.ones_like(values) if counts is None else counts\n    self._sort_order = None\n    self._extra_values = extra_values or {}\n\n    # Yuk...\n    if data_names is None:\n        if indices is not None:\n            self._names = np.copy(indices)\n        else:\n            self._names = np.arange(len(self._values), dtype=np.int_)\n    elif not isinstance(data_names, np.ndarray):\n        self._names = np.array(data_names)\n    else:\n        self._names = data_names.copy()\n    if len(np.unique(self._names)) != len(self._names):\n        raise ValueError(\"Data names must be unique\")\n\n    if indices is None:\n        indices = np.arange(len(self._values), dtype=np.int_)\n    self._indices = indices\n    self._positions = {idx: pos for pos, idx in enumerate(indices)}\n\n    self._sort_positions: NDArray[np.int_] = np.arange(\n        len(self._values), dtype=np.int_\n    )\n    if sort:\n        self.sort()\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.counts","title":"counts  <code>property</code>","text":"<pre><code>counts: NDArray[int_]\n</code></pre> <p>The raw counts, possibly sorted.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.indices","title":"indices  <code>property</code>","text":"<pre><code>indices: NDArray[IndexT]\n</code></pre> <p>The indices for the values, possibly sorted.</p> <p>If the object is unsorted, then these are the same as declared at construction or <code>np.arange(len(values))</code> if none were passed.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.names","title":"names  <code>property</code>","text":"<pre><code>names: NDArray[NameT]\n</code></pre> <p>The names for the values, possibly sorted. If the object is unsorted, then these are the same as declared at construction or <code>np.arange(len(values))</code> if none were passed.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.stderr","title":"stderr  <code>property</code>","text":"<pre><code>stderr: NDArray[float64]\n</code></pre> <p>The raw standard errors, possibly sorted.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.values","title":"values  <code>property</code>","text":"<pre><code>values: NDArray[float64]\n</code></pre> <p>The values, possibly sorted.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.variances","title":"variances  <code>property</code>","text":"<pre><code>variances: NDArray[float64]\n</code></pre> <p>The variances, possibly sorted.</p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.__add__","title":"__add__","text":"<pre><code>__add__(\n    other: ValuationResult[IndexT, NameT]\n) -&gt; ValuationResult[IndexT, NameT]\n</code></pre> <p>Adds two ValuationResults.</p> <p>The values must have been computed with the same algorithm. An exception to this is if one argument has empty values, in which case the other argument is returned.</p> <p>Warning</p> <p>Abusing this will introduce numerical errors.</p> <p>Means and standard errors are correctly handled. Statuses are added with bit-wise <code>&amp;</code>, see Status. <code>data_names</code> are taken from the left summand, or if unavailable from the right one. The <code>algorithm</code> string is carried over if both terms have the same one or concatenated.</p> <p>It is possible to add ValuationResults of different lengths, and with different or overlapping indices. The result will have the union of indices, and the values.</p> <p>Warning</p> <p>FIXME: Arbitrary <code>extra_values</code> aren't handled.</p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>def __add__(\n    self, other: ValuationResult[IndexT, NameT]\n) -&gt; ValuationResult[IndexT, NameT]:\n    \"\"\"Adds two ValuationResults.\n\n    The values must have been computed with the same algorithm. An exception\n    to this is if one argument has empty values, in which case the other\n    argument is returned.\n\n    !!! Warning\n        Abusing this will introduce numerical errors.\n\n    Means and standard errors are correctly handled. Statuses are added with\n    bit-wise `&amp;`, see [Status][pydvl.value.result.Status].\n    `data_names` are taken from the left summand, or if unavailable from\n    the right one. The `algorithm` string is carried over if both terms\n    have the same one or concatenated.\n\n    It is possible to add ValuationResults of different lengths, and with\n    different or overlapping indices. The result will have the union of\n    indices, and the values.\n\n    !!! Warning\n        FIXME: Arbitrary `extra_values` aren't handled.\n\n    \"\"\"\n    # empty results\n    if len(self.values) == 0:\n        return other\n    if len(other.values) == 0:\n        return self\n\n    self._check_compatible(other)\n\n    indices = np.union1d(self._indices, other._indices).astype(self._indices.dtype)\n    this_pos = np.searchsorted(indices, self._indices)\n    other_pos = np.searchsorted(indices, other._indices)\n\n    n: NDArray[np.int_] = np.zeros_like(indices, dtype=int)\n    m: NDArray[np.int_] = np.zeros_like(indices, dtype=int)\n    xn: NDArray[np.int_] = np.zeros_like(indices, dtype=float)\n    xm: NDArray[np.int_] = np.zeros_like(indices, dtype=float)\n    vn: NDArray[np.int_] = np.zeros_like(indices, dtype=float)\n    vm: NDArray[np.int_] = np.zeros_like(indices, dtype=float)\n\n    n[this_pos] = self._counts\n    xn[this_pos] = self._values\n    vn[this_pos] = self._variances\n    m[other_pos] = other._counts\n    xm[other_pos] = other._values\n    vm[other_pos] = other._variances\n\n    # np.maximum(1, n + m) covers case n = m = 0.\n    n_m_sum = np.maximum(1, n + m)\n\n    # Sample mean of n+m samples from two means of n and m samples\n    xnm = (n * xn + m * xm) / n_m_sum\n\n    # Sample variance of n+m samples from two sample variances of n and m samples\n    vnm = (n * (vn + xn**2) + m * (vm + xm**2)) / n_m_sum - xnm**2\n\n    if np.any(vnm &lt; 0):\n        if np.any(vnm &lt; -1e-6):\n            logger.warning(\n                \"Numerical error in variance computation. \"\n                f\"Negative sample variances clipped to 0 in {vnm}\"\n            )\n        vnm[np.where(vnm &lt; 0)] = 0\n\n    # Merging of names:\n    # If an index has the same name in both results, it must be the same.\n    # If an index has a name in one result but not the other, the name is\n    # taken from the result with the name.\n    if self._names.dtype != other._names.dtype:\n        if np.can_cast(other._names.dtype, self._names.dtype, casting=\"safe\"):\n            logger.warning(\n                f\"Casting ValuationResult.names from {other._names.dtype} to {self._names.dtype}\"\n            )\n            other._names = other._names.astype(self._names.dtype)\n        else:\n            raise TypeError(\n                f\"Cannot cast ValuationResult.names from \"\n                f\"{other._names.dtype} to {self._names.dtype}\"\n            )\n\n    both_pos = np.intersect1d(this_pos, other_pos)\n\n    if len(both_pos) &gt; 0:\n        this_names: NDArray = np.empty_like(indices, dtype=object)\n        other_names: NDArray = np.empty_like(indices, dtype=object)\n        this_names[this_pos] = self._names\n        other_names[other_pos] = other._names\n\n        this_shared_names = np.take(this_names, both_pos)\n        other_shared_names = np.take(other_names, both_pos)\n\n        if np.any(this_shared_names != other_shared_names):\n            raise ValueError(\"Mismatching names in ValuationResults\")\n\n    names = np.empty_like(indices, dtype=self._names.dtype)\n    names[this_pos] = self._names\n    names[other_pos] = other._names\n\n    return ValuationResult(\n        algorithm=self.algorithm or other.algorithm or \"\",\n        status=self.status &amp; other.status,\n        indices=indices,\n        values=xnm,\n        variances=vnm,\n        counts=n + m,\n        data_names=names,\n        # FIXME: What to do with extra_values? This is not commutative:\n        # extra_values=self._extra_values.update(other._extra_values),\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(attr: str) -&gt; Any\n</code></pre> <p>Allows access to extra values as if they were properties of the instance.</p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>def __getattr__(self, attr: str) -&gt; Any:\n    \"\"\"Allows access to extra values as if they were properties of the instance.\"\"\"\n    # This is here to avoid a RecursionError when copying or pickling the object\n    if attr == \"_extra_values\":\n        raise AttributeError()\n    try:\n        return self._extra_values[attr]\n    except KeyError as e:\n        raise AttributeError(\n            f\"{self.__class__.__name__} object has no attribute {attr}\"\n        ) from e\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[ValueItem[IndexT, NameT]]\n</code></pre> <p>Iterate over the results returning ValueItem objects. To sort in place before iteration, use sort().</p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>def __iter__(self) -&gt; Iterator[ValueItem[IndexT, NameT]]:\n    \"\"\"Iterate over the results returning [ValueItem][pydvl.value.result.ValueItem] objects.\n    To sort in place before iteration, use [sort()][pydvl.value.result.ValuationResult.sort].\n    \"\"\"\n    for pos in self._sort_positions:\n        yield ValueItem(\n            self._indices[pos],\n            self._names[pos],\n            self._values[pos],\n            self._variances[pos],\n            self._counts[pos],\n        )\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.empty","title":"empty  <code>classmethod</code>","text":"<pre><code>empty(\n    algorithm: str = \"\",\n    indices: Optional[Sequence[IndexT] | NDArray[IndexT]] = None,\n    data_names: Optional[Sequence[NameT] | NDArray[NameT]] = None,\n    n_samples: int = 0,\n) -&gt; ValuationResult\n</code></pre> <p>Creates an empty ValuationResult object.</p> <p>Empty results are characterised by having an empty array of values. When another result is added to an empty one, the empty one is discarded.</p> PARAMETER DESCRIPTION <code>algorithm</code> <p>Name of the algorithm used to compute the values</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>indices</code> <p>Optional sequence or array of indices.</p> <p> TYPE: <code>Optional[Sequence[IndexT] | NDArray[IndexT]]</code> DEFAULT: <code>None</code> </p> <code>data_names</code> <p>Optional sequences or array of names for the data points. Defaults to index numbers if not set.</p> <p> TYPE: <code>Optional[Sequence[NameT] | NDArray[NameT]]</code> DEFAULT: <code>None</code> </p> <code>n_samples</code> <p>Number of valuation result entries.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the results.</p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>@classmethod\ndef empty(\n    cls,\n    algorithm: str = \"\",\n    indices: Optional[Sequence[IndexT] | NDArray[IndexT]] = None,\n    data_names: Optional[Sequence[NameT] | NDArray[NameT]] = None,\n    n_samples: int = 0,\n) -&gt; ValuationResult:\n    \"\"\"Creates an empty [ValuationResult][pydvl.value.result.ValuationResult] object.\n\n    Empty results are characterised by having an empty array of values. When\n    another result is added to an empty one, the empty one is discarded.\n\n    Args:\n        algorithm: Name of the algorithm used to compute the values\n        indices: Optional sequence or array of indices.\n        data_names: Optional sequences or array of names for the data points.\n            Defaults to index numbers if not set.\n        n_samples: Number of valuation result entries.\n\n    Returns:\n        Object with the results.\n    \"\"\"\n    if indices is not None or data_names is not None or n_samples != 0:\n        return cls.zeros(\n            algorithm=algorithm,\n            indices=indices,\n            data_names=data_names,\n            n_samples=n_samples,\n        )\n    return cls(algorithm=algorithm, status=Status.Pending, values=np.array([]))\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.from_random","title":"from_random  <code>classmethod</code>","text":"<pre><code>from_random(\n    size: int,\n    total: Optional[float] = None,\n    seed: Optional[Seed] = None,\n    **kwargs: Any\n) -&gt; \"ValuationResult\"\n</code></pre> <p>Creates a ValuationResult object and fills it with an array of random values from a uniform distribution in [-1,1]. The values can be made to sum up to a given total number (doing so will change their range).</p> PARAMETER DESCRIPTION <code>size</code> <p>Number of values to generate</p> <p> TYPE: <code>int</code> </p> <code>total</code> <p>If set, the values are normalized to sum to this number (\"efficiency\" property of Shapley values).</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any Additional options to pass to the constructor of ValuationResult. Use to override status, names, etc.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>'ValuationResult'</code> <p>A valuation result with its status set to</p> <code>'ValuationResult'</code> <p>Status.Converged by default.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>size</code> is less than 1.</p> <p>Changed in version 0.6.0</p> <p>Added parameter <code>total</code>. Check for zero size</p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>@classmethod\ndef from_random(\n    cls,\n    size: int,\n    total: Optional[float] = None,\n    seed: Optional[Seed] = None,\n    **kwargs: Any,\n) -&gt; \"ValuationResult\":\n    \"\"\"Creates a [ValuationResult][pydvl.value.result.ValuationResult] object and fills it with an array\n    of random values from a uniform distribution in [-1,1]. The values can\n    be made to sum up to a given total number (doing so will change their range).\n\n    Args:\n        size: Number of values to generate\n        total: If set, the values are normalized to sum to this number\n            (\"efficiency\" property of Shapley values).\n        kwargs: Any Additional options to pass to the constructor of\n            [ValuationResult][pydvl.value.result.ValuationResult]. Use to override status, names, etc.\n\n    Returns:\n        A valuation result with its status set to\n        [Status.Converged][pydvl.utils.status.Status] by default.\n\n    Raises:\n         ValueError: If `size` is less than 1.\n\n    !!! tip \"Changed in version 0.6.0\"\n        Added parameter `total`. Check for zero size\n    \"\"\"\n    if size &lt; 1:\n        raise ValueError(\"Size must be a positive integer\")\n\n    rng = np.random.default_rng(seed)\n    values = rng.uniform(low=-1, high=1, size=size)\n    if total is not None:\n        values *= total / np.sum(values)\n\n    options = dict(values=values, status=Status.Converged, algorithm=\"random\")\n    options.update(kwargs)\n    return cls(**options)  # type: ignore\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.get","title":"get","text":"<pre><code>get(idx: Integral) -&gt; ValueItem\n</code></pre> <p>Retrieves a ValueItem by data index, as opposed to sort index, like the indexing operator.</p> RAISES DESCRIPTION <code>IndexError</code> <p>If the index is not found.</p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>def get(self, idx: Integral) -&gt; ValueItem:\n    \"\"\"Retrieves a ValueItem by data index, as opposed to sort index, like\n    the indexing operator.\n\n    Raises:\n         IndexError: If the index is not found.\n    \"\"\"\n    try:\n        pos = self._positions[idx]\n    except KeyError:\n        raise IndexError(f\"Index {idx} not found in ValuationResult\")\n\n    return ValueItem(\n        self._indices[pos],\n        self._names[pos],\n        self._values[pos],\n        self._variances[pos],\n        self._counts[pos],\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.scale","title":"scale","text":"<pre><code>scale(factor: float, indices: Optional[NDArray[IndexT]] = None)\n</code></pre> <p>Scales the values and variances of the result by a coefficient.</p> PARAMETER DESCRIPTION <code>factor</code> <p>Factor to scale by.</p> <p> TYPE: <code>float</code> </p> <code>indices</code> <p>Indices to scale. If None, all values are scaled.</p> <p> TYPE: <code>Optional[NDArray[IndexT]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>def scale(self, factor: float, indices: Optional[NDArray[IndexT]] = None):\n    \"\"\"\n    Scales the values and variances of the result by a coefficient.\n\n    Args:\n        factor: Factor to scale by.\n        indices: Indices to scale. If None, all values are scaled.\n    \"\"\"\n    self._values[self._sort_positions[indices]] *= factor\n    self._variances[self._sort_positions[indices]] *= factor**2\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.sort","title":"sort","text":"<pre><code>sort(\n    reverse: bool = False,\n    key: Literal[\"value\", \"variance\", \"index\", \"name\"] = \"value\",\n) -&gt; None\n</code></pre> <p>Sorts the indices in place by <code>key</code>.</p> <p>Once sorted, iteration over the results, and indexing of all the properties ValuationResult.values, ValuationResult.variances, ValuationResult.counts, ValuationResult.indices and ValuationResult.names will follow the same order.</p> PARAMETER DESCRIPTION <code>reverse</code> <p>Whether to sort in descending order by value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>key</code> <p>The key to sort by. Defaults to ValueItem.value.</p> <p> TYPE: <code>Literal['value', 'variance', 'index', 'name']</code> DEFAULT: <code>'value'</code> </p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>def sort(\n    self,\n    reverse: bool = False,\n    # Need a \"Comparable\" type here\n    key: Literal[\"value\", \"variance\", \"index\", \"name\"] = \"value\",\n) -&gt; None:\n    \"\"\"Sorts the indices in place by `key`.\n\n    Once sorted, iteration over the results, and indexing of all the\n    properties\n    [ValuationResult.values][pydvl.value.result.ValuationResult.values],\n    [ValuationResult.variances][pydvl.value.result.ValuationResult.variances],\n    [ValuationResult.counts][pydvl.value.result.ValuationResult.counts],\n    [ValuationResult.indices][pydvl.value.result.ValuationResult.indices]\n    and [ValuationResult.names][pydvl.value.result.ValuationResult.names]\n    will follow the same order.\n\n    Args:\n        reverse: Whether to sort in descending order by value.\n        key: The key to sort by. Defaults to\n            [ValueItem.value][pydvl.value.result.ValueItem].\n    \"\"\"\n    keymap = {\n        \"index\": \"_indices\",\n        \"value\": \"_values\",\n        \"variance\": \"_variances\",\n        \"name\": \"_names\",\n    }\n    self._sort_positions = np.argsort(getattr(self, keymap[key]))\n    if reverse:\n        self._sort_positions = self._sort_positions[::-1]\n    self._sort_order = reverse\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe(\n    column: Optional[str] = None, use_names: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Returns values as a dataframe.</p> PARAMETER DESCRIPTION <code>column</code> <p>Name for the column holding the data value. Defaults to the name of the algorithm used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>use_names</code> <p>Whether to use data names instead of indices for the DataFrame's index.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A dataframe with two columns, one for the values, with name given as explained in <code>column</code>, and another with standard errors for approximate algorithms. The latter will be named <code>column+'_stderr'</code>.</p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>def to_dataframe(\n    self, column: Optional[str] = None, use_names: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"Returns values as a dataframe.\n\n    Args:\n        column: Name for the column holding the data value. Defaults to\n            the name of the algorithm used.\n        use_names: Whether to use data names instead of indices for the\n            DataFrame's index.\n\n    Returns:\n        A dataframe with two columns, one for the values, with name\n            given as explained in `column`, and another with standard errors for\n            approximate algorithms. The latter will be named `column+'_stderr'`.\n    \"\"\"\n    column = column or self._algorithm\n    df = pd.DataFrame(\n        self._values[self._sort_positions],\n        index=(\n            self._names[self._sort_positions]\n            if use_names\n            else self._indices[self._sort_positions]\n        ),\n        columns=[column],\n    )\n    df[column + \"_stderr\"] = self.stderr[self._sort_positions]\n    df[column + \"_updates\"] = self.counts[self._sort_positions]\n    # HACK for compatibility with updated support code in the notebooks\n    df[column + \"_variances\"] = self.variances[self._sort_positions]\n    df[column + \"_counts\"] = self.counts[self._sort_positions]\n    return df\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.update","title":"update","text":"<pre><code>update(idx: int, new_value: float) -&gt; ValuationResult[IndexT, NameT]\n</code></pre> <p>Updates the result in place with a new value, using running mean and variance.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Data index of the value to update.</p> <p> TYPE: <code>int</code> </p> <code>new_value</code> <p>New value to add to the result.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ValuationResult[IndexT, NameT]</code> <p>A reference to the same, modified result.</p> RAISES DESCRIPTION <code>IndexError</code> <p>If the index is not found.</p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>def update(self, idx: int, new_value: float) -&gt; ValuationResult[IndexT, NameT]:\n    \"\"\"Updates the result in place with a new value, using running mean\n    and variance.\n\n    Args:\n        idx: Data index of the value to update.\n        new_value: New value to add to the result.\n\n    Returns:\n        A reference to the same, modified result.\n\n    Raises:\n        IndexError: If the index is not found.\n    \"\"\"\n    try:\n        pos = self._positions[idx]\n    except KeyError:\n        raise IndexError(f\"Index {idx} not found in ValuationResult\")\n    val, var = running_moments(\n        self._values[pos],\n        self._variances[pos],\n        self._counts[pos],\n        new_value,\n        unbiased=False,\n    )\n    self[pos] = ValueItem(\n        index=cast(IndexT, idx),  # FIXME\n        name=self._names[pos],\n        value=val,\n        variance=var,\n        count=self._counts[pos] + 1,\n    )\n    return self\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValuationResult.zeros","title":"zeros  <code>classmethod</code>","text":"<pre><code>zeros(\n    algorithm: str = \"\",\n    indices: Optional[Sequence[IndexT] | NDArray[IndexT]] = None,\n    data_names: Optional[Sequence[NameT] | NDArray[NameT]] = None,\n    n_samples: int = 0,\n) -&gt; ValuationResult\n</code></pre> <p>Creates an empty ValuationResult object.</p> <p>Empty results are characterised by having an empty array of values. When another result is added to an empty one, the empty one is ignored.</p> PARAMETER DESCRIPTION <code>algorithm</code> <p>Name of the algorithm used to compute the values</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>indices</code> <p>Data indices to use. A copy will be made. If not given, the indices will be set to the range <code>[0, n_samples)</code>.</p> <p> TYPE: <code>Optional[Sequence[IndexT] | NDArray[IndexT]]</code> DEFAULT: <code>None</code> </p> <code>data_names</code> <p>Data names to use. A copy will be made. If not given, the names will be set to the string representation of the indices.</p> <p> TYPE: <code>Optional[Sequence[NameT] | NDArray[NameT]]</code> DEFAULT: <code>None</code> </p> <code>n_samples</code> <p>Number of data points whose values are computed. If not given, the length of <code>indices</code> will be used.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the results.</p> Source code in <code>src/pydvl/value/result.py</code> <pre><code>@classmethod\ndef zeros(\n    cls,\n    algorithm: str = \"\",\n    indices: Optional[Sequence[IndexT] | NDArray[IndexT]] = None,\n    data_names: Optional[Sequence[NameT] | NDArray[NameT]] = None,\n    n_samples: int = 0,\n) -&gt; ValuationResult:\n    \"\"\"Creates an empty [ValuationResult][pydvl.value.result.ValuationResult] object.\n\n    Empty results are characterised by having an empty array of values. When\n    another result is added to an empty one, the empty one is ignored.\n\n    Args:\n        algorithm: Name of the algorithm used to compute the values\n        indices: Data indices to use. A copy will be made. If not given,\n            the indices will be set to the range `[0, n_samples)`.\n        data_names: Data names to use. A copy will be made. If not given,\n            the names will be set to the string representation of the indices.\n        n_samples: Number of data points whose values are computed. If\n            not given, the length of `indices` will be used.\n\n    Returns:\n        Object with the results.\n    \"\"\"\n    if indices is None:\n        indices = np.arange(n_samples, dtype=np.int_)\n    else:\n        indices = np.array(indices, dtype=np.int_)\n\n    if data_names is None:\n        data_names = np.array(indices)\n    else:\n        data_names = np.array(data_names)\n\n    return cls(\n        algorithm=algorithm,\n        status=Status.Pending,\n        indices=indices,\n        data_names=data_names,\n        values=np.zeros(len(indices)),\n        variances=np.zeros(len(indices)),\n        counts=np.zeros(len(indices), dtype=np.int_),\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValueItem","title":"ValueItem  <code>dataclass</code>","text":"<pre><code>ValueItem(\n    index: IndexT,\n    name: NameT,\n    value: float,\n    variance: Optional[float],\n    count: Optional[int],\n)\n</code></pre> <p>               Bases: <code>Generic[IndexT, NameT]</code></p> <p>The result of a value computation for one datum.</p> <p><code>ValueItems</code> can be compared with the usual operators, forming a total order. Comparisons take only the <code>value</code> into account.</p> <p>Todo</p> <p>Maybe have a mode of comparing similar to <code>np.isclose</code>, or taking the <code>variance</code> into account.</p> ATTRIBUTE DESCRIPTION <code>index</code> <p>Index of the sample with this value in the original Dataset</p> <p> TYPE: <code>IndexT</code> </p> <code>name</code> <p>Name of the sample if it was provided. Otherwise, <code>str(index)</code></p> <p> TYPE: <code>NameT</code> </p> <code>value</code> <p>The value</p> <p> TYPE: <code>float</code> </p> <code>variance</code> <p>Variance of the value if it was computed with an approximate method</p> <p> TYPE: <code>Optional[float]</code> </p> <code>count</code> <p>Number of updates for this value</p> <p> TYPE: <code>Optional[int]</code> </p>"},{"location":"deprecated/pydvl/value/result/#pydvl.value.result.ValueItem.stderr","title":"stderr  <code>property</code>","text":"<pre><code>stderr: Optional[float]\n</code></pre> <p>Standard error of the value.</p>"},{"location":"deprecated/pydvl/value/sampler/","title":"Sampler","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler","title":"pydvl.value.sampler","text":"<p>Samplers iterate over subsets of indices.</p> <p>The classes in this module are used to iterate over indices and subsets of their complement in the whole set, as required for the computation of marginal utility for semi-values. The elements returned when iterating over any subclass of PowersetSampler are tuples of the form <code>(idx, subset)</code>, where <code>idx</code> is the index of the element being added to the subset, and <code>subset</code> is the subset of the complement of <code>idx</code>. The classes in this module are used to iterate over an index set \\(I\\) as required for the computation of marginal utility for semi-values. The elements returned when iterating over any subclass of :class:<code>PowersetSampler</code> are tuples of the form \\((i, S)\\), where \\(i\\) is an index of interest, and \\(S \\subset I \\setminus \\{i\\}\\) is a subset of the complement of \\(i\\).</p> <p>The iteration happens in two nested loops. An outer loop iterates over \\(I\\), and an inner loop iterates over the powerset of \\(I \\setminus \\{i\\}\\). The outer iteration can be either sequential or at random.</p> <p>Note</p> <p>This is the natural mode of iteration for the combinatorial definition of semi-values, in particular Shapley value. For the computation using permutations, adhering to this interface is not ideal, but we stick to it for consistency.</p> <p>The samplers are used in the semivalues module to compute any semi-value, in particular Shapley and Beta values, and Banzhaf indices.</p>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler--slicing-of-samplers","title":"Slicing of samplers","text":"<p>The samplers can be sliced for parallel computation. For those which are embarrassingly parallel, this is done by slicing the set of \"outer\" indices and returning new samplers over those slices. This includes all truly powerset-based samplers, such as DeterministicUniformSampler and UniformSampler. In contrast, slicing a PermutationSampler creates a new sampler which iterates over the same indices.</p>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler--references","title":"References","text":"<ol> <li> <p>Mitchell, Rory, Joshua Cooper, Eibe   Frank, and Geoffrey Holmes. Sampling Permutations for Shapley Value   Estimation. Journal of Machine   Learning Research 23, no. 43 (2022): 1\u201346.\u00a0\u21a9</p> </li> <li> <p>Wang, J.T. and Jia, R., 2023. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning. In: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, pp. 6388-6421.\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.AntitheticPermutationSampler","title":"AntitheticPermutationSampler","text":"<pre><code>AntitheticPermutationSampler(*args, seed: Optional[Seed] = None, **kwargs)\n</code></pre> <p>               Bases: <code>PermutationSampler[IndexT]</code></p> <p>Samples permutations like PermutationSampler, but after each permutation, it returns the same permutation in reverse order.</p> <p>This sampler was suggested in (Mitchell et al. 2022)<sup>1</sup></p> <p>New in version 0.7.1</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __init__(self, *args, seed: Optional[Seed] = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.AntitheticPermutationSampler.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: slice | list[int]) -&gt; PowersetSampler[IndexT]\n</code></pre> <p>Permutation samplers cannot be split across indices, so we return a copy of the full sampler.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __getitem__(self, key: slice | list[int]) -&gt; PowersetSampler[IndexT]:\n    \"\"\"Permutation samplers cannot be split across indices, so we return\n    a copy of the full sampler.\"\"\"\n    return super().__getitem__(slice(None))\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.AntitheticPermutationSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of outer indices over which the sampler iterates.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of outer indices over which the sampler iterates.\"\"\"\n    return len(self._outer_indices)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.AntitheticPermutationSampler.iterindices","title":"iterindices","text":"<pre><code>iterindices() -&gt; Iterator[IndexT]\n</code></pre> <p>Iterates over indices in the order specified at construction.</p> this is probably not very useful, but I couldn't decide <p>which method is better</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def iterindices(self) -&gt; Iterator[IndexT]:\n    \"\"\"Iterates over indices in the order specified at construction.\n\n    FIXME: this is probably not very useful, but I couldn't decide\n      which method is better\n    \"\"\"\n    if self._index_iteration is PowersetSampler.IndexIteration.Sequential:\n        for idx in self._outer_indices:\n            yield idx\n    elif self._index_iteration is PowersetSampler.IndexIteration.Random:\n        while True:\n            yield np.random.choice(self._outer_indices, size=1).item()\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.AntitheticSampler","title":"AntitheticSampler","text":"<pre><code>AntitheticSampler(*args, seed: Optional[Seed] = None, **kwargs)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>PowersetSampler[IndexT]</code></p> <p>An iterator to perform uniform random sampling of subsets, and their complements.</p> <p>Works as UniformSampler, but for every tuple \\((i,S)\\), it subsequently returns \\((i,S^c)\\), where \\(S^c\\) is the complement of the set \\(S\\) in the set of indices, excluding \\(i\\).</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __init__(self, *args, seed: Optional[Seed] = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.AntitheticSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of outer indices over which the sampler iterates.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of outer indices over which the sampler iterates.\"\"\"\n    return len(self._outer_indices)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.AntitheticSampler.iterindices","title":"iterindices","text":"<pre><code>iterindices() -&gt; Iterator[IndexT]\n</code></pre> <p>Iterates over indices in the order specified at construction.</p> this is probably not very useful, but I couldn't decide <p>which method is better</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def iterindices(self) -&gt; Iterator[IndexT]:\n    \"\"\"Iterates over indices in the order specified at construction.\n\n    FIXME: this is probably not very useful, but I couldn't decide\n      which method is better\n    \"\"\"\n    if self._index_iteration is PowersetSampler.IndexIteration.Sequential:\n        for idx in self._outer_indices:\n            yield idx\n    elif self._index_iteration is PowersetSampler.IndexIteration.Random:\n        while True:\n            yield np.random.choice(self._outer_indices, size=1).item()\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.DeterministicPermutationSampler","title":"DeterministicPermutationSampler","text":"<pre><code>DeterministicPermutationSampler(*args, seed: Optional[Seed] = None, **kwargs)\n</code></pre> <p>               Bases: <code>PermutationSampler[IndexT]</code></p> <p>Samples all n! permutations of the indices deterministically, and iterates through them, returning sets as required for the permutation-based definition of semi-values.</p> <p>Warning</p> <p>This sampler requires caching to be enabled or computation will be doubled wrt. a \"direct\" implementation of permutation MC</p> <p>Warning</p> <p>This sampler is not parallelizable, as it always iterates over the whole set of permutations in the same order. Different processes would always return the same values for all indices.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __init__(self, *args, seed: Optional[Seed] = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.DeterministicPermutationSampler.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: slice | list[int]) -&gt; PowersetSampler[IndexT]\n</code></pre> <p>Permutation samplers cannot be split across indices, so we return a copy of the full sampler.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __getitem__(self, key: slice | list[int]) -&gt; PowersetSampler[IndexT]:\n    \"\"\"Permutation samplers cannot be split across indices, so we return\n    a copy of the full sampler.\"\"\"\n    return super().__getitem__(slice(None))\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.DeterministicPermutationSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of outer indices over which the sampler iterates.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of outer indices over which the sampler iterates.\"\"\"\n    return len(self._outer_indices)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.DeterministicPermutationSampler.iterindices","title":"iterindices","text":"<pre><code>iterindices() -&gt; Iterator[IndexT]\n</code></pre> <p>Iterates over indices in the order specified at construction.</p> this is probably not very useful, but I couldn't decide <p>which method is better</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def iterindices(self) -&gt; Iterator[IndexT]:\n    \"\"\"Iterates over indices in the order specified at construction.\n\n    FIXME: this is probably not very useful, but I couldn't decide\n      which method is better\n    \"\"\"\n    if self._index_iteration is PowersetSampler.IndexIteration.Sequential:\n        for idx in self._outer_indices:\n            yield idx\n    elif self._index_iteration is PowersetSampler.IndexIteration.Random:\n        while True:\n            yield np.random.choice(self._outer_indices, size=1).item()\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.DeterministicUniformSampler","title":"DeterministicUniformSampler","text":"<pre><code>DeterministicUniformSampler(indices: NDArray[IndexT], *args, **kwargs)\n</code></pre> <p>               Bases: <code>PowersetSampler[IndexT]</code></p> <p>An iterator to perform uniform deterministic sampling of subsets.</p> <p>For every index \\(i\\), each subset of the complement <code>indices - {i}</code> is returned.</p> <p>Note</p> <p>Indices are always iterated over sequentially, irrespective of the value of <code>index_iteration</code> upon construction.</p> Example <pre><code>&gt;&gt;&gt; for idx, s in DeterministicUniformSampler(np.arange(2)):\n&gt;&gt;&gt;    print(f\"{idx} - {s}\", end=\", \")\n1 - [], 1 - [2], 2 - [], 2 - [1],\n</code></pre> PARAMETER DESCRIPTION <code>indices</code> <p>The set of items (indices) to sample from.</p> <p> TYPE: <code>NDArray[IndexT]</code> </p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __init__(self, indices: NDArray[IndexT], *args, **kwargs):\n    \"\"\"An iterator to perform uniform deterministic sampling of subsets.\n\n    For every index $i$, each subset of the complement `indices - {i}` is\n    returned.\n\n    !!! Note\n        Indices are always iterated over sequentially, irrespective of\n        the value of `index_iteration` upon construction.\n\n    ??? Example\n        ``` pycon\n        &gt;&gt;&gt; for idx, s in DeterministicUniformSampler(np.arange(2)):\n        &gt;&gt;&gt;    print(f\"{idx} - {s}\", end=\", \")\n        1 - [], 1 - [2], 2 - [], 2 - [1],\n        ```\n\n    Args:\n        indices: The set of items (indices) to sample from.\n    \"\"\"\n    # Force sequential iteration\n    kwargs.update({\"index_iteration\": PowersetSampler.IndexIteration.Sequential})\n    super().__init__(indices, *args, **kwargs)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.DeterministicUniformSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of outer indices over which the sampler iterates.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of outer indices over which the sampler iterates.\"\"\"\n    return len(self._outer_indices)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.DeterministicUniformSampler.iterindices","title":"iterindices","text":"<pre><code>iterindices() -&gt; Iterator[IndexT]\n</code></pre> <p>Iterates over indices in the order specified at construction.</p> this is probably not very useful, but I couldn't decide <p>which method is better</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def iterindices(self) -&gt; Iterator[IndexT]:\n    \"\"\"Iterates over indices in the order specified at construction.\n\n    FIXME: this is probably not very useful, but I couldn't decide\n      which method is better\n    \"\"\"\n    if self._index_iteration is PowersetSampler.IndexIteration.Sequential:\n        for idx in self._outer_indices:\n            yield idx\n    elif self._index_iteration is PowersetSampler.IndexIteration.Random:\n        while True:\n            yield np.random.choice(self._outer_indices, size=1).item()\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.MSRSampler","title":"MSRSampler","text":"<pre><code>MSRSampler(*args, seed: Optional[Seed] = None, **kwargs)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>PowersetSampler[IndexT]</code></p> <p>An iterator to perform sampling of random subsets.</p> <p>This sampler does not return any index, it only returns subsets of the data. This sampler is used in (Wang et. al.)<sup>2</sup>.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __init__(self, *args, seed: Optional[Seed] = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.MSRSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of outer indices over which the sampler iterates.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of outer indices over which the sampler iterates.\"\"\"\n    return len(self._outer_indices)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.MSRSampler.iterindices","title":"iterindices","text":"<pre><code>iterindices() -&gt; Iterator[IndexT]\n</code></pre> <p>Iterates over indices in the order specified at construction.</p> this is probably not very useful, but I couldn't decide <p>which method is better</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def iterindices(self) -&gt; Iterator[IndexT]:\n    \"\"\"Iterates over indices in the order specified at construction.\n\n    FIXME: this is probably not very useful, but I couldn't decide\n      which method is better\n    \"\"\"\n    if self._index_iteration is PowersetSampler.IndexIteration.Sequential:\n        for idx in self._outer_indices:\n            yield idx\n    elif self._index_iteration is PowersetSampler.IndexIteration.Random:\n        while True:\n            yield np.random.choice(self._outer_indices, size=1).item()\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.PermutationSampler","title":"PermutationSampler","text":"<pre><code>PermutationSampler(*args, seed: Optional[Seed] = None, **kwargs)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>PowersetSampler[IndexT]</code></p> <p>Sample permutations of indices and iterate through each returning increasing subsets, as required for the permutation definition of semi-values.</p> <p>This sampler does not implement the two loops described in PowersetSampler. Instead, for a permutation <code>(3,1,4,2)</code>, it returns in sequence the tuples of index and sets:  <code>(3, {})</code>, <code>(1, {3})</code>, <code>(4, {3,1})</code> and <code>(2, {3,1,4})</code>.</p> <p>Note that the full index set is never returned.</p> <p>Warning</p> <p>This sampler requires caching to be enabled or computation will be doubled wrt. a \"direct\" implementation of permutation MC</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __init__(self, *args, seed: Optional[Seed] = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.PermutationSampler.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: slice | list[int]) -&gt; PowersetSampler[IndexT]\n</code></pre> <p>Permutation samplers cannot be split across indices, so we return a copy of the full sampler.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __getitem__(self, key: slice | list[int]) -&gt; PowersetSampler[IndexT]:\n    \"\"\"Permutation samplers cannot be split across indices, so we return\n    a copy of the full sampler.\"\"\"\n    return super().__getitem__(slice(None))\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.PermutationSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of outer indices over which the sampler iterates.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of outer indices over which the sampler iterates.\"\"\"\n    return len(self._outer_indices)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.PermutationSampler.iterindices","title":"iterindices","text":"<pre><code>iterindices() -&gt; Iterator[IndexT]\n</code></pre> <p>Iterates over indices in the order specified at construction.</p> this is probably not very useful, but I couldn't decide <p>which method is better</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def iterindices(self) -&gt; Iterator[IndexT]:\n    \"\"\"Iterates over indices in the order specified at construction.\n\n    FIXME: this is probably not very useful, but I couldn't decide\n      which method is better\n    \"\"\"\n    if self._index_iteration is PowersetSampler.IndexIteration.Sequential:\n        for idx in self._outer_indices:\n            yield idx\n    elif self._index_iteration is PowersetSampler.IndexIteration.Random:\n        while True:\n            yield np.random.choice(self._outer_indices, size=1).item()\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.PowersetSampler","title":"PowersetSampler","text":"<pre><code>PowersetSampler(\n    indices: NDArray[IndexT],\n    index_iteration: IndexIteration = Sequential,\n    outer_indices: NDArray[IndexT] | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Iterable[SampleT]</code>, <code>Generic[IndexT]</code></p> <p>Samplers are custom iterables over subsets of indices.</p> <p>Calling <code>iter()</code> on a sampler returns an iterator over tuples of the form \\((i, S)\\), where \\(i\\) is an index of interest, and \\(S \\subset I \\setminus \\{i\\}\\) is a subset of the complement of \\(i\\).</p> <p>This is done in two nested loops, where the outer loop iterates over the set of indices, and the inner loop iterates over subsets of the complement of the current index. The outer iteration can be either sequential or at random.</p> <p>Note</p> <p>Samplers are not iterators themselves, so that each call to <code>iter()</code> e.g. in a for loop creates a new iterator.</p> Example <pre><code>&gt;&gt;&gt;for idx, s in DeterministicUniformSampler(np.arange(2)):\n&gt;&gt;&gt;    print(s, end=\"\")\n[][2,][][1,]\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.PowersetSampler--methods-required-in-subclasses","title":"Methods required in subclasses","text":"<p>Samplers must implement a weight() function to be used as a multiplier in Monte Carlo sums, so that the limit expectation coincides with the semi-value.</p>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.PowersetSampler--slicing-of-samplers","title":"Slicing of samplers","text":"<p>The samplers can be sliced for parallel computation. For those which are embarrassingly parallel, this is done by slicing the set of \"outer\" indices and returning new samplers over those slices.</p> PARAMETER DESCRIPTION <code>indices</code> <p>The set of items (indices) to sample from.</p> <p> TYPE: <code>NDArray[IndexT]</code> </p> <code>index_iteration</code> <p>the order in which indices are iterated over</p> <p> TYPE: <code>IndexIteration</code> DEFAULT: <code>Sequential</code> </p> <code>outer_indices</code> <p>The set of items (indices) over which to iterate when sampling. Subsets are taken from the complement of each index in succession. For embarrassingly parallel computations, this set is sliced and the samplers are used to iterate over the slices.</p> <p> TYPE: <code>NDArray[IndexT] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __init__(\n    self,\n    indices: NDArray[IndexT],\n    index_iteration: IndexIteration = IndexIteration.Sequential,\n    outer_indices: NDArray[IndexT] | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        indices: The set of items (indices) to sample from.\n        index_iteration: the order in which indices are iterated over\n        outer_indices: The set of items (indices) over which to iterate\n            when sampling. Subsets are taken from the complement of each index\n            in succession. For embarrassingly parallel computations, this set\n            is sliced and the samplers are used to iterate over the slices.\n    \"\"\"\n    self._indices = indices\n    self._index_iteration = index_iteration\n    self._outer_indices = outer_indices if outer_indices is not None else indices\n    self._n = len(indices)\n    self._n_samples = 0\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.PowersetSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of outer indices over which the sampler iterates.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of outer indices over which the sampler iterates.\"\"\"\n    return len(self._outer_indices)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.PowersetSampler.iterindices","title":"iterindices","text":"<pre><code>iterindices() -&gt; Iterator[IndexT]\n</code></pre> <p>Iterates over indices in the order specified at construction.</p> this is probably not very useful, but I couldn't decide <p>which method is better</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def iterindices(self) -&gt; Iterator[IndexT]:\n    \"\"\"Iterates over indices in the order specified at construction.\n\n    FIXME: this is probably not very useful, but I couldn't decide\n      which method is better\n    \"\"\"\n    if self._index_iteration is PowersetSampler.IndexIteration.Sequential:\n        for idx in self._outer_indices:\n            yield idx\n    elif self._index_iteration is PowersetSampler.IndexIteration.Random:\n        while True:\n            yield np.random.choice(self._outer_indices, size=1).item()\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.PowersetSampler.weight","title":"weight  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Factor by which to multiply Monte Carlo samples, so that the mean converges to the desired expression.</p> <p>By the Law of Large Numbers, the sample mean of \\(\\Delta_i(S_j)\\) converges to the expectation under the distribution from which \\(S_j\\) is sampled.</p> \\[ \\frac{1}{m}  \\sum_{j = 1}^m \\Delta_i (S_j) c (S_j) \\longrightarrow    \\underset{S \\sim \\mathcal{D}_{- i}}{\\mathbb{E}} [\\Delta_i (S) c (    S)]\\] <p>We add a factor \\(c(S_j)\\) in order to have this expectation coincide with the desired expression.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef weight(cls, n: int, subset_len: int) -&gt; float:\n    r\"\"\"Factor by which to multiply Monte Carlo samples, so that the\n    mean converges to the desired expression.\n\n    By the Law of Large Numbers, the sample mean of $\\Delta_i(S_j)$\n    converges to the expectation under the distribution from which $S_j$ is\n    sampled.\n\n    $$ \\frac{1}{m}  \\sum_{j = 1}^m \\Delta_i (S_j) c (S_j) \\longrightarrow\n       \\underset{S \\sim \\mathcal{D}_{- i}}{\\mathbb{E}} [\\Delta_i (S) c (\n       S)]$$\n\n    We add a factor $c(S_j)$ in order to have this expectation coincide with\n    the desired expression.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.RandomHierarchicalSampler","title":"RandomHierarchicalSampler","text":"<pre><code>RandomHierarchicalSampler(*args, seed: Optional[Seed] = None, **kwargs)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>PowersetSampler[IndexT]</code></p> <p>For every index, sample a set size, then a set of that size.</p> <p>Todo</p> <p>This is unnecessary, but a step towards proper stratified sampling.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __init__(self, *args, seed: Optional[Seed] = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.RandomHierarchicalSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of outer indices over which the sampler iterates.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of outer indices over which the sampler iterates.\"\"\"\n    return len(self._outer_indices)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.RandomHierarchicalSampler.iterindices","title":"iterindices","text":"<pre><code>iterindices() -&gt; Iterator[IndexT]\n</code></pre> <p>Iterates over indices in the order specified at construction.</p> this is probably not very useful, but I couldn't decide <p>which method is better</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def iterindices(self) -&gt; Iterator[IndexT]:\n    \"\"\"Iterates over indices in the order specified at construction.\n\n    FIXME: this is probably not very useful, but I couldn't decide\n      which method is better\n    \"\"\"\n    if self._index_iteration is PowersetSampler.IndexIteration.Sequential:\n        for idx in self._outer_indices:\n            yield idx\n    elif self._index_iteration is PowersetSampler.IndexIteration.Random:\n        while True:\n            yield np.random.choice(self._outer_indices, size=1).item()\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.StochasticSamplerMixin","title":"StochasticSamplerMixin","text":"<pre><code>StochasticSamplerMixin(*args, seed: Optional[Seed] = None, **kwargs)\n</code></pre> <p>Mixin class for samplers which use a random number generator.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __init__(self, *args, seed: Optional[Seed] = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.UniformSampler","title":"UniformSampler","text":"<pre><code>UniformSampler(*args, seed: Optional[Seed] = None, **kwargs)\n</code></pre> <p>               Bases: <code>StochasticSamplerMixin</code>, <code>PowersetSampler[IndexT]</code></p> <p>An iterator to perform uniform random sampling of subsets.</p> <p>Iterating over every index \\(i\\), either in sequence or at random depending on the value of <code>index_iteration</code>, one subset of the complement <code>indices - {i}</code> is sampled with equal probability \\(2^{n-1}\\). The iterator never ends.</p> Example <p>The code <pre><code>for idx, s in UniformSampler(np.arange(3)):\n   print(f\"{idx} - {s}\", end=\", \")\n</code></pre> Produces the output: <pre><code>0 - [1 4], 1 - [2 3], 2 - [0 1 3], 3 - [], 4 - [2], 0 - [1 3 4], 1 - [0 2]\n(...)\n</code></pre></p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __init__(self, *args, seed: Optional[Seed] = None, **kwargs):\n    super().__init__(*args, **kwargs)\n    self._rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.UniformSampler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of outer indices over which the sampler iterates.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of outer indices over which the sampler iterates.\"\"\"\n    return len(self._outer_indices)\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.UniformSampler.iterindices","title":"iterindices","text":"<pre><code>iterindices() -&gt; Iterator[IndexT]\n</code></pre> <p>Iterates over indices in the order specified at construction.</p> this is probably not very useful, but I couldn't decide <p>which method is better</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>def iterindices(self) -&gt; Iterator[IndexT]:\n    \"\"\"Iterates over indices in the order specified at construction.\n\n    FIXME: this is probably not very useful, but I couldn't decide\n      which method is better\n    \"\"\"\n    if self._index_iteration is PowersetSampler.IndexIteration.Sequential:\n        for idx in self._outer_indices:\n            yield idx\n    elif self._index_iteration is PowersetSampler.IndexIteration.Random:\n        while True:\n            yield np.random.choice(self._outer_indices, size=1).item()\n</code></pre>"},{"location":"deprecated/pydvl/value/sampler/#pydvl.value.sampler.UniformSampler.weight","title":"weight  <code>classmethod</code>","text":"<pre><code>weight(n: int, subset_len: int) -&gt; float\n</code></pre> <p>Correction coming from Monte Carlo integration so that the mean of the marginals converges to the value: the uniform distribution over the powerset of a set with n-1 elements has mass 2^{n-1} over each subset.</p> Source code in <code>src/pydvl/value/sampler.py</code> <pre><code>@classmethod\ndef weight(cls, n: int, subset_len: int) -&gt; float:\n    \"\"\"Correction coming from Monte Carlo integration so that the mean of\n    the marginals converges to the value: the uniform distribution over the\n    powerset of a set with n-1 elements has mass 2^{n-1} over each subset.\"\"\"\n    return float(2 ** (n - 1)) if n &gt; 0 else 1.0\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/","title":"Semivalues","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues","title":"pydvl.value.semivalues","text":"<p>This module provides the core functionality for the computation of generic semi-values. A semi-value is any valuation function with the form:</p> \\[v_\\text{semi}(i) = \\sum_{i=1}^n w(k) \\sum_{S \\subset D_{-i}^{(k)}} [U(S_{+i})-U(S)],\\] <p>where the coefficients \\(w(k)\\) satisfy the property:</p> \\[\\sum_{k=1}^n w(k) = 1.\\] Note <p>For implementation consistency, we slightly depart from the common definition of semi-values, which includes a factor \\(1/n\\) in the sum over subsets. Instead, we subsume this factor into the coefficient \\(w(k)\\).</p>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues--main-components","title":"Main components","text":"<p>The computation of a semi-value requires two components:</p> <ol> <li>A subset sampler that generates subsets of the set \\(D\\) of interest.</li> <li>A coefficient \\(w(k)\\) that assigns a weight to each subset size \\(k\\).</li> </ol> <p>Samplers can be found in sampler, and can be classified into two categories: powerset samplers and permutation samplers. Powerset samplers generate subsets of \\(D_{-i}\\), while the permutation sampler generates permutations of \\(D\\). The former conform to the above definition of semi-values, while the latter reformulates it as:</p> \\[ v(i) = \\frac{1}{n!} \\sum_{\\sigma \\in \\Pi(n)} \\tilde{w}( | \\sigma_{:i} | )[U(\\sigma_{:i} \\cup \\{i\\}) \u2212 U(\\sigma_{:i})], \\] <p>where \\(\\sigma_{:i}\\) denotes the set of indices in permutation sigma before the position where \\(i\\) appears (see Data valuation for details), and</p> \\[ \\tilde{w} (k) = n \\binom{n - 1}{k} w (k) \\] <p>is the weight correction due to the reformulation.</p> <p>Warning</p> <p>Both PermutationSampler and DeterministicPermutationSampler require caching to be enabled or computation will be doubled wrt. a 'direct' implementation of permutation MC.</p>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues--computing-semi-values","title":"Computing semi-values","text":"<p>Samplers and coefficients can be arbitrarily mixed by means of the main entry point of this module, compute_generic_semivalues. There are several pre-defined coefficients, including the Shapley value of (Ghorbani and Zou, 2019)<sup>1</sup>, the Banzhaf index of (Wang and Jia)<sup>3</sup>, and the Beta coefficient of (Kwon and Zou, 2022)<sup>2</sup>. For each of these methods, there is a convenience wrapper function. Respectively, these are: compute_shapley_semivalues, compute_banzhaf_semivalues, and compute_beta_shapley_semivalues. instead.</p> <p>Parallelization and batching</p> <p>In order to ensure reproducibility and fine-grained control of parallelization, samples are generated in the main process and then distributed to worker processes for evaluation. For small sample sizes, this can lead to a significant overhead. To avoid this, we temporarily provide an additional argument <code>batch_size</code> to all methods which can improve performance with small models up to an order of magnitude. Note that this argument will be removed before version 1.0 in favour of a more general solution.</p>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues--references","title":"References","text":"<ol> <li> <p>Ghorbani, A., Zou, J., 2019. Data Shapley: Equitable Valuation of Data for Machine Learning. In: Proceedings of the 36th International Conference on Machine Learning, PMLR, pp. 2242\u20132251.\u00a0\u21a9</p> </li> <li> <p>Kwon, Y. and Zou, J., 2022. Beta Shapley: A Unified and Noise-reduced Data Valuation Framework for Machine Learning. In: Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022, Vol. 151. PMLR, Valencia, Spain.\u00a0\u21a9</p> </li> <li> <p>Wang, J.T. and Jia, R., 2023. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning. In: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, pp. 6388-6421.\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.DefaultMarginal","title":"DefaultMarginal","text":"<p>               Bases: <code>MarginalFunction</code></p>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.DefaultMarginal.__call__","title":"__call__","text":"<pre><code>__call__(\n    u: Utility, coefficient: SVCoefficient, samples: Iterable[SampleT]\n) -&gt; Tuple[MarginalT, ...]\n</code></pre> <p>Computation of marginal utility. This is a helper function for compute_generic_semivalues.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>coefficient</code> <p>The semivalue coefficient and sampler weight</p> <p> TYPE: <code>SVCoefficient</code> </p> <code>samples</code> <p>A collection of samples. Each sample is a tuple of index and subset of indices to compute a marginal utility.</p> <p> TYPE: <code>Iterable[SampleT]</code> </p> RETURNS DESCRIPTION <code>MarginalT</code> <p>A collection of marginals. Each marginal is a tuple with index and its marginal</p> <code>...</code> <p>utility.</p> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>def __call__(\n    self, u: Utility, coefficient: SVCoefficient, samples: Iterable[SampleT]\n) -&gt; Tuple[MarginalT, ...]:\n    \"\"\"Computation of marginal utility. This is a helper function for\n    [compute_generic_semivalues][pydvl.value.semivalues.compute_generic_semivalues].\n\n    Args:\n        u: Utility object with model, data, and scoring function.\n        coefficient: The semivalue coefficient and sampler weight\n        samples: A collection of samples. Each sample is a tuple of index and subset of\n            indices to compute a marginal utility.\n\n    Returns:\n        A collection of marginals. Each marginal is a tuple with index and its marginal\n        utility.\n    \"\"\"\n    n = len(u.data)\n    marginals: List[MarginalT] = []\n    for idx, s in samples:\n        marginal = (u({idx}.union(s)) - u(s)) * coefficient(n, len(s))\n        marginals.append((idx, marginal))\n    return tuple(marginals)\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.FutureProcessor","title":"FutureProcessor","text":"<p>The FutureProcessor class used to process the results of the parallel marginal evaluations.</p> <p>The marginals are evaluated in parallel by <code>n_jobs</code> threads, but some algorithms require a central method to postprocess the marginal results. This can be achieved through the future processor. This base class does not perform any postprocessing, it is a noop used in most data valuation algorithms.</p>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.MSRFutureProcessor","title":"MSRFutureProcessor","text":"<pre><code>MSRFutureProcessor(u: Utility)\n</code></pre> <p>               Bases: <code>FutureProcessor</code></p> <p>This FutureProcessor processes the raw marginals in a way that MSR sampling requires.</p> <p>MSR sampling evaluates the utility once, and then updates all data semivalues based on this one evaluation. In order to do this, the RawUtility value needs to be postprocessed through this class. For more details on MSR, please refer to the paper (Wang et. al.)<sup>3</sup>. This processor keeps track of the current values and computes marginals for all data points, so that the values in the ValuationResult can be updated properly down the line.</p> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>def __init__(self, u: Utility):\n    self.n = len(u.data)\n    self.all_indices = u.data.indices.copy()\n    self.point_in_subset = np.zeros((self.n,))\n    self.positive_sums = np.zeros((self.n,))\n    self.negative_sums = np.zeros((self.n,))\n    self.total_evaluations = 0\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.MSRFutureProcessor.__call__","title":"__call__","text":"<pre><code>__call__(\n    future_result: List[Tuple[List[IndexT], float]]\n) -&gt; List[List[MarginalT]]\n</code></pre> <p>Computation of marginal utility using Maximum Sample Reuse.</p> <pre><code>    This processor requires the Marginal Function to be set to RawUtility.\n    Then, this processor computes marginals based on the utility value and the index set provided.\n\n    The final formula that gives the Banzhaf semivalue using MSR is:\n    $$\\hat{\\phi}_{MSR}(i) = \frac{1}{|\\mathbf{S}_{\n</code></pre> <p>i i}|} \\sum_{S \\in \\mathbf{S}{ i i}} U(S)         - \frac{1}{|\\mathbf{S}{ ot{ i} i}|} \\sum_{S \\in \\mathbf{S}_{ ot{ i} i}} U(S)$$</p> <pre><code>    Args:\n        future_result: Result of the parallel computing jobs comprised of\n            a list of indices that were used to evaluate the utility, and the evaluation result (metric).\n\n    Returns:\n        A collection of marginals. Each marginal is a tuple with index and its marginal\n        utility.\n</code></pre> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>def __call__(\n    self, future_result: List[Tuple[List[IndexT], float]]\n) -&gt; List[List[MarginalT]]:\n    \"\"\"Computation of marginal utility using Maximum Sample Reuse.\n\n    This processor requires the Marginal Function to be set to RawUtility.\n    Then, this processor computes marginals based on the utility value and the index set provided.\n\n    The final formula that gives the Banzhaf semivalue using MSR is:\n    $$\\\\hat{\\\\phi}_{MSR}(i) = \\frac{1}{|\\\\mathbf{S}_{\\ni i}|} \\\\sum_{S \\\\in \\\\mathbf{S}_{\\ni i}} U(S)\n    - \\frac{1}{|\\\\mathbf{S}_{\\not{\\ni} i}|} \\\\sum_{S \\\\in \\\\mathbf{S}_{\\not{\\ni} i}} U(S)$$\n\n    Args:\n        future_result: Result of the parallel computing jobs comprised of\n            a list of indices that were used to evaluate the utility, and the evaluation result (metric).\n\n    Returns:\n        A collection of marginals. Each marginal is a tuple with index and its marginal\n        utility.\n    \"\"\"\n    marginals: List[List[MarginalT]] = []\n    for batch_id, (s, evaluation) in enumerate(future_result):\n        previous_values = self.compute_values()\n        self.total_evaluations += 1\n        self.point_in_subset[s] += 1\n        self.positive_sums[s] += evaluation\n        not_s = np.setdiff1d(self.all_indices, s)\n        self.negative_sums[not_s] += evaluation\n        new_values = self.compute_values()\n        # Hack to work around the update mechanic that does not work out of the box for MSR\n        marginal_vals = (\n            self.total_evaluations * new_values\n            - (self.total_evaluations - 1) * previous_values\n        )\n        marginals.append([])\n        for data_index in range(self.n):\n            marginals[batch_id].append(\n                (data_index, float(marginal_vals[data_index]))\n            )\n    return marginals\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.RawUtility","title":"RawUtility","text":"<p>               Bases: <code>MarginalFunction</code></p>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.RawUtility.__call__","title":"__call__","text":"<pre><code>__call__(\n    u: Utility, coefficient: SVCoefficient, samples: Iterable[SampleT]\n) -&gt; Tuple[MarginalT, ...]\n</code></pre> <p>Computation of raw utility without marginalization. This is a helper function for compute_generic_semivalues.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>coefficient</code> <p>The semivalue coefficient and sampler weight</p> <p> TYPE: <code>SVCoefficient</code> </p> <code>samples</code> <p>A collection of samples. Each sample is a tuple of index and subset of indices to compute a marginal utility.</p> <p> TYPE: <code>Iterable[SampleT]</code> </p> RETURNS DESCRIPTION <code>Tuple[MarginalT, ...]</code> <p>A collection of marginals. Each marginal is a tuple with index and its raw utility.</p> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>def __call__(\n    self, u: Utility, coefficient: SVCoefficient, samples: Iterable[SampleT]\n) -&gt; Tuple[MarginalT, ...]:\n    \"\"\"Computation of raw utility without marginalization. This is a helper function for\n    [compute_generic_semivalues][pydvl.value.semivalues.compute_generic_semivalues].\n\n    Args:\n        u: Utility object with model, data, and scoring function.\n        coefficient: The semivalue coefficient and sampler weight\n        samples: A collection of samples. Each sample is a tuple of index and subset of\n            indices to compute a marginal utility.\n\n    Returns:\n        A collection of marginals. Each marginal is a tuple with index and its raw utility.\n    \"\"\"\n    marginals: List[MarginalT] = []\n    for idx, s in samples:\n        marginals.append((s, u(s)))\n    return tuple(marginals)\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.SVCoefficient","title":"SVCoefficient","text":"<p>               Bases: <code>Protocol</code></p> <p>The protocol that coefficients for the computation of semi-values must fulfill.</p>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.SVCoefficient.__call__","title":"__call__","text":"<pre><code>__call__(n: int, k: int) -&gt; float\n</code></pre> <p>Computes the coefficient for a given subset size.</p> PARAMETER DESCRIPTION <code>n</code> <p>Total number of elements in the set.</p> <p> TYPE: <code>int</code> </p> <code>k</code> <p>Size of the subset for which the coefficient is being computed</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>def __call__(self, n: int, k: int) -&gt; float:\n    \"\"\"Computes the coefficient for a given subset size.\n\n    Args:\n        n: Total number of elements in the set.\n        k: Size of the subset for which the coefficient is being computed\n    \"\"\"\n    ...\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.SemiValueMode","title":"SemiValueMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of semi-value modes.</p> <p>Deprecation notice</p> <p>This enum and the associated methods are deprecated and will be removed in 0.8.0.</p>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.compute_banzhaf_semivalues","title":"compute_banzhaf_semivalues","text":"<pre><code>compute_banzhaf_semivalues(\n    u: Utility,\n    *,\n    done: StoppingCriterion,\n    sampler_t: Type[StochasticSampler] = PermutationSampler,\n    batch_size: int = 1,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>Computes Banzhaf values for a given utility function.</p> <p>This is a convenience wrapper for compute_generic_semivalues with the Banzhaf coefficient.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>done</code> <p>Stopping criterion.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>sampler_t</code> <p>The sampler type to use. See the sampler module for a list.</p> <p> TYPE: <code>Type[StochasticSampler]</code> DEFAULT: <code>PermutationSampler</code> </p> <code>batch_size</code> <p>Number of marginal evaluations per single parallel job.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the results.</p> <p>Deprecation notice</p> <p>Parameter <code>batch_size</code> is for experimental use and will be removed in future versions.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef compute_banzhaf_semivalues(\n    u: Utility,\n    *,\n    done: StoppingCriterion,\n    sampler_t: Type[StochasticSampler] = PermutationSampler,\n    batch_size: int = 1,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    \"\"\"Computes Banzhaf values for a given utility function.\n\n    This is a convenience wrapper for\n    [compute_generic_semivalues][pydvl.value.semivalues.compute_generic_semivalues]\n    with the Banzhaf coefficient.\n\n    Args:\n        u: Utility object with model, data, and scoring function.\n        done: Stopping criterion.\n        sampler_t: The sampler type to use. See the\n            [sampler][pydvl.value.sampler] module for a list.\n        batch_size: Number of marginal evaluations per single parallel job.\n        n_jobs: Number of parallel jobs to use.\n        seed: Either an instance of a numpy random number generator or a seed\n            for it.\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        progress: Whether to display a progress bar.\n\n    Returns:\n        Object with the results.\n\n    !!! warning \"Deprecation notice\"\n        Parameter `batch_size` is for experimental use and will be removed in\n        future versions.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n    # HACK: cannot infer return type because of useless IndexT, NameT\n    return compute_generic_semivalues(  # type: ignore\n        sampler_t(u.data.indices, seed=seed),\n        u,\n        banzhaf_coefficient,\n        done,\n        batch_size=batch_size,\n        n_jobs=n_jobs,\n        parallel_backend=parallel_backend,\n        config=config,\n        progress=progress,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.compute_beta_shapley_semivalues","title":"compute_beta_shapley_semivalues","text":"<pre><code>compute_beta_shapley_semivalues(\n    u: Utility,\n    *,\n    alpha: float = 1,\n    beta: float = 1,\n    done: StoppingCriterion,\n    sampler_t: Type[StochasticSampler] = PermutationSampler,\n    batch_size: int = 1,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>Computes Beta Shapley values for a given utility function.</p> <p>This is a convenience wrapper for compute_generic_semivalues with the Beta Shapley coefficient.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>alpha</code> <p>Alpha parameter of the Beta distribution.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>beta</code> <p>Beta parameter of the Beta distribution.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>done</code> <p>Stopping criterion.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>sampler_t</code> <p>The sampler type to use. See the sampler module for a list.</p> <p> TYPE: <code>Type[StochasticSampler]</code> DEFAULT: <code>PermutationSampler</code> </p> <code>batch_size</code> <p>Number of marginal evaluations per (parallelized) task.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the results.</p> <p>Deprecation notice</p> <p>Parameter <code>batch_size</code> is for experimental use and will be removed in future versions.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef compute_beta_shapley_semivalues(\n    u: Utility,\n    *,\n    alpha: float = 1,\n    beta: float = 1,\n    done: StoppingCriterion,\n    sampler_t: Type[StochasticSampler] = PermutationSampler,\n    batch_size: int = 1,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    \"\"\"Computes Beta Shapley values for a given utility function.\n\n    This is a convenience wrapper for\n    [compute_generic_semivalues][pydvl.value.semivalues.compute_generic_semivalues]\n    with the Beta Shapley coefficient.\n\n    Args:\n        u: Utility object with model, data, and scoring function.\n        alpha: Alpha parameter of the Beta distribution.\n        beta: Beta parameter of the Beta distribution.\n        done: Stopping criterion.\n        sampler_t: The sampler type to use. See the\n            [sampler][pydvl.value.sampler] module for a list.\n        batch_size: Number of marginal evaluations per (parallelized) task.\n        n_jobs: Number of parallel jobs to use.\n        seed: Either an instance of a numpy random number generator or a seed for it.\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        progress: Whether to display a progress bar.\n\n    Returns:\n        Object with the results.\n\n    !!! warning \"Deprecation notice\"\n        Parameter `batch_size` is for experimental use and will be removed in\n        future versions.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n    # HACK: cannot infer return type because of useless IndexT, NameT\n    return compute_generic_semivalues(  # type: ignore\n        sampler_t(u.data.indices, seed=seed),\n        u,\n        beta_coefficient(alpha, beta),\n        done,\n        batch_size=batch_size,\n        n_jobs=n_jobs,\n        parallel_backend=parallel_backend,\n        config=config,\n        progress=progress,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.compute_generic_semivalues","title":"compute_generic_semivalues","text":"<pre><code>compute_generic_semivalues(\n    sampler: PowersetSampler[IndexT],\n    u: Utility,\n    coefficient: SVCoefficient,\n    done: StoppingCriterion,\n    *,\n    marginal: MarginalFunction = DefaultMarginal(),\n    future_processor: FutureProcessor = FutureProcessor(),\n    batch_size: int = 1,\n    skip_converged: bool = False,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False\n) -&gt; ValuationResult\n</code></pre> <p>Computes semi-values for a given utility function and subset sampler.</p> PARAMETER DESCRIPTION <code>sampler</code> <p>The subset sampler to use for utility computations.</p> <p> TYPE: <code>PowersetSampler[IndexT]</code> </p> <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>coefficient</code> <p>The semi-value coefficient</p> <p> TYPE: <code>SVCoefficient</code> </p> <code>done</code> <p>Stopping criterion.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>marginal</code> <p>Marginal function to be used for computing the semivalues</p> <p> TYPE: <code>MarginalFunction</code> DEFAULT: <code>DefaultMarginal()</code> </p> <code>future_processor</code> <p>Additional postprocessing steps required for some algorithms</p> <p> TYPE: <code>FutureProcessor</code> DEFAULT: <code>FutureProcessor()</code> </p> <code>batch_size</code> <p>Number of marginal evaluations per single parallel job.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>skip_converged</code> <p>Whether to skip marginal evaluations for indices that have already converged. CAUTION: This is only entirely safe if the stopping criterion is MaxUpdates. For any other stopping criterion, the convergence status of indices may change during the computation, or they may be marked as having converged even though in fact the estimated values are far from the true values (e.g. for AbsoluteStandardError, you will probably have to carefully adjust the threshold).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the results.</p> <p>Deprecation notice</p> <p>Parameter <code>batch_size</code> is for experimental use and will be removed in future versions.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef compute_generic_semivalues(\n    sampler: PowersetSampler[IndexT],\n    u: Utility,\n    coefficient: SVCoefficient,\n    done: StoppingCriterion,\n    *,\n    marginal: MarginalFunction = DefaultMarginal(),\n    future_processor: FutureProcessor = FutureProcessor(),\n    batch_size: int = 1,\n    skip_converged: bool = False,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n) -&gt; ValuationResult:\n    \"\"\"Computes semi-values for a given utility function and subset sampler.\n\n    Args:\n        sampler: The subset sampler to use for utility computations.\n        u: Utility object with model, data, and scoring function.\n        coefficient: The semi-value coefficient\n        done: Stopping criterion.\n        marginal: Marginal function to be used for computing the semivalues\n        future_processor: Additional postprocessing steps required for some algorithms\n        batch_size: Number of marginal evaluations per single parallel job.\n        skip_converged: Whether to skip marginal evaluations for indices that\n            have already converged. **CAUTION**: This is only entirely safe if\n            the stopping criterion is [MaxUpdates][pydvl.value.stopping.MaxUpdates].\n            For any other stopping criterion, the convergence status of indices\n            may change during the computation, or they may be marked as having\n            converged even though in fact the estimated values are far from the\n            true values (e.g. for\n            [AbsoluteStandardError][pydvl.value.stopping.AbsoluteStandardError],\n            you will probably have to carefully adjust the threshold).\n        n_jobs: Number of parallel jobs to use.\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        progress: Whether to display a progress bar.\n\n    Returns:\n        Object with the results.\n\n    !!! warning \"Deprecation notice\"\n        Parameter `batch_size` is for experimental use and will be removed in\n        future versions.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n    if isinstance(sampler, PermutationSampler) and u.cache is None:\n        log.warning(\n            \"PermutationSampler requires caching to be enabled or computation \"\n            \"will be doubled wrt. a 'direct' implementation of permutation MC\"\n        )\n\n    if batch_size != 1:\n        warnings.warn(\n            \"Parameter `batch_size` is for experimental use and will be\"\n            \" removed in future versions\",\n            DeprecationWarning,\n        )\n\n    result = ValuationResult.zeros(\n        algorithm=f\"semivalue-{str(sampler)}-{coefficient.__name__}\",  # type: ignore\n        indices=u.data.indices,\n        data_names=u.data.data_names,\n    )\n\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n    u = parallel_backend.put(u)\n    correction = parallel_backend.put(\n        lambda n, k: coefficient(n, k) * sampler.weight(n, k)\n    )\n\n    max_workers = parallel_backend.effective_n_jobs(n_jobs)\n    n_submitted_jobs = 2 * max_workers  # number of jobs in the queue\n\n    sampler_it = iter(sampler)\n    pbar = tqdm(disable=not progress, total=100, unit=\"%\")\n\n    with parallel_backend.executor(\n        max_workers=max_workers, cancel_futures=True\n    ) as executor:\n        pending: set[Future] = set()\n        while True:\n            pbar.n = 100 * done.completion()\n            pbar.refresh()\n\n            completed, pending = wait(pending, timeout=1, return_when=FIRST_COMPLETED)\n            for future in completed:\n                processed_future = future_processor(\n                    future.result()\n                )  # List of tuples or\n                for batch_future in processed_future:\n                    if isinstance(batch_future, list):  # Case when batch size is &gt; 1\n                        for idx, marginal_val in batch_future:\n                            result.update(idx, marginal_val)\n                    else:  # Batch size 1\n                        idx, marginal_val = batch_future\n                        result.update(idx, marginal_val)\n                    if done(result):\n                        return result\n\n            # Ensure that we always have n_submitted_jobs running\n            try:\n                while len(pending) &lt; n_submitted_jobs:\n                    samples = tuple(islice(sampler_it, batch_size))\n                    if len(samples) == 0:\n                        raise StopIteration\n\n                    # Filter out samples for indices that have already converged\n                    filtered_samples = samples\n                    if skip_converged and np.count_nonzero(done.converged) &gt; 0:\n                        # TODO: cloudpickle can't pickle result of `filter` on python 3.8\n                        filtered_samples = tuple(\n                            filter(lambda t: not done.converged[t[0]], samples)\n                        )\n\n                    if filtered_samples:\n                        pending.add(\n                            executor.submit(\n                                marginal,\n                                u=u,\n                                coefficient=correction,\n                                samples=filtered_samples,\n                            )\n                        )\n            except StopIteration:\n                if len(pending) == 0:\n                    return result\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.compute_msr_banzhaf_semivalues","title":"compute_msr_banzhaf_semivalues","text":"<pre><code>compute_msr_banzhaf_semivalues(\n    u: Utility,\n    *,\n    done: StoppingCriterion,\n    sampler_t: Type[StochasticSampler] = MSRSampler,\n    batch_size: int = 1,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>Computes MSR sampled Banzhaf values for a given utility function.</p> <p>This is a convenience wrapper for compute_generic_semivalues with the Banzhaf coefficient and MSR sampling.</p> <p>This algorithm works by sampling random subsets and then evaluating the utility on that subset only once. Based on the evaluation and the subset indices, the MSRFutureProcessor then computes the marginal updates like in the paper (Wang et. al.)<sup>3</sup>. Their approach updates the semivalues for all data points every time a new evaluation is computed. This increases sample efficiency compared to normal Monte Carlo updates.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>done</code> <p>Stopping criterion.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>sampler_t</code> <p>The sampler type to use. See the sampler module for a list.</p> <p> TYPE: <code>Type[StochasticSampler]</code> DEFAULT: <code>MSRSampler</code> </p> <code>batch_size</code> <p>Number of marginal evaluations per single parallel job.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the results.</p> <p>Deprecation notice</p> <p>Parameter <code>batch_size</code> is for experimental use and will be removed in future versions.</p> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef compute_msr_banzhaf_semivalues(\n    u: Utility,\n    *,\n    done: StoppingCriterion,\n    sampler_t: Type[StochasticSampler] = MSRSampler,\n    batch_size: int = 1,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    \"\"\"Computes MSR sampled Banzhaf values for a given utility function.\n\n    This is a convenience wrapper for\n    [compute_generic_semivalues][pydvl.value.semivalues.compute_generic_semivalues]\n    with the Banzhaf coefficient and MSR sampling.\n\n    This algorithm works by sampling random subsets and then evaluating the utility\n    on that subset only once. Based on the evaluation and the subset indices,\n    the MSRFutureProcessor then computes the marginal updates like in the paper\n    (Wang et. al.)&lt;sup&gt;&lt;a href=\"wang_data_2023\"&gt;3&lt;/a&gt;&lt;/sup&gt;.\n    Their approach updates the semivalues for all data points every time a new evaluation\n    is computed. This increases sample efficiency compared to normal Monte Carlo updates.\n\n    Args:\n        u: Utility object with model, data, and scoring function.\n        done: Stopping criterion.\n        sampler_t: The sampler type to use. See the\n            [sampler][pydvl.value.sampler] module for a list.\n        batch_size: Number of marginal evaluations per single parallel job.\n        n_jobs: Number of parallel jobs to use.\n        seed: Either an instance of a numpy random number generator or a seed\n            for it.\n        config: Object configuring parallel computation, with cluster address,\n            number of cpus, etc.\n        progress: Whether to display a progress bar.\n\n    Returns:\n        Object with the results.\n\n    !!! warning \"Deprecation notice\"\n        Parameter `batch_size` is for experimental use and will be removed in\n        future versions.\n    \"\"\"\n    # HACK: cannot infer return type because of useless IndexT, NameT\n    return compute_generic_semivalues(  # type: ignore\n        sampler_t(u.data.indices, seed=seed),\n        u,\n        always_one_coefficient,\n        done,\n        marginal=RawUtility(),\n        future_processor=MSRFutureProcessor(u),\n        batch_size=batch_size,\n        n_jobs=n_jobs,\n        parallel_backend=parallel_backend,\n        config=config,\n        progress=progress,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.compute_semivalues","title":"compute_semivalues","text":"<pre><code>compute_semivalues(\n    u: Utility,\n    *,\n    done: StoppingCriterion,\n    mode: SemiValueMode = Shapley,\n    sampler_t: Type[StochasticSampler] = PermutationSampler,\n    batch_size: int = 1,\n    n_jobs: int = 1,\n    seed: Optional[Seed] = None,\n    **kwargs: Any\n) -&gt; ValuationResult\n</code></pre> <p>Convenience entry point for most common semi-value computations.</p> <p>Deprecation warning</p> <p>This method is deprecated and will be replaced in 0.8.0 by the more general implementation of compute_generic_semivalues. Use compute_shapley_semivalues, compute_banzhaf_semivalues, or compute_beta_shapley_semivalues instead.</p> <p>The modes supported with this interface are the following. For greater flexibility use compute_generic_semivalues directly.</p> <ul> <li>SemiValueMode.Shapley:   Shapley values.</li> <li>SemiValueMode.BetaShapley:   Implements the Beta Shapley semi-value as introduced in   (Kwon and Zou, 2022)<sup>1</sup>.   Pass additional keyword arguments <code>alpha</code> and <code>beta</code> to set the   parameters of the Beta distribution (both default to 1).</li> <li>SemiValueMode.Banzhaf: Implements   the Banzhaf semi-value as introduced in (Wang and Jia, 2022)<sup>1</sup>.</li> </ul> <p>See Data valuation for an overview of valuation.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>done</code> <p>Stopping criterion.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>mode</code> <p>The semi-value mode to use. See SemiValueMode for a list.</p> <p> TYPE: <code>SemiValueMode</code> DEFAULT: <code>Shapley</code> </p> <code>sampler_t</code> <p>The sampler type to use. See sampler for a list.</p> <p> TYPE: <code>Type[StochasticSampler]</code> DEFAULT: <code>PermutationSampler</code> </p> <code>batch_size</code> <p>Number of marginal evaluations per (parallelized) task.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Additional keyword arguments passed to compute_generic_semivalues.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the results.</p> <p>Deprecation notice</p> <p>Parameter <code>batch_size</code> is for experimental use and will be removed in future versions.</p> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>@deprecated(target=True, deprecated_in=\"0.7.0\", remove_in=\"0.8.0\")\ndef compute_semivalues(\n    u: Utility,\n    *,\n    done: StoppingCriterion,\n    mode: SemiValueMode = SemiValueMode.Shapley,\n    sampler_t: Type[StochasticSampler] = PermutationSampler,\n    batch_size: int = 1,\n    n_jobs: int = 1,\n    seed: Optional[Seed] = None,\n    **kwargs: Any,\n) -&gt; ValuationResult:\n    \"\"\"Convenience entry point for most common semi-value computations.\n\n    !!! warning \"Deprecation warning\"\n        This method is deprecated and will be replaced in 0.8.0 by the more\n        general implementation of\n        [compute_generic_semivalues][pydvl.value.semivalues.compute_generic_semivalues].\n        Use\n        [compute_shapley_semivalues][pydvl.value.semivalues.compute_shapley_semivalues],\n        [compute_banzhaf_semivalues][pydvl.value.semivalues.compute_banzhaf_semivalues],\n        or\n        [compute_beta_shapley_semivalues][pydvl.value.semivalues.compute_beta_shapley_semivalues]\n        instead.\n\n    The modes supported with this interface are the following. For greater\n    flexibility use\n    [compute_generic_semivalues][pydvl.value.semivalues.compute_generic_semivalues]\n    directly.\n\n    - [SemiValueMode.Shapley][pydvl.value.semivalues.SemiValueMode]:\n      Shapley values.\n    - [SemiValueMode.BetaShapley][pydvl.value.semivalues.SemiValueMode]:\n      Implements the Beta Shapley semi-value as introduced in\n      (Kwon and Zou, 2022)&lt;sup&gt;&lt;a href=\"#kwon_beta_2022\"&gt;1&lt;/a&gt;&lt;/sup&gt;.\n      Pass additional keyword arguments `alpha` and `beta` to set the\n      parameters of the Beta distribution (both default to 1).\n    - [SemiValueMode.Banzhaf][pydvl.value.semivalues.SemiValueMode]: Implements\n      the Banzhaf semi-value as introduced in (Wang and Jia, 2022)&lt;sup&gt;&lt;a\n      href=\"#wang_data_2023\"&gt;1&lt;/a&gt;&lt;/sup&gt;.\n\n    See [Data valuation][data-valuation-intro] for an overview of valuation.\n\n    Args:\n        u: Utility object with model, data, and scoring function.\n        done: Stopping criterion.\n        mode: The semi-value mode to use. See\n            [SemiValueMode][pydvl.value.semivalues.SemiValueMode] for a list.\n        sampler_t: The sampler type to use. See [sampler][pydvl.value.sampler]\n            for a list.\n        batch_size: Number of marginal evaluations per (parallelized) task.\n        n_jobs: Number of parallel jobs to use.\n        seed: Either an instance of a numpy random number generator or a seed for it.\n        kwargs: Additional keyword arguments passed to\n            [compute_generic_semivalues][pydvl.value.semivalues.compute_generic_semivalues].\n\n    Returns:\n        Object with the results.\n\n    !!! warning \"Deprecation notice\"\n        Parameter `batch_size` is for experimental use and will be removed in\n        future versions.\n    \"\"\"\n    if mode == SemiValueMode.Shapley:\n        coefficient = shapley_coefficient\n    elif mode == SemiValueMode.BetaShapley:\n        alpha = kwargs.pop(\"alpha\", 1)\n        beta = kwargs.pop(\"beta\", 1)\n        coefficient = beta_coefficient(alpha, beta)\n    elif mode == SemiValueMode.Banzhaf:\n        coefficient = banzhaf_coefficient\n    else:\n        raise ValueError(f\"Unknown mode {mode}\")\n    coefficient = cast(SVCoefficient, coefficient)\n\n    # HACK: cannot infer return type because of useless IndexT, NameT\n    return compute_generic_semivalues(  # type: ignore\n        sampler_t(u.data.indices, seed=seed),\n        u,\n        coefficient,\n        done,\n        n_jobs=n_jobs,\n        batch_size=batch_size,\n        **kwargs,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/semivalues/#pydvl.value.semivalues.compute_shapley_semivalues","title":"compute_shapley_semivalues","text":"<pre><code>compute_shapley_semivalues(\n    u: Utility,\n    *,\n    done: StoppingCriterion,\n    sampler_t: Type[StochasticSampler] = PermutationSampler,\n    batch_size: int = 1,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>Computes Shapley values for a given utility function.</p> <p>This is a convenience wrapper for compute_generic_semivalues with the Shapley coefficient. Use compute_shapley_values for a more flexible interface and additional methods, including TMCS.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>done</code> <p>Stopping criterion.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>sampler_t</code> <p>The sampler type to use. See the sampler module for a list.</p> <p> TYPE: <code>Type[StochasticSampler]</code> DEFAULT: <code>PermutationSampler</code> </p> <code>batch_size</code> <p>Number of marginal evaluations per single parallel job.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the results.</p> <p>Deprecation notice</p> <p>Parameter <code>batch_size</code> is for experimental use and will be removed in future versions.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/semivalues.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef compute_shapley_semivalues(\n    u: Utility,\n    *,\n    done: StoppingCriterion,\n    sampler_t: Type[StochasticSampler] = PermutationSampler,\n    batch_size: int = 1,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    \"\"\"Computes Shapley values for a given utility function.\n\n    This is a convenience wrapper for\n    [compute_generic_semivalues][pydvl.value.semivalues.compute_generic_semivalues]\n    with the Shapley coefficient. Use\n    [compute_shapley_values][pydvl.value.shapley.common.compute_shapley_values]\n    for a more flexible interface and additional methods, including TMCS.\n\n    Args:\n        u: Utility object with model, data, and scoring function.\n        done: Stopping criterion.\n        sampler_t: The sampler type to use. See the\n            [sampler][pydvl.value.sampler] module for a list.\n        batch_size: Number of marginal evaluations per single parallel job.\n        n_jobs: Number of parallel jobs to use.\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        seed: Either an instance of a numpy random number generator or a seed\n            for it.\n        progress: Whether to display a progress bar.\n\n    Returns:\n        Object with the results.\n\n    !!! warning \"Deprecation notice\"\n        Parameter `batch_size` is for experimental use and will be removed in\n        future versions.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n    # HACK: cannot infer return type because of useless IndexT, NameT\n    return compute_generic_semivalues(  # type: ignore\n        sampler_t(u.data.indices, seed=seed),\n        u,\n        shapley_coefficient,\n        done,\n        batch_size=batch_size,\n        n_jobs=n_jobs,\n        parallel_backend=parallel_backend,\n        config=config,\n        progress=progress,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/","title":"Stopping","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping","title":"pydvl.value.stopping","text":"<p>Stopping criteria for value computations.</p> <p>This module provides a basic set of stopping criteria, like MaxUpdates, MaxTime, or HistoryDeviation among others. These can behave in different ways depending on the context. For example, MaxUpdates limits the number of updates to values, which depending on the algorithm may mean a different number of utility evaluations or imply other computations like solving a linear or quadratic program.</p> <p>Stopping criteria are callables that are evaluated on a ValuationResult and return a Status object. They can be combined using boolean operators.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping--how-convergence-is-determined","title":"How convergence is determined","text":"<p>Most stopping criteria keep track of the convergence of each index separately but make global decisions based on the overall convergence of some fraction of all indices. For example, if we have a stopping criterion that checks whether the standard error of 90% of values is below a threshold, then methods will keep updating all indices until 90% of them have converged, irrespective of the quality of the individual estimates, and without freezing updates for indices along the way as values individually attain low standard error.</p> <p>This has some practical implications, because some values do tend to converge sooner than others. For example, assume we use the criterion <code>AbsoluteStandardError(0.02) | MaxUpdates(1000)</code>. Then values close to 0 might be marked as \"converged\" rather quickly because they fulfill the first criterion, say after 20 iterations, despite being poor estimates. Because other indices take much longer to have low standard error and the criterion is a global check, the \"converged\" ones keep being updated and end up being good estimates. In this case, this has been beneficial, but one might not wish for converged values to be updated, if one is sure that the criterion is adequate for individual values.</p> <p>Semi-value methods include a parameter <code>skip_converged</code> that allows to skip the computation of values that have converged. The way to avoid doing this too early is to use a more stringent check, e.g. <code>AbsoluteStandardError(1e-3) | MaxUpdates(1000)</code>. With <code>skip_converged=True</code> this check can still take less time than the first one, despite requiring more iterations for some indices.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping--choosing-a-stopping-criterion","title":"Choosing a stopping criterion","text":"<p>The choice of a stopping criterion greatly depends on the algorithm and the context. A safe bet is to combine a MaxUpdates or a MaxTime with a HistoryDeviation or an AbsoluteStandardError. The former will ensure that the computation does not run for too long, while the latter will try to achieve results that are stable enough. Note however that if the threshold is too strict, one will always end up running until a maximum number of iterations or time. Also keep in mind that different values converge at different times, so you might want to use tight thresholds and <code>skip_converged</code> as described above for semi-values.</p> Example <p><pre><code>from pydvl.value import AbsoluteStandardError, MaxUpdates, compute_banzhaf_semivalues\n\nutility = ...  # some utility object\ncriterion = AbsoluteStandardError(threshold=1e-3, burn_in=32) | MaxUpdates(1000)\nvalues = compute_banzhaf_semivalues(\n    utility,\n    criterion,\n    skip_converged=True,  # skip values that have converged (CAREFUL!)\n)\n</code></pre> This will compute the Banzhaf semivalues for <code>utility</code> until either the absolute standard error is below <code>1e-3</code> or <code>1000</code> updates have been performed. The <code>burn_in</code> parameter is used to discard the first <code>32</code> updates from the computation of the standard error. The <code>skip_converged</code> parameter is used to avoid computing more marginals for indices that have converged, which is useful if AbsoluteStandardError is met before MaxUpdates for some indices.</p> <p>Warning</p> <p>Be careful not to reuse the same stopping criterion for different computations. The object has state and will not be reset between calls to value computation methods. If you need to reuse the same criterion, you should create a new instance.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping--creating-stopping-criteria","title":"Creating stopping criteria","text":"<p>The easiest way is to declare a function implementing the interface StoppingCriterionCallable and wrap it with make_criterion(). This creates a StoppingCriterion object that can be composed with other stopping criteria.</p> <p>Alternatively, and in particular if reporting of completion is required, one can inherit from this class and implement the abstract methods <code>_check</code> and completion.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping--combining-stopping-criteria","title":"Combining stopping criteria","text":"<p>Objects of type StoppingCriterion can be combined with the binary operators <code>&amp;</code> (and), and <code>|</code> (or), following the truth tables of Status. The unary operator <code>~</code> (not) is also supported. See StoppingCriterion for details on how these operations affect the behavior of the stopping criteria.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping--references","title":"References","text":"<ol> <li> <p>Ghorbani, A., Zou, J., 2019. Data Shapley: Equitable Valuation of Data for Machine Learning. In: Proceedings of the 36th International Conference on Machine Learning, PMLR, pp. 2242\u20132251.\u00a0\u21a9</p> </li> <li> <p>Wang, J.T. and Jia, R., 2023. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning. In: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, pp. 6388-6421.\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.AbsoluteStandardError","title":"AbsoluteStandardError","text":"<pre><code>AbsoluteStandardError(\n    threshold: float,\n    fraction: float = 1.0,\n    burn_in: int = 4,\n    modify_result: bool = True,\n)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Determine convergence based on the standard error of the values.</p> <p>If \\(s_i\\) is the standard error for datum \\(i\\), then this criterion returns Converged if \\(s_i &lt; \\epsilon\\) for all \\(i\\) and a threshold value \\(\\epsilon \\gt 0\\).</p> PARAMETER DESCRIPTION <code>threshold</code> <p>A value is considered to have converged if the standard error is below this threshold. A way of choosing it is to pick some percentage of the range of the values. For Shapley values this is the difference between the maximum and minimum of the utility function (to see this substitute the maximum and minimum values of the utility into the marginal contribution formula).</p> <p> TYPE: <code>float</code> </p> <code>fraction</code> <p>The fraction of values that must have converged for the criterion to return Converged.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>burn_in</code> <p>The number of iterations to ignore before checking for convergence. This is required because computations typically start with zero variance, as a result of using zeros(). The default is set to an arbitrary minimum which is usually enough but may need to be increased.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __init__(\n    self,\n    threshold: float,\n    fraction: float = 1.0,\n    burn_in: int = 4,\n    modify_result: bool = True,\n):\n    super().__init__(modify_result=modify_result)\n    self.threshold = threshold\n    self.fraction = fraction\n    self.burn_in = burn_in\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.AbsoluteStandardError.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.AbsoluteStandardError.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.AbsoluteStandardError.completion","title":"completion","text":"<pre><code>completion() -&gt; float\n</code></pre> <p>Returns a value between 0 and 1 indicating the completion of the computation.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def completion(self) -&gt; float:\n    \"\"\"Returns a value between 0 and 1 indicating the completion of the\n    computation.\n    \"\"\"\n    if self.converged.size == 0:\n        return 0.0\n    return float(np.mean(self.converged).item())\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.HistoryDeviation","title":"HistoryDeviation","text":"<pre><code>HistoryDeviation(\n    n_steps: int,\n    rtol: float,\n    pin_converged: bool = True,\n    modify_result: bool = True,\n)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>A simple check for relative distance to a previous step in the computation.</p> <p>The method used by (Ghorbani and Zou, 2019)<sup>1</sup> computes the relative distances between the current values \\(v_i^t\\) and the values at the previous checkpoint \\(v_i^{t-\\tau}\\). If the sum is below a given threshold, the computation is terminated.</p> \\[\\sum_{i=1}^n \\frac{\\left| v_i^t - v_i^{t-\\tau} \\right|}{v_i^t} &lt; \\epsilon.\\] <p>When the denominator is zero, the summand is set to the value of \\(v_i^{ t-\\tau}\\).</p> <p>This implementation is slightly generalised to allow for different number of updates to individual indices, as happens with powerset samplers instead of permutations. Every subset of indices that is found to converge can be pinned to that state. Once all indices have converged the method has converged.</p> <p>Warning</p> <p>This criterion is meant for the reproduction of the results in the paper, but we do not recommend using it in practice.</p> PARAMETER DESCRIPTION <code>n_steps</code> <p>Checkpoint values every so many updates and use these saved values to compare.</p> <p> TYPE: <code>int</code> </p> <code>rtol</code> <p>Relative tolerance for convergence (\\(\\epsilon\\) in the formula).</p> <p> TYPE: <code>float</code> </p> <code>pin_converged</code> <p>If <code>True</code>, once an index has converged, it is pinned</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __init__(\n    self,\n    n_steps: int,\n    rtol: float,\n    pin_converged: bool = True,\n    modify_result: bool = True,\n):\n    super().__init__(modify_result=modify_result)\n    if n_steps &lt; 1:\n        raise ValueError(\"n_steps must be at least 1\")\n    if rtol &lt;= 0 or rtol &gt;= 1:\n        raise ValueError(\"rtol must be in (0, 1)\")\n\n    self.n_steps = n_steps\n    self.rtol = rtol\n    self.update_op = np.logical_or if pin_converged else np.logical_and\n    self._memory = None  # type: ignore\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.HistoryDeviation.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.HistoryDeviation.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.HistoryDeviation.completion","title":"completion","text":"<pre><code>completion() -&gt; float\n</code></pre> <p>Returns a value between 0 and 1 indicating the completion of the computation.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def completion(self) -&gt; float:\n    \"\"\"Returns a value between 0 and 1 indicating the completion of the\n    computation.\n    \"\"\"\n    if self.converged.size == 0:\n        return 0.0\n    return float(np.mean(self.converged).item())\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MaxChecks","title":"MaxChecks","text":"<pre><code>MaxChecks(n_checks: Optional[int], modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Terminate as soon as the number of checks exceeds the threshold.</p> <p>A \"check\" is one call to the criterion.</p> PARAMETER DESCRIPTION <code>n_checks</code> <p>Threshold: if <code>None</code>, no _check is performed, effectively creating a (never) stopping criterion that always returns <code>Pending</code>.</p> <p> TYPE: <code>Optional[int]</code> </p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __init__(self, n_checks: Optional[int], modify_result: bool = True):\n    super().__init__(modify_result=modify_result)\n    if n_checks is not None and n_checks &lt; 1:\n        raise ValueError(\"n_iterations must be at least 1 or None\")\n    self.n_checks = n_checks\n    self._count = 0\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MaxChecks.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MaxChecks.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MaxTime","title":"MaxTime","text":"<pre><code>MaxTime(seconds: Optional[float], modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Terminate if the computation time exceeds the given number of seconds.</p> <p>Checks the elapsed time since construction</p> PARAMETER DESCRIPTION <code>seconds</code> <p>Threshold: The computation is terminated if the elapsed time between object construction and a _check exceeds this value. If <code>None</code>, no _check is performed, effectively creating a (never) stopping criterion that always returns <code>Pending</code>.</p> <p> TYPE: <code>Optional[float]</code> </p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __init__(self, seconds: Optional[float], modify_result: bool = True):\n    super().__init__(modify_result=modify_result)\n    self.max_seconds = seconds or np.inf\n    if self.max_seconds &lt;= 0:\n        raise ValueError(\"Number of seconds for MaxTime must be positive or None\")\n    self.start = time()\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MaxTime.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MaxTime.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MaxUpdates","title":"MaxUpdates","text":"<pre><code>MaxUpdates(n_updates: Optional[int], modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Terminate if any number of value updates exceeds or equals the given threshold.</p> <p>Note</p> <p>If you want to ensure that all values have been updated, you probably want MinUpdates instead.</p> <p>This checks the <code>counts</code> field of a ValuationResult, i.e. the number of times that each index has been updated. For powerset samplers, the maximum of this number coincides with the maximum number of subsets sampled. For permutation samplers, it coincides with the number of permutations sampled.</p> PARAMETER DESCRIPTION <code>n_updates</code> <p>Threshold: if <code>None</code>, no _check is performed, effectively creating a (never) stopping criterion that always returns <code>Pending</code>.</p> <p> TYPE: <code>Optional[int]</code> </p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __init__(self, n_updates: Optional[int], modify_result: bool = True):\n    super().__init__(modify_result=modify_result)\n    if n_updates is not None and n_updates &lt; 1:\n        raise ValueError(\"n_updates must be at least 1 or None\")\n    self.n_updates = n_updates\n    self.last_max = 0\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MaxUpdates.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MaxUpdates.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MinUpdates","title":"MinUpdates","text":"<pre><code>MinUpdates(n_updates: Optional[int], modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>Terminate as soon as all value updates exceed or equal the given threshold.</p> <p>This checks the <code>counts</code> field of a ValuationResult, i.e. the number of times that each index has been updated. For powerset samplers, the minimum of this number is a lower bound for the number of subsets sampled. For permutation samplers, it lower-bounds the amount of permutations sampled.</p> PARAMETER DESCRIPTION <code>n_updates</code> <p>Threshold: if <code>None</code>, no _check is performed, effectively creating a (never) stopping criterion that always returns <code>Pending</code>.</p> <p> TYPE: <code>Optional[int]</code> </p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __init__(self, n_updates: Optional[int], modify_result: bool = True):\n    super().__init__(modify_result=modify_result)\n    self.n_updates = n_updates\n    self.last_min = 0\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MinUpdates.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.MinUpdates.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.RankCorrelation","title":"RankCorrelation","text":"<pre><code>RankCorrelation(rtol: float, burn_in: int, modify_result: bool = True)\n</code></pre> <p>               Bases: <code>StoppingCriterion</code></p> <p>A check for stability of Spearman correlation between checks.</p> <p>When the change in rank correlation between two successive iterations is below a given threshold, the computation is terminated. The criterion computes the Spearman correlation between two successive iterations. The Spearman correlation uses the ordering indices of the given values and correlates them. This means it focuses on the order of the elements instead of their exact values. If the order stops changing (meaning the Banzhaf semivalues estimates converge), the criterion stops the algorithm.</p> <p>This criterion is used in (Wang et. al.)<sup>2</sup>.</p> PARAMETER DESCRIPTION <code>rtol</code> <p>Relative tolerance for convergence (\\(\\epsilon\\) in the formula)</p> <p> TYPE: <code>float</code> </p> <code>modify_result</code> <p>If <code>True</code>, the status of the input ValuationResult is modified in place after the call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>burn_in</code> <p>The minimum number of iterations before checking for convergence. This is required because the first correlation is meaningless.</p> <p> TYPE: <code>int</code> </p> <p>Added in 0.9.0</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __init__(\n    self,\n    rtol: float,\n    burn_in: int,\n    modify_result: bool = True,\n):\n    super().__init__(modify_result=modify_result)\n    if rtol &lt;= 0 or rtol &gt;= 1:\n        raise ValueError(\"rtol must be in (0, 1)\")\n    self.rtol = rtol\n    self.burn_in = burn_in\n    self._memory: NDArray[np.float64] | None = None\n    self._corr = 0.0\n    self._completion = 0.0\n    self._iterations = 0\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.RankCorrelation.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.RankCorrelation.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.StoppingCriterion","title":"StoppingCriterion","text":"<pre><code>StoppingCriterion(modify_result: bool = True)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A composable callable object to determine whether a computation must stop.</p> <p>A <code>StoppingCriterion</code> is a callable taking a ValuationResult and returning a Status. It also keeps track of individual convergence of values with converged, and reports the overall completion of the computation with completion.</p> <p>Instances of <code>StoppingCriterion</code> can be composed with the binary operators <code>&amp;</code> (and), and <code>|</code> (or), following the truth tables of Status. The unary operator <code>~</code> (not) is also supported. These boolean operations act according to the following rules:</p> <ul> <li>The results of <code>check()</code> are combined with the operator. See   Status for the truth tables.</li> <li>The results of   converged are combined   with the operator (returning another boolean array).</li> <li>The completion   method returns the min, max, or the complement to 1 of the completions of   the operands, for AND, OR and NOT respectively. This is required for cases   where one of the criteria does not keep track of the convergence of single   values, e.g. MaxUpdates, because   completion by   default returns the mean of the boolean convergence array.</li> </ul>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.StoppingCriterion--subclassing","title":"Subclassing","text":"<p>Subclassing this class requires implementing a <code>check()</code> method that returns a Status object based on a given ValuationResult. This method should update the attribute <code>_converged</code>, which is a boolean array indicating whether the value for each index has converged. When this does not make sense for a particular stopping criterion, completion should be overridden to provide an overall completion value, since its default implementation attempts to compute the mean of <code>_converged</code>.</p> PARAMETER DESCRIPTION <code>modify_result</code> <p>If <code>True</code> the status of the input ValuationResult is modified in place after the call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __init__(self, modify_result: bool = True):\n    self.modify_result = modify_result\n    self._converged = np.full(0, False)\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.StoppingCriterion.converged","title":"converged  <code>property</code>","text":"<pre><code>converged: NDArray[bool_]\n</code></pre> <p>Returns a boolean array indicating whether the values have converged for each data point.</p> <p>Inheriting classes must set the <code>_converged</code> attribute in their <code>check()</code>.</p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>A boolean array indicating whether the values have converged for</p> <code>NDArray[bool_]</code> <p>each data point.</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.StoppingCriterion.__call__","title":"__call__","text":"<pre><code>__call__(result: ValuationResult) -&gt; Status\n</code></pre> <p>Calls <code>check()</code>, maybe updating the result.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def __call__(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Calls `check()`, maybe updating the result.\"\"\"\n    if len(result) == 0:\n        logger.warning(\n            \"At least one iteration finished but no results where generated. \"\n            \"Please check that your scorer and utility return valid numbers.\"\n        )\n    status = self._check(result)\n    if self.modify_result:  # FIXME: this is not nice\n        result._status = status\n    return status\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.StoppingCriterion._check","title":"_check  <code>abstractmethod</code>","text":"<pre><code>_check(result: ValuationResult) -&gt; Status\n</code></pre> <p>Check whether the computation should stop.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>@abc.abstractmethod\ndef _check(self, result: ValuationResult) -&gt; Status:\n    \"\"\"Check whether the computation should stop.\"\"\"\n    ...\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.StoppingCriterion.completion","title":"completion","text":"<pre><code>completion() -&gt; float\n</code></pre> <p>Returns a value between 0 and 1 indicating the completion of the computation.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def completion(self) -&gt; float:\n    \"\"\"Returns a value between 0 and 1 indicating the completion of the\n    computation.\n    \"\"\"\n    if self.converged.size == 0:\n        return 0.0\n    return float(np.mean(self.converged).item())\n</code></pre>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.StoppingCriterionCallable","title":"StoppingCriterionCallable","text":"<p>               Bases: <code>Protocol</code></p> <p>Signature for a stopping criterion</p>"},{"location":"deprecated/pydvl/value/stopping/#pydvl.value.stopping.make_criterion","title":"make_criterion","text":"<pre><code>make_criterion(\n    fun: StoppingCriterionCallable,\n    converged: Callable[[], NDArray[bool_]] | None = None,\n    completion: Callable[[], float] | None = None,\n    name: str | None = None,\n) -&gt; Type[StoppingCriterion]\n</code></pre> <p>Create a new StoppingCriterion from a function. Use this to enable simpler functions to be composed with bitwise operators</p> PARAMETER DESCRIPTION <code>fun</code> <p>The callable to wrap.</p> <p> TYPE: <code>StoppingCriterionCallable</code> </p> <code>converged</code> <p>A callable that returns a boolean array indicating what values have converged.</p> <p> TYPE: <code>Callable[[], NDArray[bool_]] | None</code> DEFAULT: <code>None</code> </p> <code>completion</code> <p>A callable that returns a value between 0 and 1 indicating the rate of completion of the computation. If not provided, the fraction of converged values is used.</p> <p> TYPE: <code>Callable[[], float] | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the new criterion. If <code>None</code>, the <code>__name__</code> of the function is used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Type[StoppingCriterion]</code> <p>A new subclass of StoppingCriterion.</p> Source code in <code>src/pydvl/value/stopping.py</code> <pre><code>def make_criterion(\n    fun: StoppingCriterionCallable,\n    converged: Callable[[], NDArray[np.bool_]] | None = None,\n    completion: Callable[[], float] | None = None,\n    name: str | None = None,\n) -&gt; Type[StoppingCriterion]:\n    \"\"\"Create a new [StoppingCriterion][pydvl.value.stopping.StoppingCriterion] from a function.\n    Use this to enable simpler functions to be composed with bitwise operators\n\n    Args:\n        fun: The callable to wrap.\n        converged: A callable that returns a boolean array indicating what\n            values have converged.\n        completion: A callable that returns a value between 0 and 1 indicating\n            the rate of completion of the computation. If not provided, the fraction\n            of converged values is used.\n        name: The name of the new criterion. If `None`, the `__name__` of\n            the function is used.\n\n    Returns:\n        A new subclass of [StoppingCriterion][pydvl.value.stopping.StoppingCriterion].\n    \"\"\"\n\n    class WrappedCriterion(StoppingCriterion):\n        def __init__(self, modify_result: bool = True):\n            super().__init__(modify_result=modify_result)\n            self._name = name or getattr(fun, \"__name__\", \"WrappedCriterion\")\n\n        def _check(self, result: ValuationResult) -&gt; Status:\n            return fun(result)\n\n        @property\n        def converged(self) -&gt; NDArray[np.bool_]:\n            if converged is None:\n                return super().converged\n            return converged()\n\n        def __str__(self):\n            return self._name\n\n        def completion(self) -&gt; float:\n            if completion is None:\n                return super().completion()\n            return completion()\n\n    return WrappedCriterion\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/","title":"Least core","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0 in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/least_core/#pydvl.value.least_core","title":"pydvl.value.least_core","text":"<p>New in version 0.4.0</p> <p>This package holds all routines for the computation of Least Core data values.</p> <p>Please refer to Data valuation for an overview.</p> <p>In addition to the standard interface via compute_least_core_values(), because computing the Least Core values requires the solution of a linear and a quadratic problem after computing all the utility values, there is the possibility of performing each step separately. This is useful when running multiple experiments: use lc_prepare_problem() or mclc_prepare_problem() to prepare a list of problems to solve, then solve them in parallel with lc_solve_problems().</p> <p>Note that mclc_prepare_problem() is parallelized itself, so preparing the problems should be done in sequence in this case. The solution of the linear systems can then be done in parallel.</p>"},{"location":"deprecated/pydvl/value/least_core/#pydvl.value.least_core.LeastCoreMode","title":"LeastCoreMode","text":"<p>               Bases: <code>Enum</code></p> <p>Available Least Core algorithms.</p>"},{"location":"deprecated/pydvl/value/least_core/#pydvl.value.least_core.compute_least_core_values","title":"compute_least_core_values","text":"<pre><code>compute_least_core_values(\n    u: Utility,\n    *,\n    n_jobs: int = 1,\n    n_iterations: Optional[int] = None,\n    mode: LeastCoreMode = MonteCarlo,\n    non_negative_subsidy: bool = False,\n    solver_options: Optional[dict] = None,\n    progress: bool = False,\n    **kwargs\n) -&gt; ValuationResult\n</code></pre> <p>Umbrella method to compute Least Core values with any of the available algorithms.</p> <p>See Data valuation for an overview.</p> <p>The following algorithms are available. Note that the exact method can only work with very small datasets and is thus intended only for testing.</p> <ul> <li><code>exact</code>: uses the complete powerset of the training set for the constraints   combinatorial_exact_shapley().</li> <li><code>montecarlo</code>:  uses the approximate Monte Carlo Least Core algorithm.   Implemented in montecarlo_least_core().</li> </ul> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>n_jobs</code> <p>Number of jobs to run in parallel. Only used for Monte Carlo Least Core.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_iterations</code> <p>Number of subsets to sample and evaluate the utility on. Only used for Monte Carlo Least Core.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>Algorithm to use. See LeastCoreMode for available options.</p> <p> TYPE: <code>LeastCoreMode</code> DEFAULT: <code>MonteCarlo</code> </p> <code>non_negative_subsidy</code> <p>If True, the least core subsidy \\(e\\) is constrained to be non-negative.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>solver_options</code> <p>Optional dictionary of options passed to the solvers.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the computed values.</p> <p>New in version 0.5.0</p> Source code in <code>src/pydvl/value/least_core/__init__.py</code> <pre><code>def compute_least_core_values(\n    u: Utility,\n    *,\n    n_jobs: int = 1,\n    n_iterations: Optional[int] = None,\n    mode: LeastCoreMode = LeastCoreMode.MonteCarlo,\n    non_negative_subsidy: bool = False,\n    solver_options: Optional[dict] = None,\n    progress: bool = False,\n    **kwargs,\n) -&gt; ValuationResult:\n    \"\"\"Umbrella method to compute Least Core values with any of the available\n    algorithms.\n\n    See [Data valuation][data-valuation-intro] for an overview.\n\n    The following algorithms are available. Note that the exact method can only\n    work with very small datasets and is thus intended only for testing.\n\n    - `exact`: uses the complete powerset of the training set for the constraints\n      [combinatorial_exact_shapley()][pydvl.value.shapley.naive.combinatorial_exact_shapley].\n    - `montecarlo`:  uses the approximate Monte Carlo Least Core algorithm.\n      Implemented in [montecarlo_least_core()][pydvl.value.least_core.montecarlo.montecarlo_least_core].\n\n    Args:\n        u: Utility object with model, data, and scoring function\n        n_jobs: Number of jobs to run in parallel. Only used for Monte Carlo\n            Least Core.\n        n_iterations: Number of subsets to sample and evaluate the utility on.\n            Only used for Monte Carlo Least Core.\n        mode: Algorithm to use. See\n            [LeastCoreMode][pydvl.value.least_core.LeastCoreMode] for available\n            options.\n        non_negative_subsidy: If True, the least core subsidy $e$ is constrained\n            to be non-negative.\n        solver_options: Optional dictionary of options passed to the solvers.\n\n    Returns:\n        Object with the computed values.\n\n    !!! tip \"New in version 0.5.0\"\n    \"\"\"\n\n    if mode == LeastCoreMode.MonteCarlo:\n        # TODO fix progress showing in remote case\n        progress = False\n        if n_iterations is None:\n            raise ValueError(\"n_iterations cannot be None for Monte Carlo Least Core\")\n        return montecarlo_least_core(  # type: ignore\n            u=u,\n            n_iterations=n_iterations,\n            n_jobs=n_jobs,\n            progress=progress,\n            non_negative_subsidy=non_negative_subsidy,\n            solver_options=solver_options,\n            **kwargs,\n        )\n    elif mode == LeastCoreMode.Exact:\n        return exact_least_core(\n            u=u,\n            progress=progress,\n            non_negative_subsidy=non_negative_subsidy,\n            solver_options=solver_options,\n        )\n\n    raise ValueError(f\"Invalid value encountered in {mode=}\")\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/common/","title":"Common","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/least_core/common/#pydvl.value.least_core.common","title":"pydvl.value.least_core.common","text":""},{"location":"deprecated/pydvl/value/least_core/common/#pydvl.value.least_core.common._solve_egalitarian_least_core_quadratic_program","title":"_solve_egalitarian_least_core_quadratic_program","text":"<pre><code>_solve_egalitarian_least_core_quadratic_program(\n    subsidy: float,\n    A_eq: NDArray[float64],\n    b_eq: NDArray[float64],\n    A_lb: NDArray[float64],\n    b_lb: NDArray[float64],\n    solver_options: dict,\n) -&gt; Optional[NDArray[float64]]\n</code></pre> <p>Solves the egalitarian Least Core's quadratic program using cvxopt.</p> <p>$$     \\text{minimize} \\ &amp; | x |2 \\     \\mbox{such that} \\ &amp; A, \\     &amp; A_{lb} x + e \\ge b_{lb},\\     &amp; A_{eq} x = b_{eq},\\     &amp; x in \\mathcal{R}^n , \\     &amp; e \\text{ is a constant.} $$  where } x = b_{eq\\(x\\) is a vector of decision variables; , \\(b_{ub}\\), \\(b_{eq}\\), \\(l\\), and \\(u\\) are vectors; and \\(A_{ub}\\) and \\(A_{eq}\\) are matrices.</p> PARAMETER DESCRIPTION <code>subsidy</code> <p>Minimal subsidy returned by _solve_least_core_linear_program()</p> <p> TYPE: <code>float</code> </p> <code>A_eq</code> <p>The equality constraint matrix. Each row of <code>A_eq</code> specifies the coefficients of a linear equality constraint on <code>x</code>.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>b_eq</code> <p>The equality constraint vector. Each element of <code>A_eq @ x</code> must equal the corresponding element of <code>b_eq</code>.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>A_lb</code> <p>The inequality constraint matrix. Each row of <code>A_lb</code> specifies the coefficients of a linear inequality constraint on <code>x</code>.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>b_lb</code> <p>The inequality constraint vector. Each element represents a lower bound on the corresponding value of <code>A_lb @ x</code>.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>solver_options</code> <p>Keyword arguments that will be used to select a solver and to configure it. Refer to cvxpy's documentation for all possible options.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>src/pydvl/value/least_core/common.py</code> <pre><code>def _solve_egalitarian_least_core_quadratic_program(\n    subsidy: float,\n    A_eq: NDArray[np.float64],\n    b_eq: NDArray[np.float64],\n    A_lb: NDArray[np.float64],\n    b_lb: NDArray[np.float64],\n    solver_options: dict,\n) -&gt; Optional[NDArray[np.float64]]:\n    r\"\"\"Solves the egalitarian Least Core's quadratic program using cvxopt.\n\n    $$\n        \\text{minimize} \\ &amp; \\| x \\|_2 \\\\\n        \\mbox{such that} \\ &amp; A_{eq} x = b_{eq}, \\\\\n        &amp; A_{lb} x + e \\ge b_{lb},\\\\\n        &amp; A_{eq} x = b_{eq},\\\\\n        &amp; x in \\mathcal{R}^n , \\\\\n        &amp; e \\text{ is a constant.}\n    $$\n     where $x$ is a vector of decision variables; ,\n    $b_{ub}$, $b_{eq}$, $l$, and $u$ are vectors; and\n    $A_{ub}$ and $A_{eq}$ are matrices.\n\n    Args:\n        subsidy: Minimal subsidy returned by\n            [_solve_least_core_linear_program()][pydvl.value.least_core.common._solve_least_core_linear_program]\n        A_eq: The equality constraint matrix. Each row of `A_eq` specifies the\n            coefficients of a linear equality constraint on `x`.\n        b_eq: The equality constraint vector. Each element of `A_eq @ x` must equal\n            the corresponding element of `b_eq`.\n        A_lb: The inequality constraint matrix. Each row of `A_lb` specifies the\n            coefficients of a linear inequality constraint on `x`.\n        b_lb: The inequality constraint vector. Each element represents a\n            lower bound on the corresponding value of `A_lb @ x`.\n        solver_options: Keyword arguments that will be used to select a solver\n            and to configure it. Refer to [cvxpy's\n            documentation](https://www.cvxpy.org/tutorial/advanced/index.html#setting-solver-options)\n            for all possible options.\n    \"\"\"\n    logger.debug(f\"Solving quadratic program : {A_eq=}, {b_eq=}, {A_lb=}, {b_lb=}\")\n\n    n_variables = A_eq.shape[1]\n\n    x = cp.Variable(n_variables)\n\n    objective = cp.Minimize(cp.norm2(x))\n    constraints = [A_eq @ x == b_eq, (A_lb @ x + subsidy * np.ones(len(A_lb))) &gt;= b_lb]\n    problem = cp.Problem(objective, constraints)\n\n    try:\n        problem.solve(**solver_options)\n    except cp.error.SolverError as err:\n        raise ValueError(\"Could not solve quadratic program\") from err\n\n    if problem.status in cp.settings.SOLUTION_PRESENT:\n        logger.debug(\"Problem was solved\")\n        if problem.status == cp.settings.USER_LIMIT:\n            warnings.warn(\n                \"Solver terminated early. Consider increasing the solver's \"\n                \"maximum number of iterations in solver_options\",\n                RuntimeWarning,\n            )\n        return x.value  # type: ignore\n\n    if problem.status in cp.settings.INF_OR_UNB:\n        warnings.warn(\n            \"Could not find solution due to infeasibility or unboundedness of problem.\",\n            RuntimeWarning,\n        )\n    return None\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/common/#pydvl.value.least_core.common._solve_least_core_linear_program","title":"_solve_least_core_linear_program","text":"<pre><code>_solve_least_core_linear_program(\n    A_eq: NDArray[float64],\n    b_eq: NDArray[float64],\n    A_lb: NDArray[float64],\n    b_lb: NDArray[float64],\n    solver_options: dict,\n    non_negative_subsidy: bool = False,\n) -&gt; Tuple[Optional[NDArray[float64]], Optional[float]]\n</code></pre> <p>Solves the Least Core's linear program using cvxopt.</p> <p>$$     \\text{minimize} \\ &amp; e \\     \\mbox{such that} \\ &amp; A_{eq} x = b_{eq}, \\     &amp; A_{lb} x + e \\ge b_{lb},\\     &amp; A_{eq} x = b_{eq},\\     &amp; x in \\mathcal{R}^n , \\ $$  where \\(x\\) is a vector of decision variables; , \\(b_{ub}\\), \\(b_{eq}\\), \\(l\\), and \\(u\\) are vectors; and \\(A_{ub}\\) and \\(A_{eq}\\) are matrices.</p> <p>if <code>non_negative_subsidy</code> is True, then an additional constraint \\(e \\ge 0\\) is used.</p> PARAMETER DESCRIPTION <code>A_eq</code> <p>The equality constraint matrix. Each row of <code>A_eq</code> specifies the coefficients of a linear equality constraint on <code>x</code>.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>b_eq</code> <p>The equality constraint vector. Each element of <code>A_eq @ x</code> must equal the corresponding element of <code>b_eq</code>.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>A_lb</code> <p>The inequality constraint matrix. Each row of <code>A_lb</code> specifies the coefficients of a linear inequality constraint on <code>x</code>.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>b_lb</code> <p>The inequality constraint vector. Each element represents a lower bound on the corresponding value of <code>A_lb @ x</code>. non_negative_subsidy: If True, the least core subsidy \\(e\\) is constrained to be non-negative.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>options</code> <p>Keyword arguments that will be used to select a solver and to configure it. For all possible options, refer to cvxpy's documentation.</p> <p> </p> Source code in <code>src/pydvl/value/least_core/common.py</code> <pre><code>def _solve_least_core_linear_program(\n    A_eq: NDArray[np.float64],\n    b_eq: NDArray[np.float64],\n    A_lb: NDArray[np.float64],\n    b_lb: NDArray[np.float64],\n    solver_options: dict,\n    non_negative_subsidy: bool = False,\n) -&gt; Tuple[Optional[NDArray[np.float64]], Optional[float]]:\n    r\"\"\"Solves the Least Core's linear program using cvxopt.\n\n    $$\n        \\text{minimize} \\ &amp; e \\\\\n        \\mbox{such that} \\ &amp; A_{eq} x = b_{eq}, \\\\\n        &amp; A_{lb} x + e \\ge b_{lb},\\\\\n        &amp; A_{eq} x = b_{eq},\\\\\n        &amp; x in \\mathcal{R}^n , \\\\\n    $$\n     where $x$ is a vector of decision variables; ,\n    $b_{ub}$, $b_{eq}$, $l$, and $u$ are vectors; and\n    $A_{ub}$ and $A_{eq}$ are matrices.\n\n    if `non_negative_subsidy` is True, then an additional constraint $e \\ge 0$ is used.\n\n    Args:\n        A_eq: The equality constraint matrix. Each row of `A_eq` specifies the\n            coefficients of a linear equality constraint on `x`.\n        b_eq: The equality constraint vector. Each element of `A_eq @ x` must equal\n            the corresponding element of `b_eq`.\n        A_lb: The inequality constraint matrix. Each row of `A_lb` specifies the\n            coefficients of a linear inequality constraint on `x`.\n        b_lb: The inequality constraint vector. Each element represents a\n            lower bound on the corresponding value of `A_lb @ x`.\n            non_negative_subsidy: If True, the least core subsidy $e$ is constrained\n            to be non-negative.\n        options: Keyword arguments that will be used to select a solver\n            and to configure it. For all possible options, refer to [cvxpy's\n            documentation](https://www.cvxpy.org/tutorial/advanced/index.html#setting-solver-options).\n    \"\"\"\n    logger.debug(f\"Solving linear program : {A_eq=}, {b_eq=}, {A_lb=}, {b_lb=}\")\n\n    n_variables = A_eq.shape[1]\n\n    x = cp.Variable(n_variables)\n    e = cp.Variable()\n\n    objective = cp.Minimize(e)\n    constraints = [A_eq @ x == b_eq, (A_lb @ x + e * np.ones(len(A_lb))) &gt;= b_lb]\n\n    if non_negative_subsidy:\n        constraints += [e &gt;= 0]\n\n    problem = cp.Problem(objective, constraints)\n\n    try:\n        problem.solve(**solver_options)\n    except cp.error.SolverError as err:\n        raise ValueError(\"Could not solve linear program\") from err\n\n    if problem.status in cp.settings.SOLUTION_PRESENT:\n        logger.debug(\"Problem was solved\")\n        if problem.status == cp.settings.USER_LIMIT:\n            warnings.warn(\n                \"Solver terminated early. Consider increasing the solver's \"\n                \"maximum number of iterations in solver_options\",\n                RuntimeWarning,\n            )\n        subsidy = cast(NDArray[np.float64], e.value).item()\n        return x.value, subsidy\n\n    if problem.status in cp.settings.INF_OR_UNB:\n        warnings.warn(\n            \"Could not find solution due to infeasibility or unboundedness of problem.\",\n            RuntimeWarning,\n        )\n    return None, None\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/common/#pydvl.value.least_core.common.lc_solve_problem","title":"lc_solve_problem","text":"<pre><code>lc_solve_problem(\n    problem: LeastCoreProblem,\n    *,\n    u: Utility,\n    algorithm: str,\n    non_negative_subsidy: bool = False,\n    solver_options: Optional[dict] = None\n) -&gt; ValuationResult\n</code></pre> <p>Solves a linear problem as prepared by mclc_prepare_problem(). Useful for parallel execution of multiple experiments by running this as a remote task.</p> <p>See exact_least_core() or montecarlo_least_core() for argument descriptions.</p> Source code in <code>src/pydvl/value/least_core/common.py</code> <pre><code>def lc_solve_problem(\n    problem: LeastCoreProblem,\n    *,\n    u: Utility,\n    algorithm: str,\n    non_negative_subsidy: bool = False,\n    solver_options: Optional[dict] = None,\n) -&gt; ValuationResult:\n    \"\"\"Solves a linear problem as prepared by\n    [mclc_prepare_problem()][pydvl.value.least_core.montecarlo.mclc_prepare_problem].\n    Useful for parallel execution of multiple experiments by running this as a\n    remote task.\n\n    See [exact_least_core()][pydvl.value.least_core.naive.exact_least_core] or\n    [montecarlo_least_core()][pydvl.value.least_core.montecarlo.montecarlo_least_core] for\n    argument descriptions.\n    \"\"\"\n    n = len(u.data)\n\n    if np.any(np.isnan(problem.utility_values)):\n        warnings.warn(\n            f\"Calculation returned \"\n            f\"{np.sum(np.isnan(problem.utility_values))} NaN \"\n            f\"values out of {problem.utility_values.size}\",\n            RuntimeWarning,\n        )\n\n    if solver_options is None:\n        solver_options = {}\n    else:\n        solver_options = solver_options.copy()\n\n    if \"solver\" not in solver_options:\n        solver_options[\"solver\"] = cp.SCS\n\n    if \"max_iters\" not in solver_options and solver_options[\"solver\"] == cp.SCS:\n        solver_options[\"max_iters\"] = 10000\n\n    logger.debug(\"Removing possible duplicate values in lower bound array\")\n    b_lb = problem.utility_values\n    A_lb, unique_indices = np.unique(problem.A_lb, return_index=True, axis=0)\n    b_lb = b_lb[unique_indices]\n\n    logger.debug(\"Building equality constraint\")\n    A_eq = np.ones((1, n))\n    # We might have already computed the total utility one or more times.\n    # This is the index of the row(s) in A_lb with all ones.\n    total_utility_indices = np.where(A_lb.sum(axis=1) == n)[0]\n    if len(total_utility_indices) == 0:\n        b_eq = np.array([u(u.data.indices)])\n    else:\n        b_eq = b_lb[total_utility_indices]\n        # Remove the row(s) corresponding to the total utility\n        # from the lower bound constraints\n        # because given the equality constraint\n        # it is the same as using the constraint e &gt;= 0\n        # (i.e. setting non_negative_subsidy = True).\n        mask: NDArray[np.bool_] = np.ones_like(b_lb, dtype=bool)\n        mask[total_utility_indices] = False\n        b_lb = b_lb[mask]\n        A_lb = A_lb[mask]\n\n    # Remove the row(s) corresponding to the empty subset\n    # because, given u(\u2205) = (which is almost always the case,\n    # it is the same as using the constraint e &gt;= 0\n    # (i.e. setting non_negative_subsidy = True).\n    emptyset_utility_indices = np.where(A_lb.sum(axis=1) == 0)[0]\n    if len(emptyset_utility_indices) &gt; 0:\n        mask = np.ones_like(b_lb, dtype=bool)\n        mask[emptyset_utility_indices] = False\n        b_lb = b_lb[mask]\n        A_lb = A_lb[mask]\n\n    _, subsidy = _solve_least_core_linear_program(\n        A_eq=A_eq,\n        b_eq=b_eq,\n        A_lb=A_lb,\n        b_lb=b_lb,\n        non_negative_subsidy=non_negative_subsidy,\n        solver_options=solver_options,\n    )\n\n    values: Optional[NDArray[np.float64]]\n\n    if subsidy is None:\n        logger.debug(\"No values were found\")\n        status = Status.Failed\n        values = np.empty(n)\n        values[:] = np.nan\n        subsidy = np.nan\n    else:\n        values = _solve_egalitarian_least_core_quadratic_program(\n            subsidy,\n            A_eq=A_eq,\n            b_eq=b_eq,\n            A_lb=A_lb,\n            b_lb=b_lb,\n            solver_options=solver_options,\n        )\n\n        if values is None:\n            logger.debug(\"No values were found\")\n            status = Status.Failed\n            values = np.empty(n)\n            values[:] = np.nan\n            subsidy = np.nan\n        else:\n            status = Status.Converged\n\n    return ValuationResult(\n        algorithm=algorithm,\n        status=status,\n        values=values,\n        subsidy=subsidy,\n        stderr=None,\n        data_names=u.data.data_names,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/common/#pydvl.value.least_core.common.lc_solve_problems","title":"lc_solve_problems","text":"<pre><code>lc_solve_problems(\n    problems: Sequence[LeastCoreProblem],\n    u: Utility,\n    algorithm: str,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    n_jobs: int = 1,\n    non_negative_subsidy: bool = True,\n    solver_options: Optional[dict] = None,\n    **options\n) -&gt; List[ValuationResult]\n</code></pre> <p>Solves a list of linear problems in parallel.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility.</p> <p> TYPE: <code>Utility</code> </p> <code>problems</code> <p>Least Core problems to solve, as returned by mclc_prepare_problem().</p> <p> TYPE: <code>Sequence[LeastCoreProblem]</code> </p> <code>algorithm</code> <p>Name of the valuation algorithm.</p> <p> TYPE: <code>str</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to run.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>non_negative_subsidy</code> <p>If True, the least core subsidy \\(e\\) is constrained to be non-negative.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>solver_options</code> <p>Additional options to pass to the solver.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[ValuationResult]</code> <p>List of solutions.</p> Source code in <code>src/pydvl/value/least_core/common.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef lc_solve_problems(\n    problems: Sequence[LeastCoreProblem],\n    u: Utility,\n    algorithm: str,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    n_jobs: int = 1,\n    non_negative_subsidy: bool = True,\n    solver_options: Optional[dict] = None,\n    **options,\n) -&gt; List[ValuationResult]:\n    \"\"\"Solves a list of linear problems in parallel.\n\n    Args:\n        u: Utility.\n        problems: Least Core problems to solve, as returned by\n            [mclc_prepare_problem()][pydvl.value.least_core.montecarlo.mclc_prepare_problem].\n        algorithm: Name of the valuation algorithm.\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        n_jobs: Number of parallel jobs to run.\n        non_negative_subsidy: If True, the least core subsidy $e$ is constrained\n            to be non-negative.\n        solver_options: Additional options to pass to the solver.\n\n    Returns:\n        List of solutions.\n    \"\"\"\n\n    def _map_func(\n        problems: List[LeastCoreProblem], *args, **kwargs\n    ) -&gt; List[ValuationResult]:\n        return [lc_solve_problem(p, *args, **kwargs) for p in problems]\n\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n\n    map_reduce_job: MapReduceJob[\"LeastCoreProblem\", \"List[ValuationResult]\"] = (\n        MapReduceJob(\n            inputs=problems,\n            map_func=_map_func,\n            map_kwargs=dict(\n                u=u,\n                algorithm=algorithm,\n                non_negative_subsidy=non_negative_subsidy,\n                solver_options=solver_options,\n                **options,\n            ),\n            reduce_func=lambda x: list(itertools.chain(*x)),\n            parallel_backend=parallel_backend,\n            n_jobs=n_jobs,\n        )\n    )\n    solutions = map_reduce_job()\n\n    return solutions\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/montecarlo/","title":"Montecarlo","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/least_core/montecarlo/#pydvl.value.least_core.montecarlo","title":"pydvl.value.least_core.montecarlo","text":""},{"location":"deprecated/pydvl/value/least_core/montecarlo/#pydvl.value.least_core.montecarlo._montecarlo_least_core","title":"_montecarlo_least_core","text":"<pre><code>_montecarlo_least_core(\n    u: Utility,\n    n_iterations: int,\n    *,\n    progress: bool = False,\n    job_id: int = 1,\n    seed: Optional[Seed] = None\n) -&gt; LeastCoreProblem\n</code></pre> <p>Computes utility values and the Least Core upper bound matrix for a given number of iterations.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>n_iterations</code> <p>total number of iterations to use</p> <p> TYPE: <code>int</code> </p> <code>progress</code> <p>If True, shows a tqdm progress bar</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>job_id</code> <p>Integer id used to determine the position of the progress bar</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LeastCoreProblem</code> <p>A solution</p> Source code in <code>src/pydvl/value/least_core/montecarlo.py</code> <pre><code>def _montecarlo_least_core(\n    u: Utility,\n    n_iterations: int,\n    *,\n    progress: bool = False,\n    job_id: int = 1,\n    seed: Optional[Seed] = None,\n) -&gt; LeastCoreProblem:\n    \"\"\"Computes utility values and the Least Core upper bound matrix for a given\n    number of iterations.\n\n    Args:\n        u: Utility object with model, data, and scoring function\n        n_iterations: total number of iterations to use\n        progress: If True, shows a tqdm progress bar\n        job_id: Integer id used to determine the position of the progress bar\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        A solution\n    \"\"\"\n    n = len(u.data)\n\n    utility_values = np.zeros(n_iterations)\n\n    # Randomly sample subsets of full dataset\n    rng = np.random.default_rng(seed)\n    power_set = random_powerset(u.data.indices, n_samples=n_iterations, seed=rng)\n\n    A_lb = np.zeros((n_iterations, n))\n\n    for i, subset in enumerate(\n        tqdm(power_set, disable=not progress, total=n_iterations, position=job_id)\n    ):\n        indices: NDArray[np.bool_] = np.zeros(n, dtype=bool)\n        indices[list(subset)] = True\n        A_lb[i, indices] = 1\n        utility_values[i] = u(subset)\n\n    return LeastCoreProblem(utility_values, A_lb)\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/montecarlo/#pydvl.value.least_core.montecarlo._reduce_func","title":"_reduce_func","text":"<pre><code>_reduce_func(results: Iterable[LeastCoreProblem]) -&gt; LeastCoreProblem\n</code></pre> <p>Combines the results from different parallel runs of _montecarlo_least_core()</p> Source code in <code>src/pydvl/value/least_core/montecarlo.py</code> <pre><code>def _reduce_func(results: Iterable[LeastCoreProblem]) -&gt; LeastCoreProblem:\n    \"\"\"Combines the results from different parallel runs of\n    [_montecarlo_least_core()][pydvl.value.least_core.montecarlo._montecarlo_least_core]\n    \"\"\"\n    utility_values_list, A_lb_list = zip(*results)\n    utility_values = np.concatenate(utility_values_list)\n    A_lb = np.concatenate(A_lb_list)\n    return LeastCoreProblem(utility_values, A_lb)\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/montecarlo/#pydvl.value.least_core.montecarlo.mclc_prepare_problem","title":"mclc_prepare_problem","text":"<pre><code>mclc_prepare_problem(\n    u: Utility,\n    n_iterations: int,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None\n) -&gt; LeastCoreProblem\n</code></pre> <p>Prepares a linear problem by sampling subsets of the data. Use this to separate the problem preparation from the solving with lc_solve_problem(). Useful for parallel execution of multiple experiments.</p> <p>See montecarlo_least_core for argument descriptions.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/least_core/montecarlo.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef mclc_prepare_problem(\n    u: Utility,\n    n_iterations: int,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n) -&gt; LeastCoreProblem:\n    \"\"\"Prepares a linear problem by sampling subsets of the data. Use this to\n    separate the problem preparation from the solving with\n    [lc_solve_problem()][pydvl.value.least_core.common.lc_solve_problem]. Useful\n    for parallel execution of multiple experiments.\n\n    See\n    [montecarlo_least_core][pydvl.value.least_core.montecarlo.montecarlo_least_core]\n    for argument descriptions.\n\n    !!! note \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n    n = len(u.data)\n\n    if n_iterations &lt; n:\n        warnings.warn(\n            f\"Number of iterations '{n_iterations}' is smaller the size of the dataset '{n}'. \"\n            f\"This is not optimal because in the worst case we need at least '{n}' constraints \"\n            \"to satisfy the individual rationality condition.\"\n        )\n\n    if n_iterations &gt; 2**n:\n        warnings.warn(\n            f\"Passed n_iterations is greater than the number subsets! \"\n            f\"Setting it to 2^{n}\",\n            RuntimeWarning,\n        )\n        n_iterations = 2**n\n\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n\n    iterations_per_job = max(\n        1, n_iterations // parallel_backend.effective_n_jobs(n_jobs)\n    )\n\n    map_reduce_job: MapReduceJob[\"Utility\", \"LeastCoreProblem\"] = MapReduceJob(\n        inputs=u,\n        map_func=_montecarlo_least_core,\n        reduce_func=_reduce_func,\n        map_kwargs=dict(n_iterations=iterations_per_job, progress=progress),\n        n_jobs=n_jobs,\n        parallel_backend=parallel_backend,\n    )\n\n    return map_reduce_job(seed=seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/montecarlo/#pydvl.value.least_core.montecarlo.montecarlo_least_core","title":"montecarlo_least_core","text":"<pre><code>montecarlo_least_core(\n    u: Utility,\n    n_iterations: int,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    non_negative_subsidy: bool = False,\n    solver_options: Optional[dict] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>Computes approximate Least Core values using a Monte Carlo approach.</p> \\[ \\begin{array}{lll} \\text{minimize} &amp; \\displaystyle{e} &amp; \\\\ \\text{subject to} &amp; \\displaystyle\\sum_{i\\in N} x_{i} = v(N) &amp; \\\\ &amp; \\displaystyle\\sum_{i\\in S} x_{i} + e \\geq v(S) &amp; , \\forall S \\in \\{S_1, S_2, \\dots, S_m \\overset{\\mathrm{iid}}{\\sim} U(2^N) \\} \\end{array} \\] <p>Where:</p> <ul> <li>\\(U(2^N)\\) is the uniform distribution over the powerset of \\(N\\).</li> <li>\\(m\\) is the number of subsets that will be sampled and whose utility will   be computed and used to compute the data values.</li> </ul> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>n_iterations</code> <p>total number of iterations to use</p> <p> TYPE: <code>int</code> </p> <code>n_jobs</code> <p>number of jobs across which to distribute the computation</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>non_negative_subsidy</code> <p>If True, the least core subsidy \\(e\\) is constrained to be non-negative.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>solver_options</code> <p>Dictionary of options that will be used to select a solver and to configure it. Refer to cvxpy's documentation for all possible options.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, shows a tqdm progress bar</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values and the least core value.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/least_core/montecarlo.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef montecarlo_least_core(\n    u: Utility,\n    n_iterations: int,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    non_negative_subsidy: bool = False,\n    solver_options: Optional[dict] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    r\"\"\"Computes approximate Least Core values using a Monte Carlo approach.\n\n    $$\n    \\begin{array}{lll}\n    \\text{minimize} &amp; \\displaystyle{e} &amp; \\\\\n    \\text{subject to} &amp; \\displaystyle\\sum_{i\\in N} x_{i} = v(N) &amp; \\\\\n    &amp; \\displaystyle\\sum_{i\\in S} x_{i} + e \\geq v(S) &amp; ,\n    \\forall S \\in \\{S_1, S_2, \\dots, S_m \\overset{\\mathrm{iid}}{\\sim} U(2^N) \\}\n    \\end{array}\n    $$\n\n    Where:\n\n    * $U(2^N)$ is the uniform distribution over the powerset of $N$.\n    * $m$ is the number of subsets that will be sampled and whose utility will\n      be computed and used to compute the data values.\n\n    Args:\n        u: Utility object with model, data, and scoring function\n        n_iterations: total number of iterations to use\n        n_jobs: number of jobs across which to distribute the computation\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        non_negative_subsidy: If True, the least core subsidy $e$ is constrained\n            to be non-negative.\n        solver_options: Dictionary of options that will be used to select a solver\n            and to configure it. Refer to [cvxpy's\n            documentation](https://www.cvxpy.org/tutorial/advanced/index.html#setting-solver-options)\n            for all possible options.\n        progress: If True, shows a tqdm progress bar\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        Object with the data values and the least core value.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n    problem = mclc_prepare_problem(\n        u,\n        n_iterations,\n        n_jobs=n_jobs,\n        parallel_backend=parallel_backend,\n        config=config,\n        progress=progress,\n        seed=seed,\n    )\n    return lc_solve_problem(\n        problem,\n        u=u,\n        algorithm=\"montecarlo_least_core\",\n        non_negative_subsidy=non_negative_subsidy,\n        solver_options=solver_options,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/naive/","title":"Naive","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/least_core/naive/#pydvl.value.least_core.naive","title":"pydvl.value.least_core.naive","text":""},{"location":"deprecated/pydvl/value/least_core/naive/#pydvl.value.least_core.naive.exact_least_core","title":"exact_least_core","text":"<pre><code>exact_least_core(\n    u: Utility,\n    *,\n    non_negative_subsidy: bool = False,\n    solver_options: Optional[dict] = None,\n    progress: bool = True\n) -&gt; ValuationResult\n</code></pre> <p>Computes the exact Least Core values.</p> <p>Note</p> <p>If the training set contains more than 20 instances a warning is printed because the computation is very expensive. This method is mostly used for internal testing and simple use cases. Please refer to the Monte Carlo method for practical applications.</p> <p>The least core is the solution to the following Linear Programming problem:</p> \\[ \\begin{array}{lll} \\text{minimize} &amp; \\displaystyle{e} &amp; \\\\ \\text{subject to} &amp; \\displaystyle\\sum_{i\\in N} x_{i} = v(N) &amp; \\\\ &amp; \\displaystyle\\sum_{i\\in S} x_{i} + e \\geq v(S) &amp;, \\forall S \\subseteq N \\\\ \\end{array} \\] <p>Where \\(N = \\{1, 2, \\dots, n\\}\\) are the training set's indices.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>non_negative_subsidy</code> <p>If True, the least core subsidy \\(e\\) is constrained to be non-negative.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>solver_options</code> <p>Dictionary of options that will be used to select a solver and to configure it. Refer to the cvxpy's documentation for all possible options.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, shows a tqdm progress bar</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values and the least core value.</p> Source code in <code>src/pydvl/value/least_core/naive.py</code> <pre><code>def exact_least_core(\n    u: Utility,\n    *,\n    non_negative_subsidy: bool = False,\n    solver_options: Optional[dict] = None,\n    progress: bool = True,\n) -&gt; ValuationResult:\n    r\"\"\"Computes the exact Least Core values.\n\n    !!! Note\n        If the training set contains more than 20 instances a warning is printed\n        because the computation is very expensive. This method is mostly used for\n        internal testing and simple use cases. Please refer to the\n        [Monte Carlo method][pydvl.value.least_core.montecarlo.montecarlo_least_core]\n        for practical applications.\n\n    The least core is the solution to the following Linear Programming problem:\n\n    $$\n    \\begin{array}{lll}\n    \\text{minimize} &amp; \\displaystyle{e} &amp; \\\\\n    \\text{subject to} &amp; \\displaystyle\\sum_{i\\in N} x_{i} = v(N) &amp; \\\\\n    &amp; \\displaystyle\\sum_{i\\in S} x_{i} + e \\geq v(S) &amp;, \\forall S \\subseteq N \\\\\n    \\end{array}\n    $$\n\n    Where $N = \\{1, 2, \\dots, n\\}$ are the training set's indices.\n\n    Args:\n        u: Utility object with model, data, and scoring function\n        non_negative_subsidy: If True, the least core subsidy $e$ is constrained\n            to be non-negative.\n        solver_options: Dictionary of options that will be used to select a solver\n            and to configure it. Refer to the [cvxpy's\n            documentation](https://www.cvxpy.org/tutorial/advanced/index.html#setting-solver-options)\n            for all possible options.\n        progress: If True, shows a tqdm progress bar\n\n    Returns:\n        Object with the data values and the least core value.\n    \"\"\"\n    n = len(u.data)\n    if n &gt; 20:  # Arbitrary choice, will depend on time required, caching, etc.\n        warnings.warn(f\"Large dataset! Computation requires 2^{n} calls to model.fit()\")\n\n    problem = lc_prepare_problem(u, progress=progress)\n    return lc_solve_problem(\n        problem=problem,\n        u=u,\n        algorithm=\"exact_least_core\",\n        non_negative_subsidy=non_negative_subsidy,\n        solver_options=solver_options,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/least_core/naive/#pydvl.value.least_core.naive.lc_prepare_problem","title":"lc_prepare_problem","text":"<pre><code>lc_prepare_problem(u: Utility, progress: bool = False) -&gt; LeastCoreProblem\n</code></pre> <p>Prepares a linear problem with all subsets of the data Use this to separate the problem preparation from the solving with lc_solve_problem(). Useful for parallel execution of multiple experiments.</p> <p>See exact_least_core() for argument descriptions.</p> Source code in <code>src/pydvl/value/least_core/naive.py</code> <pre><code>def lc_prepare_problem(u: Utility, progress: bool = False) -&gt; LeastCoreProblem:\n    \"\"\"Prepares a linear problem with all subsets of the data\n    Use this to separate the problem preparation from the solving with\n    [lc_solve_problem()][pydvl.value.least_core.common.lc_solve_problem]. Useful for\n    parallel execution of multiple experiments.\n\n    See [exact_least_core()][pydvl.value.least_core.naive.exact_least_core] for argument\n    descriptions.\n    \"\"\"\n    n = len(u.data)\n\n    logger.debug(\"Building vectors and matrices for linear programming problem\")\n    powerset_size = 2**n\n    A_lb = np.zeros((powerset_size, n))\n\n    logger.debug(\"Iterating over all subsets\")\n    utility_values = np.zeros(powerset_size)\n    for i, subset in enumerate(  # type: ignore\n        tqdm(\n            powerset(u.data.indices),\n            disable=not progress,\n            total=powerset_size - 1,\n            position=0,\n        )\n    ):\n        indices: NDArray[np.bool_] = np.zeros(n, dtype=bool)\n        indices[list(subset)] = True\n        A_lb[i, indices] = 1\n        utility_values[i] = u(subset)  # type: ignore\n\n    return LeastCoreProblem(utility_values, A_lb)\n</code></pre>"},{"location":"deprecated/pydvl/value/loo/","title":"Loo","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/loo/#pydvl.value.loo","title":"pydvl.value.loo","text":""},{"location":"deprecated/pydvl/value/loo/loo/","title":"Loo","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/loo/loo/#pydvl.value.loo.loo","title":"pydvl.value.loo.loo","text":""},{"location":"deprecated/pydvl/value/loo/loo/#pydvl.value.loo.loo.compute_loo","title":"compute_loo","text":"<pre><code>compute_loo(\n    u: Utility,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = True\n) -&gt; ValuationResult\n</code></pre> <p>Computes leave one out value:</p> \\[v(i) = u(D) - u(D \\setminus \\{i\\}) \\] PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>progress</code> <p>If True, display a progress bar</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to use</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, display a progress bar</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values.</p> <p>New in version 0.7.0</p> <p>Renamed from <code>naive_loo</code> and added parallel computation.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/loo/loo.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef compute_loo(\n    u: Utility,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = True,\n) -&gt; ValuationResult:\n    r\"\"\"Computes leave one out value:\n\n    $$v(i) = u(D) - u(D \\setminus \\{i\\}) $$\n\n    Args:\n        u: Utility object with model, data, and scoring function\n        progress: If True, display a progress bar\n        n_jobs: Number of parallel jobs to use\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        progress: If True, display a progress bar\n\n    Returns:\n        Object with the data values.\n\n    !!! tip \"New in version 0.7.0\"\n        Renamed from `naive_loo` and added parallel computation.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n    if len(u.data) &lt; 3:\n        raise ValueError(\"Dataset must have at least 2 elements\")\n\n    result = ValuationResult.zeros(\n        algorithm=\"loo\",\n        indices=u.data.indices,\n        data_names=u.data.data_names,\n    )\n\n    all_indices = set(u.data.indices)\n    total_utility = u(u.data.indices)\n\n    def fun(idx: int) -&gt; tuple[int, float]:\n        return idx, total_utility - u(all_indices.difference({idx}))\n\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n    max_workers = parallel_backend.effective_n_jobs(n_jobs)\n    n_submitted_jobs = 2 * max_workers  # number of jobs in the queue\n\n    # NOTE: this could be done with a simple executor.map(), but we want to\n    # display a progress bar\n\n    with parallel_backend.executor(\n        max_workers=max_workers, cancel_futures=True\n    ) as executor:\n        pending: set[Future] = set()\n        index_it = iter(u.data.indices)\n\n        pbar = tqdm(disable=not progress, total=100, unit=\"%\")\n        while True:\n            pbar.n = 100 * sum(result.counts) / len(u.data)\n            pbar.refresh()\n            completed, pending = wait(pending, timeout=0.1, return_when=FIRST_COMPLETED)\n            for future in completed:\n                idx, marginal = future.result()\n                result.update(idx, marginal)\n\n            # Ensure that we always have n_submitted_jobs running\n            try:\n                for _ in range(n_submitted_jobs - len(pending)):\n                    pending.add(executor.submit(fun, next(index_it)))\n            except StopIteration:\n                if len(pending) == 0:\n                    return result\n</code></pre>"},{"location":"deprecated/pydvl/value/oob/","title":"Oob","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/oob/#pydvl.value.oob","title":"pydvl.value.oob","text":""},{"location":"deprecated/pydvl/value/oob/oob/","title":"Oob","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/oob/oob/#pydvl.value.oob.oob","title":"pydvl.value.oob.oob","text":""},{"location":"deprecated/pydvl/value/oob/oob/#pydvl.value.oob.oob--references","title":"References","text":"<ol> <li> <p>Kwon et al. Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value. In: Published at ICML 2023\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/oob/oob/#pydvl.value.oob.oob.compute_data_oob","title":"compute_data_oob","text":"<pre><code>compute_data_oob(\n    u: Utility,\n    *,\n    n_est: int = 10,\n    max_samples: float = 0.8,\n    loss: Optional[PointwiseScore] = None,\n    n_jobs: Optional[int] = None,\n    seed: Optional[Seed] = None,\n    progress: bool = False\n) -&gt; ValuationResult\n</code></pre> <p>Computes Data out of bag values</p> <p>This implements the method described in (Kwon and Zou, 2023)<sup>1</sup>. It fits several base estimators provided through u.model through a bagging process. The point value corresponds to the average loss of estimators which were not fit on it.</p> <p>\\(w_{bj}\\in Z\\) is the number of times the j-th datum \\((x_j, y_j)\\) is selected in the b-th bootstrap dataset.</p> \\[\\psi((x_i,y_i),\\Theta_B):=\\frac{\\sum_{b=1}^{B}\\mathbb{1}(w_{bi}=0)T(y_i, \\hat{f}_b(x_i))}{\\sum_{b=1}^{B} \\mathbb{1} (w_{bi}=0)}\\] <p>With:</p> \\[ T: Y \\times Y \\rightarrow \\mathbb{R} \\] <p>T is a score function that represents the goodness of a weak learner \\(\\hat{f}_b\\) at the i-th datum \\((x_i, y_i)\\).</p> <p><code>n_est</code> and <code>max_samples</code> must be tuned jointly to ensure that all samples are at least 1 time out-of-bag, otherwise the result could include a NaN value for that datum.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>n_est</code> <p>Number of estimator used in the bagging procedure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>max_samples</code> <p>The fraction of samples to draw to train each base estimator.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>loss</code> <p>A function taking as parameters model prediction and corresponding data labels(y_true, y_pred) and returning an array of point-wise errors.</p> <p> TYPE: <code>Optional[PointwiseScore]</code> DEFAULT: <code>None</code> </p> <code>n_jobs</code> <p>The number of jobs to run in parallel used in the bagging procedure for both fit and predict.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>If True, display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values.</p> Source code in <code>src/pydvl/value/oob/oob.py</code> <pre><code>def compute_data_oob(\n    u: Utility,\n    *,\n    n_est: int = 10,\n    max_samples: float = 0.8,\n    loss: Optional[PointwiseScore] = None,\n    n_jobs: Optional[int] = None,\n    seed: Optional[Seed] = None,\n    progress: bool = False,\n) -&gt; ValuationResult:\n    r\"\"\"Computes Data out of bag values\n\n    This implements the method described in\n    (Kwon and Zou, 2023)&lt;sup&gt;&lt;a href=\"#kwon_dataoob_2023\"&gt;1&lt;/a&gt;&lt;/sup&gt;.\n    It fits several base estimators provided through u.model through a bagging\n    process. The point value corresponds to the average loss of estimators which\n    were not fit on it.\n\n    $w_{bj}\\in Z$ is the number of times the j-th datum $(x_j, y_j)$ is selected\n    in the b-th bootstrap dataset.\n\n    $$\\psi((x_i,y_i),\\Theta_B):=\\frac{\\sum_{b=1}^{B}\\mathbb{1}(w_{bi}=0)T(y_i,\n    \\hat{f}_b(x_i))}{\\sum_{b=1}^{B}\n    \\mathbb{1}\n    (w_{bi}=0)}$$\n\n    With:\n\n    $$\n    T: Y \\times Y\n    \\rightarrow \\mathbb{R}\n    $$\n\n    T is a score function that represents the goodness of a weak learner\n    $\\hat{f}_b$ at the i-th datum $(x_i, y_i)$.\n\n    `n_est` and `max_samples` must be tuned jointly to ensure that all samples\n    are at least 1 time out-of-bag, otherwise the result could include a NaN\n    value for that datum.\n\n    Args:\n        u: Utility object with model, data, and scoring function.\n        n_est: Number of estimator used in the bagging procedure.\n        max_samples: The fraction of samples to draw to train each base\n            estimator.\n        loss: A function taking as parameters model prediction and corresponding\n            data labels(y_true, y_pred) and returning an array of point-wise errors.\n        n_jobs: The number of jobs to run in parallel used in the bagging\n            procedure for both fit and predict.\n        seed: Either an instance of a numpy random number generator or a seed\n            for it.\n        progress: If True, display a progress bar.\n\n    Returns:\n        Object with the data values.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    random_state = np.random.RandomState(rng.bit_generator)\n\n    result: ValuationResult[np.int_, np.object_] = ValuationResult.empty(\n        algorithm=\"data_oob\", indices=u.data.indices, data_names=u.data.data_names\n    )\n\n    if is_classifier(u.model):\n        bag = BaggingClassifier(\n            u.model,\n            n_estimators=n_est,\n            max_samples=max_samples,\n            n_jobs=n_jobs,\n            random_state=random_state,\n        )\n        if loss is None:\n            loss = point_wise_accuracy\n    elif is_regressor(u.model):\n        bag = BaggingRegressor(\n            u.model,\n            n_estimators=n_est,\n            max_samples=max_samples,\n            n_jobs=n_jobs,\n            random_state=random_state,\n        )\n        if loss is None:\n            loss = neg_l2_distance\n    else:\n        raise Exception(\n            \"Model has to be a classifier or a regressor in sklearn format.\"\n        )\n\n    bag.fit(u.data.x_train, u.data.y_train)\n\n    for est, samples in tqdm(\n        zip(bag.estimators_, bag.estimators_samples_), disable=not progress, total=n_est\n    ):  # The bottleneck is the bag fitting not this part so TQDM is not very useful here\n        oob_idx = np.setxor1d(u.data.indices, np.unique(samples))\n        array_loss = loss(\n            y_true=u.data.y_train[oob_idx],\n            y_pred=est.predict(u.data.x_train[oob_idx]),\n        )\n        result += ValuationResult(\n            algorithm=\"data_oob\",\n            indices=oob_idx,\n            values=array_loss,\n            counts=np.ones_like(array_loss, dtype=u.data.indices.dtype),\n        )\n    return result\n</code></pre>"},{"location":"deprecated/pydvl/value/oob/oob/#pydvl.value.oob.oob.neg_l2_distance","title":"neg_l2_distance","text":"<pre><code>neg_l2_distance(y_true: NDArray[T], y_pred: NDArray[T]) -&gt; NDArray[T]\n</code></pre> <p>Point-wise negative \\(l_2\\) distance between two arrays</p> PARAMETER DESCRIPTION <code>y_true</code> <p>Array of true values (e.g. labels)</p> <p> TYPE: <code>NDArray[T]</code> </p> <code>y_pred</code> <p>Array of estimated values (e.g. model predictions)</p> <p> TYPE: <code>NDArray[T]</code> </p> RETURNS DESCRIPTION <code>NDArray[T]</code> <p>Array with point-wise negative \\(l_2\\) distances between labels and model</p> <code>NDArray[T]</code> <p>predictions</p> Source code in <code>src/pydvl/value/oob/oob.py</code> <pre><code>def neg_l2_distance(y_true: NDArray[T], y_pred: NDArray[T]) -&gt; NDArray[T]:\n    r\"\"\"Point-wise negative $l_2$ distance between two arrays\n\n    Args:\n        y_true: Array of true values (e.g. labels)\n        y_pred: Array of estimated values (e.g. model predictions)\n\n    Returns:\n        Array with point-wise negative $l_2$ distances between labels and model\n        predictions\n    \"\"\"\n    return -np.square(np.array(y_pred - y_true), dtype=y_pred.dtype)\n</code></pre>"},{"location":"deprecated/pydvl/value/oob/oob/#pydvl.value.oob.oob.point_wise_accuracy","title":"point_wise_accuracy","text":"<pre><code>point_wise_accuracy(y_true: NDArray[T], y_pred: NDArray[T]) -&gt; NDArray[T]\n</code></pre> <p>Point-wise 0-1 loss between two arrays</p> PARAMETER DESCRIPTION <code>y_true</code> <p>Array of true values (e.g. labels)</p> <p> TYPE: <code>NDArray[T]</code> </p> <code>y_pred</code> <p>Array of estimated values (e.g. model predictions)</p> <p> TYPE: <code>NDArray[T]</code> </p> RETURNS DESCRIPTION <code>NDArray[T]</code> <p>Array with point-wise 0-1 losses between labels and model predictions</p> Source code in <code>src/pydvl/value/oob/oob.py</code> <pre><code>def point_wise_accuracy(y_true: NDArray[T], y_pred: NDArray[T]) -&gt; NDArray[T]:\n    r\"\"\"Point-wise 0-1 loss between two arrays\n\n    Args:\n        y_true: Array of true values (e.g. labels)\n        y_pred: Array of estimated values (e.g. model predictions)\n\n    Returns:\n        Array with point-wise 0-1 losses between labels and model predictions\n    \"\"\"\n    return np.array(y_pred == y_true, dtype=y_pred.dtype)\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/","title":"Shapley","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/shapley/#pydvl.value.shapley","title":"pydvl.value.shapley","text":"<p>This package holds all routines for the computation of Shapley Data value. Users will want to use compute_shapley_values or compute_semivalues as interfaces to most methods defined in the modules.</p> <p>Please refer to the guide on data valuation for an overview of all methods.</p>"},{"location":"deprecated/pydvl/value/shapley/classwise/","title":"Classwise","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/shapley/classwise/#pydvl.value.shapley.classwise","title":"pydvl.value.shapley.classwise","text":"<p>Class-wise Shapley (Schoch et al., 2022)<sup>1</sup> offers a Shapley framework tailored for classification problems. Let \\(D\\) be a dataset, \\(D_{y_i}\\) be the subset of \\(D\\) with labels \\(y_i\\), and \\(D_{-y_i}\\) be the complement of \\(D_{y_i}\\) in \\(D\\). The key idea is that a sample \\((x_i, y_i)\\), might enhance the overall performance on \\(D\\), while being detrimental for the performance on \\(D_{y_i}\\). The Class-wise value is defined as:</p> \\[ v_u(i) = \\frac{1}{2^{|D_{-y_i}|}} \\sum_{S_{-y_i}} \\frac{1}{|D_{y_i}|!} \\sum_{S_{y_i}} \\binom{|D_{y_i}|-1}{|S_{y_i}|}^{-1} [u( S_{y_i} \\cup \\{i\\} | S_{-y_i} ) \u2212 u( S_{y_i} | S_{-y_i})], \\] <p>where \\(S_{y_i} \\subseteq D_{y_i} \\setminus \\{i\\}\\) and \\(S_{-y_i} \\subseteq D_{-y_i}\\).</p> <p>Analysis of Class-wise Shapley</p> <p>For a detailed analysis of the method, with comparison to other valuation techniques, please refer to the main documentation.</p> <p>In practice, the quantity above is estimated using Monte Carlo sampling of the powerset and the set of index permutations. This results in the estimator</p> \\[ v_u(i) = \\frac{1}{K} \\sum_k \\frac{1}{L} \\sum_l [u(\\sigma^{(l)}_{:i} \\cup \\{i\\} | S^{(k)} ) \u2212 u( \\sigma^{(l)}_{:i} | S^{(k)})], \\] <p>with \\(S^{(1)}, \\dots, S^{(K)} \\subseteq T_{-y_i},\\) \\(\\sigma^{(1)}, \\dots, \\sigma^{(L)} \\in \\Pi(T_{y_i}\\setminus\\{i\\}),\\) and \\(\\sigma^{(l)}_{:i}\\) denoting the set of indices in permutation \\(\\sigma^{(l)}\\) before the position where \\(i\\) appears. The sets \\(T_{y_i}\\) and \\(T_{-y_i}\\) are the training sets for the labels \\(y_i\\) and \\(-y_i\\), respectively.</p> Notes for derivation of test cases <p>The unit tests include the following manually constructed data: Let \\(D=\\{(1,0),(2,0),(3,0),(4,1)\\}\\) be the test set and \\(T=\\{(1,0),(2,0),(3,1),(4,1)\\}\\) the train set. This specific dataset is chosen as it allows to solve the model</p> \\[y = \\max(0, \\min(1, \\text{round}(\\beta^T x)))\\] <p>in closed form \\(\\beta = \\frac{\\text{dot}(x, y)}{\\text{dot}(x, x)}\\). From the closed-form solution, the tables for in-class accuracy \\(a_S(D_{y_i})\\) and out-of-class accuracy \\(a_S(D_{-y_i})\\) can be calculated. By using these tables and setting \\(\\{S^{(1)}, \\dots, S^{(K)}\\} = 2^{T_{-y_i}}\\) and \\(\\{\\sigma^{(1)}, \\dots, \\sigma^{(L)}\\} = \\Pi(T_{y_i}\\setminus\\{i\\})\\), the Monte Carlo estimator can be evaluated (\\(2^M\\) is the powerset of \\(M\\)). The details of the derivation are left to the eager reader.</p>"},{"location":"deprecated/pydvl/value/shapley/classwise/#pydvl.value.shapley.classwise--references","title":"References","text":"<ol> <li> <p>Schoch, Stephanie, Haifeng Xu, and Yangfeng Ji. CS-Shapley: Class-wise Shapley Values for Data Valuation in Classification. In Proc. of the Thirty-Sixth Conference on Neural Information Processing Systems (NeurIPS). New Orleans, Louisiana, USA, 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/shapley/classwise/#pydvl.value.shapley.classwise.ClasswiseScorer","title":"ClasswiseScorer","text":"<pre><code>ClasswiseScorer(\n    scoring: Union[str, ScorerCallable] = \"accuracy\",\n    default: float = 0.0,\n    range: Tuple[float, float] = (0, 1),\n    in_class_discount_fn: Callable[[float], float] = lambda x: x,\n    out_of_class_discount_fn: Callable[[float], float] = exp,\n    initial_label: Optional[int] = None,\n    name: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>Scorer</code></p> <p>A Scorer designed for evaluation in classification problems. Its value is computed from an in-class and an out-of-class \"inner score\" (Schoch et al., 2022) <sup>1</sup>. Let \\(S\\) be the training set and \\(D\\) be the valuation set. For each label \\(c\\), \\(D\\) is factorized into two disjoint sets: \\(D_c\\) for in-class instances and \\(D_{-c}\\) for out-of-class instances. The score combines an in-class metric of performance, adjusted by a discounted out-of-class metric. These inner scores must be provided upon construction or default to accuracy. They are combined into:</p> \\[ u(S_{y_i}) = f(a_S(D_{y_i}))\\ g(a_S(D_{-y_i})), \\] <p>where \\(f\\) and \\(g\\) are continuous, monotonic functions. For a detailed explanation, refer to section four of (Schoch et al., 2022)<sup> 1</sup>.</p> <p>Warning</p> <p>Metrics must support multiple class labels if you intend to apply them to a multi-class problem. For instance, the metric 'accuracy' supports multiple classes, but the metric <code>f1</code> does not. For a two-class classification problem, using <code>f1_weighted</code> is essentially equivalent to using <code>accuracy</code>.</p> PARAMETER DESCRIPTION <code>scoring</code> <p>Name of the scoring function or a callable that can be passed to Scorer.</p> <p> TYPE: <code>Union[str, ScorerCallable]</code> DEFAULT: <code>'accuracy'</code> </p> <code>default</code> <p>Score to use when a model fails to provide a number, e.g. when too little was used to train it, or errors arise.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>range</code> <p>Numerical range of the score function. Some Monte Carlo methods can use this to estimate the number of samples required for a certain quality of approximation. If not provided, it can be read from the <code>scoring</code> object if it provides it, for instance if it was constructed with compose_score.</p> <p> TYPE: <code>Tuple[float, float]</code> DEFAULT: <code>(0, 1)</code> </p> <code>in_class_discount_fn</code> <p>Continuous, monotonic increasing function used to discount the in-class score.</p> <p> TYPE: <code>Callable[[float], float]</code> DEFAULT: <code>lambda x: x</code> </p> <code>out_of_class_discount_fn</code> <p>Continuous, monotonic increasing function used to discount the out-of-class score.</p> <p> TYPE: <code>Callable[[float], float]</code> DEFAULT: <code>exp</code> </p> <code>initial_label</code> <p>Set initial label (for the first iteration)</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the scorer. If not provided, the name of the inner scoring function will be prefixed by <code>classwise</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <p>New in version 0.7.1</p> Source code in <code>src/pydvl/value/shapley/classwise.py</code> <pre><code>def __init__(\n    self,\n    scoring: Union[str, ScorerCallable] = \"accuracy\",\n    default: float = 0.0,\n    range: Tuple[float, float] = (0, 1),\n    in_class_discount_fn: Callable[[float], float] = lambda x: x,\n    out_of_class_discount_fn: Callable[[float], float] = np.exp,\n    initial_label: Optional[int] = None,\n    name: Optional[str] = None,\n):\n    disc_score_in_class = in_class_discount_fn(range[1])\n    disc_score_out_of_class = out_of_class_discount_fn(range[1])\n    transformed_range = (0, disc_score_in_class * disc_score_out_of_class)\n    super().__init__(\n        scoring=scoring,\n        range=transformed_range,\n        default=default,\n        name=name or f\"classwise {str(scoring)}\",\n    )\n    self._in_class_discount_fn = in_class_discount_fn\n    self._out_of_class_discount_fn = out_of_class_discount_fn\n    self.label = initial_label\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/classwise/#pydvl.value.shapley.classwise.ClasswiseScorer.estimate_in_class_and_out_of_class_score","title":"estimate_in_class_and_out_of_class_score","text":"<pre><code>estimate_in_class_and_out_of_class_score(\n    model: SupervisedModel,\n    x_test: NDArray[float64],\n    y_test: NDArray[int_],\n    rescale_scores: bool = True,\n) -&gt; Tuple[float, float]\n</code></pre> <p>Computes in-class and out-of-class scores using the provided inner scoring function. The result is</p> \\[ a_S(D=\\{(x_1, y_1), \\dots, (x_K, y_K)\\}) = \\frac{1}{N} \\sum_k s(y(x_k), y_k). \\] <p>In this context, for label \\(c\\) calculations are executed twice: once for \\(D_c\\) and once for \\(D_{-c}\\) to determine the in-class and out-of-class scores, respectively. By default, the raw scores are multiplied by \\(\\frac{|D_c|}{|D|}\\) and \\(\\frac{|D_{-c}|}{|D|}\\), respectively. This is done to ensure that both scores are of the same order of magnitude. This normalization is particularly useful when the inner score function \\(a_S\\) is calculated by an estimator of the form \\(\\frac{1}{N} \\sum_i x_i\\), e.g. the accuracy.</p> PARAMETER DESCRIPTION <code>model</code> <p>Model used for computing the score on the validation set.</p> <p> TYPE: <code>SupervisedModel</code> </p> <code>x_test</code> <p>Array containing the features of the classification problem.</p> <p> TYPE: <code>NDArray[float64]</code> </p> <code>y_test</code> <p>Array containing the labels of the classification problem.</p> <p> TYPE: <code>NDArray[int_]</code> </p> <code>rescale_scores</code> <p>If set to True, the scores will be denormalized. This is particularly useful when the inner score function \\(a_S\\) is calculated by an estimator of the form \\(\\frac{1}{N} \\sum_i x_i\\).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Tuple[float, float]</code> <p>Tuple containing the in-class and out-of-class scores.</p> Source code in <code>src/pydvl/value/shapley/classwise.py</code> <pre><code>def estimate_in_class_and_out_of_class_score(\n    self,\n    model: SupervisedModel,\n    x_test: NDArray[np.float64],\n    y_test: NDArray[np.int_],\n    rescale_scores: bool = True,\n) -&gt; Tuple[float, float]:\n    r\"\"\"\n    Computes in-class and out-of-class scores using the provided inner\n    scoring function. The result is\n\n    $$\n    a_S(D=\\{(x_1, y_1), \\dots, (x_K, y_K)\\}) = \\frac{1}{N} \\sum_k s(y(x_k), y_k).\n    $$\n\n    In this context, for label $c$ calculations are executed twice: once for $D_c$\n    and once for $D_{-c}$ to determine the in-class and out-of-class scores,\n    respectively. By default, the raw scores are multiplied by $\\frac{|D_c|}{|D|}$\n    and $\\frac{|D_{-c}|}{|D|}$, respectively. This is done to ensure that both\n    scores are of the same order of magnitude. This normalization is particularly\n    useful when the inner score function $a_S$ is calculated by an estimator of the\n    form $\\frac{1}{N} \\sum_i x_i$, e.g. the accuracy.\n\n    Args:\n        model: Model used for computing the score on the validation set.\n        x_test: Array containing the features of the classification problem.\n        y_test: Array containing the labels of the classification problem.\n        rescale_scores: If set to True, the scores will be denormalized. This is\n            particularly useful when the inner score function $a_S$ is calculated by\n            an estimator of the form $\\frac{1}{N} \\sum_i x_i$.\n\n    Returns:\n        Tuple containing the in-class and out-of-class scores.\n    \"\"\"\n    scorer = self._scorer\n    label_set_match = y_test == self.label\n    label_set = np.where(label_set_match)[0]\n    num_classes = len(np.unique(y_test))\n\n    if len(label_set) == 0:\n        return 0, 1 / (num_classes - 1)\n\n    complement_label_set = np.where(~label_set_match)[0]\n    in_class_score = scorer(model, x_test[label_set], y_test[label_set])\n    out_of_class_score = scorer(\n        model, x_test[complement_label_set], y_test[complement_label_set]\n    )\n\n    if rescale_scores:\n        n_in_class = np.count_nonzero(y_test == self.label)\n        n_out_of_class = len(y_test) - n_in_class\n        in_class_score *= n_in_class / (n_in_class + n_out_of_class)\n        out_of_class_score *= n_out_of_class / (n_in_class + n_out_of_class)\n\n    return in_class_score, out_of_class_score\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/classwise/#pydvl.value.shapley.classwise._normalize_classwise_shapley_values","title":"_normalize_classwise_shapley_values","text":"<pre><code>_normalize_classwise_shapley_values(\n    result: ValuationResult, u: Utility\n) -&gt; ValuationResult\n</code></pre> <p>Normalize a valuation result specific to classwise Shapley.</p> <p>Each value \\(v_i\\) associated with the sample \\((x_i, y_i)\\) is normalized by multiplying it with \\(a_S(D_{y_i})\\) and dividing by \\(\\sum_{j \\in D_{y_i}} v_j\\). For more details, see (Schoch et al., 2022) <sup>1 </sup>.</p> PARAMETER DESCRIPTION <code>result</code> <p>ValuationResult object to be normalized.</p> <p> TYPE: <code>ValuationResult</code> </p> <code>u</code> <p>Utility object containing model, data, and scoring function. The scorer must be of type ClasswiseScorer.</p> <p> TYPE: <code>Utility</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Normalized ValuationResult object.</p> Source code in <code>src/pydvl/value/shapley/classwise.py</code> <pre><code>def _normalize_classwise_shapley_values(\n    result: ValuationResult, u: Utility\n) -&gt; ValuationResult:\n    r\"\"\"\n    Normalize a valuation result specific to classwise Shapley.\n\n    Each value $v_i$ associated with the sample $(x_i, y_i)$ is normalized by\n    multiplying it with $a_S(D_{y_i})$ and dividing by $\\sum_{j \\in D_{y_i}} v_j$. For\n    more details, see (Schoch et al., 2022) &lt;sup&gt;&lt;a href=\"#schoch_csshapley_2022\"&gt;1&lt;/a&gt;\n    &lt;/sup&gt;.\n\n    Args:\n        result: ValuationResult object to be normalized.\n        u: Utility object containing model, data, and scoring function. The\n            scorer must be of type [ClasswiseScorer]\n            [pydvl.value.shapley.classwise.ClasswiseScorer].\n\n    Returns:\n        Normalized ValuationResult object.\n    \"\"\"\n    y_train = u.data.y_train\n    unique_labels = np.unique(np.concatenate((y_train, u.data.y_test)))\n    scorer = cast(ClasswiseScorer, u.scorer)\n\n    for idx_label, label in enumerate(unique_labels):\n        scorer.label = label\n        active_elements = y_train == label\n        indices_label_set = np.where(active_elements)[0]\n        indices_label_set = u.data.indices[indices_label_set]\n\n        u.model.fit(u.data.x_train, u.data.y_train)\n        scorer.label = label\n        in_class_acc, _ = scorer.estimate_in_class_and_out_of_class_score(\n            u.model, u.data.x_test, u.data.y_test\n        )\n\n        sigma = np.sum(result.values[indices_label_set])\n        if sigma != 0:\n            result.scale(in_class_acc / sigma, indices=indices_label_set)\n\n    return result\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/classwise/#pydvl.value.shapley.classwise._permutation_montecarlo_classwise_shapley_one_step","title":"_permutation_montecarlo_classwise_shapley_one_step","text":"<pre><code>_permutation_montecarlo_classwise_shapley_one_step(\n    u: Utility,\n    *,\n    done_sample_complements: Optional[StoppingCriterion] = None,\n    truncation: TruncationPolicy,\n    use_default_scorer_value: bool = True,\n    min_elements_per_label: int = 1,\n    algorithm_name: str = \"classwise_shapley\",\n    seed: Optional[SeedSequence] = None\n) -&gt; ValuationResult\n</code></pre> <p>Helper function for compute_classwise_shapley_values().</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object containing model, data, and scoring function. The scorer must be of type ClasswiseScorer.</p> <p> TYPE: <code>Utility</code> </p> <code>done_sample_complements</code> <p>Function checking whether computation needs to stop. Otherwise, it will resample conditional sets until the stopping criterion is met.</p> <p> TYPE: <code>Optional[StoppingCriterion]</code> DEFAULT: <code>None</code> </p> <code>truncation</code> <p>Callable function that decides whether to interrupt processing a permutation and set subsequent marginals to zero.</p> <p> TYPE: <code>TruncationPolicy</code> </p> <code>use_default_scorer_value</code> <p>The first set of indices is the sampled complement set. Unless not otherwise specified, the default scorer value is used for this. If it is set to false, the base score is calculated from the utility.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>min_elements_per_label</code> <p>The minimum number of elements for each opposite label.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>algorithm_name</code> <p>For the results object.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'classwise_shapley'</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[SeedSequence]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>ValuationResult object containing computed data values.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend configuration.</p> Source code in <code>src/pydvl/value/shapley/classwise.py</code> <pre><code>def _permutation_montecarlo_classwise_shapley_one_step(\n    u: Utility,\n    *,\n    done_sample_complements: Optional[StoppingCriterion] = None,\n    truncation: TruncationPolicy,\n    use_default_scorer_value: bool = True,\n    min_elements_per_label: int = 1,\n    algorithm_name: str = \"classwise_shapley\",\n    seed: Optional[SeedSequence] = None,\n) -&gt; ValuationResult:\n    \"\"\"Helper function for [compute_classwise_shapley_values()]\n    [pydvl.value.shapley.classwise.compute_classwise_shapley_values].\n\n    Args:\n        u: Utility object containing model, data, and scoring function. The\n            scorer must be of type [ClasswiseScorer]\n            [pydvl.value.shapley.classwise.ClasswiseScorer].\n        done_sample_complements: Function checking whether computation needs to stop.\n            Otherwise, it will resample conditional sets until the stopping criterion is\n            met.\n        truncation: Callable function that decides whether to interrupt processing a\n            permutation and set subsequent marginals to zero.\n        use_default_scorer_value: The first set of indices is the sampled complement\n            set. Unless not otherwise specified, the default scorer value is used for\n            this. If it is set to false, the base score is calculated from the utility.\n        min_elements_per_label: The minimum number of elements for each opposite\n            label.\n        algorithm_name: For the results object.\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        ValuationResult object containing computed data values.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend configuration.\n    \"\"\"\n    if done_sample_complements is None:\n        done_sample_complements = MaxChecks(1)\n\n    result = ValuationResult.zeros(\n        algorithm=algorithm_name, indices=u.data.indices, data_names=u.data.data_names\n    )\n    rng = np.random.default_rng(seed)\n    x_train, y_train = u.data.get_training_data(u.data.indices)\n    unique_labels = np.unique(y_train)\n    scorer = cast(ClasswiseScorer, copy(u.scorer))\n    u.scorer = scorer\n\n    for label in unique_labels:\n        u.scorer.label = label\n        class_indices_set, class_complement_indices_set = _split_indices_by_label(\n            u.data.indices, y_train, label\n        )\n        _, complement_y_train = u.data.get_training_data(class_complement_indices_set)\n        indices_permutation = rng.permutation(class_indices_set)\n        done_sample_complements.reset()\n\n        for subset_idx, subset_complement in enumerate(\n            random_powerset_label_min(\n                class_complement_indices_set,\n                complement_y_train,\n                min_elements_per_label=min_elements_per_label,\n                seed=rng,\n            )\n        ):\n            result += _permutation_montecarlo_shapley_rollout(\n                u,\n                indices_permutation,\n                additional_indices=subset_complement,\n                truncation=truncation,\n                algorithm_name=algorithm_name,\n                use_default_scorer_value=use_default_scorer_value,\n            )\n            if done_sample_complements(result):\n                break\n\n    return result\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/classwise/#pydvl.value.shapley.classwise._permutation_montecarlo_shapley_rollout","title":"_permutation_montecarlo_shapley_rollout","text":"<pre><code>_permutation_montecarlo_shapley_rollout(\n    u: Utility,\n    permutation: NDArray[int_],\n    truncation: TruncationPolicy,\n    algorithm_name: str,\n    additional_indices: Optional[NDArray[int_]] = None,\n    use_default_scorer_value: bool = True,\n) -&gt; ValuationResult\n</code></pre> <p>Represents a truncated version of a permutation-based MC estimator. It iterates over all subsets starting from the empty set to the full set of indices as specified by <code>permutation</code>. For each subset, the marginal contribution is computed and added to the result. The computation is interrupted if the truncation policy returns <code>True</code>.</p> <p>Todo</p> <p>Reuse in permutation_montecarlo_shapley()</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object containing model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>permutation</code> <p>Permutation of indices to be considered.</p> <p> TYPE: <code>NDArray[int_]</code> </p> <code>truncation</code> <p>Callable which decides whether to interrupt processing a permutation and set all subsequent marginals to zero.</p> <p> TYPE: <code>TruncationPolicy</code> </p> <code>algorithm_name</code> <p>For the results object. Used internally by different variants of Shapley using this subroutine</p> <p> TYPE: <code>str</code> </p> <code>additional_indices</code> <p>Set of additional indices for data points which should be always considered.</p> <p> TYPE: <code>Optional[NDArray[int_]]</code> DEFAULT: <code>None</code> </p> <code>use_default_scorer_value</code> <p>Use default scorer value even if additional_indices is not None.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>ValuationResult object containing computed data values.</p> Source code in <code>src/pydvl/value/shapley/classwise.py</code> <pre><code>def _permutation_montecarlo_shapley_rollout(\n    u: Utility,\n    permutation: NDArray[np.int_],\n    truncation: TruncationPolicy,\n    algorithm_name: str,\n    additional_indices: Optional[NDArray[np.int_]] = None,\n    use_default_scorer_value: bool = True,\n) -&gt; ValuationResult:\n    \"\"\"\n    Represents a truncated version of a permutation-based MC estimator. It iterates over\n    all subsets starting from the empty set to the full set of indices as specified by\n    `permutation`. For each subset, the marginal contribution is computed and added to\n    the result. The computation is interrupted if the truncation policy returns `True`.\n\n    !!! Todo\n        Reuse in [permutation_montecarlo_shapley()]\n        [pydvl.value.shapley.montecarlo.permutation_montecarlo_shapley]\n\n    Args:\n        u: Utility object containing model, data, and scoring function.\n        permutation: Permutation of indices to be considered.\n        truncation: Callable which decides whether to interrupt processing a\n            permutation and set all subsequent marginals to zero.\n        algorithm_name: For the results object. Used internally by different\n            variants of Shapley using this subroutine\n        additional_indices: Set of additional indices for data points which should be\n            always considered.\n        use_default_scorer_value: Use default scorer value even if additional_indices\n            is not None.\n\n    Returns:\n         ValuationResult object containing computed data values.\n    \"\"\"\n    if (\n        additional_indices is not None\n        and len(np.intersect1d(permutation, additional_indices)) &gt; 0\n    ):\n        raise ValueError(\n            \"The class label set and the complement set have to be disjoint.\"\n        )\n\n    result = ValuationResult.zeros(\n        algorithm=algorithm_name, indices=u.data.indices, data_names=u.data.data_names\n    )\n\n    prev_score = (\n        u.default_score\n        if (\n            use_default_scorer_value\n            or additional_indices is None\n            or additional_indices is not None\n            and len(additional_indices) == 0\n        )\n        else u(additional_indices)\n    )\n\n    truncation_u = u\n    if additional_indices is not None:\n        # hack to calculate the correct value in reset.\n        truncation_indices = np.sort(np.concatenate((permutation, additional_indices)))\n        truncation_u = Utility(\n            u.model,\n            Dataset(\n                u.data.x_train[truncation_indices],\n                u.data.y_train[truncation_indices],\n                u.data.x_test,\n                u.data.y_test,\n            ),\n            u.scorer,\n        )\n    truncation.reset(truncation_u)\n\n    is_terminated = False\n    for i, idx in enumerate(permutation):\n        if is_terminated or (is_terminated := truncation(i, prev_score)):\n            score = prev_score\n        else:\n            score = u(\n                np.concatenate((permutation[: i + 1], additional_indices))\n                if additional_indices is not None and len(additional_indices) &gt; 0\n                else permutation[: i + 1]\n            )\n\n        marginal = score - prev_score\n        result.update(idx, marginal)\n        prev_score = score\n\n    return result\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/classwise/#pydvl.value.shapley.classwise._split_indices_by_label","title":"_split_indices_by_label","text":"<pre><code>_split_indices_by_label(\n    indices: NDArray[int_], labels: NDArray[int_], label: int\n) -&gt; Tuple[NDArray[int_], NDArray[int_]]\n</code></pre> <p>Splits the indices into two sets based on the value of <code>label</code>, e.g. those samples with and without that label.</p> PARAMETER DESCRIPTION <code>indices</code> <p>The indices to be used for referring to the data.</p> <p> TYPE: <code>NDArray[int_]</code> </p> <code>labels</code> <p>Corresponding labels for the indices.</p> <p> TYPE: <code>NDArray[int_]</code> </p> <code>label</code> <p>Label to be used for splitting.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tuple[NDArray[int_], NDArray[int_]]</code> <p>Tuple with two sets of indices.</p> Source code in <code>src/pydvl/value/shapley/classwise.py</code> <pre><code>def _split_indices_by_label(\n    indices: NDArray[np.int_], labels: NDArray[np.int_], label: int\n) -&gt; Tuple[NDArray[np.int_], NDArray[np.int_]]:\n    \"\"\"\n    Splits the indices into two sets based on the value of `label`, e.g. those samples\n    with and without that label.\n\n    Args:\n        indices: The indices to be used for referring to the data.\n        labels: Corresponding labels for the indices.\n        label: Label to be used for splitting.\n\n    Returns:\n        Tuple with two sets of indices.\n    \"\"\"\n    active_elements = labels == label\n    class_indices_set = np.where(active_elements)[0]\n    class_complement_indices_set = np.where(~active_elements)[0]\n    class_indices_set = indices[class_indices_set]\n    class_complement_indices_set = indices[class_complement_indices_set]\n    return class_indices_set, class_complement_indices_set\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/classwise/#pydvl.value.shapley.classwise.compute_classwise_shapley_values","title":"compute_classwise_shapley_values","text":"<pre><code>compute_classwise_shapley_values(\n    u: Utility,\n    *,\n    done: StoppingCriterion,\n    truncation: TruncationPolicy,\n    done_sample_complements: Optional[StoppingCriterion] = None,\n    normalize_values: bool = True,\n    use_default_scorer_value: bool = True,\n    min_elements_per_label: int = 1,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>Computes an approximate Class-wise Shapley value by sampling independent permutations of the index set for each label and index sets sampled from the powerset of the complement (with respect to the currently evaluated label), approximating the sum:</p> \\[ v_u(i) = \\frac{1}{K} \\sum_k \\frac{1}{L} \\sum_l [u(\\sigma^{(l)}_{:i} \\cup \\{i\\} | S^{(k)} ) \u2212 u( \\sigma^{(l)}_{:i} | S^{(k)})], \\] <p>where \\(\\sigma_{:i}\\) denotes the set of indices in permutation sigma before the position where \\(i\\) appears and \\(S\\) is a subset of the index set of all other labels (see the main documentation for details).</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object containing model, data, and scoring function. The scorer must be of type ClasswiseScorer.</p> <p> TYPE: <code>Utility</code> </p> <code>done</code> <p>Function that checks whether the computation needs to stop.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>truncation</code> <p>Callable function that decides whether to interrupt processing a permutation and set subsequent marginals to zero.</p> <p> TYPE: <code>TruncationPolicy</code> </p> <code>done_sample_complements</code> <p>Function checking whether computation needs to stop. Otherwise, it will resample conditional sets until the stopping criterion is met.</p> <p> TYPE: <code>Optional[StoppingCriterion]</code> DEFAULT: <code>None</code> </p> <code>normalize_values</code> <p>Indicates whether to normalize the values by the variation in each class times their in-class accuracy.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>done_sample_complements</code> <p>Number of times to resample the complement set for each permutation.</p> <p> TYPE: <code>Optional[StoppingCriterion]</code> DEFAULT: <code>None</code> </p> <code>use_default_scorer_value</code> <p>The first set of indices is the sampled complement set. Unless not otherwise specified, the default scorer value is used for this. If it is set to false, the base score is calculated from the utility.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>min_elements_per_label</code> <p>The minimum number of elements for each opposite label.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to run.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>ValuationResult object containing computed data values.</p> <p>New in version 0.7.1</p> Source code in <code>src/pydvl/value/shapley/classwise.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef compute_classwise_shapley_values(\n    u: Utility,\n    *,\n    done: StoppingCriterion,\n    truncation: TruncationPolicy,\n    done_sample_complements: Optional[StoppingCriterion] = None,\n    normalize_values: bool = True,\n    use_default_scorer_value: bool = True,\n    min_elements_per_label: int = 1,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    r\"\"\"\n    Computes an approximate Class-wise Shapley value by sampling independent\n    permutations of the index set for each label and index sets sampled from the\n    powerset of the complement (with respect to the currently evaluated label),\n    approximating the sum:\n\n    $$\n    v_u(i) = \\frac{1}{K} \\sum_k \\frac{1}{L} \\sum_l\n    [u(\\sigma^{(l)}_{:i} \\cup \\{i\\} | S^{(k)} ) \u2212 u( \\sigma^{(l)}_{:i} | S^{(k)})],\n    $$\n\n    where $\\sigma_{:i}$ denotes the set of indices in permutation sigma before\n    the position where $i$ appears and $S$ is a subset of the index set of all\n    other labels (see [the main documentation][classwise-shapley-intro] for\n    details).\n\n    Args:\n        u: Utility object containing model, data, and scoring function. The\n            scorer must be of type\n            [ClasswiseScorer][pydvl.value.shapley.classwise.ClasswiseScorer].\n        done: Function that checks whether the computation needs to stop.\n        truncation: Callable function that decides whether to interrupt processing a\n            permutation and set subsequent marginals to zero.\n        done_sample_complements: Function checking whether computation needs to stop.\n            Otherwise, it will resample conditional sets until the stopping criterion is\n            met.\n        normalize_values: Indicates whether to normalize the values by the variation\n            in each class times their in-class accuracy.\n        done_sample_complements: Number of times to resample the complement set\n            for each permutation.\n        use_default_scorer_value: The first set of indices is the sampled complement\n            set. Unless not otherwise specified, the default scorer value is used for\n            this. If it is set to false, the base score is calculated from the utility.\n        min_elements_per_label: The minimum number of elements for each opposite\n            label.\n        n_jobs: Number of parallel jobs to run.\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        progress: Whether to display a progress bar.\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        ValuationResult object containing computed data values.\n\n    !!! tip \"New in version 0.7.1\"\n    \"\"\"\n    dim_correct = u.data.y_train.ndim == 1 and u.data.y_test.ndim == 1\n    is_integral = all(\n        map(\n            lambda v: isinstance(v, numbers.Integral), (*u.data.y_train, *u.data.y_test)\n        )\n    )\n    if not dim_correct or not is_integral:\n        raise ValueError(\n            \"The supplied dataset has to be a 1-dimensional classification dataset.\"\n        )\n\n    if not isinstance(u.scorer, ClasswiseScorer):\n        raise ValueError(\n            \"Please set a subclass of ClasswiseScorer object as scorer object of the\"\n            \" utility. See scoring argument of Utility.\"\n        )\n\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n    u_ref = parallel_backend.put(u)\n    n_jobs = parallel_backend.effective_n_jobs(n_jobs)\n    n_submitted_jobs = 2 * n_jobs\n\n    pbar = tqdm(disable=not progress, position=0, total=100, unit=\"%\")\n    algorithm = \"classwise_shapley\"\n    accumulated_result = ValuationResult.zeros(\n        algorithm=algorithm, indices=u.data.indices, data_names=u.data.data_names\n    )\n    terminate_exec = False\n    seed_sequence = ensure_seed_sequence(seed)\n\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n\n    with parallel_backend.executor(max_workers=n_jobs) as executor:\n        pending: Set[Future] = set()\n        while True:\n            completed_futures, pending = wait(\n                pending, timeout=60, return_when=FIRST_COMPLETED\n            )\n            for future in completed_futures:\n                accumulated_result += future.result()\n                if done(accumulated_result):\n                    terminate_exec = True\n                    break\n\n            pbar.n = 100 * done.completion()\n            pbar.refresh()\n            if terminate_exec:\n                break\n\n            n_remaining_slots = n_submitted_jobs - len(pending)\n            seeds = seed_sequence.spawn(n_remaining_slots)\n            for i in range(n_remaining_slots):\n                future = executor.submit(\n                    _permutation_montecarlo_classwise_shapley_one_step,\n                    u_ref,\n                    truncation=truncation,\n                    done_sample_complements=done_sample_complements,\n                    use_default_scorer_value=use_default_scorer_value,\n                    min_elements_per_label=min_elements_per_label,\n                    algorithm_name=algorithm,\n                    seed=seeds[i],\n                )\n                pending.add(future)\n\n    result = accumulated_result\n    if normalize_values:\n        result = _normalize_classwise_shapley_values(result, u)\n\n    return result\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/common/","title":"Common","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/shapley/common/#pydvl.value.shapley.common","title":"pydvl.value.shapley.common","text":""},{"location":"deprecated/pydvl/value/shapley/common/#pydvl.value.shapley.common.compute_shapley_values","title":"compute_shapley_values","text":"<pre><code>compute_shapley_values(\n    u: Utility,\n    *,\n    done: StoppingCriterion = MaxChecks(None),\n    mode: ShapleyMode = TruncatedMontecarlo,\n    n_jobs: int = 1,\n    seed: Optional[Seed] = None,\n    **kwargs\n) -&gt; ValuationResult\n</code></pre> <p>Umbrella method to compute Shapley values with any of the available algorithms.</p> <p>See Data valuation for an overview.</p> <p>The following algorithms are available. Note that the exact methods can only work with very small datasets and are thus intended only for testing. Some algorithms also accept additional arguments, please refer to the documentation of each particular method.</p> <ul> <li><code>combinatorial_exact</code>: uses the combinatorial implementation of data   Shapley. Implemented in   combinatorial_exact_shapley().</li> <li><code>combinatorial_montecarlo</code>:  uses the approximate Monte Carlo   implementation of combinatorial data Shapley. Implemented in   combinatorial_montecarlo_shapley().</li> <li><code>permutation_exact</code>: uses the permutation-based implementation of data   Shapley. Computation is not parallelized. Implemented in   permutation_exact_shapley().</li> <li><code>permutation_montecarlo</code>: uses the approximate Monte Carlo   implementation of permutation data Shapley. Accepts a   TruncationPolicy to stop   computing marginals. Implemented in   permutation_montecarlo_shapley().</li> <li><code>owen_sampling</code>: Uses the Owen continuous extension of the utility   function to the unit cube. Implemented in   owen_sampling_shapley(). This   method does not take a StoppingCriterion   but instead requires a parameter <code>q_max</code> for the number of subdivisions   of the unit interval to use for integration, and another parameter   <code>n_samples</code> for the number of subsets to sample for each \\(q\\).</li> <li><code>owen_halved</code>: Same as 'owen_sampling' but uses correlated samples in the   expectation. Implemented in   owen_sampling_shapley().   This method  requires an additional parameter <code>q_max</code> for the number of   subdivisions of the interval [0,0.5] to use for integration, and another   parameter <code>n_samples</code> for the number of subsets to sample for each \\(q\\).</li> <li><code>group_testing</code>: estimates differences of Shapley values and solves a   constraint satisfaction problem. High sample complexity, not recommended.   Implemented in group_testing_shapley(). This   method does not take a StoppingCriterion   but instead requires a parameter <code>n_samples</code> for the number of   iterations to run.</li> </ul> <p>Additionally, one can use model-specific methods:</p> <ul> <li><code>knn</code>: Exact method for K-Nearest neighbour models. Implemented in   knn_shapley().</li> </ul> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>done</code> <p>Object used to determine when to stop the computation for Monte Carlo methods. The default is to stop after 100 iterations. See the available criteria in stopping. It is possible to combine several of them using boolean operators. Some methods ignore this argument, others require specific subtypes.</p> <p> TYPE: <code>StoppingCriterion</code> DEFAULT: <code>MaxChecks(None)</code> </p> <code>n_jobs</code> <p>Number of parallel jobs (available only to some methods)</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>Choose which shapley algorithm to use. See ShapleyMode for a list of allowed value.</p> <p> TYPE: <code>ShapleyMode</code> DEFAULT: <code>TruncatedMontecarlo</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the results.</p> Source code in <code>src/pydvl/value/shapley/common.py</code> <pre><code>def compute_shapley_values(\n    u: Utility,\n    *,\n    done: StoppingCriterion = MaxChecks(None),\n    mode: ShapleyMode = ShapleyMode.TruncatedMontecarlo,\n    n_jobs: int = 1,\n    seed: Optional[Seed] = None,\n    **kwargs,\n) -&gt; ValuationResult:\n    \"\"\"Umbrella method to compute Shapley values with any of the available\n    algorithms.\n\n    See [Data valuation][data-valuation-intro] for an overview.\n\n    The following algorithms are available. Note that the exact methods can only\n    work with very small datasets and are thus intended only for testing. Some\n    algorithms also accept additional arguments, please refer to the\n    documentation of each particular method.\n\n    - `combinatorial_exact`: uses the combinatorial implementation of data\n      Shapley. Implemented in\n      [combinatorial_exact_shapley()][pydvl.value.shapley.naive.combinatorial_exact_shapley].\n    - `combinatorial_montecarlo`:  uses the approximate Monte Carlo\n      implementation of combinatorial data Shapley. Implemented in\n      [combinatorial_montecarlo_shapley()][pydvl.value.shapley.montecarlo.combinatorial_montecarlo_shapley].\n    - `permutation_exact`: uses the permutation-based implementation of data\n      Shapley. Computation is **not parallelized**. Implemented in\n      [permutation_exact_shapley()][pydvl.value.shapley.naive.permutation_exact_shapley].\n    - `permutation_montecarlo`: uses the approximate Monte Carlo\n      implementation of permutation data Shapley. Accepts a\n      [TruncationPolicy][pydvl.value.shapley.truncated.TruncationPolicy] to stop\n      computing marginals. Implemented in\n      [permutation_montecarlo_shapley()][pydvl.value.shapley.montecarlo.permutation_montecarlo_shapley].\n    - `owen_sampling`: Uses the Owen continuous extension of the utility\n      function to the unit cube. Implemented in\n      [owen_sampling_shapley()][pydvl.value.shapley.owen.owen_sampling_shapley]. This\n      method does not take a [StoppingCriterion][pydvl.value.stopping.StoppingCriterion]\n      but instead requires a parameter `q_max` for the number of subdivisions\n      of the unit interval to use for integration, and another parameter\n      `n_samples` for the number of subsets to sample for each $q$.\n    - `owen_halved`: Same as 'owen_sampling' but uses correlated samples in the\n      expectation. Implemented in\n      [owen_sampling_shapley()][pydvl.value.shapley.owen.owen_sampling_shapley].\n      This method  requires an additional parameter `q_max` for the number of\n      subdivisions of the interval [0,0.5] to use for integration, and another\n      parameter `n_samples` for the number of subsets to sample for each $q$.\n    - `group_testing`: estimates differences of Shapley values and solves a\n      constraint satisfaction problem. High sample complexity, not recommended.\n      Implemented in [group_testing_shapley()][pydvl.value.shapley.gt.group_testing_shapley]. This\n      method does not take a [StoppingCriterion][pydvl.value.stopping.StoppingCriterion]\n      but instead requires a parameter `n_samples` for the number of\n      iterations to run.\n\n    Additionally, one can use model-specific methods:\n\n    - `knn`: Exact method for K-Nearest neighbour models. Implemented in\n      [knn_shapley()][pydvl.value.shapley.knn.knn_shapley].\n\n    Args:\n        u: [Utility][pydvl.utils.utility.Utility] object with model, data, and\n            scoring function.\n        done: Object used to determine when to stop the computation for Monte\n            Carlo methods. The default is to stop after 100 iterations. See the\n            available criteria in [stopping][pydvl.value.stopping]. It is\n            possible to combine several of them using boolean operators. Some\n            methods ignore this argument, others require specific subtypes.\n        n_jobs: Number of parallel jobs (available only to some methods)\n        seed: Either an instance of a numpy random number generator or a seed\n            for it.\n        mode: Choose which shapley algorithm to use. See\n            [ShapleyMode][pydvl.value.shapley.ShapleyMode] for a list of allowed\n            value.\n\n    Returns:\n        Object with the results.\n\n    \"\"\"\n    progress: bool = kwargs.pop(\"progress\", False)\n\n    if mode not in list(ShapleyMode):\n        raise ValueError(f\"Invalid value encountered in {mode=}\")\n\n    if mode in (\n        ShapleyMode.PermutationMontecarlo,\n        ShapleyMode.ApproShapley,\n        ShapleyMode.TruncatedMontecarlo,\n    ):\n        truncation = kwargs.pop(\"truncation\", NoTruncation())\n        return permutation_montecarlo_shapley(  # type: ignore\n            u=u,\n            done=done,\n            truncation=truncation,\n            n_jobs=n_jobs,\n            seed=seed,\n            progress=progress,\n            **kwargs,\n        )\n    elif mode == ShapleyMode.CombinatorialMontecarlo:\n        return combinatorial_montecarlo_shapley(  # type: ignore\n            u, done=done, n_jobs=n_jobs, seed=seed, progress=progress\n        )\n    elif mode == ShapleyMode.CombinatorialExact:\n        return combinatorial_exact_shapley(u, n_jobs=n_jobs, progress=progress)  # type: ignore\n    elif mode == ShapleyMode.PermutationExact:\n        return permutation_exact_shapley(u, progress=progress)\n    elif mode == ShapleyMode.Owen or mode == ShapleyMode.OwenAntithetic:\n        if kwargs.get(\"n_samples\") is None:\n            raise ValueError(\"n_samples cannot be None for Owen methods\")\n        if kwargs.get(\"max_q\") is None:\n            raise ValueError(\"Owen Sampling requires max_q for the outer integral\")\n\n        method = (\n            OwenAlgorithm.Standard\n            if mode == ShapleyMode.Owen\n            else OwenAlgorithm.Antithetic\n        )\n        return owen_sampling_shapley(  # type: ignore\n            u,\n            n_samples=int(kwargs.get(\"n_samples\", -1)),\n            max_q=int(kwargs.get(\"max_q\", -1)),\n            method=method,\n            n_jobs=n_jobs,\n            seed=seed,\n        )\n    elif mode == ShapleyMode.KNN:\n        return knn_shapley(u, progress=progress)\n    elif mode == ShapleyMode.GroupTesting:\n        n_samples = kwargs.pop(\"n_samples\")\n        if n_samples is None:\n            raise ValueError(\"n_samples cannot be None for Group Testing\")\n        epsilon = kwargs.pop(\"epsilon\")\n        if epsilon is None:\n            raise ValueError(\"Group Testing requires error bound epsilon\")\n        delta = kwargs.pop(\"delta\", 0.05)\n        return group_testing_shapley(  # type: ignore\n            u,\n            epsilon=float(epsilon),\n            delta=delta,\n            n_samples=int(n_samples),\n            n_jobs=n_jobs,\n            progress=progress,\n            seed=seed,\n            **kwargs,\n        )\n    else:\n        raise ValueError(f\"Invalid value encountered in {mode=}\")\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/gt/","title":"Gt","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/shapley/gt/#pydvl.value.shapley.gt","title":"pydvl.value.shapley.gt","text":"<p>This module implements Group Testing for the approximation of Shapley values, as introduced in (Jia, R. et al., 2019)<sup>1</sup>. The sampling of index subsets is done in such a way that an approximation to the true Shapley values can be computed with guarantees.</p> <p>Warning</p> <p>This method is very inefficient. Potential improvements to the implementation notwithstanding, convergence seems to be very slow (in terms of evaluations of the utility required). We recommend other Monte Carlo methods instead.</p> <p>You can read more in the documentation.</p> <p>New in version 0.4.0</p>"},{"location":"deprecated/pydvl/value/shapley/gt/#pydvl.value.shapley.gt--references","title":"References","text":"<ol> <li> <p>Jia, R. et al., 2019. Towards Efficient Data Valuation Based on the Shapley Value. In: Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, pp. 1167\u20131176. PMLR.\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/shapley/gt/#pydvl.value.shapley.gt._constants","title":"_constants","text":"<pre><code>_constants(\n    n: int, epsilon: float, delta: float, utility_range: float\n) -&gt; GTConstants\n</code></pre> <p>A helper function returning the constants for the algorithm. Pretty ugly, yes.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of data points.</p> <p> TYPE: <code>int</code> </p> <code>epsilon</code> <p>The error tolerance.</p> <p> TYPE: <code>float</code> </p> <code>delta</code> <p>The confidence level.</p> <p> TYPE: <code>float</code> </p> <code>utility_range</code> <p>The range of the utility function.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>GTConstants</code> <p>A namedtuple with the constants. The fields are the same as in the paper: - kk: the sample sizes (i.e. an array of 1, 2, ..., n - 1) - Z: the normalization constant - q: the probability of drawing a sample of size k - q_tot: another normalization constant - T: the number of iterations. This will be -1 if the utility_range is     infinite. E.g. because the Scorer does     not define a range.</p> Source code in <code>src/pydvl/value/shapley/gt.py</code> <pre><code>def _constants(\n    n: int, epsilon: float, delta: float, utility_range: float\n) -&gt; GTConstants:\n    \"\"\"A helper function returning the constants for the algorithm. Pretty ugly,\n    yes.\n\n    Args:\n        n: The number of data points.\n        epsilon: The error tolerance.\n        delta: The confidence level.\n        utility_range: The range of the utility function.\n\n    Returns:\n        A namedtuple with the constants. The fields are the same as in the paper:\n            - kk: the sample sizes (i.e. an array of 1, 2, ..., n - 1)\n            - Z: the normalization constant\n            - q: the probability of drawing a sample of size k\n            - q_tot: another normalization constant\n            - T: the number of iterations. This will be -1 if the utility_range is\n                infinite. E.g. because the [Scorer][pydvl.utils.score.Scorer] does\n                not define a range.\n    \"\"\"\n    r = utility_range\n\n    kk = np.arange(1, n)  # sample sizes\n    Z = 2 * (1.0 / kk).sum()\n    q = (1 / kk + 1 / (n - kk)) / Z\n    q_tot = (n - 2) / n * q[0] + np.inner(\n        q[1:], 1 + 2 * kk[1:] * (kk[1:] - n) / (n * (n - 1))\n    )\n\n    def h(u: T) -&gt; T:\n        return cast(T, (1 + u) * np.log(1 + u) - u)\n\n    # The implementation in GitHub defines a different bound:\n    # T_code = int( 4\n    #     / (1 - q_tot**2)\n    #     / h(2 * epsilon / Z / r / (1 - q_tot**2))\n    #     * np.log(n * (n - 1) / (2 * delta))\n    # )\n    if r == np.inf:\n        log.warning(\n            \"Group Testing: cannot estimate minimum number of iterations for \"\n            \"unbounded utilities. Please specify a range in the Scorer if \"\n            \"you need this estimate.\"\n        )\n        min_iter = -1\n    else:\n        min_iter = 8 * np.log(n * (n - 1) / (2 * delta)) / (1 - q_tot**2)\n        min_iter /= h(2 * epsilon / (np.sqrt(n) * Z * r * (1 - q_tot**2)))\n\n    return GTConstants(kk=kk, Z=Z, q=q, q_tot=q_tot, T=int(min_iter))\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/gt/#pydvl.value.shapley.gt._group_testing_shapley","title":"_group_testing_shapley","text":"<pre><code>_group_testing_shapley(\n    u: Utility,\n    n_samples: int,\n    progress: bool = False,\n    job_id: int = 1,\n    seed: Optional[Union[Seed, SeedSequence]] = None,\n)\n</code></pre> <p>Helper function for group_testing_shapley().</p> <p>Computes utilities of sets sampled using the strategy for estimating the differences in Shapley values.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>n_samples</code> <p>total number of samples (subsets) to use.</p> <p> TYPE: <code>int</code> </p> <code>progress</code> <p>Whether to display progress bars for each job.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>job_id</code> <p>id to use for reporting progress (e.g. to place progres bars)</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Union[Seed, SeedSequence]]</code> DEFAULT: <code>None</code> </p> <p>Returns:</p> Source code in <code>src/pydvl/value/shapley/gt.py</code> <pre><code>def _group_testing_shapley(\n    u: Utility,\n    n_samples: int,\n    progress: bool = False,\n    job_id: int = 1,\n    seed: Optional[Union[Seed, SeedSequence]] = None,\n):\n    \"\"\"Helper function for\n    [group_testing_shapley()][pydvl.value.shapley.gt.group_testing_shapley].\n\n    Computes utilities of sets sampled using the strategy for estimating the\n    differences in Shapley values.\n\n    Args:\n        u: Utility object with model, data, and scoring function.\n        n_samples: total number of samples (subsets) to use.\n        progress: Whether to display progress bars for each job.\n        job_id: id to use for reporting progress (e.g. to place progres bars)\n        seed: Either an instance of a numpy random number generator or a seed for it.\n    Returns:\n\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n = len(u.data.indices)\n    const = _constants(n, 1, 1, 1)  # don't care about eps,delta,range\n\n    betas: NDArray[np.int_] = np.zeros(\n        shape=(n_samples, n), dtype=np.int_\n    )  # indicator vars\n    uu = np.empty(n_samples)  # utilities\n\n    for t in trange(n_samples, disable=not progress, position=job_id):\n        k = rng.choice(const.kk, size=1, p=const.q).item()\n        s = random_subset_of_size(u.data.indices, k, seed=rng)\n        uu[t] = u(s)\n        betas[t, s] = 1\n    return uu, betas\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/gt/#pydvl.value.shapley.gt.group_testing_shapley","title":"group_testing_shapley","text":"<pre><code>group_testing_shapley(\n    u: Utility,\n    n_samples: int,\n    epsilon: float,\n    delta: float,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n    **options: dict\n) -&gt; ValuationResult\n</code></pre> <p>Implements group testing for approximation of Shapley values as described in (Jia, R. et al., 2019)<sup>1</sup>.</p> <p>Warning</p> <p>This method is very inefficient. It requires several orders of magnitude more evaluations of the utility than others in montecarlo. It also uses several intermediate objects like the results from the runners and the constraint matrices which can become rather large.</p> <p>By picking a specific distribution over subsets, the differences in Shapley values can be approximated with a Monte Carlo sum. These are then used to solve for the individual values in a feasibility problem.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>n_samples</code> <p>Number of tests to perform. Use num_samples_eps_delta to estimate this.</p> <p> TYPE: <code>int</code> </p> <code>epsilon</code> <p>From the (\u03b5,\u03b4) sample bound. Use the same as for the estimation of <code>n_iterations</code>.</p> <p> TYPE: <code>float</code> </p> <code>delta</code> <p>From the (\u03b5,\u03b4) sample bound. Use the same as for the estimation of <code>n_iterations</code>.</p> <p> TYPE: <code>float</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to use. Each worker performs a chunk of all tests (i.e. utility evaluations).</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display progress bars for each job.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Additional options to pass to cvxpy.Problem.solve(). E.g. to change the solver (which defaults to <code>cvxpy.SCS</code>) pass <code>solver=cvxpy.CVXOPT</code>.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values.</p> <p>New in version 0.4.0</p> <p>Changed in version 0.5.0</p> <p>Changed the solver to cvxpy instead of scipy's linprog. Added the ability to pass arbitrary options to it.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/shapley/gt.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef group_testing_shapley(\n    u: Utility,\n    n_samples: int,\n    epsilon: float,\n    delta: float,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n    **options: dict,\n) -&gt; ValuationResult:\n    \"\"\"Implements group testing for approximation of Shapley values as described\n    in (Jia, R. et al., 2019)&lt;sup&gt;&lt;a href=\"#jia_efficient_2019\"&gt;1&lt;/a&gt;&lt;/sup&gt;.\n\n    !!! Warning\n        This method is very inefficient. It requires several orders of magnitude\n        more evaluations of the utility than others in\n        [montecarlo][pydvl.value.shapley.montecarlo]. It also uses several intermediate\n        objects like the results from the runners and the constraint matrices\n        which can become rather large.\n\n    By picking a specific distribution over subsets, the differences in Shapley\n    values can be approximated with a Monte Carlo sum. These are then used to\n    solve for the individual values in a feasibility problem.\n\n    Args:\n        u: Utility object with model, data, and scoring function\n        n_samples: Number of tests to perform. Use\n            [num_samples_eps_delta][pydvl.value.shapley.gt.num_samples_eps_delta]\n            to estimate this.\n        epsilon: From the (\u03b5,\u03b4) sample bound. Use the same as for the\n            estimation of `n_iterations`.\n        delta: From the (\u03b5,\u03b4) sample bound. Use the same as for the\n            estimation of `n_iterations`.\n        n_jobs: Number of parallel jobs to use. Each worker performs a chunk\n            of all tests (i.e. utility evaluations).\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        progress: Whether to display progress bars for each job.\n        seed: Either an instance of a numpy random number generator or a seed for it.\n        options: Additional options to pass to\n            [cvxpy.Problem.solve()](https://www.cvxpy.org/tutorial/advanced/index.html#solve-method-options).\n            E.g. to change the solver (which defaults to `cvxpy.SCS`) pass\n            `solver=cvxpy.CVXOPT`.\n\n    Returns:\n        Object with the data values.\n\n    !!! tip \"New in version 0.4.0\"\n\n    !!! tip \"Changed in version 0.5.0\"\n        Changed the solver to cvxpy instead of scipy's linprog. Added the ability\n        to pass arbitrary options to it.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n\n    n = len(u.data.indices)\n\n    const = _constants(\n        n=n,\n        epsilon=epsilon,\n        delta=delta,\n        utility_range=u.score_range.max() - u.score_range.min(),\n    )\n    T = n_samples\n    if T &lt; const.T:\n        log.warning(\n            f\"n_samples of {T} are below the required {const.T} for the \"\n            f\"\u03b5={epsilon:.02f} guarantee at \u03b4={1 - delta:.02f} probability\"\n        )\n\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n\n    samples_per_job = max(1, n_samples // parallel_backend.effective_n_jobs(n_jobs))\n\n    def reducer(\n        results_it: Iterable[Tuple[NDArray, NDArray]],\n    ) -&gt; Tuple[NDArray, NDArray]:\n        return np.concatenate(list(x[0] for x in results_it)).astype(\n            np.float64\n        ), np.concatenate(list(x[1] for x in results_it)).astype(np.int_)\n\n    seed_sequence = ensure_seed_sequence(seed)\n    map_reduce_seed_sequence, cvxpy_seed = tuple(seed_sequence.spawn(2))\n\n    map_reduce_job: MapReduceJob[Utility, Tuple[NDArray, NDArray]] = MapReduceJob(\n        u,\n        map_func=_group_testing_shapley,\n        reduce_func=reducer,\n        map_kwargs=dict(n_samples=samples_per_job, progress=progress),\n        parallel_backend=parallel_backend,\n        n_jobs=n_jobs,\n    )\n    uu, betas = map_reduce_job(seed=map_reduce_seed_sequence)\n\n    # Matrix of estimated differences. See Eqs. (3) and (4) in the paper.\n    C = np.zeros(shape=(n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            C[i, j] = np.dot(uu, betas[:, i] - betas[:, j])\n    C *= const.Z / T\n    total_utility = u(u.data.indices)\n\n    ###########################################################################\n    # Solution of the constraint problem with cvxpy\n\n    v = cp.Variable(n)\n    constraints = [cp.sum(v) == total_utility]\n    for i in range(n):\n        for j in range(i + 1, n):\n            constraints.append(v[i] - v[j] &lt;= epsilon + C[i, j])\n            constraints.append(v[j] - v[i] &lt;= epsilon - C[i, j])\n\n    problem = cp.Problem(cp.Minimize(0), constraints)\n    solver = options.pop(\"solver\", cp.SCS)\n    problem.solve(solver=solver, **options)\n\n    if problem.status != \"optimal\":\n        log.warning(f\"cvxpy returned status {problem.status}\")\n        values = (\n            np.nan * np.ones_like(u.data.indices)\n            if not hasattr(v.value, \"__len__\")\n            else cast(NDArray[np.float64], v.value)\n        )\n        status = Status.Failed\n    else:\n        values = cast(NDArray[np.float64], v.value)\n        status = Status.Converged\n\n    return ValuationResult(\n        algorithm=\"group_testing_shapley\",\n        status=status,\n        values=values,\n        data_names=u.data.data_names,\n        solver_status=problem.status,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/gt/#pydvl.value.shapley.gt.num_samples_eps_delta","title":"num_samples_eps_delta","text":"<pre><code>num_samples_eps_delta(\n    eps: float, delta: float, n: int, utility_range: float\n) -&gt; int\n</code></pre> <p>Implements the formula in Theorem 3 of (Jia, R. et al., 2019)<sup>1</sup> which gives a lower bound on the number of samples required to obtain an (\u03b5/\u221an,\u03b4/(N(N-1))-approximation to all pair-wise differences of Shapley values, wrt. \\(\\ell_2\\) norm.</p> PARAMETER DESCRIPTION <code>eps</code> <p>\u03b5</p> <p> TYPE: <code>float</code> </p> <code>delta</code> <p>\u03b4</p> <p> TYPE: <code>float</code> </p> <code>n</code> <p>Number of data points</p> <p> TYPE: <code>int</code> </p> <code>utility_range</code> <p>Range of the Utility function</p> <p> TYPE: <code>float</code> </p> <p>Returns:     Number of samples from \\(2^{[n]}\\) guaranteeing \u03b5/\u221an-correct Shapley         pair-wise differences of values with probability 1-\u03b4/(N(N-1)).</p> <p>New in version 0.4.0</p> Source code in <code>src/pydvl/value/shapley/gt.py</code> <pre><code>def num_samples_eps_delta(\n    eps: float, delta: float, n: int, utility_range: float\n) -&gt; int:\n    r\"\"\"Implements the formula in Theorem 3 of (Jia, R. et al., 2019)&lt;sup&gt;&lt;a href=\"#jia_efficient_2019\"&gt;1&lt;/a&gt;&lt;/sup&gt;\n    which gives a lower bound on the number of samples required to obtain an\n    (\u03b5/\u221an,\u03b4/(N(N-1))-approximation to all pair-wise differences of Shapley\n    values, wrt. $\\ell_2$ norm.\n\n    Args:\n        eps: \u03b5\n        delta: \u03b4\n        n: Number of data points\n        utility_range: Range of the [Utility][pydvl.utils.utility.Utility] function\n    Returns:\n        Number of samples from $2^{[n]}$ guaranteeing \u03b5/\u221an-correct Shapley\n            pair-wise differences of values with probability 1-\u03b4/(N(N-1)).\n\n    !!! tip \"New in version 0.4.0\"\n\n    \"\"\"\n    constants = _constants(n=n, epsilon=eps, delta=delta, utility_range=utility_range)\n    return int(constants.T)\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/knn/","title":"Knn","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/shapley/knn/#pydvl.value.shapley.knn","title":"pydvl.value.shapley.knn","text":"<p>This module contains Shapley computations for K-Nearest Neighbours.</p> <p>Todo</p> <p>Implement approximate KNN computation for sublinear complexity</p>"},{"location":"deprecated/pydvl/value/shapley/knn/#pydvl.value.shapley.knn--references","title":"References","text":"<ol> <li> <p>Jia, R. et al., 2019. Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms. In: Proceedings of the VLDB Endowment, Vol. 12, No. 11, pp. 1610\u20131623.\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/shapley/knn/#pydvl.value.shapley.knn.knn_shapley","title":"knn_shapley","text":"<pre><code>knn_shapley(u: Utility, *, progress: bool = True) -&gt; ValuationResult\n</code></pre> <p>Computes exact Shapley values for a KNN classifier.</p> <p>This implements the method described in (Jia, R. et al., 2019)<sup>1</sup>. It exploits the local structure of K-Nearest Neighbours to reduce the value computation to sorting of the training points by distance to the test point and applying a recursive formula, thus reducing computation time to \\(O(n_test n_train log(n_train)\\).</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility with a KNN model to extract parameters from. The object will not be modified nor used other than to call get_params()</p> <p> TYPE: <code>Utility</code> </p> <code>progress</code> <p>Whether to display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values.</p> RAISES DESCRIPTION <code>TypeError</code> <p>If the model in the utility is not a sklearn.neighbors.KNeighborsClassifier.</p> <p>New in version 0.1.0</p> Source code in <code>src/pydvl/value/shapley/knn.py</code> <pre><code>def knn_shapley(u: Utility, *, progress: bool = True) -&gt; ValuationResult:\n    \"\"\"Computes exact Shapley values for a KNN classifier.\n\n    This implements the method described in (Jia, R. et al., 2019)&lt;sup&gt;&lt;a\n    href=\"#jia_efficient_2019a\"&gt;1&lt;/a&gt;&lt;/sup&gt;. It exploits the local structure of\n    K-Nearest Neighbours to reduce the value computation to sorting of the training\n    points by distance to the test point and applying a recursive formula,\n    thus reducing computation time to $O(n_test n_train log(n_train)$.\n\n    Args:\n        u: Utility with a KNN model to extract parameters from. The object\n            will not be modified nor used other than to call [get_params()](\n            &lt;https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator.get_params&gt;)\n        progress: Whether to display a progress bar.\n\n    Returns:\n        Object with the data values.\n\n    Raises:\n        TypeError: If the model in the utility is not a\n            [sklearn.neighbors.KNeighborsClassifier][].\n\n    !!! tip \"New in version 0.1.0\"\n\n    \"\"\"\n    if not isinstance(u.model, KNeighborsClassifier):\n        raise TypeError(\"KNN Shapley requires a K-Nearest Neighbours model\")\n\n    defaults: Dict[str, Union[int, str]] = {\n        \"algorithm\": \"ball_tree\" if u.data.dim &gt;= 20 else \"kd_tree\",\n        \"metric\": \"minkowski\",\n        \"p\": 2,\n    }\n    defaults.update(u.model.get_params())\n    # HACK: NearestNeighbors doesn't support this. There will be more...\n    del defaults[\"weights\"]\n    n_neighbors: int = int(defaults[\"n_neighbors\"])\n    defaults[\"n_neighbors\"] = len(u.data)  # We want all training points sorted\n\n    assert n_neighbors &lt; len(u.data)\n    # assert data.target_dim == 1\n\n    nns = NearestNeighbors(**defaults).fit(u.data.x_train)\n    # closest to farthest\n    _, indices = nns.kneighbors(u.data.x_test)\n\n    res = np.zeros_like(u.data.indices, dtype=np.float64)\n    n = len(u.data)\n    yt = u.data.y_train\n    iterator = enumerate(zip(u.data.y_test, indices), start=1)\n    for j, (y, ii) in tqdm(iterator, disable=not progress):\n        values = np.zeros_like(u.data.indices, dtype=np.float64)\n        idx = ii[-1]\n        values[idx] = int(yt[idx] == y) / n\n\n        for i in range(n - 1, 0, -1):\n            prev_idx = idx\n            idx = ii[i - 1]\n            values[idx] = values[prev_idx] + (\n                int(yt[idx] == y) - int(yt[prev_idx] == y)\n            ) / max(n_neighbors, i)\n        res += values\n\n    return ValuationResult(\n        algorithm=\"knn_shapley\",\n        status=Status.Converged,\n        values=res,\n        data_names=u.data.data_names,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/montecarlo/","title":"Montecarlo","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/shapley/montecarlo/#pydvl.value.shapley.montecarlo","title":"pydvl.value.shapley.montecarlo","text":"<p>Monte Carlo approximations to Shapley Data values.</p> <p>Warning</p> <p>You probably want to use the common interface provided by compute_shapley_values() instead of directly using the functions in this module.</p> <p>Because exact computation of Shapley values requires \\(\\mathcal{O}(2^n)\\) re-trainings of the model, several Monte Carlo approximations are available. The first two sample from the powerset of the training data directly: combinatorial_montecarlo_shapley() and owen_sampling_shapley(). The latter uses a reformulation in terms of a continuous extension of the utility.</p> <p>Alternatively, employing another reformulation of the expression above as a sum over permutations, one has the implementation in permutation_montecarlo_shapley() with the option to pass an early stopping strategy to reduce computation as done in Truncated MonteCarlo Shapley (TMCS).</p> <p>Also see</p> <p>It is also possible to use group_testing_shapley() to reduce the number of evaluations of the utility. The method is however typically outperformed by others in this module.</p> <p>Also see</p> <p>Additionally, you can consider grouping your data points using GroupedDataset and computing the values of the groups instead. This is not to be confused with \"group testing\" as implemented in group_testing_shapley(): any of the algorithms mentioned above, including Group Testing, can work to valuate groups of samples as units.</p>"},{"location":"deprecated/pydvl/value/shapley/montecarlo/#pydvl.value.shapley.montecarlo--references","title":"References","text":"<ol> <li> <p>Ghorbani, A., Zou, J., 2019. Data Shapley: Equitable Valuation of Data for Machine Learning. In: Proceedings of the 36th International Conference on Machine Learning, PMLR, pp. 2242\u20132251.\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/shapley/montecarlo/#pydvl.value.shapley.montecarlo._combinatorial_montecarlo_shapley","title":"_combinatorial_montecarlo_shapley","text":"<pre><code>_combinatorial_montecarlo_shapley(\n    indices: Sequence[int],\n    u: Utility,\n    done: StoppingCriterion,\n    *,\n    progress: bool = False,\n    job_id: int = 1,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>Helper function for combinatorial_montecarlo_shapley.</p> <p>This is the code that is sent to workers to compute values using the combinatorial definition.</p> PARAMETER DESCRIPTION <code>indices</code> <p>Indices of the samples to compute values for.</p> <p> TYPE: <code>Sequence[int]</code> </p> <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>done</code> <p>Check on the results which decides when to stop sampling subsets for an index.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>progress</code> <p>Whether to display progress bars for each job.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> <code>job_id</code> <p>id to use for reporting progress</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>The results for the indices.</p> Source code in <code>src/pydvl/value/shapley/montecarlo.py</code> <pre><code>def _combinatorial_montecarlo_shapley(\n    indices: Sequence[int],\n    u: Utility,\n    done: StoppingCriterion,\n    *,\n    progress: bool = False,\n    job_id: int = 1,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    \"\"\"Helper function for\n    [combinatorial_montecarlo_shapley][pydvl.value.shapley.montecarlo.combinatorial_montecarlo_shapley].\n\n    This is the code that is sent to workers to compute values using the\n    combinatorial definition.\n\n    Args:\n        indices: Indices of the samples to compute values for.\n        u: Utility object with model, data, and scoring function\n        done: Check on the results which decides when to stop sampling\n            subsets for an index.\n        progress: Whether to display progress bars for each job.\n        seed: Either an instance of a numpy random number generator or a seed\n            for it.\n        job_id: id to use for reporting progress\n\n    Returns:\n        The results for the indices.\n    \"\"\"\n    n = len(u.data)\n\n    # Correction coming from Monte Carlo integration so that the mean of the\n    # marginals converges to the value: the uniform distribution over the\n    # powerset of a set with n-1 elements has mass 2^{n-1} over each subset. The\n    # additional factor n corresponds to the one in the Shapley definition\n    correction = 2 ** (n - 1) / n\n    result = ValuationResult.zeros(\n        algorithm=\"combinatorial_montecarlo_shapley\",\n        indices=np.array(indices, dtype=np.int_),\n        data_names=[u.data.data_names[i] for i in indices],\n    )\n\n    rng = np.random.default_rng(seed)\n\n    for idx in repeat_indices(\n        indices,\n        result=result,  # type:ignore\n        done=done,  # type:ignore\n        disable=not progress,\n        position=job_id,\n    ):\n        # Randomly sample subsets of full dataset without idx\n        subset = np.setxor1d(u.data.indices, [idx], assume_unique=True)\n        s = next(random_powerset(subset, n_samples=1, seed=rng))\n        marginal = (u({idx}.union(s)) - u(s)) / math.comb(n - 1, len(s))\n        result.update(idx, correction * marginal)\n\n    return result\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/montecarlo/#pydvl.value.shapley.montecarlo._permutation_montecarlo_one_step","title":"_permutation_montecarlo_one_step","text":"<pre><code>_permutation_montecarlo_one_step(\n    u: Utility,\n    truncation: TruncationPolicy,\n    algorithm_name: str,\n    seed: Optional[Union[Seed, SeedSequence]] = None,\n) -&gt; ValuationResult\n</code></pre> <p>Helper function for permutation_montecarlo_shapley().</p> <p>Computes marginal utilities of each training sample in a randomly sampled permutation.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>truncation</code> <p>A callable which decides whether to interrupt processing a permutation and set all subsequent marginals to zero.</p> <p> TYPE: <code>TruncationPolicy</code> </p> <code>algorithm_name</code> <p>For the results object. Used internally by different variants of Shapley using this subroutine</p> <p> TYPE: <code>str</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Union[Seed, SeedSequence]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>An object with the results</p> Source code in <code>src/pydvl/value/shapley/montecarlo.py</code> <pre><code>def _permutation_montecarlo_one_step(\n    u: Utility,\n    truncation: TruncationPolicy,\n    algorithm_name: str,\n    seed: Optional[Union[Seed, SeedSequence]] = None,\n) -&gt; ValuationResult:\n    \"\"\"Helper function for\n    [permutation_montecarlo_shapley()][pydvl.value.shapley.montecarlo.permutation_montecarlo_shapley].\n\n    Computes marginal utilities of each training sample in a randomly sampled\n    permutation.\n\n    Args:\n        u: Utility object with model, data, and scoring function\n        truncation: A callable which decides whether to interrupt\n            processing a permutation and set all subsequent marginals to zero.\n        algorithm_name: For the results object. Used internally by different\n            variants of Shapley using this subroutine\n        seed: Either an instance of a numpy random number generator or a seed\n            for it.\n\n    Returns:\n        An object with the results\n    \"\"\"\n\n    result = ValuationResult.zeros(\n        algorithm=algorithm_name, indices=u.data.indices, data_names=u.data.data_names\n    )\n    prev_score = 0.0\n    permutation = np.random.default_rng(seed).permutation(u.data.indices)\n    permutation_done = False\n    truncation.reset()\n    for i, idx in enumerate(permutation):\n        if permutation_done:\n            score = prev_score\n        else:\n            score = u(permutation[: i + 1])\n        marginal = score - prev_score\n        result.update(idx, marginal)\n        prev_score = score\n        if not permutation_done and truncation(i, score):\n            permutation_done = True\n    nans = np.isnan(result.values).sum()\n    if nans &gt; 0:\n        logger.warning(\n            f\"{nans} NaN values in current permutation, ignoring. \"\n            \"Consider setting a default value for the Scorer\"\n        )\n        result = ValuationResult.empty(algorithm=algorithm_name)\n    return result\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/montecarlo/#pydvl.value.shapley.montecarlo.combinatorial_montecarlo_shapley","title":"combinatorial_montecarlo_shapley","text":"<pre><code>combinatorial_montecarlo_shapley(\n    u: Utility,\n    done: StoppingCriterion,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>Computes an approximate Shapley value using the combinatorial definition:</p> \\[v_u(i) = \\frac{1}{n} \\sum_{S \\subseteq N \\setminus \\{i\\}} \\binom{n-1}{ | S | }^{-1} [u(S \\cup \\{i\\}) \u2212 u(S)]\\] <p>This consists of randomly sampling subsets of the power set of the training indices in u.data, and computing their marginal utilities. See Data valuation for details.</p> <p>Note that because sampling is done with replacement, the approximation is poor even for \\(2^{m}\\) subsets with \\(m&gt;n\\), even though there are \\(2^{n-1}\\) subsets for each \\(i\\). Prefer permutation_montecarlo_shapley().</p> <p>Parallelization is done by splitting the set of indices across processes and computing the sum over subsets \\(S \\subseteq N \\setminus \\{i\\}\\) separately.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>done</code> <p>Stopping criterion for the computation.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>n_jobs</code> <p>number of parallel jobs across which to distribute the computation. Each worker receives a chunk of indices</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display progress bars for each job.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/shapley/montecarlo.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef combinatorial_montecarlo_shapley(\n    u: Utility,\n    done: StoppingCriterion,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    r\"\"\"Computes an approximate Shapley value using the combinatorial\n    definition:\n\n    $$v_u(i) = \\frac{1}{n} \\sum_{S \\subseteq N \\setminus \\{i\\}}\n    \\binom{n-1}{ | S | }^{-1} [u(S \\cup \\{i\\}) \u2212 u(S)]$$\n\n    This consists of randomly sampling subsets of the power set of the training\n    indices in [u.data][pydvl.utils.utility.Utility], and computing their\n    marginal utilities. See [Data valuation][data-valuation-intro] for details.\n\n    Note that because sampling is done with replacement, the approximation is\n    poor even for $2^{m}$ subsets with $m&gt;n$, even though there are $2^{n-1}$\n    subsets for each $i$. Prefer\n    [permutation_montecarlo_shapley()][pydvl.value.shapley.montecarlo.permutation_montecarlo_shapley].\n\n    Parallelization is done by splitting the set of indices across processes and\n    computing the sum over subsets $S \\subseteq N \\setminus \\{i\\}$ separately.\n\n    Args:\n        u: Utility object with model, data, and scoring function\n        done: Stopping criterion for the computation.\n        n_jobs: number of parallel jobs across which to distribute the\n            computation. Each worker receives a chunk of\n            [indices][pydvl.utils.dataset.Dataset.indices]\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        progress: Whether to display progress bars for each job.\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        Object with the data values.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n\n    map_reduce_job: MapReduceJob[NDArray, ValuationResult] = MapReduceJob(\n        u.data.indices,\n        map_func=_combinatorial_montecarlo_shapley,\n        reduce_func=lambda results: reduce(operator.add, results),\n        map_kwargs=dict(u=u, done=done, progress=progress),\n        n_jobs=n_jobs,\n        parallel_backend=parallel_backend,\n    )\n    return map_reduce_job(seed=seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/montecarlo/#pydvl.value.shapley.montecarlo.permutation_montecarlo_shapley","title":"permutation_montecarlo_shapley","text":"<pre><code>permutation_montecarlo_shapley(\n    u: Utility,\n    done: StoppingCriterion,\n    *,\n    truncation: TruncationPolicy = NoTruncation(),\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>Computes an approximate Shapley value by sampling independent permutations of the index set, approximating the sum:</p> \\[ v_u(x_i) = \\frac{1}{n!} \\sum_{\\sigma \\in \\Pi(n)} \\tilde{w}( | \\sigma_{:i} | )[u(\\sigma_{:i} \\cup \\{i\\}) \u2212 u(\\sigma_{:i})], \\] <p>where \\(\\sigma_{:i}\\) denotes the set of indices in permutation sigma before the position where \\(i\\) appears (see [[data-valuation-intro]] for details).</p> <p>This implements the method described in (Ghorbani and Zou, 2019)<sup>1</sup> with a double stopping criterion.</p> <p>Todo</p> <p>Think of how to add Robin-Gelman or some other more principled stopping criterion.</p> <p>Instead of naively implementing the expectation, we sequentially add points to coalitions from a permutation and incrementally compute marginal utilities. We stop computing marginals for a given permutation based on a TruncationPolicy. (Ghorbani and Zou, 2019)<sup>1</sup> mention two policies: one that stops after a certain fraction of marginals are computed, implemented in FixedTruncation, and one that stops if the last computed utility (\"score\") is close to the total utility using the standard deviation of the utility as a measure of proximity, implemented in BootstrapTruncation.</p> <p>We keep sampling permutations and updating all shapley values until the StoppingCriterion returns <code>True</code>.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>done</code> <p>function checking whether computation must stop.</p> <p> TYPE: <code>StoppingCriterion</code> </p> <code>truncation</code> <p>An optional callable which decides whether to interrupt processing a permutation and set all subsequent marginals to zero. Typically used to stop computation when the marginal is small.</p> <p> TYPE: <code>TruncationPolicy</code> DEFAULT: <code>NoTruncation()</code> </p> <code>n_jobs</code> <p>number of jobs across which to distribute the computation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display a progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/shapley/montecarlo.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef permutation_montecarlo_shapley(\n    u: Utility,\n    done: StoppingCriterion,\n    *,\n    truncation: TruncationPolicy = NoTruncation(),\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    r\"\"\"Computes an approximate Shapley value by sampling independent\n    permutations of the index set, approximating the sum:\n\n    $$\n    v_u(x_i) = \\frac{1}{n!} \\sum_{\\sigma \\in \\Pi(n)}\n    \\tilde{w}( | \\sigma_{:i} | )[u(\\sigma_{:i} \\cup \\{i\\}) \u2212 u(\\sigma_{:i})],\n    $$\n\n    where $\\sigma_{:i}$ denotes the set of indices in permutation sigma before\n    the position where $i$ appears (see [[data-valuation-intro]] for details).\n\n    This implements the method described in (Ghorbani and Zou, 2019)&lt;sup&gt;&lt;a\n    href=\"#ghorbani_data_2019\"&gt;1&lt;/a&gt;&lt;/sup&gt; with a double stopping criterion.\n\n    !!! Todo\n        Think of how to add Robin-Gelman or some other more principled stopping\n        criterion.\n\n    Instead of naively implementing the expectation, we sequentially add points\n    to coalitions from a permutation and incrementally compute marginal utilities.\n    We stop computing marginals for a given permutation based on a\n    [TruncationPolicy][pydvl.value.shapley.truncated.TruncationPolicy].\n    (Ghorbani and Zou, 2019)&lt;sup&gt;&lt;a href=\"#ghorbani_data_2019\"&gt;1&lt;/a&gt;&lt;/sup&gt;\n    mention two policies: one that stops after a certain\n    fraction of marginals are computed, implemented in\n    [FixedTruncation][pydvl.value.shapley.truncated.FixedTruncation],\n    and one that stops if the last computed utility (\"score\") is close to the\n    total utility using the standard deviation of the utility as a measure of\n    proximity, implemented in\n    [BootstrapTruncation][pydvl.value.shapley.truncated.BootstrapTruncation].\n\n    We keep sampling permutations and updating all shapley values\n    until the [StoppingCriterion][pydvl.value.stopping.StoppingCriterion] returns\n    `True`.\n\n    Args:\n        u: Utility object with model, data, and scoring function.\n        done: function checking whether computation must stop.\n        truncation: An optional callable which decides whether to interrupt\n            processing a permutation and set all subsequent marginals to\n            zero. Typically used to stop computation when the marginal is small.\n        n_jobs: number of jobs across which to distribute the computation.\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        progress: Whether to display a progress bar.\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        Object with the data values.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n    algorithm = \"permutation_montecarlo_shapley\"\n\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n    u = parallel_backend.put(u)\n    max_workers = parallel_backend.effective_n_jobs(n_jobs)\n    n_submitted_jobs = 2 * max_workers  # number of jobs in the executor's queue\n\n    seed_sequence = ensure_seed_sequence(seed)\n    result = ValuationResult.zeros(\n        algorithm=algorithm, indices=u.data.indices, data_names=u.data.data_names\n    )\n\n    pbar = tqdm(disable=not progress, total=100, unit=\"%\")\n\n    with parallel_backend.executor(\n        max_workers=max_workers, cancel_futures=CancellationPolicy.ALL\n    ) as executor:\n        pending: set[Future] = set()\n        while True:\n            pbar.n = 100 * done.completion()\n            pbar.refresh()\n\n            completed, pending = wait(pending, timeout=1.0, return_when=FIRST_COMPLETED)\n            for future in completed:\n                result += future.result()\n                # we could check outside the loop, but that means more\n                # submissions if the stopping criterion is unstable\n                if done(result):\n                    return result\n\n            # Ensure that we always have n_submitted_jobs in the queue or running\n            n_remaining_slots = n_submitted_jobs - len(pending)\n            seeds = seed_sequence.spawn(n_remaining_slots)\n            for i in range(n_remaining_slots):\n                future = executor.submit(\n                    _permutation_montecarlo_one_step,\n                    u,\n                    truncation,\n                    algorithm,\n                    seed=seeds[i],\n                )\n                pending.add(future)\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/naive/","title":"Naive","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/shapley/naive/#pydvl.value.shapley.naive","title":"pydvl.value.shapley.naive","text":"<p>This module implements exact Shapley values using either the combinatorial or permutation definition.</p> <p>The exact computation of \\(n\\) values takes \\(\\mathcal{O}(2^n)\\) evaluations of the utility and is therefore only possible for small datasets. For larger datasets, consider using any of the approximations, such as Monte Carlo, or proxy models like kNN.</p> <p>See Data valuation for details.</p>"},{"location":"deprecated/pydvl/value/shapley/naive/#pydvl.value.shapley.naive._combinatorial_exact_shapley","title":"_combinatorial_exact_shapley","text":"<pre><code>_combinatorial_exact_shapley(\n    indices: NDArray[int_], u: Utility, progress: bool\n) -&gt; NDArray\n</code></pre> <p>Helper function for combinatorial_exact_shapley().</p> <p>Computes the marginal utilities for the set of indices passed and returns the value of the samples according to the exact combinatorial definition.</p> Source code in <code>src/pydvl/value/shapley/naive.py</code> <pre><code>def _combinatorial_exact_shapley(\n    indices: NDArray[np.int_], u: Utility, progress: bool\n) -&gt; NDArray:\n    \"\"\"Helper function for\n    [combinatorial_exact_shapley()][pydvl.value.shapley.naive.combinatorial_exact_shapley].\n\n    Computes the marginal utilities for the set of indices passed and returns\n    the value of the samples according to the exact combinatorial definition.\n    \"\"\"\n    n = len(u.data)\n    local_values = np.zeros(n)\n    for i in indices:\n        subset: NDArray[np.int_] = np.setxor1d(\n            u.data.indices, [i], assume_unique=True\n        ).astype(np.int_)\n        for s in tqdm(  # type: ignore\n            powerset(subset),\n            disable=not progress,\n            desc=f\"Index {i}\",\n            total=2 ** (n - 1),\n            position=0,\n        ):\n            local_values[i] += (u({i}.union(s)) - u(s)) / math.comb(n - 1, len(s))  # type: ignore\n    return local_values / n\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/naive/#pydvl.value.shapley.naive.combinatorial_exact_shapley","title":"combinatorial_exact_shapley","text":"<pre><code>combinatorial_exact_shapley(\n    u: Utility,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False\n) -&gt; ValuationResult\n</code></pre> <p>Computes the exact Shapley value using the combinatorial definition.</p> \\[v_u(i) = \\frac{1}{n} \\sum_{S \\subseteq N \\setminus \\{i\\}} \\binom{n-1}{ | S | }^{-1} [u(S \\cup \\{i\\}) \u2212 u(S)].\\] <p>See Data valuation for details.</p> <p>Note</p> <p>If the length of the training set is &gt; n_jobs*20 this prints a warning because the computation is very expensive. Used mostly for internal testing and simple use cases. Please refer to the Monte Carlo approximations for practical applications.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to use</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display progress bars for each job.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/shapley/naive.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef combinatorial_exact_shapley(\n    u: Utility,\n    *,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n) -&gt; ValuationResult:\n    r\"\"\"Computes the exact Shapley value using the combinatorial definition.\n\n    $$v_u(i) = \\frac{1}{n} \\sum_{S \\subseteq N \\setminus \\{i\\}}\n    \\binom{n-1}{ | S | }^{-1} [u(S \\cup \\{i\\}) \u2212 u(S)].$$\n\n    See [Data valuation][data-valuation-intro] for details.\n\n    !!! Note\n        If the length of the training set is &gt; n_jobs*20 this prints a warning\n        because the computation is very expensive. Used mostly for internal\n        testing and simple use cases. Please refer to the\n        [Monte Carlo][pydvl.value.shapley.montecarlo] approximations for\n        practical applications.\n\n    Args:\n        u: Utility object with model, data, and scoring function\n        n_jobs: Number of parallel jobs to use\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        progress: Whether to display progress bars for each job.\n\n    Returns:\n        Object with the data values.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n    \"\"\"\n    # Arbitrary choice, will depend on time required, caching, etc.\n    if len(u.data) // n_jobs &gt; 20:\n        warnings.warn(\n            f\"Large dataset! Computation requires 2^{len(u.data)} calls to model.fit()\"\n        )\n\n    def reduce_fun(results: List[NDArray]) -&gt; NDArray:\n        return np.array(results).sum(axis=0)  # type: ignore\n\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n\n    map_reduce_job: MapReduceJob[NDArray, NDArray] = MapReduceJob(\n        u.data.indices,\n        map_func=_combinatorial_exact_shapley,\n        map_kwargs=dict(u=u, progress=progress),\n        reduce_func=reduce_fun,\n        n_jobs=n_jobs,\n        parallel_backend=parallel_backend,\n    )\n    values = map_reduce_job()\n    return ValuationResult(\n        algorithm=\"combinatorial_exact_shapley\",\n        status=Status.Converged,\n        values=values,\n        data_names=u.data.data_names,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/naive/#pydvl.value.shapley.naive.permutation_exact_shapley","title":"permutation_exact_shapley","text":"<pre><code>permutation_exact_shapley(\n    u: Utility, *, progress: bool = True\n) -&gt; ValuationResult\n</code></pre> <p>Computes the exact Shapley value using the formulation with permutations:</p> \\[v_u(x_i) = \\frac{1}{n!} \\sum_{\\sigma \\in \\Pi(n)} [u(\\sigma_{i-1} \\cup {i}) \u2212 u(\\sigma_{i})].\\] <p>See Data valuation for details.</p> <p>When the length of the training set is &gt; 10 this prints a warning since the computation becomes too expensive. Used mostly for internal testing and simple use cases. Please refer to the Monte Carlo approximations for practical applications.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>progress</code> <p>Whether to display progress bars for each job.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values.</p> Source code in <code>src/pydvl/value/shapley/naive.py</code> <pre><code>def permutation_exact_shapley(u: Utility, *, progress: bool = True) -&gt; ValuationResult:\n    r\"\"\"Computes the exact Shapley value using the formulation with permutations:\n\n    $$v_u(x_i) = \\frac{1}{n!} \\sum_{\\sigma \\in \\Pi(n)} [u(\\sigma_{i-1}\n    \\cup {i}) \u2212 u(\\sigma_{i})].$$\n\n    See [Data valuation][data-valuation-intro] for details.\n\n    When the length of the training set is &gt; 10 this prints a warning since the\n    computation becomes too expensive. Used mostly for internal testing and\n    simple use cases. Please refer to the [Monte Carlo\n    approximations][pydvl.value.shapley.montecarlo] for practical applications.\n\n    Args:\n        u: Utility object with model, data, and scoring function\n        progress: Whether to display progress bars for each job.\n\n    Returns:\n        Object with the data values.\n    \"\"\"\n\n    n = len(u.data)\n    # Note that the cache in utility saves most of the refitting because we\n    # use frozenset for the input.\n    if n &gt; 10:\n        warnings.warn(\n            f\"Large dataset! Computation requires {n}! calls to utility()\",\n            RuntimeWarning,\n        )\n\n    values = np.zeros(n)\n    for p in tqdm(\n        permutations(u.data.indices),\n        disable=not progress,\n        desc=\"Permutation\",\n        total=math.factorial(n),\n    ):\n        for i, idx in enumerate(p):\n            values[idx] += u(p[: i + 1]) - u(p[:i])\n    values /= math.factorial(n)\n\n    return ValuationResult(\n        algorithm=\"permutation_exact_shapley\",\n        status=Status.Converged,\n        values=values,\n        data_names=u.data.data_names,\n    )\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/owen/","title":"Owen","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/shapley/owen/#pydvl.value.shapley.owen","title":"pydvl.value.shapley.owen","text":""},{"location":"deprecated/pydvl/value/shapley/owen/#pydvl.value.shapley.owen--references","title":"References","text":"<ol> <li> <p>Okhrati, R., Lipani, A., 2021. A Multilinear Sampling Algorithm to Estimate Shapley Values. In: 2020 25th International Conference on Pattern Recognition (ICPR), pp. 7992\u20137999. IEEE.\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/shapley/owen/#pydvl.value.shapley.owen.OwenAlgorithm","title":"OwenAlgorithm","text":"<p>               Bases: <code>Enum</code></p> <p>Choices for the Owen sampling method.</p> ATTRIBUTE DESCRIPTION <code>Standard</code> <p>Use q \u2208 [0, 1]</p> <p> </p> <code>Antithetic</code> <p>Use q \u2208 [0, 0.5] and correlated samples</p> <p> </p>"},{"location":"deprecated/pydvl/value/shapley/owen/#pydvl.value.shapley.owen._owen_sampling_shapley","title":"_owen_sampling_shapley","text":"<pre><code>_owen_sampling_shapley(\n    indices: Sequence[int],\n    u: Utility,\n    method: OwenAlgorithm,\n    n_samples: int,\n    max_q: int,\n    *,\n    progress: bool = False,\n    job_id: int = 1,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>This is the algorithm as detailed in the paper: to compute the outer integral over q \u2208 [0,1], use uniformly distributed points for evaluation of the integrand. For the integrand (the expected marginal utility over the power set), use Monte Carlo.</p> <p>Todo</p> <p>We might want to try better quadrature rules like Gauss or Rombert or use Monte Carlo for the double integral.</p> PARAMETER DESCRIPTION <code>indices</code> <p>Indices to compute the value for</p> <p> TYPE: <code>Sequence[int]</code> </p> <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>method</code> <p>Either OwenAlgorithm.Full for q \u2208 [0, 1] or OwenAlgorithm.Halved for q \u2208 [0, 0.5] and correlated samples</p> <p> TYPE: <code>OwenAlgorithm</code> </p> <code>n_samples</code> <p>Number of subsets to sample to estimate the integrand</p> <p> TYPE: <code>int</code> </p> <code>max_q</code> <p>number of subdivisions for the integration over \\(q\\)</p> <p> TYPE: <code>int</code> </p> <code>progress</code> <p>Whether to display progress bars for each job</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>job_id</code> <p>For positioning of the progress bar</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values, errors.</p> Source code in <code>src/pydvl/value/shapley/owen.py</code> <pre><code>def _owen_sampling_shapley(\n    indices: Sequence[int],\n    u: Utility,\n    method: OwenAlgorithm,\n    n_samples: int,\n    max_q: int,\n    *,\n    progress: bool = False,\n    job_id: int = 1,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    r\"\"\"This is the algorithm as detailed in the paper: to compute the outer\n    integral over q \u2208 [0,1], use uniformly distributed points for evaluation\n    of the integrand. For the integrand (the expected marginal utility over the\n    power set), use Monte Carlo.\n\n    !!! Todo\n        We might want to try better quadrature rules like Gauss or Rombert or\n        use Monte Carlo for the double integral.\n\n    Args:\n        indices: Indices to compute the value for\n        u: Utility object with model, data, and scoring function\n        method: Either [OwenAlgorithm.Full][pydvl.value.shapley.owen.OwenAlgorithm]\n            for q \u2208 [0, 1] or [OwenAlgorithm.Halved][pydvl.value.shapley.owen.OwenAlgorithm]\n            for q \u2208 [0, 0.5] and correlated samples\n        n_samples: Number of subsets to sample to estimate the integrand\n        max_q: number of subdivisions for the integration over $q$\n        progress: Whether to display progress bars for each job\n        job_id: For positioning of the progress bar\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        Object with the data values, errors.\n    \"\"\"\n    q_stop = {OwenAlgorithm.Standard: 1.0, OwenAlgorithm.Antithetic: 0.5}\n    q_steps = np.linspace(start=0, stop=q_stop[method], num=max_q)\n\n    result = ValuationResult.zeros(\n        algorithm=\"owen_sampling_shapley_\" + str(method),\n        indices=np.array(indices, dtype=np.int_),\n        data_names=u.data.data_names[indices],\n    )\n\n    rng = np.random.default_rng(seed)\n    done = MinUpdates(1)\n\n    for idx in repeat_indices(\n        indices,\n        result=result,  # type:ignore\n        done=done,  # type:ignore\n        disable=not progress,\n        position=job_id,\n    ):\n        e = np.zeros(max_q)\n        subset = np.setxor1d(u.data.indices, [idx], assume_unique=True)\n        for j, q in enumerate(q_steps):\n            for s in random_powerset(subset, n_samples=n_samples, q=q, seed=rng):\n                marginal = u({idx}.union(s)) - u(s)\n                if method == OwenAlgorithm.Antithetic:\n                    s_complement = np.setxor1d(subset, s, assume_unique=True)\n                    marginal += u({idx}.union(s_complement)) - u(s_complement)\n                    marginal /= 2\n                e[j] += marginal\n        e /= n_samples\n        result.update(idx, e.mean())\n        # Trapezoidal rule\n        # TODO: investigate whether this or other quadrature rules are better\n        #  than a simple average\n        # result.update(idx, (e[:-1] + e[1:]).sum() / (2 * max_q))\n\n    return result\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/owen/#pydvl.value.shapley.owen.owen_sampling_shapley","title":"owen_sampling_shapley","text":"<pre><code>owen_sampling_shapley(\n    u: Utility,\n    n_samples: int,\n    max_q: int,\n    *,\n    method: OwenAlgorithm = Standard,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None\n) -&gt; ValuationResult\n</code></pre> <p>Owen sampling of Shapley values as described in (Okhrati and Lipani, 2021)<sup>1</sup>.</p> <p>This function computes a Monte Carlo approximation to</p> \\[v_u(i) = \\int_0^1 \\mathbb{E}_{S \\sim P_q(D_{\\backslash \\{i\\}})} [u(S \\cup \\{i\\}) - u(S)]\\] <p>using one of two methods. The first one, selected with the argument <code>mode = OwenAlgorithm.Standard</code>, approximates the integral with:</p> \\[\\hat{v}_u(i) = \\frac{1}{Q M} \\sum_{j=0}^Q \\sum_{m=1}^M [u(S^{(q_j)}_m \\cup \\{i\\}) - u(S^{(q_j)}_m)],\\] <p>where \\(q_j = \\frac{j}{Q} \\in [0,1]\\) and the sets \\(S^{(q_j)}\\) are such that a sample \\(x \\in S^{(q_j)}\\) if a draw from a \\(Ber(q_j)\\) distribution is 1.</p> <p>The second method, selected with the argument <code>mode = OwenAlgorithm.Antithetic</code>, uses correlated samples in the inner sum to reduce the variance:</p> \\[\\hat{v}_u(i) = \\frac{1}{2 Q M} \\sum_{j=0}^Q \\sum_{m=1}^M [u(S^{(q_j)}_m \\cup \\{i\\}) - u(S^{(q_j)}_m) + u((S^{(q_j)}_m)^c \\cup \\{i\\}) - u((S^{( q_j)}_m)^c)],\\] <p>where now \\(q_j = \\frac{j}{2Q} \\in [0,\\frac{1}{2}]\\), and \\(S^c\\) is the complement of \\(S\\).</p> <p>Note</p> <p>The outer integration could be done instead with a quadrature rule.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object holding data, model and scoring function.</p> <p> TYPE: <code>Utility</code> </p> <code>n_samples</code> <p>Numer of sets to sample for each value of q</p> <p> TYPE: <code>int</code> </p> <code>max_q</code> <p>Number of subdivisions for q \u2208 [0,1] (the element sampling probability) used to approximate the outer integral.</p> <p> TYPE: <code>int</code> </p> <code>method</code> <p>Selects the algorithm to use, see the description. Either OwenAlgorithm.Full for \\(q \\in [0,1]\\) or OwenAlgorithm.Halved for \\(q \\in [0,0.5]\\) and correlated samples</p> <p> TYPE: <code>OwenAlgorithm</code> DEFAULT: <code>Standard</code> </p> <code>n_jobs</code> <p>Number of parallel jobs to use. Each worker receives a chunk of the total of <code>max_q</code> values for q.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>parallel_backend</code> <p>Parallel backend instance to use for parallelizing computations. If <code>None</code>, use JoblibParallelBackend backend. See the Parallel Backends package for available options.</p> <p> TYPE: <code>Optional[ParallelBackend]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>(DEPRECATED) Object configuring parallel computation, with cluster address, number of cpus, etc.</p> <p> TYPE: <code>Optional[ParallelConfig]</code> DEFAULT: <code>None</code> </p> <code>progress</code> <p>Whether to display progress bars for each job.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>Either an instance of a numpy random number generator or a seed for it.</p> <p> TYPE: <code>Optional[Seed]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ValuationResult</code> <p>Object with the data values.</p> <p>New in version 0.3.0</p> <p>Changed in version 0.5.0</p> <p>Support for parallel computation and enable antithetic sampling.</p> <p>Changed in version 0.9.0</p> <p>Deprecated <code>config</code> argument and added a <code>parallel_backend</code> argument to allow users to pass the Parallel Backend instance directly.</p> Source code in <code>src/pydvl/value/shapley/owen.py</code> <pre><code>@deprecated(\n    target=True,\n    args_mapping={\"config\": \"config\"},\n    deprecated_in=\"0.9.0\",\n    remove_in=\"0.10.0\",\n)\ndef owen_sampling_shapley(\n    u: Utility,\n    n_samples: int,\n    max_q: int,\n    *,\n    method: OwenAlgorithm = OwenAlgorithm.Standard,\n    n_jobs: int = 1,\n    parallel_backend: Optional[ParallelBackend] = None,\n    config: Optional[ParallelConfig] = None,\n    progress: bool = False,\n    seed: Optional[Seed] = None,\n) -&gt; ValuationResult:\n    r\"\"\"Owen sampling of Shapley values as described in\n    (Okhrati and Lipani, 2021)&lt;sup&gt;&lt;a href=\"#okhrati_multilinear_2021\"&gt;1&lt;/a&gt;&lt;/sup&gt;.\n\n    This function computes a Monte Carlo approximation to\n\n    $$v_u(i) = \\int_0^1 \\mathbb{E}_{S \\sim P_q(D_{\\backslash \\{i\\}})}\n    [u(S \\cup \\{i\\}) - u(S)]$$\n\n    using one of two methods. The first one, selected with the argument ``mode =\n    OwenAlgorithm.Standard``, approximates the integral with:\n\n    $$\\hat{v}_u(i) = \\frac{1}{Q M} \\sum_{j=0}^Q \\sum_{m=1}^M [u(S^{(q_j)}_m\n    \\cup \\{i\\}) - u(S^{(q_j)}_m)],$$\n\n    where $q_j = \\frac{j}{Q} \\in [0,1]$ and the sets $S^{(q_j)}$ are such that a\n    sample $x \\in S^{(q_j)}$ if a draw from a $Ber(q_j)$ distribution is 1.\n\n    The second method, selected with the argument ``mode =\n    OwenAlgorithm.Antithetic``, uses correlated samples in the inner sum to\n    reduce the variance:\n\n    $$\\hat{v}_u(i) = \\frac{1}{2 Q M} \\sum_{j=0}^Q \\sum_{m=1}^M [u(S^{(q_j)}_m\n    \\cup \\{i\\}) - u(S^{(q_j)}_m) + u((S^{(q_j)}_m)^c \\cup \\{i\\}) - u((S^{(\n    q_j)}_m)^c)],$$\n\n    where now $q_j = \\frac{j}{2Q} \\in [0,\\frac{1}{2}]$, and $S^c$ is the\n    complement of $S$.\n\n    !!! Note\n        The outer integration could be done instead with a quadrature rule.\n\n    Args:\n        u: [Utility][pydvl.utils.utility.Utility] object holding data, model\n            and scoring function.\n        n_samples: Numer of sets to sample for each value of q\n        max_q: Number of subdivisions for q \u2208 [0,1] (the element sampling\n            probability) used to approximate the outer integral.\n        method: Selects the algorithm to use, see the description. Either\n            [OwenAlgorithm.Full][pydvl.value.shapley.owen.OwenAlgorithm] for\n            $q \\in [0,1]$ or\n            [OwenAlgorithm.Halved][pydvl.value.shapley.owen.OwenAlgorithm] for\n            $q \\in [0,0.5]$ and correlated samples\n        n_jobs: Number of parallel jobs to use. Each worker receives a chunk\n            of the total of `max_q` values for q.\n        parallel_backend: Parallel backend instance to use\n            for parallelizing computations. If `None`,\n            use [JoblibParallelBackend][pydvl.parallel.backends.JoblibParallelBackend] backend.\n            See the [Parallel Backends][pydvl.parallel.backends] package\n            for available options.\n        config: (**DEPRECATED**) Object configuring parallel computation,\n            with cluster address, number of cpus, etc.\n        progress: Whether to display progress bars for each job.\n        seed: Either an instance of a numpy random number generator or a seed for it.\n\n    Returns:\n        Object with the data values.\n\n    !!! tip \"New in version 0.3.0\"\n\n    !!! tip \"Changed in version 0.5.0\"\n        Support for parallel computation and enable antithetic sampling.\n\n    !!! tip \"Changed in version 0.9.0\"\n        Deprecated `config` argument and added a `parallel_backend`\n        argument to allow users to pass the Parallel Backend instance\n        directly.\n\n    \"\"\"\n    parallel_backend = _maybe_init_parallel_backend(parallel_backend, config)\n\n    map_reduce_job: MapReduceJob[NDArray, ValuationResult] = MapReduceJob(\n        u.data.indices,\n        map_func=_owen_sampling_shapley,\n        reduce_func=lambda results: reduce(operator.add, results),\n        map_kwargs=dict(\n            u=u,\n            method=OwenAlgorithm(method),\n            n_samples=n_samples,\n            max_q=max_q,\n            progress=progress,\n        ),\n        n_jobs=n_jobs,\n        parallel_backend=parallel_backend,\n    )\n\n    return map_reduce_job(seed=seed)\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/","title":"Truncated","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated","title":"pydvl.value.shapley.truncated","text":""},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated--references","title":"References","text":"<ol> <li> <p>Ghorbani, A., Zou, J., 2019. Data Shapley: Equitable Valuation of Data for Machine Learning. In: Proceedings of the 36th International Conference on Machine Learning, PMLR, pp. 2242\u20132251.\u00a0\u21a9</p> </li> </ol>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.BootstrapTruncation","title":"BootstrapTruncation","text":"<pre><code>BootstrapTruncation(u: Utility, n_samples: int, sigmas: float = 1)\n</code></pre> <p>               Bases: <code>TruncationPolicy</code></p> <p>Break a permutation if the last computed utility is close to the total utility, measured as a multiple of the standard deviation of the utilities.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>n_samples</code> <p>Number of bootstrap samples to use to compute the variance of the utilities.</p> <p> TYPE: <code>int</code> </p> <code>sigmas</code> <p>Number of standard deviations to use as a threshold.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>def __init__(self, u: Utility, n_samples: int, sigmas: float = 1):\n    super().__init__()\n    self.n_samples = n_samples\n    logger.info(\"Computing total utility for permutation truncation.\")\n    self.total_utility = u(u.data.indices)\n    self.count: int = 0\n    self.variance: float = 0\n    self.mean: float = 0\n    self.sigmas: float = sigmas\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.BootstrapTruncation.__call__","title":"__call__","text":"<pre><code>__call__(idx: int, score: float) -&gt; bool\n</code></pre> <p>Check whether the computation should be interrupted.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Position in the permutation currently being computed.</p> <p> TYPE: <code>int</code> </p> <code>score</code> <p>Last utility computed.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the computation should be interrupted.</p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>def __call__(self, idx: int, score: float) -&gt; bool:\n    \"\"\"Check whether the computation should be interrupted.\n\n    Args:\n        idx: Position in the permutation currently being computed.\n        score: Last utility computed.\n\n    Returns:\n        `True` if the computation should be interrupted.\n    \"\"\"\n    ret = self._check(idx, score)\n    self.n_calls += 1\n    self.n_truncations += 1 if ret else 0\n    return ret\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.FixedTruncation","title":"FixedTruncation","text":"<pre><code>FixedTruncation(u: Utility, fraction: float)\n</code></pre> <p>               Bases: <code>TruncationPolicy</code></p> <p>Break a permutation after computing a fixed number of marginals.</p> <p>The experiments in Appendix B of (Ghorbani and Zou, 2019)<sup>1</sup> show that when the training set size is large enough, one can simply truncate the iteration over permutations after a fixed number of steps. This happens because beyond a certain number of samples in a training set, the model becomes insensitive to new ones. Alas, this strongly depends on the data distribution and the model and there is no automatic way of estimating this number.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>fraction</code> <p>Fraction of marginals in a permutation to compute before stopping (e.g. 0.5 to compute half of the marginals).</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>def __init__(self, u: Utility, fraction: float):\n    super().__init__()\n    if fraction &lt;= 0 or fraction &gt; 1:\n        raise ValueError(\"fraction must be in (0, 1]\")\n    self.max_marginals = len(u.data) * fraction\n    self.count = 0\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.FixedTruncation.__call__","title":"__call__","text":"<pre><code>__call__(idx: int, score: float) -&gt; bool\n</code></pre> <p>Check whether the computation should be interrupted.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Position in the permutation currently being computed.</p> <p> TYPE: <code>int</code> </p> <code>score</code> <p>Last utility computed.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the computation should be interrupted.</p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>def __call__(self, idx: int, score: float) -&gt; bool:\n    \"\"\"Check whether the computation should be interrupted.\n\n    Args:\n        idx: Position in the permutation currently being computed.\n        score: Last utility computed.\n\n    Returns:\n        `True` if the computation should be interrupted.\n    \"\"\"\n    ret = self._check(idx, score)\n    self.n_calls += 1\n    self.n_truncations += 1 if ret else 0\n    return ret\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.NoTruncation","title":"NoTruncation","text":"<pre><code>NoTruncation()\n</code></pre> <p>               Bases: <code>TruncationPolicy</code></p> <p>A policy which never interrupts the computation.</p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.n_calls: int = 0\n    self.n_truncations: int = 0\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.NoTruncation.__call__","title":"__call__","text":"<pre><code>__call__(idx: int, score: float) -&gt; bool\n</code></pre> <p>Check whether the computation should be interrupted.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Position in the permutation currently being computed.</p> <p> TYPE: <code>int</code> </p> <code>score</code> <p>Last utility computed.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the computation should be interrupted.</p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>def __call__(self, idx: int, score: float) -&gt; bool:\n    \"\"\"Check whether the computation should be interrupted.\n\n    Args:\n        idx: Position in the permutation currently being computed.\n        score: Last utility computed.\n\n    Returns:\n        `True` if the computation should be interrupted.\n    \"\"\"\n    ret = self._check(idx, score)\n    self.n_calls += 1\n    self.n_truncations += 1 if ret else 0\n    return ret\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.RelativeTruncation","title":"RelativeTruncation","text":"<pre><code>RelativeTruncation(u: Utility, rtol: float)\n</code></pre> <p>               Bases: <code>TruncationPolicy</code></p> <p>Break a permutation if the marginal utility is too low.</p> <p>This is called \"performance tolerance\" in (Ghorbani and Zou, 2019)<sup>1</sup>.</p> PARAMETER DESCRIPTION <code>u</code> <p>Utility object with model, data, and scoring function</p> <p> TYPE: <code>Utility</code> </p> <code>rtol</code> <p>Relative tolerance. The permutation is broken if the last computed utility is less than <code>total_utility * rtol</code>.</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>def __init__(self, u: Utility, rtol: float):\n    super().__init__()\n    self.rtol = rtol\n    logger.info(\"Computing total utility for permutation truncation.\")\n    self.total_utility = self.reset(u)\n    self._u = u\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.RelativeTruncation.__call__","title":"__call__","text":"<pre><code>__call__(idx: int, score: float) -&gt; bool\n</code></pre> <p>Check whether the computation should be interrupted.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Position in the permutation currently being computed.</p> <p> TYPE: <code>int</code> </p> <code>score</code> <p>Last utility computed.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the computation should be interrupted.</p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>def __call__(self, idx: int, score: float) -&gt; bool:\n    \"\"\"Check whether the computation should be interrupted.\n\n    Args:\n        idx: Position in the permutation currently being computed.\n        score: Last utility computed.\n\n    Returns:\n        `True` if the computation should be interrupted.\n    \"\"\"\n    ret = self._check(idx, score)\n    self.n_calls += 1\n    self.n_truncations += 1 if ret else 0\n    return ret\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.TruncationPolicy","title":"TruncationPolicy","text":"<pre><code>TruncationPolicy()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A policy for deciding whether to stop computing marginals in a permutation.</p> <p>Statistics are kept on the number of calls and truncations as <code>n_calls</code> and <code>n_truncations</code> respectively.</p> ATTRIBUTE DESCRIPTION <code>n_calls</code> <p>Number of calls to the policy.</p> <p> TYPE: <code>int</code> </p> <code>n_truncations</code> <p>Number of truncations made by the policy.</p> <p> TYPE: <code>int</code> </p> <p>Todo</p> <p>Because the policy objects are copied to the workers, the statistics are not accessible from the coordinating process. We need to add methods for this.</p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.n_calls: int = 0\n    self.n_truncations: int = 0\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.TruncationPolicy.__call__","title":"__call__","text":"<pre><code>__call__(idx: int, score: float) -&gt; bool\n</code></pre> <p>Check whether the computation should be interrupted.</p> PARAMETER DESCRIPTION <code>idx</code> <p>Position in the permutation currently being computed.</p> <p> TYPE: <code>int</code> </p> <code>score</code> <p>Last utility computed.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the computation should be interrupted.</p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>def __call__(self, idx: int, score: float) -&gt; bool:\n    \"\"\"Check whether the computation should be interrupted.\n\n    Args:\n        idx: Position in the permutation currently being computed.\n        score: Last utility computed.\n\n    Returns:\n        `True` if the computation should be interrupted.\n    \"\"\"\n    ret = self._check(idx, score)\n    self.n_calls += 1\n    self.n_truncations += 1 if ret else 0\n    return ret\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.TruncationPolicy._check","title":"_check  <code>abstractmethod</code>","text":"<pre><code>_check(idx: int, score: float) -&gt; bool\n</code></pre> <p>Implement the policy.</p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>@abc.abstractmethod\ndef _check(self, idx: int, score: float) -&gt; bool:\n    \"\"\"Implement the policy.\"\"\"\n    ...\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/truncated/#pydvl.value.shapley.truncated.TruncationPolicy.reset","title":"reset  <code>abstractmethod</code>","text":"<pre><code>reset(u: Optional[Utility] = None)\n</code></pre> <p>Reset the policy to a state ready for a new permutation.</p> Source code in <code>src/pydvl/value/shapley/truncated.py</code> <pre><code>@abc.abstractmethod\ndef reset(self, u: Optional[Utility] = None):\n    \"\"\"Reset the policy to a state ready for a new permutation.\"\"\"\n    ...\n</code></pre>"},{"location":"deprecated/pydvl/value/shapley/types/","title":"Types","text":"<p>Deprecation notice</p> <p>This module is deprecated since v0.10.0    in favor of pydvl.valuation.</p>"},{"location":"deprecated/pydvl/value/shapley/types/#pydvl.value.shapley.types","title":"pydvl.value.shapley.types","text":""},{"location":"deprecated/pydvl/value/shapley/types/#pydvl.value.shapley.types.ShapleyMode","title":"ShapleyMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported algorithms for the computation of Shapley values.</p> <p>Todo</p> <p>Make algorithms register themselves here.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#data-valuation-example-gallery","title":"Data valuation","text":"<ul> <li> <p>Shapley values</p> <p>An introduction using the spotify dataset, showcasing grouped datasets and applied to improving model performance and identifying bogus data.</p> <p></p> </li> <li> <p>KNN Shapley</p> <p>A showcase of a fast model-specific valuation method using the iris dataset. </p> <p></p> </li> <li> <p>Data utility learning</p> <p>Learning a utility function from a few evaluations and using it to estimate the value of the remaining data.</p> <p></p> </li> <li> <p>Least Core</p> <p>An alternative solution concept from game theory, illustrated on a classification problem.</p> <p></p> </li> <li> <p>Data OOB</p> <p>A different and fast strategy for data valuation, using the out-of-bag error of a bagging model.</p> <p></p> </li> <li> <p>Faster Banzhaf values</p> <p>Using Banzhaf values to estimate the value of data points in MNIST, and evaluating convergence speed of MSR. </p> <p></p> </li> </ul>"},{"location":"examples/#influence-functions","title":"Influence functions","text":"<ul> <li> <p>For CNNs</p> <p>Detecting corrupted labels with influence functions on the ImageNet dataset.</p> <p></p> </li> <li> <p>For language models</p> <p>Using the IMDB dataset for sentiment analysis and a fine-tuned BERT model.</p> <p></p> </li> <li> <p>For mislabeled data</p> <p>Detecting corrupted labels using a synthetic dataset.</p> <p></p> </li> <li> <p>For outlier detection</p> <p>Using the wine dataset</p> <p></p> </li> </ul>"},{"location":"examples/data_oob/","title":"Data OOB","text":"<p>     This notebook illustrates the use of           Data-             OOB            from Kwon and Zou \"           Data-             OOB            : Out-of-bag Estimate as a Simple and Efficient Data Value          \" (ICML 2023), to compute values for bagged models.    </p> <p>     We will work with the           adult classification dataset          from the UCI repository. It's an imbalanced dataset where the objective is to predict whether a person earns more than $50K a year (the \"positive\" class) based on a set of features such as age, education, occupation, etc. After training a random forest on this dataset, we will compute the Data-           OOB          values and analyze them.    </p> <p>     We begin by importing the main libraries and setting some defaults.    </p> <pre><code>import os\nimport random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom support.common import load_adult_data\n\nfrom pydvl.reporting.plots import plot_ci_array, plot_ci_values\n\nmatplotlib.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\nMEAN_COLORS = [\"dodgerblue\", \"indianred\", \"limegreen\", \"darkorange\", \"darkorchid\"]\nSHADE_COLORS = [\"lightskyblue\", \"firebrick\", \"seagreen\", \"gold\", \"plum\"]\n\nis_CI = os.environ.get(\"CI\")\nrandom_state = 42\nrandom.seed(random_state);\n</code></pre> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\nfrom pydvl.valuation import DataOOBValuation, Dataset\n</code></pre> <p>     With a helper function we download the data, encode the categorical variables using           TargetEncoder          , and split it into training and testing sets. We must be careful to stratify the split by the target variable (income).    </p> <pre><code>n_est = 50  # Number of estimators for the random forest\nmax_samples = 0.2  # Use small bootstrap samples\nn_jobs = 18\nn_runs = 10\ntrain_size = 0.6\ntrain, test = load_adult_data(\n    train_size=train_size, subsample=0.2, random_state=random_state\n)\n</code></pre> <p>     Usually we would carefully look at the features, check for missing values, outliers, etc. But for the sake of this example, we will skip this step and jump straight into training a model. We will only look at the class distribution since it will matter later:    </p> <pre>\n<code>3602 samples. Class distribution: 25.0% positive, 75.0% negative\n</code>\n</pre> <p>     As a quick baseline, we train a standard sklearn           RandomForestClassifier          . Since the dataset is imbalanced, besides the accuracy we look at the confusion matrix, and notice that despite weighting the class by their inverse frequency with     <code>      class_weight=\"balanced\"     </code>     , the model is not very good at predicting the minority (\"positive\", or \"1\") class: in the left-hand side of the figure below we see a high rate of false negatives. This will play a role later in how we interpret the values that Data-           OOB          returns, and requires us to address the imbalance in the dataset. We do this with a simple random over sampling using     <code>      imblearn     </code>     's           RandomOverSampler          class.    </p> <pre><code>from imblearn.over_sampling import RandomOverSampler\n\nmodel = RandomForestClassifier(\n    n_estimators=n_est,\n    max_samples=max_samples,\n    class_weight=\"balanced\",\n    random_state=random_state,\n)\nmodel.fit(*train.data())\nbase_predictions = model.predict(test.data().x)\n\nsampler = RandomOverSampler(random_state=random_state)\nresampled_x, resampled_y = sampler.fit_resample(*train.data())\ntrain = Dataset(resampled_x, resampled_y, train.feature_names, train.target_names)\n\nmodel.fit(*train.data())\npredictions_oversampled = model.predict(test.data().x)\n</code></pre> <p>     After oversampling, the class distribution is balanced:    </p> <pre>\n<code>5406 samples. Class distribution: 50.0% positive, 50.0% negative\n</code>\n</pre> <pre><code>n_estimators = [50, 100, 200]\noob_values = []\nfor i, n_est in enumerate(n_estimators, start=1):\n    model = RandomForestClassifier(\n        n_estimators=n_est,\n        max_samples=max_samples,\n        class_weight=\"balanced\",\n        random_state=random_state,\n    )\n\n    model.fit(*train.data())\n    accuracy = model.score(*test.data())\n    print(f\"Accuracy with {n_est} estimators: {accuracy:.2f}\")\n    valuation = DataOOBValuation(model)\n    valuation.fit(train)\n    oob_values.append(valuation.values())\n</code></pre> <p>     To see this, focus on the long tails with zero variance. These are samples for which the score           \\(T(y_i, \\hat{f}_b(x_i)) = 1\\)          for           every          estimator           \\(\\hat{f}_b\\)          not trained on them, that is:           every weak learner in the ensemble correctly classifies these samples          . As we said above, this can happen because it is likely for weak estimators to be fitted to always predict the majority (negative) class:    </p> <pre>\n<code>There are 914 points with value 1 and zero variance (16.91% of the data).\nOf these, 77.46% are in the majority class.\n</code>\n</pre> <p>     Simply put, the ensemble is mostly good at classifying the majority class, and the variance of the           OOB          score for these samples is very low. This is a common issue in imbalanced datasets, and it is one of the reasons why the           OOB          score might not be a good metric for model performance in these cases. For us, it shows that the values reflect only poorly fit models. We test this at the end of the notebook by ensembling a number of constant classifiers.    </p> <p>     But first, let's evaluate the conclusion of the previous discussion: at first sight, it seems that for this imbalanced dataset and poorly performing model, the usual intuition that extreme values characterize \"easy\" or \"hard\" points might be bogus.    </p> <p>     In order to test this idea, we will use a standard experiment in the data valuation literature.    </p> <p>     We observe that if we discard the datapoints with the highest values, i.e. those which the ensemble of weak learners classifies correctly every time (maybe because we believe that those are trivial in some sense, e.g. repeated) and bias the ensemble towards stricter decisions boundaries, we obtain very mild changes in performance, even after removing 50% of the data. This is consistent with the fact that the ensemble is mostly good at classifying the majority class.    </p> <p>     However, we see a more pronounced sensitivity of the model to the removal of low-valued points, with an average drop of 5% in accuracy after removing 50% of the data with the lowest value, as opposed to barely any change when removing data at random. This is consistent with the fact that the ensemble is not very good at classifying the minority class, and the low-valued points are those that the ensemble gets wrong most of the time.    </p> <pre><code>from sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# We want to use the KNN classifier as model for the utility, not the bagged model\ndef make_knn_utility(test: Dataset, random_state: int) -&amp;gt; ModelUtility:\n    return KNNClassifierUtility(KNeighborsClassifier(n_neighbors=10), test)\n\n\ndef make_oob_knn(train: Dataset, random_state: int) -&amp;gt; DataOOBValuation:\n    model = BaggingClassifier(\n        estimator=KNeighborsClassifier(n_neighbors=10),\n        max_samples=0.4,\n        n_estimators=10,\n        random_state=random_state,\n    )\n    model.fit(*train.data())\n    return DataOOBValuation(model, point_wise_accuracy)\n\n\nlow_scores_df, high_scores_df = run_removal_experiment(\n    data_factory=make_data,\n    utility_factory=make_knn_utility,\n    valuation_factories=[make_random, make_oob_knn],\n    removal_percentages=removal_percentages,\n    n_runs=n_runs,\n    n_jobs=n_jobs,\n)\n</code></pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [05:53&lt;00:00, 17.69s/%]  \n</code>\n</pre> <p>     As we can see by the changes in performance, bagging the model and looking at           OOB          information to rank data points with Data-           OOB          does provide valuable information for data inspection, cleaning, etc. even when the model is not a bagging model. However, the cost of fitting the bagging model might be prohibitive in some cases.    </p>"},{"location":"examples/data_oob/#data-oob-for-random-forests-and-bagged-classifiers","title":"Data-           OOB          for random forests and bagged classifiers","text":""},{"location":"examples/data_oob/#setup","title":"Setup","text":"If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience."},{"location":"examples/data_oob/#computing-the-oob-values","title":"Computing the           OOB          values","text":"<p>     The main idea of Data-           OOB          is to use the out-of-bag error estimates of a bagged model to compute data values. In pyDVL, we provide a class           DataOOBValuation          that takes an existing classification or regression bagging model and uses the per-sample out-of-bag performance estimate for the value of each point.    </p> <p>     Let's compute and compare the Data-           OOB          values with three choices for the number of estimators of a random forest. After fitting the random forest, we use the           fit          method to compute the values and store them in           ValuationResult          objects.    </p> <p>     Even though it's not relevant to our discussion, notice how the accuracy barely changes with the number of estimators. Below, we will discuss using the values to identify \"easy\" or \"hard\" samples in the dataset, but first let's quickly look at the values themselves.    </p>"},{"location":"examples/data_oob/#the-distribution-of-values","title":"The distribution of values","text":"<p>     The left-hand side of the figure below depicts value as it increases with rank and a 95% t-confidence interval. The right-hand side shows the histogram of values.    </p> <p>     We observe a long tail of high values. This is because the score           \\(T\\)          used in Data-           OOB          (accuracy in this case) is a binary variable, and the value           \\(\\psi_i\\)          is the fraction of times that all weak learners not trained on the           \\(i\\)          -th point classify it correctly. Given the imbalance in the dataset, many learners will always predict the majority (\"negative\", &lt; $50K earnings / year) class and be correct on 75% of the dataset, leading to this tail.    </p> <p>     Besides the actual value           \\(\\psi_i\\)          ,           ValuationResult          objects store the number of times a sample is           OOB          \u2014the quantity           \\(\\sum_{b=1}^{B} \\mathbb{1} (w_{bi}=0)\\)          \u2014 in the     <code>      counts     </code>     attribute, and the variance of the           OOB          score in the     <code>      variances     </code>     attribute. We use the latter in the plot below in order to display the confidence intervals, but it is important to note that the interpretation varies from one valuation method to another:    </p> <p>     For Shapley-based valuation methods, the variance is that of the marginal changes in the performance of the model when trained on subsets of the data with and without a sample, evaluated on a fixed valuation dataset, and could in principle be used to see whether values have (roughly) converged. But for Data-           OOB          , it is the variance of the performance of the ensemble of weak learners on the sample when it is           OOB          . Although similar in spirit, the construction is different and can be misleading.    </p> <p>     As a matter of fact, the interpretation of the vanishing variance at the tail has little to do with valuation and everything to do with our dataset, as introduced above: As the number of estimators increases, the chance of all of them failing on the same points decreases, up to a point. The same happens when we increase the maximum depth (try it!). This behaviour is then not a deep property of Data-           OOB          from which to gain new insights, but rather a consequence of the dataset and the model, as we further elaborate below.    </p> <p>       Note that a symmetric CI is actually incorrect in this situation since all values are bounded between 0 and 1 (instead we could use a bootstrapped CI if we stored the scores of all estimators in the bagging model, but this is not implemented in pyDVL).      </p>"},{"location":"examples/data_oob/#evaluating-data-oob-values-with-data-removal","title":"Evaluating Data-           OOB          values with data removal","text":"<p>     We can systematically evaluate the impact of removing data points with high or low values on the model's performance. If the values are meaningful, we should see a significant change in the model's performance. This is a common practice in the literature.    </p> <p>     PyDVL provides           run_removal_experiment          , which is a convenience function to compute the performance of the model after removing a fraction of the data with the highest or lowest values.    </p> <p>     The details are hidden in the version of this notebook rendered for the documentation, please refer to the actual notebook for the full code.    </p>"},{"location":"examples/data_oob/#using-data-oob-with-arbitrary-models","title":"Using Data-           OOB          with arbitrary models","text":"<p>     Note that even though the method is designed for bagging models, in principle it can be used with any other estimator by fitting a bagging model on top of it. This can generally be quite expensive, but it might prove useful in some cases. Below is what happens when we do this with a k-nearest neighbors classifier.    </p>"},{"location":"examples/data_oob/#appendix-a-detour-to-further-interpret-the-oob-values","title":"Appendix: A detour to further interpret the           OOB          values","text":"<p>     We can verify that the           OOB          values in our case reflect the imbalance of the dataset by training a           BaggingClassifier          with constant estimators. A fraction of     <code>      n_estimators     </code>     will always pick class 1, and the rest class 0. This leads to a clear jump in the value rank plot, either around 25% or 75% of them, since, as we saw above, 25% of the samples are in the positive (\"1\") class, and 75% in the negative (\"0\").    </p> <p>     We will use three different probabilities for the constant estimators to predict class 0: 0.01, 0.5, and 0.99. Again, the idea is that the           OOB          values will reflect the class distribution of the dataset, and we should see a clear jump in the values around 25% and 75% of the data.    </p> <p>     The code is analogous to the above when we fitted the random forest, so it is ommitted from the documentation, but we use a custom class     <code>      ConstantBinaryClassifier     </code>     as base estimator.    </p>"},{"location":"examples/influence_imagenet/","title":"For CNNs","text":"If you are reading this in the documentation, some boilerplate has been omitted for convenience.     <pre><code>%matplotlib inline\nimport logging\nimport os\nfrom typing import Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom support.common import (\n    compute_mean_corrupted_influences,\n    corrupt_imagenet,\n    load_preprocess_imagenet,\n    plot_corrupted_influences_distribution,\n    plot_losses,\n    plot_lowest_highest_influence_images,\n    plot_sample_images,\n)\nfrom support.influence import MODEL_PATH, Losses, TrainingManager, new_resnet_model\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nlogging.basicConfig(level=logging.INFO)\ndefault_figsize = (7, 7)\nplt.rcParams[\"figure.figsize\"] = default_figsize\nplt.rcParams[\"font.size\"] = 12\nplt.rcParams[\"xtick.labelsize\"] = 12\nplt.rcParams[\"ytick.labelsize\"] = 10\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\nhessian_reg = 1e4 if os.environ.get(\"CI\") else 1e-3\nrandom_state = 42\nnp.random.seed(random_state)\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n</code></pre> <pre><code>from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score\n\nfrom pydvl.influence.torch import CgInfluence\nfrom pydvl.reporting.plots import plot_influence_distribution_by_label\n</code></pre> <pre><code>label_names = {90: \"tables\", 100: \"boats\"}\ntrain_ds, val_ds, test_ds = load_preprocess_imagenet(\n    train_size=0.8,\n    test_size=0.1,\n    keep_labels=label_names,\n    downsampling_ratio=1,\n)\n\nprint(\"Normalised image dtype:\", train_ds[\"normalized_images\"][0].dtype)\nprint(\"Label type:\", type(train_ds[\"labels\"][0]))\nprint(\"Image type:\", type(train_ds[\"images\"][0]))\ntrain_ds.info()\n</code></pre> <p>     Let's take a closer look at a few image samples    </p> <p>     Let's now further pre-process the data and prepare for model training. The helper function     <code>      process_io     </code>     converts the normalized images into tensors and the labels to the indices 0 and 1 to train the classifier.    </p> <pre><code>def process_io(df: pd.DataFrame, labels: dict) -&amp;gt; Tuple[torch.Tensor, torch.Tensor]:\n    x = df[\"normalized_images\"]\n    y = df[\"labels\"]\n    ds_label_to_model_label = {\n        ds_label: idx for idx, ds_label in enumerate(labels.values())\n    }\n    x_nn = torch.stack(x.tolist()).to(DEVICE)\n    y_nn = torch.tensor([ds_label_to_model_label[yi] for yi in y], device=DEVICE)\n    return x_nn, y_nn\n\n\ntrain_x, train_y = process_io(train_ds, label_names)\nval_x, val_y = process_io(val_ds, label_names)\ntest_x, test_y = process_io(test_ds, label_names)\n\nbatch_size = 768\ntrain_data = DataLoader(TensorDataset(train_x, train_y), batch_size=batch_size)\ntest_data = DataLoader(TensorDataset(test_x, test_y), batch_size=batch_size)\nval_data = DataLoader(TensorDataset(val_x, val_y), batch_size=batch_size)\n</code></pre> <pre><code>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel_ft = new_resnet_model(output_size=len(label_names))\nmgr = TrainingManager(\n    \"model_ft\",\n    model_ft,\n    nn.CrossEntropyLoss(),\n    train_data,\n    val_data,\n    MODEL_PATH,\n    device=device,\n)\n# Set use_cache=False to retrain the model\ntrain_loss, val_loss = mgr.train(n_epochs=50, use_cache=True)\n</code></pre> <pre><code>plot_losses(Losses(train_loss, val_loss))\n</code></pre> <p>     The confusion matrix and           \\(F_1\\)          score look good, especially considering the low resolution of the images and their complexity (they contain different objects)    </p> <pre><code>pred_y_test = np.argmax(model_ft(test_x).cpu().detach(), axis=1).cpu()\nmodel_score = f1_score(test_y.cpu(), pred_y_test, average=\"weighted\")\n\ncm = confusion_matrix(test_y.cpu(), pred_y_test)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names.values())\nprint(\"f1_score of model:\", model_score)\ndisp.plot();\n</code></pre> <pre>\n<code>f1_score of model: 0.9062805208898536\n</code>\n</pre> <pre><code>influence_model = CgInfluence(mgr.model, mgr.loss, hessian_reg, progress=True)\ninfluence_model = influence_model.fit(train_data)\n</code></pre> <p>     On the instantiated influence object, we can call the method           influences          , which takes some test data and some input dataset with labels (which typically is the training data, or a subset of it). The influence type will be     <code>      up     </code>     . The other option,     <code>      perturbation     </code>     , is beyond the scope of this notebook, but more info can be found in the notebook           using the Wine dataset          or in the documentation for pyDVL.    </p> <pre><code>influences = influence_model.influences(test_x, test_y, train_x, train_y, mode=\"up\")\n</code></pre> <p>     The output  is a matrix of size     <code>      test_set_length     </code>     x     <code>      training_set_length     </code>     . Each row represents a test data point, and each column a training data point, so that entry           \\((i,j)\\)          represents the influence of training point           \\(j\\)          on test point           \\(i\\)          .    </p> <p>     Now we plot the histogram of the influence that all training images have on the image selected above, separated by their label.    </p> <p>     Rather unsurprisingly, the training points with the highest influence have the same label. Now we can take the training images with the same label and show those with highest and lowest scores.    </p> <p>     Looking at the images, it is difficult to explain why those on the right are more influential than those on the left. At first sight, the choice seems to be random (or at the very least noisy). Let's dig in a bit more by looking at average influences:    </p> <pre><code>avg_influences = np.mean(influences.cpu().numpy(), axis=0)\n</code></pre> <p>     Once again, let's plot the histogram of influence values by label.    </p> <p>     Next, for each class (you can change value by changing label key) we can have a look at the top and bottom images by average influence, i.e. we can show the images that have the highest and lowest average influence over all test images.    </p> <p>     Once again, it is not easy to explain why the images on the left have a lower influence than the ones on the right.    </p> <pre><code>corrupted_model = new_resnet_model(output_size=len(label_names))\ncorrupted_dataset, corrupted_indices = corrupt_imagenet(\n    dataset=train_ds,\n    fraction_to_corrupt=0.1,\n    avg_influences=avg_influences,\n)\n\ncorrupted_train_x, corrupted_train_y = process_io(corrupted_dataset, label_names)\ncorrupted_data = DataLoader(\n    TensorDataset(corrupted_train_x, corrupted_train_y), batch_size=batch_size\n)\n\nmgr = TrainingManager(\n    \"corrupted_model\",\n    corrupted_model,\n    nn.CrossEntropyLoss(),\n    corrupted_data,\n    val_data,\n    MODEL_PATH,\n    device=device,\n)\ntraining_loss, validation_loss = mgr.train(n_epochs=50, use_cache=True)\n</code></pre> <pre><code>plot_losses(Losses(training_loss, validation_loss))\n</code></pre> <pre>\n<code>F1 score of model with corrupted data: 0.8541666666666666\n</code>\n</pre> <p>     Interestingly, despite being trained on a corrupted dataset, the model has a fairly high           \\(F_1\\)          score. Let's now calculate the influence of the corrupted training data points over the test data points.    </p> <pre><code>influence_model = CgInfluence(mgr.model, mgr.loss, hessian_reg, progress=True)\ninfluence_model = influence_model.fit(corrupted_data)\ninfluences = influence_model.influences(\n    test_x, test_y, corrupted_train_x, corrupted_train_y\n)\n</code></pre> <p>     As before, since we are interested in the average influence on the test dataset, we take the average of influences across rows, and then plot the highest and lowest influences for a chosen label    </p> <pre><code>avg_corrupted_influences = np.mean(influences.cpu().numpy(), axis=0)\n</code></pre> <p>     As expected, the samples with lowest (negative) influence for the label \"boats\" are those that have been corrupted: all the images on the left are tables! We can compare the average influence of corrupted data with non-corrupted ones    </p>            label                      avg_non_corrupted_infl                      avg_corrupted_infl                      score_diff                      0                      tables                      -0.405254                      -12.999691                      12.594438                      1                      boats                      -0.544211                      -13.080050                      12.535838           <p>     And indeed corrupted data have a more negative influence on average than clean ones!    </p> <p>     Despite this being a useful property, influence functions are known to be unreliable for tasks of data valuation, especially in deep learning where the fundamental assumption of the theory (convexity) is grossly violated. A lot of factors (e.g. the size of the network, the training process or the Hessian regularization term) can interfere with the computation, to the point that often the results that we obtain cannot be trusted. This has been extensively studied in the recent paper:    </p> <p>       Basu, S., P. Pope, and S. Feizi.             Influence Functions in Deep Learning Are Fragile.            International Conference on Learning Representations (ICLR). 2021          .    </p> <p>     Nevertheless, influence functions offer a relatively quick and mathematically rigorous way to evaluate (at first order) the importance of a training point for a model's prediction.    </p>"},{"location":"examples/influence_imagenet/#influence-functions-for-neural-networks","title":"Influence functions for neural networks","text":"<p>     This notebook explores the use of influence functions for convolutional neural networks. In           the first part          we will investigate the usefulness, or lack thereof, of influence functions for the interpretation of a classifier's outputs.    </p> <p>     For our study we choose a pre-trained ResNet18, fine-tuned on the           tiny-imagenet dataset          . This dataset was created for a Stanford course on           Deep Learning for Computer Vision          , and is a subset of the           famous ImageNet          with 200 classes instead of 1000, and images down-sampled to a lower resolution of 64x64 pixels.    </p> <p>     After tuning the last layers of the network, we will use           pyDVL          to find the most and the least influential training images for the test set. This can sometimes be used to explain inference errors, or to direct efforts during data collection, although we will face inconclusive results with our model and data. This illustrates well-known issues of influence functions for neural networks.    </p> <p>     However, in the           final part of the notebook          we will see that influence functions are an effective tool for finding anomalous or corrupted data points.    </p> <p>     We conclude with an           appendix          with some basic theoretical concepts used.    </p>"},{"location":"examples/influence_imagenet/#imports-and-setup","title":"Imports and setup","text":""},{"location":"examples/influence_imagenet/#loading-and-preprocessing-the-dataset","title":"Loading and preprocessing the dataset","text":"<p>     We pick two classes arbitrarily to work with: 90 and 100, corresponding respectively to dining tables, and boats in Venice (you can of course select any other two classes, or more of them, although that would imply longer training times and some modifications in the notebook below). The dataset is loaded with     <code>      load_preprocess_imagenet()     </code>     , which returns three pandas     <code>      DataFrames     </code>     with training, validation and test sets respectively. Each dataframe has three columns: normalized images, labels and the original images. Note that you can load a subset of the data decreasing downsampling_ratio.    </p>"},{"location":"examples/influence_imagenet/#model-definition-and-training","title":"Model definition and training","text":"<p>     We use a ResNet18 from     <code>      torchvision     </code>     with final layers modified for binary classification.    </p> <p>     For training, we use the convenience class     <code>      TrainingManager     </code>     which transparently handles persistence after training. It is not part of the main pyDVL package but just a way to reduce clutter in this notebook.    </p> <p>     We train the model for 50 epochs and save the results. Then we plot the train and validation loss curves.    </p>"},{"location":"examples/influence_imagenet/#influence-computation","title":"Influence computation","text":"<p>     Let's now calculate influences! The central interface for computing influences is           InfluenceFunctionModel          . Since Resnet18 is quite big, we pick the conjugate gradient implementation           CgInfluence          , which takes a trained           torch.nn.Module          , the training loss and the training data. Other important parameters are the Hessian regularization term, which should be chosen as small as possible for the computation to converge (further details on why this is important can be found in the           Appendix          ).    </p>"},{"location":"examples/influence_imagenet/#analysing-influences","title":"Analysing influences","text":"<p>     With the computed influences we can study single images or all of them together:    </p>"},{"location":"examples/influence_imagenet/#influence-on-a-single-test-image","title":"Influence on a single test image","text":"<p>     Let's take any image in the test set:    </p>"},{"location":"examples/influence_imagenet/#analysing-the-average-influence-on-test-samples","title":"Analysing the average influence on test samples","text":"<p>     By averaging across the rows of the influence matrix, we obtain the average influence of each training sample on the whole test set:    </p>"},{"location":"examples/influence_imagenet/#detecting-corrupted-data","title":"Detecting corrupted data","text":"<p>     After facing the shortcomings of influence functions for explaining decisions, we move to an application with clear-cut results. Influences can be successfully used to detect corrupted or mislabeled samples, making them an effective tool to \"debug\" training data.    </p> <p>     We begin by training a new model (with the same architecture as before) on a dataset with some corrupted labels. The method     <code>      get_corrupted_imagenet     </code>     will take the training dataset and corrupt a certain fraction of the labels by flipping them. We use the same number of epochs and optimizer as before.    </p>"},{"location":"examples/influence_imagenet/#theory-of-influence-functions-for-neural-networks","title":"Theory of influence functions for neural networks","text":"<p>     In this appendix we will briefly go through the basic ideas of influence functions adapted for neural networks as introduced in           Koh, Pang Wei, and Percy Liang.             \"Understanding Black-box Predictions via Influence Functions\"            International conference on machine learning. PMLR, 2017.      </p> <p>     Note however that this paper departs from the standard and established theory and notation for influence functions. For a rigorous introduction to the topic we recommend classical texts like           Hampel, Frank R., Elvezio M. Ronchetti, Peter J. Rousseeuw, and Werner A. Stahel. Robust Statistics: The Approach Based on Influence Functions. 1st edition. Wiley Series in Probability and Statistics. New York: Wiley-Interscience, 2005. https://doi.org/10.1002/9781118186435.      </p>"},{"location":"examples/influence_imagenet/#upweighting-points","title":"Upweighting points","text":"<p>     Let's start by considering some input space           \\(\\mathcal{X}\\)          to a model (e.g. images) and an output space           \\(\\mathcal{Y}\\)          (e.g. labels). Let's take           \\(z_i = (x_i, y_i)\\)          to be the           \\(i\\)          -th training point, and           \\(\\theta\\)          to be the (potentially highly) multi-dimensional parameters of the neural network (i.e.           \\(\\theta\\)          is a big array with very many parameters). We will indicate with           \\(L(z, \\theta)\\)          the loss of the model for point           \\(z\\)          and parameters           \\(\\theta\\)          . When training the model we minimize the loss over all points, i.e. the optimal parameters are calculated through gradient descent on the following formula:    </p>      \\[ \\hat{\\theta} = \\arg \\min_\\theta \\frac{1}{n}\\sum_{i=1}^n L(z_i, \\theta) \\]     <p>     where           \\(n\\)          is the total number of training data points.    </p> <p>     For notational  convenience, let's define    </p>      \\[ \\hat{\\theta}_{-z} = \\arg \\min_\\theta \\frac{1}{n}\\sum_{z_i \\ne z} L(z_i, \\theta) \\ , \\]     <p>     i.e.           \\(\\hat{\\theta}_{-z}\\)          are the model parameters that minimize the total loss when           \\(z\\)          is not in the training dataset.    </p> <p>     In order to check the impact of each training point on the model, we would need to calculate           \\(\\hat{\\theta}_{-z}\\)          for each           \\(z\\)          in the training dataset, thus re-training the model at least ~           \\(n\\)          times (more if model training is noisy). This is computationally very expensive, especially for big neural networks. To circumvent this problem, we can just calculate a first order approximation of           \\(\\hat{\\theta}\\)          . This can be done through single backpropagation and without re-training the full model.    </p> <p>     Let's define    </p>      \\[ \\hat{\\theta}_{\\epsilon, z} = \\arg \\min_\\theta \\frac{1}{n}\\sum_{i=1}^n L(z_i, \\theta) + \\epsilon L(z, \\theta) \\ , \\]     <p>     which is the optimal           \\(\\hat{\\theta}\\)          if we were to up-weigh           \\(z\\)          by an amount           \\(\\epsilon\\)          .    </p> <p>     From a classical result (a simple derivation is available in Appendix A of Koh and Liang's paper), we know that:    </p>      \\[ \\frac{d \\ \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon} \\Big|_{\\epsilon=0} = -H_{\\hat{\\theta}}^{-1} \\nabla_\\theta L(z, \\hat{\\theta}) \\]     <p>     where           \\(H_{\\hat{\\theta}} = \\frac{1}{n} \\sum_{i=1}^n \\nabla_\\theta^2 L(z_i, \\hat{\\theta})\\)          is the Hessian of           \\(L\\)          . Importantly, notice that this expression is only valid when           \\(\\hat{\\theta}\\)          is a minimum of           \\(L\\)          , or otherwise           \\(H_{\\hat{\\theta}}\\)          cannot be inverted!    </p>"},{"location":"examples/influence_imagenet/#approximating-the-influence-of-a-point","title":"Approximating the influence of a point","text":"<p>     We will define the influence of training point           \\(z\\)          on test point           \\(z_{\\text{test}}\\)          as           \\(\\mathcal{I}(z, z_{\\text{test}}) =  L(z_{\\text{test}}, \\hat{\\theta}_{-z}) - L(z_{\\text{test}}, \\hat{\\theta})\\)          (notice that it is higher for points           \\(z\\)          which positively impact the model score, since if they are excluded, the loss is higher). In practice, however, we will always use the infinitesimal approximation           \\(\\mathcal{I}_{up}(z, z_{\\text{test}})\\)          , defined as    </p>      \\[  \\mathcal{I}_{up}(z, z_{\\text{test}}) = - \\frac{d L(z_{\\text{test}}, \\hat{\\theta}_{\\epsilon, z})}{d \\epsilon} \\Big|_{\\epsilon=0} \\]     <p>     Using the chain rule and the results calculated above, we thus have:    </p>      \\[  \\mathcal{I}_{up}(z, z_{\\text{test}}) = - \\nabla_\\theta L(z_{\\text{test}}, \\hat{\\theta})^\\top \\ \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon} \\Big|_{\\epsilon=0} = \\nabla_\\theta L(z_{\\text{test}}, \\hat{\\theta})^\\top \\ H_{\\hat{\\theta}}^{-1} \\ \\nabla_\\theta L(z, \\hat{\\theta}) \\]     <p>     In order to calculate this expression we need the gradient and the Hessian of the loss wrt. the model parameters           \\(\\hat{\\theta}\\)          . This can be easily done through a single backpropagation pass.    </p>"},{"location":"examples/influence_imagenet/#regularizing-the-hessian","title":"Regularizing the Hessian","text":"<p>     One very important assumption that we make when approximating influence is that           \\(\\hat{\\theta}\\)          is at least a local minimum of the loss. However, we clearly cannot guarantee this except for convex models, and despite good apparent convergence,           \\(\\hat{\\theta}\\)          might be located in a region with flat curvature or close to a saddle point. In particular, the Hessian might have vanishing eigenvalues making its direct inversion impossible.    </p> <p>     To circumvent this problem, instead of inverting the true Hessian           \\(H_{\\hat{\\theta}}\\)          , one can invert a small perturbation thereof:           \\(H_{\\hat{\\theta}} + \\lambda \\mathbb{I}\\)          , with           \\(\\mathbb{I}\\)          being the identity matrix. This standard trick ensures that the eigenvalues of           \\(H_{\\hat{\\theta}}\\)          are bounded away from zero and therefore the matrix is invertible. In order for this regularization not to corrupt the outcome too much, the parameter           \\(\\lambda\\)          should be as small as possible while still allowing a reliable inversion of           \\(H_{\\hat{\\theta}} + \\lambda \\mathbb{I}\\)          .    </p>"},{"location":"examples/influence_sentiment_analysis/","title":"For language models","text":"<p>     This notebooks showcases the use of influence functions for large language models. In particular, it focuses on sentiment analysis using the           IMDB dataset          and a fine-tuned           BERT          model.    </p> <p>     Not all the methods for influence function calculation can scale to large models and datasets. In this notebook we will use the           Kronecker-Factored Approximate Curvature          method, which is the only one that can scale to current state-of-the-art language models.    </p> <p>     The notebook is structured as follows:    </p> <ul> <li>        Setup            imports the required libraries and downloads the dataset and the model.     </li> <li>        Sentiment analysis            loads the model and the dataset and goes through a few examples of sentiment analysis.     </li> <li>        Model and data preparation            prepares the model and the dataset for influence function calculation. In particular, it assigns all the linear layers to require gradients and wraps the model so that only logits are returned (and not the loss or attention masks).     </li> <li>        Influence function computation            : shows how to calculate the influence function for a few test and train examples.     </li> <li>        Analysis of influence values            : analyses the influence values, trying to extract general information about the model and how it is affected by corruption in the training data.     </li> <li>        Influence functions by layer            : since ekfac is based on a block diagonal approximation of the Fisher information matrix, we can compute the influence function separately for each layer of the neural network. This section shows how to do that and how to analyse the results.     </li> </ul> <p>     Finally, the           Appendix          shows how to select the Hessian regularization parameter to obtain the best influence function approximation.    </p>      If you are reading this in the documentation, some boilerplate has been omitted for convenience.     <p>     Let's start by importing the required libraries. If not already installed, you can install them with     <code>      pip install -r requirements-notebooks.txt     </code>     .    </p> <pre><code>import os\nfrom copy import deepcopy\nfrom typing import Sequence\n\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom IPython.display import HTML, display\nfrom sklearn.metrics import f1_score\nfrom support.influence import ImdbDataset, ModelLogitsWrapper\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nfrom pydvl.influence.torch import EkfacInfluence\n</code></pre> <pre><code>plt.rcParams[\"figure.figsize\"] = (16, 8)\nplt.rcParams[\"font.size\"] = 12\nplt.rcParams[\"xtick.labelsize\"] = 12\nplt.rcParams[\"ytick.labelsize\"] = 10\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n</code></pre> <p>     Sentiment analysis is the task of classifying a sentence as having a positive or negative sentiment. For example, the sentence \"I love this movie\" has a positive sentiment, while \"I hate this movie\" has a negative sentiment. In this notebook we will use the IMDB dataset, which contains 50,000 movie reviews with corresponding labels. The dataset is split into 25,000 reviews for training and 25,000 reviews for testing. The dataset is balanced, meaning that there are the same number of positive and negative reviews in the training and test set.    </p> <pre><code>imdb = load_dataset(\"imdb\")\n</code></pre> <p>     Let's print an example of review and its label    </p> <pre><code>sample_review = imdb[\"train\"].select([24])\n\nprint(f\"Here is a sample review with label {sample_review['label'][0]}: \\n\")\n\ndisplay(HTML(sample_review[\"text\"][0].split(\"&lt;br/&gt;\")[0]))\ndisplay(HTML(sample_review[\"text\"][0].split(\"&lt;br/&gt;\")[-1]))\n</code></pre> <pre>\n<code>Here is a sample review with label 0: \n\n</code>\n</pre>       Without wishing to be a killjoy, Brad Sykes is responsible for at least two of the most dull and clich\u00e9d films i've ever seen - this being one of them, and Camp Blood being another.            I bought this for \u00a31, but remember, you can't put a price on 71 minutes of your life. You'd do well to avoid this turkey, even at a bargain basement price.      <p>     The review is negative, and so label 0 is associated to negative sentiment.    </p> <p>     The model is a BERT model fine-tuned on the IMDB dataset. BERT is a large language model that has been pre-trained on a large corpus of text. The model was fine-tuned on the IMDB dataset by AssemblyAI and is available on the HuggingFace model hub. We also load its tokenizer, which is used to convert sentences into numeric tokens.    </p> <pre><code>tokenizer = AutoTokenizer.from_pretrained(\"assemblyai/distilbert-base-uncased-sst2\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"assemblyai/distilbert-base-uncased-sst2\"\n)\n</code></pre> <p>     Even if the model is trained on movie reviews, it can be used to classify any sentence as positive or negative. Let's try it on a simple sentence created by us.    </p> <pre><code>example_phrase = (\n    \"Pydvl is the best data valuation library, and it is fully open-source!\"\n)\n\ntokenized_example = tokenizer(\n    [example_phrase],\n    return_tensors=\"pt\",\n    truncation=True,\n)\n\nmodel_output = model(\n    input_ids=tokenized_example.input_ids,\n)\n</code></pre> <p>     The model output is a     <code>      SequenceClassificationOutput     </code>     object, which contains the logits and other information.    </p> <pre>\n<code>Model Output:\n SequenceClassifierOutput(loss=None, logits=tensor([[-2.6237,  2.8350]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n</code>\n</pre> <p>     For calculating probabilities and for the influence functions we only need the logits. Then the softmax function converts the logits into probabilities.    </p> <pre><code>model_predictions = F.softmax(model_output.logits, dim=1)\n</code></pre> <p>     The model is quite confident that the sentence has a positive sentiment, which is correct.    </p> <pre>\n<code>Positive probability: 99.6%\nNegative probability: 0.4%\n</code>\n</pre> <p>     Let's examine the model's f1 score on a small subset of the test set.    </p> <pre><code>sample_test_set = imdb[\"test\"].shuffle(seed=seed).select(range(50 if not is_CI else 5))\nsample_test_set = sample_test_set.map(\n    lambda example: tokenizer(example[\"text\"], truncation=True, padding=\"max_length\"),\n    batched=True,\n)\nsample_test_set.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\nmodel.eval()\nwith torch.no_grad():\n    logits = model(\n        input_ids=sample_test_set[\"input_ids\"],\n        attention_mask=sample_test_set[\"attention_mask\"],\n    ).logits\n    predictions = torch.argmax(logits, dim=1)\n</code></pre> <pre><code>f1_score_value = f1_score(sample_test_set[\"label\"], predictions)\nprint(f\"F1 Score: {round(f1_score_value, 3)}\")\n</code></pre> <pre>\n<code>F1 Score: 0.955\n</code>\n</pre> <p>     In this section we will define two helper function and classes that will be used in the rest of the notebook.    </p> <pre><code>def print_sentiment_preds(\n    model: ModelLogitsWrapper, model_input: torch.Tensor, true_label: int\n):\n    \"\"\"\n    Prints the sentiment predictions in a human-readable format given a model and an\n    input. It also prints the true label.\n    \"\"\"\n    model_predictions = F.softmax(model(model_input.unsqueeze(0)), dim=1)\n    print(\n        \"Positive probability: \"\n        + str(round(model_predictions[0][1].item(), 3) * 100)\n        + \"%\"\n    )\n    print(\n        \"Negative probability: \"\n        + str(round(model_predictions[0][0].item(), 3) * 100)\n        + \"%\"\n    )\n\n    true_label = \"Positive\" if true_label == 1 else \"Negative\"\n    print(f\"True label: {true_label} \\n\")\n\n\ndef strip_layer_names(param_names: Sequence[str]):\n    \"\"\"\n    Helper function that strips the parameter names of the model and the transformer,\n    so that they can be printed and compared more easily.\n    \"\"\"\n    stripped_param_names = []\n    for name in param_names:\n        name = name.replace(\"model.\", \"\")\n        if name.startswith(\"distilbert.transformer.\"):\n            name = name.replace(\"distilbert.transformer.\", \"\")\n        stripped_param_names.append(name)\n    return stripped_param_names\n</code></pre> <p>     Importantly, we will need to assign all the linear layers to require gradients, so that we can compute the influence function with respect to them. Keep in mind that the current implementation of Ekfac only supports linear layers, so if any other type of layer in the model requires gradients the initialisation of the influence function class will fail.    </p> <pre><code>for param in model.named_parameters():\n    param[1].requires_grad = False\n\nfor m_name, module in model.named_modules():\n    if len(list(module.children())) == 0 and len(list(module.parameters())) &amp;gt; 0:\n        if isinstance(module, torch.nn.Linear):\n            for p_name, param in module.named_parameters():\n                if (\n                    (\"ffn\" in m_name and not is_CI)\n                    or \"pre_classifier\" in m_name\n                    or \"classifier\" in m_name\n                ):\n                    param.requires_grad = True\n</code></pre> <p>     Albeit restrictive, linear layers constitute a large fraction of the parameters of most large language models, and so our analysis still holds a lot of information about the full neural network.    </p> <pre>\n<code>Total parameters: 66.96 millions\nParameters requiring gradients: 28.93 millions\nRatio of Linear over other layer types: 43.20%\n</code>\n</pre> <p>     We are now ready to compute the influence function for a few testing and training examples. Let's start by selecting a subset of the full training and testing dataset and wrapping them in a     <code>      DataLoader     </code>     object, so that we can easily do batching.    </p> <pre><code>NUM_TRAIN_EXAMPLES = 100 if not is_CI else 7\nNUM_TEST_EXAMPLES = 100 if not is_CI else 5\n\nsmall_train_dataset = (\n    imdb[\"train\"]\n    .shuffle(seed=seed)\n    .select([i for i in list(range(NUM_TRAIN_EXAMPLES))])\n)\nsmall_test_dataset = (\n    imdb[\"test\"].shuffle(seed=seed).select([i for i in list(range(NUM_TEST_EXAMPLES))])\n)\n\ntrain_dataset = ImdbDataset(small_train_dataset, tokenizer=tokenizer)\ntest_dataset = ImdbDataset(small_test_dataset, tokenizer=tokenizer)\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=7, shuffle=True\n)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=5, shuffle=True)\n</code></pre> <p>     For influence computation we need to take the model in evaluation mode, so that no dropout or batch normalization is applied. Then, we can fit the Ekfac representation.    </p> <pre><code>wrapped_model = ModelLogitsWrapper(model)\nwrapped_model.eval()\n\nekfac_influence_model = EkfacInfluence(\n    wrapped_model,\n    progress=True,\n)\nekfac_influence_model = ekfac_influence_model.fit(train_dataloader)\n</code></pre> <pre>\n<code>K-FAC blocks - batch progress:   0%|          | 0/15 [00:00&lt;?, ?it/s]</code>\n</pre> <p>     And the approximate Hessian is thus obtained. Considering that the model has almost 30 million parameters requiring gradients, this was very fast! Of course, this Hessian is computed using only a very small fraction (~0.4%) of the training data, and for a better approximation we should use a larger subset.    </p> <p>     Before continuing, we need to set the Hessian regularization parameter to an appropriate value. A way to decide which is better can be found in the           Appendix          . Here, we will just set it to 1e-5.    </p> <pre><code>ekfac_influence_model.hessian_regularization = 1e-5\n</code></pre> <p>     We calculate the influence of the first batch of training data over the first batch of test data. This is because influence functions are very expensive to compute, and so to keep the runtime of this notebook within a few minutes we need to restrict ourselves to a small number of examples.    </p> <pre><code>test_input, test_labels, test_text = next(iter(test_dataloader))\ntrain_input, train_labels, train_text = next(iter(train_dataloader))\n</code></pre> <p>     And let's finally compute the influence function values    </p> <pre><code>ekfac_train_influences = ekfac_influence_model.influences(\n    test_input,\n    test_labels,\n    train_input,\n    train_labels,\n)\n</code></pre> <p>     Now that we have calculated the influences for a few examples, let's analyse some of the extreme values.    </p> <p>     Let's plot the influence values as a heatmap for easily spotting patterns.    </p> <p>     Most of the test and training examples have similar influence, close to zero. However, there is one test and one training samples that stand out. In particular, their cross influence is very large and negative. Let's examine them more closely.    </p> <pre>\n<code>Training example with idx 3: \n\nPositive probability: 18.099999999999998%\nNegative probability: 81.89999999999999%\nTrue label: Positive \n\nSentence:\n</code>\n</pre>       In the process of trying to establish the audiences' empathy with Jake Roedel (Tobey Maguire) the filmmakers slander the North and the Jayhawkers. Missouri never withdrew from the Union and the Union Army was not an invading force. The Southerners fought for State's Rights: the right to own slaves, elect crooked legislatures and judges, and employ a political spoils system. There's nothing noble in that. The Missourians could have easily traveled east and joined the Confederate Army.             It seems to me that the story has nothing to do with ambiguity. When Jake leaves the Bushwhackers, it's not because he saw error in his way, he certainly doesn't give himself over to the virtue of the cause of abolition.      <p>     We can see that, despite being positive, this review is quite hard to classify. Its language is overall negative, mostly associated to the facts narrated rather than the movie itself. Notice how several terms are related to war and invasion.    </p> <pre>\n<code>Test example with idx 4: \n\nPositive probability: 39.6%\nNegative probability: 60.4%\nTrue label: Negative \n\nSentence:\n</code>\n</pre>       \"An astronaut (Michael Emmet) dies while returning from a mission and his body is recovered by the military. The base where the dead astronaut is taken to becomes the scene of a bizarre invasion plan from outer space. Alien embryos inside the dead astronaut resurrect the corpse and begin a terrifying assault on the military staff in the hopes of conquering the world,\" according to the DVD sleeve's synopsis.             A Roger Corman \"American International\" production. The man who fell to Earth impregnated, Mr. Emmet (as John Corcoran), does all right. Angela Greene is his pretty conflicted fianc\u00e9e. And, Ed Nelson (as Dave Randall) is featured as prominently. With a bigger budget, better opening, and a re-write for crisper characterizations, this could have been something approaching classic 1950s science fiction.             *** Night of the Blood Beast (1958) Bernard L. Kowalski, Roger Corman ~ Michael Emmet, Angela Greene, Ed Nelson      <p>     This review is also quite hard to classify. This time it has a negative sentiment towards the movie, but it also contains several words with positive connotation. The parallel with the previous review is quite interesting since both talk about an invasion.    </p> <p>     As it is often the case when analysing influence functions, it is hard to understand why these examples have such a large influence. We have seen some interesting patterns, mostly related to similarities in the language and words used, but it is hard to say with certainty if these are the reasons for such a large influence.    </p> <p>     A           recent paper          has explored this topic in high detail, even for much larger language models than BERT (up to ~50 billion parameters!). Among the most interesting findings is that smaller models tend to rely a lot on word-to-word correspondencies, while larger models are more capable of extracting higher level concepts, drawing connections between words across multiple phrases.    </p> <p>     For more info, you can visit our           blog on influence functions for large language models      </p> <p>     In this sections we want to get an idea of how influence functions change when training examples are corrupted. In the next cell we will flip the label of all the training examples and compute the influences on the same test batch as before.    </p> <pre><code>modified_train_labels = deepcopy(train_labels)\nmodified_train_labels = 1 - train_labels\n\ncorrupted_ekfac_train_influences = ekfac_influence_model.influences(\n    test_input,\n    test_labels,\n    train_input,\n    modified_train_labels,\n)\n</code></pre> <p>     Overall, when corrupted the influences tend to become negative, as expected. Nevertheless, there are cases where values go from slightly negative to positive, mostly isolated to the second and last test samples. Single values can be quite noisy, so it is difficult to generalise this result, but it would be interesting to see how common these cases are in the full test dataset.    </p> <p>     Since ekfac is based on a block diagonal approximation of the Fisher information matrix, we can compute the influence functions separately for each layer of the neural network. In this section we show how to do that and we briefly analyse the results.    </p> <pre><code>influences_by_layer = ekfac_influence_model.influences_by_layer(\n    test_input,\n    test_labels,\n    train_input,\n    train_labels,\n)\n</code></pre> <p>     The method     <code>      influences_by_layer     </code>     returns a dictionary containing the influence function values for each layer of the neural network as a tensor. To recover the full influence values as returned by the     <code>      influences     </code>     (as done in the previous section), we need to sum each layer's values.    </p> <pre><code>influences = torch.zeros_like(ekfac_train_influences)\nfor layer_id, value in influences_by_layer.items():\n    influences += value.detach()\n</code></pre> <p>     And if we plot the result as a heatmap we can see that the results are the same as in           Negative influence training examples      </p> <p>     Let's analyse how the influence values change across different layers for given test and train examples.    </p> <p>     The plot above shows the influences for test idx 0 and all train idx apart idx=3 (excluded for clarity since it has a very large absolute value). We can see that the scores tend to keep their sign across layers, but in almost all cases tend to decrease when approaching the output layer. This is not always the case, and in fact other test examples show different patterns. Understanding why this happens is an interesting research direction.    </p> <p>     Ekfac is a powerful approximate method for computing the influence function of models that use a cross-entropy loss. In this notebook we applied it to sentiment analysis with BERT on the IMDB dataset. However, this method can be applied to much larger models and problems, e.g. to analyse the influence of entire sentences generated by GPT, Llama or Claude. For more info, you can visit our           paper pill on influence functions for large language models      </p> <p>     The Hessian regularization value impacts a lot the quality of the influence function approximation. In general, the value should be chosen as small as possible so that the results are finite. In practice, even when finite the influence values can be too large and lead to numerical instabilities. In this section we show how to efficiently analyse the impact of the Hessian regularization value with the ekfac method.    </p> <p>     Let's start with a few additional imports.    </p> <pre><code>import pandas as pd\nfrom scipy.stats import pearsonr, spearmanr\n</code></pre> <p>     The method     <code>      explore_hessian_regularization     </code>     will calculate the influence values of the training examples with each other for a range of Hessian regularization values. The method optimises gradient calculation and Hessian inversion to minimise the computation time.    </p> <pre><code>influences_by_reg_value = ekfac_influence_model.explore_hessian_regularization(\n    train_input,\n    train_labels,\n    regularization_values=[1e-15, 1e-9, 1e-5, 1],\n)\n</code></pre> <pre>\n<code>/home/jakob/Documents/pyDVL/venv/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:222: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::masked_fill.Tensor. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n  scores = scores.masked_fill(\n</code>\n</pre> <p>     The resulting object,     <code>      influences_by_reg_value     </code>     is a dictionary that associates to each regularization value the influences for each layer of the neural network. This is a lot of data, so we will first organise it in a pandas dataframe and take the average across training examples.    </p> <pre><code>cols = [\"reg_value\", \"layer_id\", \"mean_infl\"]\ninfl_df = pd.DataFrame(influences_by_reg_value, columns=cols)\nfor reg_value in influences_by_reg_value:\n    for layer_id, layer_influences in influences_by_reg_value[reg_value].items():\n        mean_infl = torch.mean(layer_influences, dim=0).detach().numpy()\n        infl_df = pd.concat(\n            [infl_df, pd.DataFrame([[reg_value, layer_id, mean_infl]], columns=cols)]\n        )\n</code></pre> <pre>\n<code>/tmp/ipykernel_8503/1081261490.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  infl_df = pd.concat(\n</code>\n</pre> <p>     With this dataframe, we can take contiguous values of regularization and, for each layer, calculate the Pearson and Spearman correlation coefficients. This will give us an idea of how the influence values change with the regularization value.    </p> <pre><code>result_corr = {}\nfor layer_id, group_df in infl_df.groupby(\"layer_id\"):\n    result_corr[layer_id + \"_pearson\"] = {}\n    result_corr[layer_id + \"_spearman\"] = {}\n    for idx, mean_infl in enumerate(group_df[\"mean_infl\"]):\n        if idx == 0:\n            continue\n        reg_value_diff = f\"Reg: {group_df['reg_value'].iloc[idx - 1]} -&amp;gt; {group_df['reg_value'].iloc[idx]}\"\n        pearson = pearsonr(mean_infl, group_df[\"mean_infl\"].iloc[idx - 1]).statistic\n        spearman = spearmanr(mean_infl, group_df[\"mean_infl\"].iloc[idx - 1]).statistic\n        result_corr[layer_id + \"_pearson\"].update({f\"{reg_value_diff}\": pearson})\n        result_corr[layer_id + \"_spearman\"].update({f\"{reg_value_diff}\": spearman})\nresult_df = pd.DataFrame(result_corr).T\n</code></pre> <p>     Let's plot the correlations heatmap. The y-axis reports Spearman and Pearson correlations for each layer, while the x-axis reports pairs of regularization values. High correlations mean that influences are stable across regularization values.    </p> <p>     In our case, we can see that for regularization = 1 the spearman correlation becomes very bad. However, for a large range of regularization values smaller than 1 the sample rankings are stable. This is a good indicator that the model is not too sensitive to the regularization value. We therefore chose the value 1e-5 for our analysis.    </p>"},{"location":"examples/influence_sentiment_analysis/#influence-functions-for-large-language-models","title":"Influence functions for Large Language Models","text":""},{"location":"examples/influence_sentiment_analysis/#setup","title":"Setup","text":""},{"location":"examples/influence_sentiment_analysis/#sentiment-analysis","title":"Sentiment Analysis","text":""},{"location":"examples/influence_sentiment_analysis/#model-and-data-preparation","title":"Model and Data Preparation","text":""},{"location":"examples/influence_sentiment_analysis/#influence-function-computation","title":"Influence function computation","text":""},{"location":"examples/influence_sentiment_analysis/#analysis-of-influence-values","title":"Analysis of influence values","text":""},{"location":"examples/influence_sentiment_analysis/#negative-influence-training-examples","title":"Negative influence training examples","text":""},{"location":"examples/influence_sentiment_analysis/#influence-of-corrupted-training-examples","title":"Influence of corrupted training examples","text":""},{"location":"examples/influence_sentiment_analysis/#influence-functions-by-layer","title":"Influence functions by layer","text":""},{"location":"examples/influence_sentiment_analysis/#conclusion","title":"Conclusion","text":""},{"location":"examples/influence_sentiment_analysis/#appendix","title":"Appendix","text":""},{"location":"examples/influence_sentiment_analysis/#choosing-the-hessian-regularization-value","title":"Choosing the Hessian regularization value","text":""},{"location":"examples/influence_synthetic/","title":"For mislabeled data","text":"If you are reading this in the documentation, some boilerplate has been omitted for convenience.     <pre><code>%matplotlib inline\n\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nfrom support.common import (\n    plot_gaussian_blobs,\n    plot_influences,\n    plot_losses,\n)\nfrom support.influence import TorchLogisticRegression, fit_torch_model\nfrom support.shapley import (\n    decision_boundary_fixed_variance_2d,\n    synthetic_classification_dataset,\n)\nfrom torch.optim import AdamW, lr_scheduler\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom pydvl.influence.torch import CgInfluence, DirectInfluence\n</code></pre> <pre><code>plt.rcParams[\"figure.figsize\"] = (16, 8)\nplt.rcParams[\"font.size\"] = 12\nplt.rcParams[\"xtick.labelsize\"] = 12\nplt.rcParams[\"ytick.labelsize\"] = 10\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\n</code></pre> <p>     The following code snippet generates the aforementioned dataset.    </p> <pre><code>train_data, val_data, test_data = synthetic_classification_dataset(\n    means, sigma, num_samples, train_size=0.7, test_size=0.2\n)\n</code></pre> <p>     Given the simplicity of the dataset, we can calculate exactly the optimal decision boundary(that which maximizes our accuracy). The following code maps a continuous line of z values to a 2-dimensional vector in feature space (More details are in the appendix to this notebook.)    </p> <pre><code>decision_boundary_fn = decision_boundary_fixed_variance_2d(means[0], means[1])\ndecision_boundary = decision_boundary_fn(np.linspace(-1.5, 1.5, 100))\n</code></pre> <pre><code>plot_gaussian_blobs(\n    train_data,\n    test_data,\n    xlabel=\"$x_0$\",\n    ylabel=\"$x_1$\",\n    legend_title=\"$y - labels$\",\n    line=decision_boundary,\n    s=10,\n    suptitle=\"Plot of train-test data\",\n)\n</code></pre> <p>     Note that there are samples which go across the optimal decision boundary and will be wrongly labelled. The optimal decision boundary can not discriminate these as the mislabelling is a consequence of the presence of random noise.    </p> <pre><code>model = TorchLogisticRegression(num_features)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 50\nlr = 0.05\nweight_decay = 0.05\nbatch_size = 256\n\ntrain_data_loader = DataLoader(\n    TensorDataset(\n        torch.as_tensor(train_data[0]),\n        torch.as_tensor(train_data[1], dtype=torch.float64).unsqueeze(-1),\n    ),\n    batch_size=batch_size,\n    shuffle=True,\n)\n\nval_data_loader = DataLoader(\n    TensorDataset(\n        torch.as_tensor(val_data[0]),\n        torch.as_tensor(val_data[1], dtype=torch.float64).unsqueeze(-1),\n    ),\n    batch_size=batch_size,\n    shuffle=True,\n)\n\noptimizer = AdamW(params=model.parameters(), lr=lr, weight_decay=weight_decay)\nscheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\nlosses = fit_torch_model(\n    model=model,\n    training_data=train_data_loader,\n    val_data=val_data_loader,\n    loss=F.binary_cross_entropy,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    num_epochs=num_epochs,\n    device=device,\n)\n</code></pre> <p>     And let's check that the model is not overfitting    </p> <pre><code>plot_losses(losses)\n</code></pre> <p>     A look at the confusion matrix also shows good results    </p> <p>     It is important that the model converges to a point near the optimum, since the influence values assume that we are at a minimum (or close) in the loss landscape. The function    </p>      \\[I(x_1, y_1, x_2, y_2) \\colon \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}\\]     <p>     measures the influence of the data point           \\(x_1\\)          onto           \\(x_2\\)          conditioned on the training targets           \\(y_1\\)          and           \\(y_2\\)          trough some model parameters           \\(\\theta\\)          . If the loss function L is differentiable, we can take           \\(I\\)          to be    </p> <p>     $$ I(x_1, x_2) = \\nabla_\\theta\\; L(x_1, y_1) ^\\mathsf{T} \\; H_\\theta^{-1} \\; \\nabla_\\theta \\; L(x_2, y_2) $$ See           \"Understanding Black-box Predictions via Influence Functions\"          for a detailed derivation of this formula    </p> <p>     Let's take a subset of the training data points, which we will calculate the influence values of.    </p> <pre><code>x = train_data[0][:100]\ny = train_data[1][:100]\n</code></pre> <p>     In pyDVL, the influence of the training points on the test points can be calculated with the following    </p> <pre><code>train_x = torch.as_tensor(x)\ntrain_y = torch.as_tensor(y, dtype=torch.float64).unsqueeze(-1)\ntest_x = torch.as_tensor(test_data[0])\ntest_y = torch.as_tensor(test_data[1], dtype=torch.float64).unsqueeze(-1)\n\ntrain_data_loader = DataLoader(\n    TensorDataset(train_x, train_y),\n    batch_size=batch_size,\n)\n\ninfluence_model = DirectInfluence(\n    model,\n    F.binary_cross_entropy,\n    regularization=0.0,\n)\ninfluence_model = influence_model.fit(train_data_loader)\n\ninfluence_values = influence_model.influences(\n    test_x, test_y, train_x, train_y, mode=\"up\"\n)\n</code></pre> <p>     The above explicitly constructs the Hessian. This can often be computationally expensive and conjugate gradient approximate calculation should be used for bigger models.    </p> <p>     With the influence type 'up', training influences have shape [NxM] where N is the number of test samples and M is the number of training samples. They therefore associate to each training sample its influence on each test sample.  Influence type 'perturbation', instead, return an array of shape  [NxMxF], where F is the number of features in input, ie. the length of x.    </p> <p>     In our case, in order to have a value of the total average influence of a point we can just average across training samples.    </p> <pre><code>mean_train_influences = np.mean(influence_values.cpu().numpy(), axis=0)\n</code></pre> <p>     Let's plot the results (adjust colorbar_limits for better color gradient)    </p> <pre><code>plot_influences(\n    x,\n    mean_train_influences,\n    line=decision_boundary,\n    xlabel=\"$x_0$\",\n    ylabel=\"$x_1$\",\n    suptitle=\"Influences of input points\",\n    legend_title=\"influence values\",\n    # colorbar_limits=(-0.3,),\n);\n</code></pre> <p>     We can see that, as we approach the separation line, the influences tend to move away from zero, i.e. the points become more decisive for model training, some in a positive way, some negative.    </p> <p>     As a further test, let's introduce some labelling errors into           \\(y\\)          and see how the distribution of the influences changes. Let's flip the first 10 labels and calculate influences    </p> <pre><code>y_corrupted = np.copy(y)\ny_corrupted[:10] = [1 - yi for yi in y[:10]]\ntrain_y_corrupted = torch.as_tensor(y_corrupted, dtype=torch.float64).unsqueeze(-1)\ntrain_corrupted_data_loader = DataLoader(\n    TensorDataset(\n        train_x,\n        train_y_corrupted,\n    ),\n    batch_size=batch_size,\n)\n\ninfluence_model = DirectInfluence(\n    model,\n    F.binary_cross_entropy,\n    regularization=0.0,\n)\ninfluence_model = influence_model.fit(train_corrupted_data_loader)\ninfluence_values = influence_model.influences(\n    test_x, test_y, train_x, train_y_corrupted, mode=\"up\"\n)\n\nmean_train_influences = np.mean(influence_values.cpu().numpy(), axis=0)\n</code></pre> <pre>\n<code>Average mislabelled data influence: -0.8618301488627411\nAverage correct data influence: 0.011604730452803018\n</code>\n</pre> <p>     Red circles indicate the points which have been corrupted. We can see that the mislabelled data have a more negative average influence on the model, especially those that are farther away from the decision boundary.    </p> <p>     The \"direct\" method that we have used above involves the inversion of the Hessian matrix of the model. If a model has           \\(n\\)          training points and           \\(\\theta  \\in \\mathbb{R}^p\\)          parameters, this requires           \\(O(n \\ p^2 + p^3)\\)          operations, which for larger models, like neural networks, becomes quickly unfeasible. Conjugate gradient avoids the explicit computation of the Hessian via a technique called implicit Hessian-vector products (HVPs), which typically takes           \\(O(n \\ p)\\)          operations.    </p> <p>     In the next cell we will use conjugate gradient to compute the influence factors. Since logistic regression is a very simple model, \"cg\" actually slows computation with respect to the direct method, which in this case is a much better choice. Nevertheless, we are able to verify that the influences calculated with \"cg\" are the same (to a minor error) as those calculated directly.    </p> <pre><code>influence_model = CgInfluence(\n    model,\n    F.binary_cross_entropy,\n    regularization=0.0,\n)\ninfluence_model = influence_model.fit(train_corrupted_data_loader)\ninfluence_values = influence_model.influences(\n    test_x, test_y, train_x, train_y_corrupted\n)\nmean_train_influences = np.mean(influence_values.cpu().numpy(), axis=0)\n\nprint(\"Average mislabelled data influence:\", np.mean(mean_train_influences[:10]))\nprint(\"Average correct data influence:\", np.mean(mean_train_influences[10:]))\n</code></pre> <pre>\n<code>Average mislabelled data influence: -0.30172696155741363\nAverage correct data influence: 0.0\n</code>\n</pre> <p>     Averages are very similar to the ones calculated through direct method. Same is true for the plot    </p>"},{"location":"examples/influence_synthetic/#influence-functions-for-data-mislabeling","title":"Influence functions for data mislabeling","text":"<p>     In this notebook, we will take a closer look at the theory of influence functions with the help of a synthetic dataset. Data mislabeling occurs whenever some examples from a usually big dataset are wrongly-labeled. In real-life this happens fairly often, e.g. as a consequence of human error, or noise in the data.    </p> <p>     Let's consider a classification problem with the following notation:    </p>      \\[ \\begin{align*} x_i &amp;\\in \\mathbb{R}^d \\\\ y_i &amp;\\in \\{0, 1\\} \\\\ \\forall i &amp;\\in [ N ] \\end{align*} \\]     <p>     In other words, we have a dataset containing           \\(N\\)          samples, each with label 1 or 0. As typical example you can think of y indicating whether a patient has a disease based on some feature representation           \\(x\\)          .    </p> <p>     Let's now introduce a toy model that will help us delve into the theory and practical utility of influence functions. We will assume that           \\(y\\)          is a Bernoulli binary random variable while the input           \\(x\\)          is d-dimensional Gaussian distribution which depends on the label           \\(y\\)          . More precisely:    </p>      \\[ y_i \\sim \\text{Ber}\\left (0.5 \\right) \\\\ x_i \\sim \\mathcal{N}\\left ((1 - y_i) \\mu_1 + y_i \\mu_2, \\sigma^2 I \\right), \\]     <p>     with fixed means and diagonal covariance. Implementing the sampling scheme in python is straightforward and can be achieved by first sampling           \\(y\\)          and afterward           \\(x\\)          .    </p>"},{"location":"examples/influence_synthetic/#imports","title":"Imports","text":""},{"location":"examples/influence_synthetic/#dataset","title":"Dataset","text":""},{"location":"examples/influence_synthetic/#plotting-the-dataset","title":"Plotting the dataset","text":"<p>     Let's plot the dataset is plotted with their respective labels and the optimal decision line    </p>"},{"location":"examples/influence_synthetic/#training-the-model","title":"Training the model","text":"<p>     We will now train a logistic regression model on the training data. This can be done with the following    </p>"},{"location":"examples/influence_synthetic/#calculating-influences","title":"Calculating influences","text":""},{"location":"examples/influence_synthetic/#inversion-through-conjugate-gradient","title":"Inversion through conjugate gradient","text":""},{"location":"examples/influence_synthetic/#appendix-calculating-the-decision-boundary","title":"Appendix: Calculating the decision boundary","text":"<p>     For obtaining the optimal discriminator one has to solve the equation    </p>      \\[p(x|y=0)=p(x|y=1)\\]     <p>     and determine the solution set           \\(X\\)          . Let's take the following probabilities    </p>      \\[ \\begin{align*} p(x|y=0)&amp;=\\mathcal{N}\\left (\\mu_1, \\sigma^2 I \\right) \\\\ p(x|y=1)&amp;=\\mathcal{N}\\left (\\mu_2, \\sigma^2 I \\right) \\end{align*} \\]     <p>     For a single fixed diagonal variance parameterized by           \\(\\sigma\\)          , the optimal discriminator lays at points which are equidistant from the means of the two distributions, i.e.    </p>      \\[ \\begin{align*} \\| x - \\mu_1 \\|^2 &amp;= \\| x - \\mu_2 \\|^2 \\\\ \\| \\mu_1 \\|^2 -2 x^\\mathsf{T} \\mu_1 &amp;= \\| \\mu_2 \\|^2 -2 x^\\mathsf{T} \\mu_2 \\\\ \\implies 0 &amp;= 2 (\\mu_2 - \\mu_1)^\\mathsf{T} x + \\| \\mu_1 \\|^2 - \\| \\mu_2 \\|^2 \\\\ 0 &amp;= \\mu_1^\\mathsf{T}x - \\mu_2^\\mathsf{T}x - \\frac{1}{2} \\mu_1^\\mathsf{T} \\mu_1 + \\frac{1}{2} \\mu_2^\\mathsf{T} \\mu_2 \\end{align*} \\]     <p>     This is just the implicit description of the line. Solving for the explicit form can be achieved by enforcing a functional form           \\(f(z) = x = a z + b\\)          with           \\(z \\in \\mathbb{R}\\)          onto           \\(x\\)          . After the term is inserted in the previous equation    </p>      \\[ 0 = (\\mu_2 - \\mu_1)^\\mathsf{T} (az + b) + \\frac{1}{2} \\| \\mu_1 \\|^2 - \\| \\mu_2 \\|^2 \\]     <p>     We can write           \\(a\\)          since, by symmetry, it is expected to be explicitly orthogonal to           \\(\\mu_2 - \\mu_1\\)          . Then, solving for           \\(b\\)          , the solution can be found to be    </p>      \\[ f(z) = \\underbrace{\\begin{bmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{bmatrix} (\\mu_2 - \\mu_1)}_a z + \\underbrace{\\frac{\\mu_1 + \\mu_2}{2}}_b \\]"},{"location":"examples/influence_wine/","title":"For outlier detection","text":"If you are reading this in the documentation, some boilerplate has been omitted for convenience.     <p>     Let's start by loading the imports, the dataset and splitting it into train, validation and test sets. We will use a large test set to have a less noisy estimate of the average influence.    </p> <pre><code>%matplotlib inline\n\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score\nfrom support.common import plot_losses\nfrom support.influence import TorchMLP, fit_torch_model\nfrom support.shapley import load_wine_dataset\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom pydvl.influence.torch import (\n    ArnoldiInfluence,\n    CgInfluence,\n    DirectInfluence,\n    EkfacInfluence,\n    LissaInfluence,\n    NystroemSketchInfluence,\n)\n</code></pre> <pre><code>plt.rcParams[\"figure.figsize\"] = (16, 8)\nplt.rcParams[\"font.size\"] = 12\nplt.rcParams[\"xtick.labelsize\"] = 12\nplt.rcParams[\"ytick.labelsize\"] = 10\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\n</code></pre> <pre><code>training_data, val_data, test_data, feature_names = load_wine_dataset(\n    train_size=0.6, test_size=0.3\n)\n</code></pre> <p>     We will corrupt some of the training points by flipping their labels    </p> <pre><code>num_corrupted_idxs = 10\ntraining_data[1][:num_corrupted_idxs] = torch.tensor(\n    [(val + 1) % 3 for val in training_data[1][:num_corrupted_idxs]]\n)\n</code></pre> <p>     and let's wrap it in a pytorch data loader    </p> <pre><code>training_data_loader = DataLoader(\n    TensorDataset(*training_data), batch_size=32, shuffle=False\n)\nval_data_loader = DataLoader(TensorDataset(*val_data), batch_size=32, shuffle=False)\ntest_data_loader = DataLoader(TensorDataset(*test_data), batch_size=32, shuffle=False)\n</code></pre> <pre><code>feature_dimension = 13\nnum_classes = 3\nnetwork_size = [16, 16]\nlayers_size = [feature_dimension, *network_size, num_classes]\nnum_epochs = 300\nlr = 0.005\nweight_decay = 0.01\n\nnn_model = TorchMLP(layers_size)\nnn_model.to(device)\n\noptimizer = Adam(params=nn_model.parameters(), lr=lr, weight_decay=weight_decay)\nscheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\nlosses = fit_torch_model(\n    model=nn_model,\n    training_data=training_data_loader,\n    val_data=val_data_loader,\n    loss=F.cross_entropy,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    num_epochs=num_epochs,\n    device=device,\n)\n</code></pre> <p>     Let's check that the training has found a stable minimum by plotting the training and validation loss    </p> <pre><code>plot_losses(losses)\n</code></pre> <p>     Since it is a classification problem, let's also take a look at the confusion matrix on the test set    </p> <p>     And let's compute the f1 score of the model    </p> <pre><code>f1_score(test_data[1], pred_y_test, average=\"weighted\")\n</code></pre> <pre>\n<code>0.9633110554163186</code>\n</pre> <p>     Let's now move to calculating influences of each point on the total score.    </p> <pre><code>influence_model = DirectInfluence(\n    nn_model,\n    F.cross_entropy,\n    regularization=0.1,\n)\ninfluence_model = influence_model.fit(training_data_loader)\ntrain_influences = influence_model.influences(*test_data, *training_data, mode=\"up\")\n</code></pre> <p>     the returned matrix, train_influences, has a quantity of columns equal to the points in the training set, and a number of rows equal to the points in the test set. At each element           \\(a_{i,j}\\)          it stores the influence that training point           \\(j\\)          has on the classification of test point           \\(i\\)          .    </p> <p>     If we take the average across every column of the influences matrix, we obtain an estimate of the overall influence of a training point on the total accuracy of the network.    </p> <pre><code>mean_train_influences = np.mean(train_influences.cpu().numpy(), axis=0)\nmean_train_influences.shape\n</code></pre> <pre>\n<code>(106,)</code>\n</pre> <p>     The following histogram shows that there are big differences in score within the training set (notice the log-scale on the y axis).    </p> <p>     We can see that the corrupted points tend to have a negative effect on the model, as expected    </p> <pre>\n<code>Average influence of corrupted points:  -1.076019\nAverage influence of other points:  0.10683939\n</code>\n</pre> <p>     We have seen how to calculate the influence of single training points on each test point using influence_type 'up'. Using influence_type 'perturbation' we can also calculate the influence of the input features of each point. In the next cell we will calculate the average influence of each feature on training and test points, and ultimately assess which are the most relevant to model performance.    </p> <pre><code>influence_model.regularization = 1.0\nfeature_influences = influence_model.influences(\n    *test_data, *training_data, mode=\"perturbation\"\n)\n</code></pre> <p>     The explicit calculation of the Hessian matrix is numerically challenging, and due to the high memory need infeasible for larger models.  PyDVL allows to use several approximation methods for the action of the inverse Hessian matrix to overcome this bottleneck:    </p> <ul> <li>      Iteration-based:      <ul> <li>        Conjugate Gradients (Cg)       </li> <li>        Linear time Stochastic Second-Order Approximation (                 LiSSA                )       </li> </ul> </li> <li>      Low-rank Approximations:      <ul> <li>        Arnoldi       </li> <li>        Nystr\u00f6m Sketch-and-Solve (Nystr\u00f6m)       </li> </ul> </li> <li>      Factorization-based:      <ul> <li>        Eigenvalue-corrected Kronecker Factorization (                 EKFAC                )       </li> </ul> </li> </ul> <p>     In the following, we show the usage of these approximation methods and investigate their performance.    </p> <p>     Since the Hessian is symmetric and positive definite (at least after applying a sufficient regularization), we can utilize the           Conjugate Gradients Algorithm          to approximately solve the equations    </p>      \\[ (H + \\lambda \\operatorname{I}) x = b\\]     <p>     Most importantly, the algorithm do not require the computation of the full Hessian matrix, but only requires the implementation of Hessian vector products. pyDVL implements a stable block variant of preconditioned conjugate gradients algorithm.    </p> <pre><code>from pydvl.influence.torch.preconditioner import NystroemPreconditioner\n\nnn_model.to(\"cpu\")\ncg_influence_model = CgInfluence(\n    nn_model,\n    F.cross_entropy,\n    regularization=0.1,\n    progress=True,\n    solve_simultaneously=True,\n    preconditioner=NystroemPreconditioner(rank=5),\n)\ncg_influence_model = cg_influence_model.fit(training_data_loader)\ncg_train_influences = cg_influence_model.influences(\n    *test_data, *training_data, mode=\"up\"\n)\nmean_cg_train_influences = np.mean(cg_train_influences.numpy(), axis=0)\n</code></pre> <p>     Let's compare the results obtained through conjugate gradient with those from the direct method    </p> <pre>\n<code>Percentage error of Cg over direct method:30.799928307533264 %\n</code>\n</pre> <pre>\n<code>Pearson Correlation Cg vs direct 0.9975120139679169\nSpearman Correlation Cg vs direct 0.9968058039650353\n</code>\n</pre> <p>     The           LiSSA          method is a stochastic approximation of the inverse Hessian vector product. Compared to conjugate gradient it is faster but less accurate and typically suffers from instability.    </p> <p>     In order to find the solution of the HVP,           LiSSA          iteratively approximates the inverse of the Hessian matrix with the following update:    </p>      \\[H^{-1}_{j+1} b = b + (I - d) \\ H - \\frac{H^{-1}_j b}{s},\\]     <p>     where           \\(d\\)          and           \\(s\\)          are a dampening and a scaling factor.    </p> <pre><code>lissa_influence_model = LissaInfluence(\n    nn_model,\n    F.cross_entropy,\n    regularization=0.1,\n    progress=True,\n)\nlissa_influence_model = lissa_influence_model.fit(training_data_loader)\nlissa_train_influences = lissa_influence_model.influences(\n    *test_data, *training_data, mode=\"up\"\n)\nmean_lissa_train_influences = np.mean(lissa_train_influences.numpy(), axis=0)\n</code></pre> <pre>\n<code>Percentage error of Lissa over direct method:43.32989752292633 %\n</code>\n</pre> <pre>\n<code>Pearson Correlation Lissa vs direct 0.9940533986762107\nSpearman Correlation Lissa vs direct 0.9942061112930448\n</code>\n</pre> <p>     The Arnoldi method leverages a low rank approximation of the Hessian matrix to reduce the memory requirements. It is generally much faster than the conjugate gradient method and can achieve similar accuracy.    </p> <pre><code>arnoldi_influence_model = ArnoldiInfluence(\n    nn_model,\n    F.cross_entropy,\n    rank=30,\n    regularization=0.1,\n)\narnoldi_influence_model = arnoldi_influence_model.fit(training_data_loader)\narnoldi_train_influences = arnoldi_influence_model.influences(\n    *test_data, *training_data, mode=\"up\"\n)\nmean_arnoldi_train_influences = np.mean(arnoldi_train_influences.numpy(), axis=0)\n</code></pre> <pre>\n<code>Percentage error of Arnoldi over direct method:44.838228821754456 %\n</code>\n</pre> <pre>\n<code>Pearson Correlation Arnoldi vs direct 0.9876463041029632\nSpearman Correlation Arnoldi vs direct 0.9772879562687357\n</code>\n</pre> <p>     Similar to the Arnoldi method. the Nystr\u00f6m method uses a low-rank approximation, which is computed from random projections of the Hessian matrix. In general the approximation is expected to be worse then the Arnoldi approximation, but is cheaper to compute.    </p> <pre><code>nystroem_influence_model = NystroemSketchInfluence(\n    nn_model,\n    F.cross_entropy,\n    rank=30,\n    regularization=0.1,\n)\nnystroem_influence_model = nystroem_influence_model.fit(training_data_loader)\nnystroem_train_influences = nystroem_influence_model.influences(\n    *test_data, *training_data, mode=\"up\"\n)\nmean_nystroem_train_influences = np.mean(nystroem_train_influences.numpy(), axis=0)\n</code></pre> <pre>\n<code>Percentage error of Nystr\u00f6m over direct method:52.49669551849365 %\n</code>\n</pre> <pre>\n<code>Pearson Correlation Nystr\u00f6m vs direct 0.9949392350441726\nSpearman Correlation Nystr\u00f6m vs direct 0.9883719172733456\n</code>\n</pre> <p>     The           EKFAC          method is a more recent technique that leverages the Kronecker product structure of the Hessian matrix to reduce the memory requirements. It is generally much faster than iterative methods like conjugate gradient and Arnoldi and it allows for an easier handling of memory. Therefore, it is the only technique that can scale to very large models (e.g. billions of parameters). Its accuracy is however much worse. Let's see how it performs on our example.    </p> <pre><code>ekfac_influence_model = EkfacInfluence(\n    nn_model,\n    update_diagonal=True,\n    hessian_regularization=0.1,\n)\nekfac_influence_model = ekfac_influence_model.fit(training_data_loader)\nekfac_train_influences = ekfac_influence_model.influences(\n    *test_data, *training_data, mode=\"up\"\n)\nmean_ekfac_train_influences = np.mean(ekfac_train_influences.numpy(), axis=0)\n</code></pre> <pre>\n<code>Percentage error of EK-FAC over direct method:1528.7002563476562 %\n</code>\n</pre> <p>     The accuracy is not good, and it is not recommended to use this method for small models. Nevertheless, a look at the actual influence values reveals that the EK-FAC estimates are not completely off.    </p> <p>     The above plot shows a good correlation between the EK-FAC and the direct method. Corrupted points have been circled in red, and in both the direct and approximate case they are correcly identified as having negative influence on the model's accuracy. This is confirmed by explicit calculation of the Pearson and Spearman correlation coefficients.    </p> <pre>\n<code>Pearson Correlation EK-FAC vs direct 0.9579606771217988\nSpearman Correlation EK-FAC vs direct 0.895971987807643\n</code>\n</pre> <p>     The correlation between the EK-FAC and the direct method is quite good, and it improves significantly if we just keep top-20 highest absolute influences.    </p> <pre>\n<code>Pearson Correlation EK-FAC vs direct - top-20 influences 0.9729110282877721\nSpearman Correlation EK-FAC vs direct - top-20 influences 0.9759398496240601\n</code>\n</pre> <p>     When we calculate influence scores, typically we are more interested in assessing which training points have the highest or lowest impact on the model rather than having a precise estimate of the influence value. EK-FAC then provides a fast and memory-efficient way to calculate a coarse influence ranking of the training points which scales very well even to the largest neural networks.    </p> <p>     This was a quick introduction to the pyDVL interface for influence functions. Despite their speed and simplicity, influence functions are known to be a very noisy estimator of data quality, as pointed out in the paper           \"Influence functions in deep learning are fragile\"          . The size of the network, the weight decay, the inversion method used for calculating influences, the size of the test set: they all add up to the total amount of noise. Experiments may therefore give quantitative and qualitatively different results if not averaged across several realisations. Shapley values, on the contrary, have shown to be a more robust, but this comes at the cost of high computational requirements. PyDVL employs several parallelization and caching techniques to optimize such calculations.    </p>"},{"location":"examples/influence_wine/#influence-functions-for-outlier-detection","title":"Influence functions for outlier detection","text":"<p>     This notebook shows how to calculate influences on a NN model using pyDVL for an arbitrary dataset, and how this can be used to find anomalous or corrupted data points.    </p> <p>     It uses the wine dataset from sklearn: given a set of 13 different input parameters regarding a particular bottle, each related to some physical property (e.g. concentration of magnesium, malic acidity, alcoholic percentage, etc.), the model will need to predict to which of 3 classes the wine belongs to. For more details, please refer to the           sklearn documentation          .    </p>"},{"location":"examples/influence_wine/#imports","title":"Imports","text":""},{"location":"examples/influence_wine/#dataset","title":"Dataset","text":""},{"location":"examples/influence_wine/#fit-a-neural-network-to-the-data","title":"Fit a neural network to the data","text":"<p>     We will train a 2-layer neural network. PyDVL has some convenience wrappers to initialize a pytorch NN. If you already have a model loaded and trained, you can skip this section.    </p>"},{"location":"examples/influence_wine/#calculating-influences-for-small-neural-networks","title":"Calculating influences for small neural networks","text":"<p>     The following cell calculates the influences of each training data point on the neural network.  Neural networks have typically a very bumpy parameter space, which, during training, is explored until the configuration that minimises the loss is found. There is an important assumption in influence functions that the model lays at a (at least local) minimum of such loss, and if this is not fulfilled many issues can arise. In order to avoid this scenario, a regularisation term should be used whenever dealing with big and noisy models.    </p>"},{"location":"examples/influence_wine/#influence-of-training-features","title":"Influence of training features","text":""},{"location":"examples/influence_wine/#speeding-up-influences-for-big-models","title":"Speeding up influences for big models","text":""},{"location":"examples/influence_wine/#cg","title":"Cg","text":""},{"location":"examples/influence_wine/#lissa","title":"Lissa","text":""},{"location":"examples/influence_wine/#arnoldi","title":"Arnoldi","text":""},{"location":"examples/influence_wine/#nystrom","title":"Nystr\u00f6m","text":""},{"location":"examples/influence_wine/#ekfac","title":"EKFAC","text":""},{"location":"examples/influence_wine/#conclusions","title":"Conclusions","text":""},{"location":"examples/least_core_basic/","title":"Least Core","text":"<p>     We will be using the following functions and classes from pyDVL.    </p> <pre><code>from pydvl.reporting.plots import shaded_mean_std\nfrom pydvl.reporting.scores import compute_removal_score\nfrom pydvl.valuation import (\n    Dataset,\n    ExactLeastCoreValuation,\n    ModelUtility,\n    MonteCarloLeastCoreValuation,\n    SupervisedScorer,\n)\n</code></pre> <pre><code>X, y = make_classification(\n    n_samples=dataset_size,\n    n_features=50,\n    n_informative=25,\n    n_classes=3,\n    random_state=seed,\n)\n\ntrain, test = Dataset.from_arrays(X, y, stratify_by_target=True, random_state=seed)\n</code></pre> <pre><code>model = LogisticRegression(max_iter=500, solver=\"liblinear\")\nmodel.fit(train.data().x, train.data().y);\n</code></pre> <pre><code>train_mini = train[:train_mini_size]\nscorer = SupervisedScorer(\"accuracy\", test_data=test, default=0, range=(0, 1))\nutility = ModelUtility(model=model, scorer=scorer)\nvaluation = ExactLeastCoreValuation(utility=utility)\n</code></pre> <pre><code>from joblib import parallel_config\n\nwith parallel_config(n_jobs=n_jobs):\n    valuation.fit(train_mini)\n</code></pre> <pre><code>exact_result = valuation.values()\nexact_df = exact_result.to_dataframe(column=\"exact\").T\nexact_df = exact_df[sorted(exact_df.columns)]\n</code></pre> <pre><code>def relative_error(x, estimate, norm):\n    return np.linalg.norm(x - estimate, ord=norm) / np.linalg.norm(x, ord=norm)\n\n\nbudget_array = np.logspace(8, len(train_mini), base=2, num=8, endpoint=False, dtype=int)\n\nall_estimated_values_df = []\nall_errors = {budget: [] for budget in budget_array}\n\nwith tqdm(total=len(budget_array) * n_runs) as pbar:\n    for budget in budget_array:\n        pbar.set_description(f\"Computing values with {budget} constraints\")\n        dfs = []\n        errors = []\n        column_name = f\"estimated_{budget}\"\n        valuation = MonteCarloLeastCoreValuation(\n            utility=utility, n_samples=budget, progress=False\n        )\n        for i in range(n_runs):\n            with parallel_config(n_jobs=n_jobs):\n                valuation.fit(train_mini)\n            df = (\n                valuation.values()\n                .to_dataframe(column=column_name)\n                .drop(columns=[f\"{column_name}_variances\", f\"{column_name}_counts\"])\n                .T\n            )\n            df = df[sorted(df.columns)]\n            error = relative_error(\n                exact_df.loc[\"exact\"].values, np.nan_to_num(df.values.ravel()), norm=1\n            )\n            all_errors[budget].append(error)\n            df[\"budget\"] = budget\n            dfs.append(df)\n            pbar.update(1)\n        estimated_values_df = pd.concat(dfs)\n        all_estimated_values_df.append(estimated_values_df)\n\nvalues_df = pd.concat(all_estimated_values_df)\nerrors_df = pd.DataFrame(all_errors)\n</code></pre> <p>     We can see that the approximation error decreases as the number of constraints increases, but that there are diminishing returns for increasing the budget beyond a certain point.    </p> <pre><code>from pydvl.valuation.methods.random import RandomValuation\n\nremoval_percentages = np.arange(0, 0.41, 0.05)\nmethods = [\n    RandomValuation(random_state=seed),\n    MonteCarloLeastCoreValuation(\n        utility=utility, n_samples=n_iterations, progress=False, seed=seed\n    ),\n]\nall_scores = []\nfor i in trange(n_runs, position=0, desc=f\"Removing best points, {n_runs} times\"):\n    for method in methods:\n        with parallel_config(n_jobs=n_jobs):\n            valuation.fit(train)\n        result = valuation.values()\n\n        scores = compute_removal_score(\n            utility,\n            result,\n            train,\n            removal_percentages,\n            remove_best=True,\n            progress=False,\n        )\n\n        scores[\"method_name\"] = method.__class__.__name__\n        all_scores.append(scores)\n\nscores_df = pd.DataFrame(all_scores)\n</code></pre> <p>     We can clearly see that removing the most valuable data points, as given by the Least Core method, leads to, on average, a decrease in the model's performance and that the method outperforms random removal of data points.    </p> <pre><code>all_scores = []\nfor i in trange(n_runs, position=0, desc=f\"Removing best points, {n_runs} times\"):\n    for method in methods:\n        with parallel_config(n_jobs=n_jobs):\n            valuation.fit(train)\n        result = valuation.values()\n\n        scores = compute_removal_score(\n            utility,\n            result,\n            train,\n            removal_percentages,\n            remove_best=False,\n            progress=False,\n        )\n\n        scores[\"method_name\"] = method.__class__.__name__\n        all_scores.append(scores)\n\nscores_df = pd.DataFrame(all_scores)\n</code></pre> <p>     We can clearly see that removing the least valuable data points, as given by the Least Core method, leads to, on average, an increase in the model's performance and that the method outperforms the random removal of data points.    </p>"},{"location":"examples/least_core_basic/#least-core-for-data-valuation","title":"Least Core for Data Valuation","text":"<p>     This notebook introduces Least Core methods for the computation of data values using pyDVL.    </p> <p>     Shapley values define a fair way of distributing the worth of the whole training set when every data point is part of it. But they do not consider the question of stability of subsets: Could some data points obtain a higher payoff if they formed smaller subsets? It is argued that this might be relevant if data providers are paid based on data value, since Shapley values can incentivise them not to contribute their data to the \"grand coalition\", but instead try to form smaller ones. Whether this is of actual practical relevance is debatable, but in any case, the least core is an alternative tool available for any task of Data Valuation    </p> <p>     The Core is another approach to compute data values originating in cooperative game theory that attempts to answer those questions. It is the set of feasible payoffs that cannot be improved upon by a coalition of the participants.    </p> <p>     Its use for Data Valuation was first described in the paper             If You Like Shapley Then You\u2019ll Love the Core            by Tom Yan and Ariel D. Procaccia.    </p> <p>     The Least Core value           \\(v\\)          of the           \\(i\\)          -th sample in dataset           \\(D\\)          wrt. utility           \\(u\\)          is computed by solving the following Linear Program:    </p>      \\[ \\begin{array}{lll} \\text{minimize} &amp; \\displaystyle{e} &amp; \\\\ \\text{subject to} &amp; \\displaystyle\\sum_{x_i\\in D} v_u(x_i) = u(D) &amp; \\\\ &amp; \\displaystyle\\sum_{x_i\\in S} v_u(x_i) + e \\geq u(S) &amp;, \\forall S \\subset D, S \\neq \\emptyset \\\\ \\end{array} \\]     <p>     To illustrate this method we will use a synthetic dataset. We will first use a subset of 10 data point to compute the exact values and use them to assess the Monte Carlo approximation. Afterwards, we will conduct the data removal experiments as described by Ghorbani and Zou in their paper           Data Shapley: Equitable Valuation of Data for Machine Learning          : We compute the data valuation given different computation budgets and incrementally remove a percentage of the best, respectively worst, data points and observe how that affects the utility.    </p>"},{"location":"examples/least_core_basic/#setup","title":"Setup","text":"<p>     We begin by importing the main libraries and setting some defaults.    </p>      If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience."},{"location":"examples/least_core_basic/#dataset","title":"Dataset","text":"<p>     We generate a synthetic dataset using the      <code>       make_classification      </code>      function from scikit-learn.    </p> <p>     We sample 200 data points from a 50-dimensional Gaussian distribution with 25 informative features and 25 non-informative features (generated as random linear combinations of the informative features).    </p> <p>     The 200 samples are uniformly distributed across 3 classes with a small percentage of noise added to the labels to make the task a bit more difficult.    </p>"},{"location":"examples/least_core_basic/#estimating-least-core-values","title":"Estimating Least Core Values","text":"<p>     In this first section we use a smaller subset of the dataset containing 12 samples in order to be able to compute exact values reasonably fast. Recall that, in order to assemble the problem, for every subset           \\(S \\subset D\\)          we must compute the utility           \\(u(S).\\)          We then have a linear problem with           \\(2^{|D|}\\)          constraints to solve. After doing this, we use the Monte Carlo method with a limited budget (maximum number of constraints) to approximate the           LC          values on the same reduced dataset, and we repeat this several times to assess the stability of the approximation.    </p>"},{"location":"examples/least_core_basic/#data-removal","title":"Data Removal","text":"<p>     In the final two experiments, we rank the training set according to the value estimates obtained with Monte Carlo Least Core. Then, we incrementally remove up to 40% of the most / least valuable training points, train the model on this subset and compute its accuracy on the previously held-out test set.    </p>"},{"location":"examples/least_core_basic/#remove-best","title":"Remove Best","text":"<p>     We start by removing the best data points and seeing how the model's accuracy evolves. We repeat the whole process (valuation and removal) several times to assess the stability of the results.    </p>"},{"location":"examples/least_core_basic/#remove-worst","title":"Remove Worst","text":"<p>     We then proceed to removing the worst data points and seeing how the model's accuracy evolves.    </p>"},{"location":"examples/msr_banzhaf_digits/","title":"Banzhaf semivalues","text":"<pre><code>train, test = load_digits_dataset(train_size=0.7, random_state=random_state)\n</code></pre> <pre><code>from support.banzhaf import TorchCNNModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TorchCNNModel(lr=0.001, epochs=n_epochs, batch_size=batch_size, device=device)\nmodel.fit(*train.data())\n</code></pre> <pre>\n<code>Training accuracy: 0.698\nTest accuracy: 0.711\n</code>\n</pre> <p>     As with all other model-based valuation methods, for Data Banzhaf we need a scoring function to measure performance of the model over the test set. We will use accuracy, but it can be anything, like e.g.           \\(R^2\\)          , using strings from the           standard sklearn scoring methods          , passed to           SupervisedScorer          .    </p> <p>     We group our torch model and the scoring function into an instance of           ModelUtility          .    </p> <pre><code>from pydvl.valuation.samplers import PermutationSampler, RelativeTruncation\nfrom pydvl.valuation.scorers import SupervisedScorer\nfrom pydvl.valuation.stopping import MinUpdates\nfrom pydvl.valuation.utility import ModelUtility\n\naccuracy_over_test_set = SupervisedScorer(\n    \"accuracy\", test_data=test, default=0.0, range=(0, 1)\n)\n\nutility = ModelUtility(model=model, scorer=accuracy_over_test_set)\n</code></pre> <p>     In order to compute the Banzhaf semi-values, we use           BanzhafValuation          , which also requires choosing a sampler and a stopping criterion.    </p> <p>     We use the standard           PermutationSampler          , and choose to stop computation using the           MinUpdates          stopping criterion, which terminates after a fixed number of value updates. This is a simple stopping criterion, but it is not very efficient. We will later compare it to           RankCorrelation          , which terminates after the change in Spearman correlation between two successive iterations is below a certain threshold.    </p> <p>     We also define a relative           TruncationPolicy          , which is a policy used to early stop computation of marginal values in permutations, once the utility is close to the total utility. This is a heuristic to speed up computation introduced in the Data-Shapley paper called Truncated Monte Carlo Shapley. Note how we tell it to wait until at least 50% of every permutation has been processed in order to start evaluation. This is to ensure that noise doesn't stop the computation too early.    </p> <pre><code>truncation = RelativeTruncation(rtol=0.05, burn_in_fraction=0.5)\nsampler = PermutationSampler(truncation=truncation)\nstopping = MinUpdates(100)\n</code></pre> <p>     We now instantiate and fit the valuation. Note how parallelization is just a matter of using joblib's context manager     <code>      parallel_config     </code>     in order to set the number of jobs.    </p> <pre><code>from joblib import parallel_config\n\nfrom pydvl.valuation.methods import BanzhafValuation\n\nvaluation = BanzhafValuation(utility, sampler=sampler, is_done=stopping, progress=True)\n\nwith parallel_config(n_jobs=n_jobs):\n    valuation.fit(train)\n\nvalues = valuation.values()\nvalues.sort(key=\"value\")\ndf = values.to_dataframe(column=\"banzhaf_value\")\n</code></pre> <p>     For convenience, we have transformed the values into a dataframe. It includes columns with the mean, variance and number of updates of the Monte Carlo estimates for the values:    </p>            banzhaf_value                      banzhaf_value_variances                      banzhaf_value_counts                      240                      -7.488506e-42                      1.109645e-94                      100                      991                      -4.254834e-42                      3.424364e-94                      100                      792                      -2.627003e-42                      1.318692e-100                      100                      265                      -2.535799e-42                      2.243686e-95                      100                      1020                      -1.209766e-42                      1.270162e-93                      100                      ...                      ...                      ...                      ...                      1021                      3.573181e-42                      1.264143e-93                      100                      312                      6.377917e-42                      8.087408e-97                      100                      1018                      1.126661e-41                      1.038240e-93                      100                      653                      1.610007e-41                      2.148715e-110                      100                      433                      2.065544e-41                      2.471744e-102                      100           <p>        1257 rows \u00d7 3 columns       </p> <p>     Let us plot the results. In the next cell we will take the 10 images with the lowest score and plot their values with 95% Normal confidence intervals. Keep in mind that Permutation Monte Carlo Banzhaf is typically very noisy, and it can take many steps to arrive at a clean estimate.    </p> <p>     For the first 5 images, we will falsify their label, for images 6-10, we will add some noise.    </p> <pre><code>x_train_anomalous = train.data().x.copy()\ny_train_anomalous = train.data().y.copy()\nanomalous_indices = high_values.index.map(int).values\n\n# Change the label of the first 5 images\ny_train_anomalous[anomalous_indices[:5]] = np.mod(\n    y_train_anomalous[anomalous_indices[:5]] + 1, 10\n)\n\n# Add noise to images 6-10\ncurrent_images = x_train_anomalous[anomalous_indices[5:10]]\nnoisy_images = current_images + 0.5 * np.random.randn(*current_images.shape)\nnoisy_images[noisy_images &amp;lt; 0] = 0.0\nnoisy_images[noisy_images &amp;gt; 1] = 1.0\nx_train_anomalous[anomalous_indices[5:10]] = noisy_images\n</code></pre> <pre><code>from pydvl.valuation.dataset import Dataset\n\nanomalous_dataset = Dataset(x=x_train_anomalous, y=y_train_anomalous)\n\n# Note that we reuse the same stopping criterion. fit() resets it, but\n# to be sure we can always call stopping.reset()\nanomalous_valuation = BanzhafValuation(\n    utility, sampler=sampler, is_done=stopping.reset(), progress=True\n)\n\nwith parallel_config(n_jobs=n_jobs):\n    anomalous_valuation.fit(anomalous_dataset)\n\nanomalous_values = anomalous_valuation.values()\nanomalous_values.sort(key=\"value\")\nanomalous_df = anomalous_values.to_dataframe(column=\"banzhaf_value\")\n</code></pre> <p>     Let us now look at how the value has changed for the images that we manipulated:    </p> <p>     As can be seen in this figure, the valuation of the data points has decreased significantly by adding noise or falsifying their labels. This shows the potential of using Banzhaf values or other data valuation methods to detect mislabeled data points or noisy input data.    </p> <p>     All that is required to compute the values with           MSR          is using           MSRSampler          as sampler.    </p> <p>     Because values converge much faster, we can use a better stopping criterion. Instead of fixing the number of value updates with           MinUpdates          , we use           RankCorrelation          to stop when the change in Spearman correlation between the ranking of two successive iterations is below a threshold. Despite the much stricter stopping criterion, fitting the Banzhaf values with the           MSR          sampler is much faster.    </p> <pre><code>from pydvl.valuation.samplers import MSRSampler\nfrom pydvl.valuation.stopping import RankCorrelation\n\nvaluation = BanzhafValuation(\n    utility,\n    sampler=MSRSampler(batch_size=32, seed=random_state),\n    is_done=RankCorrelation(rtol=1e-4, burn_in=64),\n    progress=True,\n)\n\nwith parallel_config(n_jobs=n_jobs):\n    valuation.fit(train)\n\nmsr_values = valuation.values()\nmsr_values.sort(key=\"value\")\nmsr_df = msr_values.to_dataframe(column=\"banzhaf_value\")\n</code></pre> <p>     Inspection of the values reveals (generally) much lower variances. Notice the number of updates to each value as well.    </p> <pre><code>max_checks = 1000\nmoving_avg = 200\n</code></pre> <pre><code>from pydvl.valuation import IndexSampler, SemivalueValuation, ValuationResult\nfrom pydvl.valuation.stopping import HistoryDeviation, MaxChecks\n\n\ndef compute_semivalues_and_history(\n    method_t: Type[SemivalueValuation],\n    sampler_t: Type[IndexSampler],\n    sampler_args: dict,\n    max_checks: int,\n    progress: bool = True,\n):\n    history = HistoryDeviation(n_steps=max_checks, rtol=1e-6)\n    valuation = method_t(\n        utility,\n        sampler=sampler_t(**sampler_args, seed=random_state),\n        is_done=MaxChecks(max_checks + 2) | history,\n        progress=progress,\n    )\n    with parallel_config(n_jobs=n_jobs):\n        valuation.fit(train)\n\n    return history, valuation.values()\n</code></pre> <pre><code>from pydvl.valuation.samplers import (\n    AntitheticSampler,\n    HarmonicSampleSize,\n    RandomIndexIteration,\n    RandomSizeIteration,\n    StratifiedSampler,\n    UniformSampler,\n)\n\nexperiments = OrderedDict(\n    [\n        (\n            PermutationSampler,\n            {\n                \"name\": \"Permutation\",\n                \"truncation\": RelativeTruncation(rtol=0.05, burn_in_fraction=0.5),\n            },\n        ),\n        (MSRSampler, {\"name\": \"MSR\", \"kwargs\": {\"batch_size\": 16}}),\n        (UniformSampler, {\"name\": \"Uniform\", \"kwargs\": {}}),\n        (AntitheticSampler, {\"name\": \"Antithetic\", \"kwargs\": {}}),\n        (\n            StratifiedSampler,\n            {\n                \"name\": \"Stratified\",\n                \"kwargs\": {\n                    \"sample_sizes\": HarmonicSampleSize(1),\n                    \"sample_sizes_iteration\": RandomSizeIteration,\n                    \"index_iteration\": RandomIndexIteration,\n                },\n            },\n        ),\n    ]\n)\n\nresults = {}\nhistory = {}\n\nfor sampler_t, params in experiments.items():\n    history[sampler_t], results[sampler_t] = compute_semivalues_and_history(\n        BanzhafValuation, sampler_t, params.get(\"kwargs\", {}), max_checks\n    )\n</code></pre> <p>     The plot above visualizes the convergence speed of different samplers used for Banzhaf semi-value calculation. It shows the average magnitude of how much the semi-values are updated in every step of the algorithm.    </p> <p>     As you can see,             MSR            Banzhaf          stabilizes much faster. After 1000 iterations (subsets sampled and evaluated with the utility), Permutation Monte Carlo Banzhaf has evaluated the marginal function about 5 times per data point (we are using 200 data points). For           MSR          , the semi-value of each data point was updated 1000 times. Due to this, the values converge much faster wrt. the number of utility evaluations, which is the key advantage of           MSR          sampling.    </p> <p>       MSR          sampling does come at a cost, however, which is that the updates to the semi-values are more noisy than in other methods.  We will analyze the impact of this tradeoff in the next sections. First, let us look at how similar all the computed semi-values are. They are all Banzhaf values, so in a perfect world, all samplers should result in the exact same semi-values. However, due to randomness in the utility (recall that we use a neural network) and randomness in the samplers, the resulting values are likely never exactly the same. Another quality measure is that a good sampler would lead to very consistent values, a bad one to less consistent values. Let us first examine how similar the results are, then we'll look at consistency.    </p> <p>     This plot shows that the samplers lead to quite different Banzhaf semi-values, however, all of them have some points in common. The           MSR          Sampler does not seem to be significantly worse than any others.    </p> <p>     In an ideal setting without randomness, the overlap of points would be higher, however, the stochastic nature of the CNN model that we use together with the fact that we use only 200 data points for training, might overshadow these results. As a matter of fact we have the rather discouraging following result:    </p>"},{"location":"examples/msr_banzhaf_digits/#banzhaf-semi-values-for-data-valuation","title":"Banzhaf Semi-values for data valuation","text":"<p>     This notebook showcases           Data Banzhaf: A Robust Data Valuation Framework for Machine Learning          by Wang, and Jia.    </p> <p>     Computing Banzhaf semi-values using pyDVL follows basically the same procedure as all other semi-value-based methods like Shapley values. However, Data-Banzhaf tends to be more robust to stochasticity in the training process than other semi-values. A property that we study here.    </p> <p>     Additionally, we compare two sampling techniques: the standard permutation-based Monte Carlo sampling, and the so-called           MSR          (Maximum Sample Reuse) principle.    </p> <p>     In order to highlight the strengths of Data-Banzhaf, we require a stochastic model. For this reason, we use a CNN to classify handwritten digits from the           scikit-learn toy datasets          .    </p>"},{"location":"examples/msr_banzhaf_digits/#setup","title":"Setup","text":"If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience."},{"location":"examples/msr_banzhaf_digits/#the-dataset","title":"The dataset","text":"<p>     The data consists of ~1800 grayscale images of 8x8 pixels with 16 shades of gray. These images contain handwritten digits from 0 to 9. The helper function     <code>      load_digits_dataset()     </code>     downloads and prepares it for usage returning two           Datasets          .    </p>"},{"location":"examples/msr_banzhaf_digits/#creating-the-utility-and-computing-banzhaf-semi-values","title":"Creating the utility and computing Banzhaf semi-values","text":"<p>     Now we can calculate the contribution of each training sample to the model performance. We use a simple CNN written in torch, and wrapped into an object to convert numpy arrays into tensors (as of v0.9.2 valuation methods in pyDVL work only with numpy arrays). Note that any model that implements the protocol           SupervisedModel          , which is just the standard sklearn interface of     <code>      fit()     </code>     ,     <code>      predict()     </code>     and     <code>      score()     </code>     can be used to construct the utility.    </p>"},{"location":"examples/msr_banzhaf_digits/#evaluation-on-anomalous-data","title":"Evaluation on anomalous data","text":"<p>     An interesting use-case for data valuation is finding anomalous data. Maybe some of the data is really noisy or has been mislabeled. To simulate this for our dataset, we will change some of the labels and add noise to some images. Intuitively, these anomalous data points should then have a lower value.    </p> <p>     To evaluate this, let us first check the average value of the 10 data points with the highest value, as these will be the ones that we modify:    </p>"},{"location":"examples/msr_banzhaf_digits/#maximum-sample-reuse-banzhaf","title":"Maximum Sample Reuse Banzhaf","text":"<p>     Despite the previous results already being useful, we had to retrain the model a number of times and yet the variance of the value estimates was high. This has consequences for the stability of the top-k ranking of points, which decreases the applicability of the method. We will now use a different sampling method called Maximum Sample Reuse (           MSR          ) which reuses every sample for updating the Banzhaf values. The method was introduced by the authors of Data-Banzhaf and is much more sample-efficient, as we will show.    </p>"},{"location":"examples/msr_banzhaf_digits/#compare-convergence-speed-of-banzhaf-and-msr-banzhaf-values","title":"Compare convergence speed of Banzhaf and           MSR          Banzhaf Values","text":"<p>     Conventional margin-based samplers produce require evaluating the utility twice to do one update of the value, and permutation samplers do instead           \\(n+1\\)          evaluations for           \\(n\\)          updates. Maximum Sample Reuse (           MSR          ) updates instead all indices in every sample that the utility evaluates. We compare the convergence rates of these methods.    </p> <p>     In order to do so, we will compute the semi-values using different samplers and use a high number of iterations to make sure that the values have converged.    </p>"},{"location":"examples/msr_banzhaf_digits/#similarity-of-the-semi-values-computed-using-different-samplers","title":"Similarity of the semi-values computed using different samplers","text":""},{"location":"examples/msr_banzhaf_digits/#consistency-of-the-semi-values","title":"Consistency of the semi-values","text":"<p>     Finally, we want to analyze how consistent the semi-values are when computed using the different samplers. In order to do this, we calculate them multiple times and check how many of the data points in the top and lowest 20% of the valuation overlap.    </p>"},{"location":"examples/msr_banzhaf_digits/#conclusion","title":"Conclusion","text":"<p>       MSR          sampling updates the semi-value estimates for every index in the sample, much more frequently than any other sampler available, which leads to much           faster convergence          . Additionally, the sampler is more consistent with its value estimates than the other samplers, which might be caused by the higher number of value updates.    </p> <p>     There is alas no general recommendation. It is best to try different samplers when computing semi-values and test which one is best suited for your use case. Nevertheless, the           MSR          sampler seems like a more efficient sampler which may bring fast results and is well-suited for stochastic models.    </p>"},{"location":"examples/shapley_basic_spotify/","title":"Shapley values","text":"<p>     This notebook introduces Shapley methods for the computation of data value using pyDVL.    </p> <p>     In order to illustrate the practical advantages, we will predict the popularity of songs in the dataset           Top Hits Spotify from 2000-2019          , and highlight how data valuation can help investigate and boost the performance of the models. In doing so, we will describe the basic usage patterns of pyDVL.    </p> <p>     Recall that data value is a function of three things:    </p> <ol> <li>      The dataset.     </li> <li>      The model.     </li> <li>      The performance metric or scoring function.     </li> </ol> <p>     Below we will describe how to instantiate each one of these objects and how to use them for data valuation. Please also see the           documentation on data valuation          .    </p> <pre><code>%matplotlib inline\n\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics._scorer import neg_mean_absolute_error_scorer\nfrom support.shapley import load_spotify_dataset\n\nplt.ioff()  # Prevent jupyter from automatically plotting\nplt.rcParams[\"figure.figsize\"] = (20, 6)\nplt.rcParams[\"font.size\"] = 12\nplt.rcParams[\"xtick.labelsize\"] = 12\nplt.rcParams[\"ytick.labelsize\"] = 10\nplt.rcParams[\"axes.facecolor\"] = (1, 1, 1, 0)\nplt.rcParams[\"figure.facecolor\"] = (1, 1, 1, 0)\n\nis_CI = os.environ.get(\"CI\")\nrandom_state = 24\nrandom.seed(random_state)\nn_jobs = 4\nif is_CI:\n    n_jobs = 1\n</code></pre> <p>     We will use the following classes and functions from pyDVL. The main entry point is the class           ShapleyValuation          , which provides the implementation of the Shapley method. In order to use it we need to instantiate two           Datasets          , a           PermutationSampler          (with a           RelativeTruncation          policy to stop computation early), a           SupervisedScorer          to evaluate the model on the held-out test set, and a           ModelUtility          to hold the model and the scoring function.    </p> <pre><code>from pydvl.reporting.plots import plot_shapley\nfrom pydvl.valuation.dataset import Dataset, GroupedDataset\nfrom pydvl.valuation.methods.shapley import ShapleyValuation\nfrom pydvl.valuation.samplers import PermutationSampler, RelativeTruncation\nfrom pydvl.valuation.scorers import SupervisedScorer\nfrom pydvl.valuation.stopping import HistoryDeviation, MaxUpdates\nfrom pydvl.valuation.utility import ModelUtility\n</code></pre> <pre><code>train_data, val_data, test_data = load_spotify_dataset(\n    val_size=0.3, test_size=0.2, target_column=\"popularity\", random_state=random_state\n)\n</code></pre> <pre><code># In CI we only use a subset of the training set\nif is_CI:\n    ci_test_artists = [\"Billie Eilish\", \"DJ Snake\", \"Eminem\", \"Adele\", \"Maroon 5\"]\n    train_data[0] = train_data[0][train_data[0][\"artist\"].isin(ci_test_artists)]\n    train_data[1] = train_data[1][train_data[0].index]\n</code></pre> <pre><code>train_data[0].head()\n</code></pre>            artist                      duration_ms                      explicit                      year                      danceability                      energy                      key                      loudness                      mode                      speechiness                      acousticness                      instrumentalness                      liveness                      valence                      tempo                      genre                      1588                      Joel Adams                      210580                      False                      2015                      0.513                      0.768                      4                      -4.868                      0                      0.0587                      0.0118                      0.000019                      0.2940                      0.235                      84.264                      1                      1674                      Jonas Blue                      196613                      False                      2016                      0.742                      0.819                      1                      -5.307                      1                      0.0487                      0.3720                      0.000000                      0.2770                      0.709                      117.986                      15                      1957                      Stormzy                      196266                      True                      2019                      0.682                      0.653                      8                      -6.062                      1                      0.3390                      0.1300                      0.001160                      0.1290                      0.428                      188.115                      5                      1707                      Maroon 5                      214265                      False                      2018                      0.783                      0.610                      7                      -6.124                      1                      0.0696                      0.3430                      0.000000                      0.0983                      0.418                      100.047                      14                      1536                      The Weeknd                      235653                      True                      2015                      0.480                      0.682                      7                      -4.940                      1                      0.1300                      0.0696                      0.000000                      0.0463                      0.506                      167.939                      17           <p>     The dataset has many high-level features, some quite intuitive (     <code>      duration_ms     </code>     or     <code>      tempo     </code>     ), while others are a bit more cryptic (     <code>      valence     </code>     ?).    </p> <p>     For detailed information on each feature, please consult           the dataset's website          .    </p> <p>     In our analysis, we will use every column except     <code>      artist     </code>     and     <code>      song     </code>     to predict the     <code>      popularity     </code>     of each song.    </p> <pre><code>artists = train_data[0][\"artist\"]\ntrain_data[0] = train_data[0].drop([\"artist\"], axis=1)\ntest_data[0] = test_data[0].drop([\"artist\"], axis=1)\nval_data[0] = val_data[0].drop([\"artist\"], axis=1)\n</code></pre> <p>     Input and label data are then used to instantiate           Dataset          objects:    </p> <pre><code>train_dataset = Dataset(\n    *train_data, feature_names=train_data[0].columns, target_names=[\"popularity\"]\n)\ntest_dataset = Dataset(\n    *test_data, feature_names=train_data[0].columns, target_names=[\"popularity\"]\n)\nval_dataset = Dataset(\n    *val_data, feature_names=train_data[0].columns, target_names=[\"popularity\"]\n)\n</code></pre> <p>     The calculation of exact Shapley values is computationally very expensive (exponentially so!) because it requires training the model on every possible subset of the training set. For this reason, PyDVL implements techniques to speed up the calculation, such as Monte Carlo approximations,           surrogate models          or           caching          of intermediate results and grouping of data to calculate Shapley values of groups of data points instead of single data points.    </p> <p>     In our case, we will group songs by artist and calculate the Shapley values for the artists. The class           GroupedDataset          takes an array mapping indices to group identifiers. These identifiers are 0-indexed and must be integers, so we can't use the artist names directly. Instead, we map the artist names to integers and use these as group identifiers. Note that we also cannot use the artist ids which are non-contiguous. We build the necessary mappings below.    </p> <pre><code>artist_to_gid = {\n    artist: i for i, artist in enumerate(artists.unique())\n}  # 1:1 artist name -&amp;gt; group id\ngid_to_artist = [\n    artist for artist, i in sorted(artist_to_gid.items(), key=lambda x: x[1])\n]  # 1:1 group id -&amp;gt; artist name\nsong_to_gid = [\n    artist_to_gid[x] for x in artists\n]  # n:1 song loc[] in the data -&amp;gt; group id\n\ngrouped_train_dataset = GroupedDataset.from_dataset(\n    train_dataset, data_groups=song_to_gid, group_names=gid_to_artist\n)\n</code></pre> <pre>\n<code>Size of the original dataset: 284\nSize of the grouped dataset: 184\n</code>\n</pre> <p>     The songs are now grouped by artist, and values will be computed per-group. This is a common scenario in data valuation. On the one hand the data points are naturally grouped, and it is more informative to know the value of the group than the value of each data point. On the other, it is computationally much cheaper to calculate the value of a few groups than the value of each data point.    </p> <pre><code>utility = ModelUtility(\n    model=GradientBoostingRegressor(n_estimators=3, random_state=random_state),\n    scorer=SupervisedScorer(\n        \"neg_mean_absolute_error\", test_data=val_dataset, default=0.0\n    ),\n    show_warnings=False,\n)\n</code></pre> <p>     Now we configure the valuation method. Shapley values were popularized for data valuation in machine learning with           Truncated Monte Carlo Shapley          , which is a Monte Carlo approximation of the Shapley value that uses a permutation-based definition of Shapley values and truncates the iteration over a given permutation after the marginal utility drops below a certain threshold. For more information on the method, see           Ghorbani and Zou (2019)          or           pydvl's documentation          .    </p> <p>     Like every semi-value method,     <code>      ShapleyValuation     </code>     requires a sampler and a stopping criterion. For the former we use a           PermutationSampler          , which samples permutations of indices and computes marginal contributions incrementally. By using           RelativeTruncation          , the processing of a permutation will stop once the utility of a subset is close to the total utility. Finally, the stopping condition for the whole algorithm is given as in the           TMCS          paper: we stop once the total change in the last 100 steps is below a threshold.    </p> <pre><code>from joblib import parallel_config\n\nfrom pydvl.valuation import MinUpdates\n\nvaluation = ShapleyValuation(\n    utility=utility,\n    sampler=PermutationSampler(\n        truncation=RelativeTruncation(rtol=0.01), seed=random_state\n    ),\n    is_done=MinUpdates(200)\n    &amp;amp; (HistoryDeviation(n_steps=100, rtol=0.01) | MaxUpdates(1000)),\n    progress=True,\n)\nwith parallel_config(n_jobs=n_jobs):\n    valuation.fit(grouped_train_dataset)\nresult = valuation.values()\nresult.sort(key=\"value\")\ndf = result.to_dataframe(column=\"data_value\", use_names=True)\n</code></pre> <p>     Let's take a look at the returned dataframe:    </p>            data_value                      data_value_variances                      data_value_counts                      Kungs                      -1.331776                      5.584003                      200                      AJR                      -1.263026                      2.086788                      200                      A Boogie Wit da Hoodie                      -1.049198                      0.049854                      200                      Justin Bieber                      -1.021131                      1.300413                      200                      Years &amp; Years                      -0.947536                      2.852691                      200           <p>     The first thing to notice is that we sorted the results in ascending order of Shapley value. The index holds the labels for each data group: in this case, artist names. The column     <code>      data_value     </code>     is just that: the Shapley Data value.     <code>      data_value_variance     </code>     is the sample variance of the Monte Carlo estimate, and     <code>      data_value_count     </code>     is the number of updates to the estimate.    </p> <p>     Let us plot the results. In the next cell we will take the 30 artists with the lowest score and plot their values with 95% Normal confidence intervals. Keep in mind that Monte Carlo Shapley is typically very noisy, and it can take many steps to arrive at a clean estimate.    </p> <p>     We can immediately see that many artists (groups of samples) have very low, even negative value, which means that they tend to decrease the total score of the model when present in the training set! What happens if we remove them?    </p> <p>     In the next cell we create a new training set excluding the artists within the bottom 10% scores:    </p> <pre><code>lower_bound = np.percentile(result.values, 10)\nselected_indices = result.indices[result.values &amp;gt; lower_bound]\nclean_dataset = grouped_train_dataset[selected_indices]\n</code></pre> <pre>\n<code>Size of the original dataset: 184 artists (284 songs)\nSize of the cleaned dataset: 165 artists (254 songs)\n</code>\n</pre> <p>     Now we will use this           \"cleaned\"          dataset to retrain the same model and compare its mean absolute error to the one trained on the full dataset. Notice that the score now is calculated using the test set, while in the calculation of the Shapley values we were using the validation set.    </p> <pre><code>model_clean_data = GradientBoostingRegressor(\n    n_estimators=3, random_state=random_state\n).fit(*clean_dataset.data())\nerror_good_data = neg_mean_absolute_error_scorer(model_clean_data, *test_dataset.data())\n\nmodel_all_data = GradientBoostingRegressor(n_estimators=3).fit(*train_dataset.data())\nerror_all_data = neg_mean_absolute_error_scorer(model_all_data, *test_dataset.data())\n</code></pre> <pre>\n<code>Improvement: 8.808892%\n</code>\n</pre> <p>     The score has improved by a noticeable amount! This is quite an important result, as it shows a consistent process to improve the performance of a model by excluding data points from its training set.    </p>      One must however proceed with caution instead of simply throwing away data. For one, the score on `test_dataset` is an estimate of generalization error on unseen data, and the improvement might not be as large upon deployment. It would be advisable to cross-validate this whole process to obtain more conservative estimates. It is also advisable to manually inspect the artists with low value and to try to understand the reason why the model behaves like it does. Finally, remember that **the value depends on the model!** Artists that are detrimental to the Gradient Boosting Regressor might be informative for a different model. Nevertheless, it is likely that the worst ones share some characteristic making them \"bad\" for other regressors, a property that justifies attempting to transfer values from one model to another, at least for data inspection purposes.     <p>     Let us take all the songs by Billie Eilish, set their score to 0 and re-calculate the Shapley values.    </p> <pre><code>y_train_anomalous = train_data[1].copy(deep=True)\ny_train_anomalous[artists == \"Billie Eilish\"] = 0\nanomalous_train_dataset = Dataset(train_data[0], y_train_anomalous)\ngrouped_anomalous_dataset = GroupedDataset.from_dataset(\n    anomalous_train_dataset, data_groups=song_to_gid, group_names=gid_to_artist\n)\n</code></pre> <pre><code>anomalous_utility = ModelUtility(\n    model=GradientBoostingRegressor(n_estimators=3, random_state=random_state),\n    scorer=SupervisedScorer(\n        \"neg_mean_absolute_error\", test_data=val_dataset, default=0.0\n    ),\n)\nvaluation = ShapleyValuation(\n    utility=anomalous_utility,\n    sampler=PermutationSampler(\n        truncation=RelativeTruncation(rtol=0.01), seed=random_state\n    ),\n    is_done=HistoryDeviation(n_steps=100, rtol=1e-3) | MaxUpdates(1000),\n    progress=True,\n)\nwith parallel_config(n_jobs=n_jobs):\n    valuation.fit(grouped_anomalous_dataset)\nresult = valuation.values()\nresult.sort(key=\"value\")\ndf = result.to_dataframe(column=\"data_value\", use_names=True)\n</code></pre> <p>     Let us now consider the low-value artists (at least for predictive purposes, no claims are made about their artistic value!) and plot the results    </p> <p>     And Billie Eilish (our anomalous data group) has moved from top contributor to having negative impact on the performance of the model, as expected!    </p> <p>     What is going on? A popularity of 0 for Billie Eilish's songs is inconsistent with listening patterns for other artists. In artificially setting this, we degrade the predictive power of the model.    </p> <p>     By dropping low-value groups or samples, one can often increase model performance, but by           inspecting          them, it is possible to identify bogus data sources or acquisition methods.    </p>"},{"location":"examples/shapley_basic_spotify/#shapley-for-data-valuation","title":"Shapley for data valuation","text":""},{"location":"examples/shapley_basic_spotify/#setup","title":"Setup","text":"<p>     We begin by importing the main libraries and setting some defaults.    </p>      If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience."},{"location":"examples/shapley_basic_spotify/#loading-and-grouping-the-dataset","title":"Loading and grouping the dataset","text":"<p>     pyDVL provides a support function for this notebook,     <code>      load_spotify_dataset()     </code>     , which downloads data on songs published after 2014, and splits 30% of data for testing, and 30% of the remaining data for validation. The return value is a triple of training, validation and test data as lists of the form     <code>      [X_input, Y_label]     </code>     .    </p>"},{"location":"examples/shapley_basic_spotify/#creating-the-utility-and-computing-values","title":"Creating the utility and computing values","text":"<p>     Now we can calculate the contribution of each group to the model performance.    </p> <p>     As a model, we use scikit-learn's           GradientBoostingRegressor          , but pyDVL can work with any model from sklearn, xgboost or lightgbm. More precisely, any model that implements the protocol           SupervisedModel          (which is just the standard scikit-learn interface of     <code>      fit()     </code>     ,     <code>      predict()     </code>     and     <code>      score()     </code>     ) can be used to construct the utility.    </p> <p>     The third and final component is the scoring function. It can be anything like accuracy or           \\(R^2\\)          , and is set, in the simplest way, by passing a string from the           standard sklearn scoring methods          to the     <code>      SupervisedScorer     </code>     class. Please refer to that documentation on information on how to define your own scoring function.    </p> <p>     We collect validation dataset, model and scoring function into an instance of           ModelUtility          .    </p>"},{"location":"examples/shapley_basic_spotify/#evaluation-on-anomalous-data","title":"Evaluation on anomalous data","text":"<p>     One interesting test to validate the idea that Shapley values help locate bogus data, is to corrupt some of it and to monitor how their value changes. To do this, we will take one of the artists with the highest value and set the popularity of all their songs to 0.    </p>"},{"location":"examples/shapley_knn_flowers/","title":"KNN Shapley","text":"<p>     This notebook shows how to calculate Shapley values for the K-Nearest Neighbours algorithm. By making use of the local structure of           KNN          , it is possible to compute an exact value in almost linear time, as opposed to exponential complexity of exact, model-agnostic Shapley.    </p> <p>     The main idea is to exploit the fact that adding or removing points beyond the k-ball doesn't influence the score. Because the algorithm then essentially only needs to do a search it runs in           \\(\\mathcal{O}(N \\log N)\\)          time.    </p> <p>     By further using approximate nearest neighbours, it is possible to achieve           \\((\\epsilon,\\delta)\\)          -approximations in sublinear time. However, this is not implemented in pyDVL yet.    </p> <p>     We refer to the original paper that pyDVL implements for details:           Jia, Ruoxi, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang, Costas Spanos, and Dawn Song.             Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms            . Proceedings of the VLDB Endowment 12, no. 11 (1 July 2019): 1610\u201323.      </p> <p>     The main interface is the class           KNNShapleyValuation          . In order to use it we need to construct two           Datasets          (one for training and one for evaluating), and a           KNNClassifierUtility          .    </p> <pre><code>from pydvl.valuation import Dataset\n\nsklearn_dataset = datasets.load_iris()\ntrain, test = Dataset.from_sklearn(sklearn_dataset)\nknn = sk.neighbors.KNeighborsClassifier(n_neighbors=5)\n</code></pre> <pre><code>from joblib import parallel_config\n\nfrom pydvl.valuation.methods import KNNShapleyValuation\n\nvaluation = KNNShapleyValuation(model=knn, test_data=test, progress=True)\nwith parallel_config(n_jobs=n_jobs):\n    valuation.fit(train)\n\nresult = valuation.values(sort=True)\n</code></pre> <p>     If we now look at  the distribution of Shapley values for each class, we see that each has samples with both high and low scores. This is expected, because an accurate model uses information of all classes.    </p> <pre><code>_x, _y = train.data()\nn_corrupted = 10\n_y[:n_corrupted] = (_y[:n_corrupted] + 1) % 3\ncorrupted_data = Dataset(\n    _x,\n    _y,\n    feature_names=train.feature_names,\n    target_names=train.target_names,\n    description=\"Corrupted iris dataset\",\n)\nknn = sk.neighbors.KNeighborsClassifier(n_neighbors=5)\ncontaminated_valuation = KNNShapleyValuation(\n    model=knn, test_data=test, progress=True\n).fit(corrupted_data)\ncontaminated_result = contaminated_valuation.values()\n</code></pre> <p>     Taking the average corrupted value and comparing it to uncorrupted one, we notice that on average anomalous points have a much lower score, i.e. they tend to be much less valuable to the model.    </p> <p>     To do this, first we make sure that we access the results by data index with a call to     <code>      ValuationResult.sort()     </code>     , then we split the values into two groups: corrupted and non-corrupted. Note how we access property     <code>      values     </code>     of the     <code>      ValuationResult     </code>     object. This is a numpy array of values, sorted however the object was sorted. Finally, we compute the quantiles of the two groups and compare them. We see that the corrupted mean is in the lowest percentile of the value distribution, while the correct mean is in the 70th percentile.    </p> <pre><code>contaminated_result.sort(key=\"index\")  # actually redundant\n\ncorrupted_shapley_values = contaminated_result.values[:n_corrupted]\ncorrect_shapley_values = contaminated_result.values[n_corrupted:]\n\nmean_corrupted = np.mean(corrupted_shapley_values)\nmean_correct = np.mean(correct_shapley_values)\npercentile_corrupted = np.round(100 * np.mean(result.values &amp;lt; mean_corrupted), 0)\npercentile_correct = np.round(100 * np.mean(result.values &amp;lt; mean_correct), 0)\n</code></pre> <pre>\n<code>The corrupted mean is at percentile 2 of the value distribution.\nThe correct mean is percentile 73 of the value distribution.\n</code>\n</pre> <p>     This is confirmed if we plot the distribution of Shapley values and circle corrupt points in red. They all tend to have low Shapley scores, regardless of their position in space and assigned label:    </p>"},{"location":"examples/shapley_knn_flowers/#knn-shapley","title":"KNN          Shapley","text":""},{"location":"examples/shapley_knn_flowers/#setup","title":"Setup","text":"<p>     We begin by importing the main libraries and setting some defaults.    </p>      If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience."},{"location":"examples/shapley_knn_flowers/#building-a-dataset-and-a-utility","title":"Building a Dataset and a Utility","text":"<p>     We use           the sklearn iris dataset          and wrap it into two           Datasets          calling the factory           Dataset.from_sklearn()          . This automatically creates a train / test split for us which will be used to score the model.    </p> <p>     We then create a           KNN          model from scikit-learn and instantiate a           KNNShapleyValuation          object. This valuation departs from standard usage in that it does not use a     <code>      Utility     </code>     but instead takes the scikit-learn model and the test set directly as input. This is because           KNN          -Shapley uses a recursive formula to compute the values directly, without needing to sample subsets of the training data (nevertheless, we provide           KNNClassifierUtility          for purposes of testing and experimentation).    </p>"},{"location":"examples/shapley_knn_flowers/#inspecting-the-results","title":"Inspecting the results","text":"<p>     Let us first look at the labels' distribution as a function of petal and sepal length:    </p>"},{"location":"examples/shapley_knn_flowers/#corrupting-labels","title":"Corrupting labels","text":"<p>     To test how informative values are, we can corrupt some training labels and see how their Shapley values change with respect to the non-corrupted points.    </p>"},{"location":"examples/shapley_utility_learning/","title":"Data utility learning","text":"<p>     This notebook introduces           Data Utility Learning          (           DUL          ), a method for approximate data valuation which learns a model of the utility function.    </p> <p>     The idea is to estimate the performance of the learning algorithm of interest on unseen data combinations (i.e. subsets of the dataset). The method was originally described in           Wang, Tianhao, Yu Yang, and Ruoxi Jia.             Improving Cooperative Game Theory-Based Data Valuation via Data Utility Learning            . arXiv, 2022          .    </p> <p>        Warning:            Work on Data Utility Learning is preliminary. It remains to be seen when or whether it can be put effectively into application. For this further testing and benchmarking are required.     </p> <p>     Data utility learning can be applied to any valuation method that uses a Utility, in particular any           SemivalueValuation          . For this example we will use Shapley values with the subclass           ShapleyValuation          .    </p> <pre><code>from pydvl.valuation.dataset import Dataset\n\ntrain, test = Dataset.from_sklearn(\n    load_iris(),\n    train_size=train_size,\n    random_state=random_state,\n    stratify_by_target=True,\n)\n</code></pre> <p>     We verify that, as in the paper, if we fit a Support-Vector Classifier to the training data, we obtain a high accuracy:    </p> <pre><code>from sklearn.svm import LinearSVC\n\nmodel = LinearSVC()\nmodel.fit(*train.data());\n</code></pre> <pre>\n<code>Mean accuracy: 94.07%\n</code>\n</pre> <pre><code>from pydvl.valuation import ModelUtility, SupervisedScorer\n\nscorer = SupervisedScorer(\"accuracy\", test_data=test, default=0, range=(0, 1))\nutility = ModelUtility(model=model, scorer=scorer, show_warnings=False)\n</code></pre> <p>     Then we must pick the sampling scheme. In order to compute exact values we must use either           DeterministicUniformSampler          which yields all possible subsets of the data or           DeterministicPermutationSampler          which yields all possible permutations of the data, but the latter is prohibitively expensive.    </p> <p>     Finally, we must pick a stopping criterion. Since the sampler is finite, we use           NoStopping          to run until completion and we pass it the sampler to keep track of progress.    </p> <pre><code>from pydvl.valuation import (\n    DeterministicUniformSampler,\n    NoStopping,\n    ShapleyValuation,\n)\n\nsampler = DeterministicUniformSampler(batch_size=32)\nstopping = NoStopping(sampler)\n\nvaluation = ShapleyValuation(\n    utility=utility, sampler=sampler, is_done=stopping, progress=True\n)\n</code></pre> <p>     With everything set up, we fit the valuation in parallel and obtain the exact Data Shapley values:    </p> <pre><code>from joblib import parallel_config\n\nfrom pydvl.utils.functional import timed\n\ntimed_fit = timed(valuation.fit)\nwith parallel_config(n_jobs=n_jobs):\n    timed_fit(train)\ncomputation_times[\"exact\"] = timed_fit.execution_time\n\nresult = valuation.values()\ndf = result.to_dataframe(column=\"exact\")[\"exact\"]  # We only need the values\n</code></pre> <pre>\n<code>ShapleyValuation: NoStopping(): 0.00%|          | [00:00&lt;?, ?%/s]</code>\n</pre> <p>     Below are the parameters for the model used to learn the utility. We follow the paper and use a fully connected neural network whose inputs are indicator vectors of the sets. For a set           \\(S = \\{i_1, ..., i_m\\}\\)          , the encoding is a binary vector           \\(x\\)          such that           \\(x_i = 1\\)          if           \\(i \\in S\\)          or           \\(0\\)          otherwise. The process of encoding the data and fitting the neural network is encapsulated inside an           IndicatorUtilityModel          . Other choices inherit from the ABC           UtilityModel          . We will see an example later.    </p> <pre><code>mlp_params = dict(\n    hidden_layer_sizes=(20, 10),\n    activation=\"relu\",\n    solver=\"adam\",\n    learning_rate_init=0.001,\n    batch_size=batch_size,\n    max_iter=800,\n    shuffle=False,\n    random_state=random_state,\n)\n</code></pre> <p>     For the training we use an increasing     <code>      training_budget     </code>     of utility samples, spaced on a log scale from 100 to 4000. We repeat each training procedure 10 times in order to compute confidence intervals. Each experiment is encapsulated in the function     <code>      run_once     </code>     , which takes a run identifier and a           DUL          budget, then performs the fitting and returns all information as a tuple    </p> <pre><code>from joblib import Parallel, delayed\nfrom sklearn.neural_network import MLPRegressor\n\nfrom pydvl.utils.functional import suppress_warnings, timed\nfrom pydvl.valuation.result import ValuationResult\nfrom pydvl.valuation.samplers import PermutationSampler\nfrom pydvl.valuation.samplers.truncation import RelativeTruncation\nfrom pydvl.valuation.stopping import MaxUpdates\nfrom pydvl.valuation.utility import DataUtilityLearning\nfrom pydvl.valuation.utility.learning import IndicatorUtilityModel\n\n\n@suppress_warnings(categories=(RuntimeWarning,))\ndef run_once(run: int, budget: int) -&amp;gt; tuple[int, int, ValuationResult, float]:\n    # DUL will kick in after `budget` calls to utility\n    utility_model = IndicatorUtilityModel(MLPRegressor(**mlp_params), n_data=len(train))\n    dul_utility = DataUtilityLearning(\n        utility=utility,\n        training_budget=budget,\n        model=utility_model,\n        show_warnings=False,\n    )\n\n    truncation = RelativeTruncation(rtol=0.001)\n    sampler = PermutationSampler(truncation=truncation)\n    stopping = MaxUpdates(300)\n    valuation = ShapleyValuation(dul_utility, sampler, is_done=stopping, progress=False)\n\n    # Note that DUL does not support parallel fitting (yet?)\n    timed_fit = timed(valuation.fit)\n    timed_fit(train)\n\n    return run, budget, valuation.values(), timed_fit.execution_time\n</code></pre> <pre><code>with parallel_config(n_jobs=n_jobs):\n    worker = delayed(run_once)\n\n    with Parallel(return_as=\"generator_unordered\") as parallel:\n        jobs = parallel(\n            worker(run, budget)\n            for run, budget in product(range(n_runs), training_budgets)\n        )\n\n        pbar = tqdm(jobs, total=n_runs * len(training_budgets))\n        for run, budget, result, time in pbar:\n            computation_times[budget].append(time)\n            dul_df = result.to_dataframe(column=f\"{budget}_{run}\")[f\"{budget}_{run}\"]\n            df = pd.concat([df, dul_df], axis=1)\n</code></pre> <p>     Notice how we have changed the stopping criterion to be a combination of           HistoryDeviation          and           MaxUpdates          . The former stops the valuation when the relative deviation of the values is below a certain threshold for a given number of steps, and the latter stops the valuation after a given number of updates (to ensure that the valuation does not run indefinitely).     <code>      HistoryDeviation     </code>     is the criterion used in the paper introducing Truncated Monte Carlo Shapley. Next we compute the           \\(l_2\\)          error for the different training budgets across all runs and plot mean and standard deviation. We obtain results analogous to Figure 1 of the paper, verifying that the method indeed works for estimating the Data Shapley values (at least in this context).    </p> <p>     In the plot we also display the mean and standard deviation of the computation time taken for each training budget.    </p> <pre><code>max_len = max(len(v) for v in computation_times.values() if isinstance(v, list))\nfor k, v in computation_times.items():\n    computation_times[k] = (\n        v + [v[-1]] if isinstance(v, list) and len(v) &amp;lt; max_len else v\n    )\n</code></pre> <p>     Let us next look at how well the ranking of values resulting from using the surrogate           \\(\\tilde{u}\\)          matches the ranking by the exact values. For this we fix           \\(k=3\\)          and consider the           \\(k\\)          samples with the highest value according to           \\(\\tilde{u}\\)          and           \\(u\\)          :    </p> <p>     Finally, for each sample, we look at the distance of the estimates to the exact value across runs. Boxes are centered at the 50th percentile with wiskers at the 25th and 75th. We plot relative distances, as a percentage.    </p> <pre><code>highest_value_index = df.index[df[\"exact\"].argmax()]\ny_corrupted = train.data().y.copy()\ny_corrupted[highest_value_index] = (y_corrupted[highest_value_index] + 1) % 3\n\ncorrupted_dataset = Dataset(\n    train.data().x.copy(),\n    y_corrupted,\n    feature_names=train.feature_names,\n    target_names=train.target_names,\n)\n</code></pre> <pre>\n<code>The corrupted sample has index 0\n</code>\n</pre> <p>     We retrain the model on the new dataset and verify that the accuracy decreases:    </p> <pre><code>model = LinearSVC()\nmodel.fit(*corrupted_dataset.data());\n</code></pre> <pre>\n<code>Mean accuracy over test set, with corrupted training data: 71.85%\n</code>\n</pre> <p>     Now we recompute the values of all samples using the exact method:    </p> <pre><code>sampler = DeterministicUniformSampler(batch_size=32)\nstopping = NoStopping(sampler)\nvaluation = ShapleyValuation(\n    utility=utility, sampler=sampler, is_done=stopping, progress=True\n)\n\nwith parallel_config(n_jobs=n_jobs):\n    valuation.fit(corrupted_dataset)\nresult = valuation.values()\n\ndf_corrupted = result.to_dataframe(column=\"exact\")[\"exact\"]\n</code></pre> <p>     And finally we use           DUL          with the best training budget previously obtained, and plot the resulting scores.    </p> <pre>\n<code>Best training budget was: 2361\n</code>\n</pre> <pre><code>scorer = SupervisedScorer(\"accuracy\", test_data=test, default=0, range=(0, 1))\nutility = ModelUtility(model=model, scorer=scorer, show_warnings=False)\nutility_model = IndicatorUtilityModel(MLPRegressor(**mlp_params), n_data=len(train))\ndul_utility = DataUtilityLearning(\n    utility=utility,\n    training_budget=best_budget,\n    model=utility_model,\n    show_warnings=False,\n)\n\ntruncation = RelativeTruncation(rtol=0.001)\nsampler = PermutationSampler(truncation=truncation)\nstopping = MaxUpdates(300)\nvaluation = ShapleyValuation(dul_utility, sampler, is_done=stopping, progress=False)\nvaluation.fit(corrupted_dataset)\nresult = valuation.values()\n\ndul_df = result.to_dataframe(column=\"estimated\")[\"estimated\"]\ndf_corrupted = pd.concat([df_corrupted, dul_df], axis=1)\n</code></pre> <p>     We can see in the figure that both methods assign the lowest value to the sample with the corrupted label.    </p>      As mentioned above, despite the previous results, this work is preliminary and the usefulness of Data Utility Learning remains to be tested in practice."},{"location":"examples/shapley_utility_learning/#data-utility-learning","title":"Data Utility Learning","text":""},{"location":"examples/shapley_utility_learning/#setting-and-notation","title":"Setting and notation","text":"<p>       DUL          can be applied to any semi-value-based data valuation method. Banzhaf, Beta-Shapley, etc. In this notebook we will focus on Shapley Value. Assume we have some machine learning model, say a classifier           \\(M\\)          , trained on a dataset           \\(D=\\{x_1, ..., x_n\\}\\)          . For notational convenience we identify points           \\(x_i\\)          with their indices           \\(i \\in N=\\{1, ..., n\\}\\)          . We also have a separate           valuation set            \\(D_{val}\\)          , and define the           utility          of a subset           \\(S \\subset N\\)          as the performance (e.g. accuracy) of           \\(M\\)          when trained on           \\(S\\)          and evaluated on           \\(D_{val}\\)          .    </p> <p>     Then, the definition of Shapley Value           \\(v_u(i)\\)          for           \\(i \\in N\\)          is:    </p>      \\[\\begin{equation} v_u(i) = \\frac{1}{n} \\sum_{S \\subseteq N \\setminus \\{i\\}} \\binom{n-1}{|S|}^{-1} [u(S \\cup \\{i\\}) \u2212 u(S)], \\tag{1} \\label{eq:shapley-def} \\end{equation}\\]     <p>     Because each evaluation of           \\(u(S)\\)          requires a potentially very costly training of           \\(M\\)          on           \\(S\\)          , the idea of           DUL          is to learn a surrogate model           \\(\\hat{u}\\)          for the utility. The main assumption is that it is much faster to fit and use           \\(\\hat{u}\\)          than it is to compute           \\(u\\)          and that for most           \\(i\\)          ,           \\(v_\\hat{u}(i) \\approx v_u(i)\\)          in some sense.    </p> <p>     In order to fit           \\(\\hat{u}\\)          , we start by sampling so-called           utility samples          to form a training set           \\(S_\\mathrm{train}\\)          . Each utility sample is a tuple consisting of a subset of indices           \\(S_j,\\)          and its true utility           \\(u(S_j)\\)          :    </p>      \\[\\mathcal{S}_\\mathrm{train} = \\{(S_j, u(S_j): S_j \\subset N, j = 1 , ..., m_\\mathrm{train}\\}\\]     <p>     where           \\(m_\\mathrm{train}\\)          denotes the           training budget          for the learned utility function.    </p> <p>     The           data utility model            \\(\\hat{u}\\)          takes as input sets           \\(S\\)          , which we must encode somehow. In the           DUL          paper, the authors use 1-hot encoding for the data points, i.e. they use an indicator function for           \\(S\\)          , which is a boolean vector           \\(\\phi\\)          in which a           \\(1\\)          at index           \\(k\\)          means that the           \\(k\\)          -th sample of the dataset is present in the subset:    </p>      \\[S_j \\mapsto \\phi_j \\in \\{ 0, 1 \\}^{N}, \\text{ where }\\phi_j^k = 1 \\text{ iff }x_k \\in S_j.\\]     <p>     We train           \\(\\hat{u}\\)          on the transformed utility samples           \\(\\phi (\\mathcal{S}_\\mathrm{train}) := \\{(\\phi(S_j), u(S_j): j = 1 , ..., m_\\mathrm{train}\\},\\)          and then use it to predict instead of computing the utility for any           \\(S_j \\notin \\mathcal{S}_\\mathrm{train}\\)          .    </p>"},{"location":"examples/shapley_utility_learning/#alternative-encodings","title":"Alternative encodings","text":"<p>     Training e.g. a neural network           \\(\\hat{u}\\)          on a set of corners of an           \\(N\\)          -dimensional unit cube is generally not the best approach, so one can think of alternative encodings. One such possibility are           DeepSets          , introduced by           Zaheer et al. (2017)          , which learn an encoding for the sets as part of the training procedure. This is done by ensuring that the network is permutation-invariant, which is the main property of interest when learning a function defined over sets.    </p> <p>     pyDVL offers a very simple pytorch implementation of DeepSets through           DeepSetUtilityModel          . You can implement your own by subclassing           UtilityModel          .    </p>"},{"location":"examples/shapley_utility_learning/#setup","title":"Setup","text":"<p>     We begin by importing the main libraries and setting some defaults.    </p>      If you are reading this in the documentation, some boilerplate (including most plotting code) has been omitted for convenience."},{"location":"examples/shapley_utility_learning/#dataset","title":"Dataset","text":"<p>     Closely following the paper, we take     <code>      train_size=15     </code>     samples (10%) from the           Iris dataset          and compute their Data Shapley values by using all the remaining samples as test set for computing the utility, which in this case is accuracy.    </p>"},{"location":"examples/shapley_utility_learning/#computing-exact-shapley-values","title":"Computing exact Shapley values","text":"<p>     We start by defining the utility using the main model of interest and computing the exact Data Shapley values by definition           \\(\\ref{eq:shapley-def}\\)          .    </p> <p>     We require a           SupervisedScorer          (accuracy), and a           ModelUtility          (not to be confused with           DUL          's model           for          the utility, which is     <code>      UtilityModel     </code>     !).    </p>"},{"location":"examples/shapley_utility_learning/#learning-the-utility-with-a-simple-neural-network","title":"Learning the utility with a simple neural network","text":"<p>     We now estimate the Data Shapley values with           DataUtilityLearning          . This class learns a model of the     <code>      Utility     </code>     by wrapping it and delegating calls to it, up until a given budget. Every call yields a           utility sample          which is saved under the hood for training of the given utility model. Once the budget is exhausted,     <code>      DataUtilityLearning     </code>     fits the model to the utility samples and all subsequent calls use the learned model to predict the wrapped utility. Because each evaluation of the original utility is assumed to take a long time, this results in a speedup.    </p>      Note how we use a Monte Carlo approximation instead of sampling all subsets as above. This is not only for speed reasons, but because [DeterministicUniformSampler][pydvl.valuation.samplers.powerset.DeterministicUniformSampler] yields subsets in a fixed order, from the lowest size to the largest. Because the training budget for the model to learn the utility is around 1/4th of the total number of subsets, this would mean that we would never see utility samples for the larger sizes and the model would be biased (try it!)"},{"location":"examples/shapley_utility_learning/#evaluation-on-anomalous-data","title":"Evaluation on anomalous data","text":"<p>     One interesting way to assess the Data Utility Learning approach is to corrupt some data and monitor how the value changes. To do this, we will take the sample with the highest score and change its label.    </p>"},{"location":"getting-started/","title":"Getting started","text":"<p>If you want to jump straight in, install pyDVL and then check out the examples. You will probably want to install with support for influence function computation.</p> <p>We have introductions to the ideas behind Data valuation and Influence functions, as well as a short overview of common applications.</p>"},{"location":"getting-started/#installation","title":"Installing pyDVL","text":"<p>To install the latest release use:</p> <pre><code>pip install pyDVL\n</code></pre> <p>See Extras for optional dependencies, in particular if you are interested in influence functions. You can also install the latest development version from TestPyPI:</p> <pre><code>pip install pyDVL --index-url https://test.pypi.org/simple/\n</code></pre> <p>In order to check the installation you can use:</p> <pre><code>python -c \"import pydvl; print(pydvl.__version__)\"\n</code></pre>"},{"location":"getting-started/#dependencies","title":"Dependencies","text":"<p>pyDVL requires Python &gt;= 3.9, numpy, scikit-learn, scipy, cvxpy for the core methods, and joblib for parallelization locally. Additionally,the Influence functions module requires PyTorch (see Extras below).</p>"},{"location":"getting-started/#installation-extras","title":"Extras","text":"<p>pyDVL has a few extra dependencies that can be optionally installed:</p>"},{"location":"getting-started/#installation-influences","title":"Influence functions","text":"<p>pytorch dependency</p> <p>While only pydvl.influence completely depends on PyTorch, some valuation methods in pydvl.valuation use PyTorch as well (e.g. DeepSets). If you want to use these, you can also follow the instructions below.</p> <p>To use the module on influence functions, pydvl.influence, run:</p> <pre><code>pip install pyDVL[influence]\n</code></pre> <p>This includes a dependency on PyTorch (Version 2.0 and above) and thus is left out by default.</p>"},{"location":"getting-started/#cupy","title":"CuPy","text":"<p>In case that you have a supported version of CUDA installed (v11.2 to 11.8 as of this writing), you can enable eigenvalue computations for low-rank approximations with CuPy on the GPU by using:</p> <pre><code>pip install pyDVL[cupy]\n</code></pre> <p>This installs cupy-cuda11x.</p> <p>If you use a different version of CUDA, please install CuPy manually.</p>"},{"location":"getting-started/#ray","title":"Ray","text":"<p>If you want to use Ray to distribute data valuation workloads across nodes in a cluster (it can be used locally as well, but for this we recommend joblib instead) install pyDVL using:</p> <pre><code>pip install pyDVL[ray]\n</code></pre> <p>see the intro to parallelization for more details on how to use it.</p>"},{"location":"getting-started/#memcached","title":"Memcached","text":"<p>If you want to use Memcached for caching utility evaluations, use:</p> <pre><code>pip install pyDVL[memcached]\n</code></pre> <p>This installs pymemcache additionally. Be aware that you still have to start a memcached server manually. See Setting up the Memcached cache.</p>"},{"location":"getting-started/advanced-usage/","title":"Advanced usage","text":"<p>Besides the dos and don'ts of data valuation itself, which are the subject of the examples and the documentation of each method, there are two main things to keep in mind when using pyDVL namely Parallelization and Caching.</p>"},{"location":"getting-started/advanced-usage/#setting-up-parallelization","title":"Parallelization","text":"<p>pyDVL uses parallelization to scale and speed up computations. It does so using Dask or Joblib (with any of its backends). The first is used in the influence package whereas the latter is used in  the valuation package.</p>"},{"location":"getting-started/advanced-usage/#data-valuation-parallelization","title":"Data valuation","text":"<p>For data valuation, pyDVL uses joblib for transparent parallelization of most methods using any of the backends available to joblib.</p> <p>If you want to use ray or dask as backends, please follow the instructions in joblib's documentation. Mostly it's just a matter of registering the backend with joblib.register_parallel_backend and then using it within the context manager joblib.parallel_config around the code that you want to parallelize, which is usually the call to the <code>fit</code> method of the valuation object.</p> Basic fitting in parallel <pre><code>import sklearn as sk\nfrom joblib import parallel_config, register_parallel_backend\nfrom pydvl.valuation import *\nfrom ray.util.joblib import register_ray\n\nregister_ray()\n\ntrain, test = Dataset.from_arrays(...)\nmodel = sk.svm.SVC()\nscorer = SupervisedScorer(\"accuracy\", test, default=0.0, range=(0, 1))\nutility = ModelUtility(model, scorer)\nsampler = PermutationSampler(truncation=NoTruncation())\nstopping = MinUpdates(7000) | MaxTime(3600)\nshapley = ShapleyValuation(utility, sampler, stopping, progress=True)\n\nwith parallel_config(backend=\"ray\", n_jobs=128):\n    shapley.fit(train)\n\nresults = shapley.values()\n</code></pre> <p>Note that you will have to install additional dependencies (see Extras) and to provide a running cluster (or run ray in local mode). For instance, for ray follow the instructions in Ray's documentation to set up a remote cluster. You could alternatively use a local cluster and in that case you don't have to set anything up.</p> <p>Info</p> <p>As of v0.10.0 pyDVL does not allow requesting resources per task sent to the cluster, so you will need to make sure that each worker has enough resources to handle the tasks it receives. A data valuation task using game-theoretic methods will typically make a copy of the whole model and dataset to each worker, even if the re-training only happens on a subset of the data. Some backends, like \"loky\" will use memory mapping to avoid copying the dataset to each worker, but in general you should make sure that each worker has enough memory to handle the whole dataset.</p>"},{"location":"getting-started/advanced-usage/#influence-parallelization","title":"Influence functions","text":"<p>Refer to Scaling influence computation for explanations about parallelization for Influence Functions.</p>"},{"location":"getting-started/advanced-usage/#getting-started-cache","title":"Caching","text":"<p>PyDVL can cache (memoize) the computation of the utility function and speed up some computations for data valuation. It is however disabled by default because single runs of methods rarely benefit much from it. When it is enabled it takes into account the data indices passed as argument and the utility function wrapped into the Utility object. This means that care must be taken when reusing the same utility function with different data, see the documentation for the caching package for more information.</p> <p>In general, caching won't play a major role in the computation of Shapley values because the probability of sampling the same subset twice, and hence needing the same utility function computation, is very low. However, it can be very useful when comparing methods that use the same utility function, or when running multiple experiments with the same data.</p> <p>pyDVL supports 3 different caching backends:</p> <ul> <li>InMemoryCacheBackend:   an in-memory cache backend that uses a dictionary to store and retrieve   cached values. This is used to share cached values between threads   in a single process. This backend is provided for completeness, since   parallelization is almost never done using threads, </li> <li>DiskCacheBackend:   a disk-based cache backend that uses pickled values written to and read from   disk. This is used to share cached values between processes in a single machine.   !!! warning \"Disk cache\"       The disk cache is a stub implementation which pickles each utility       evaluation and is extremely inefficient. If it proves useful, we might       implement a more efficient version in the future.</li> <li>MemcachedCacheBackend:   a Memcached-based cache backend that uses pickled values written to   and read from a Memcached server. This is used to share cached values   between processes across one or multiple machines.</li> </ul> Memcached extras <p>The Memcached backend requires optional dependencies. See  Extras for more information.</p> <p>Using the caches is as simple as passing the backend to the utility constructor. Please refer to the documentation and examples of each backend class for more details.</p> <p>Using the cache</p> <p>Continue reading about the cache in the documentation for the caching package.</p>"},{"location":"getting-started/advanced-usage/#setting-up-memcached","title":"Setting up the Memcached cache","text":"<p>Memcached is an in-memory key-value store accessible over the network.</p> <p>You can either install it as a package or run it inside a docker container (the simplest). For installation instructions, refer to the Getting started section in memcached's wiki. Then you can run it with:</p> <pre><code>memcached -u user\n</code></pre> <p>To run memcached inside a container in daemon mode instead, use:</p> <pre><code>docker container run -d --rm -p 11211:11211 memcached:latest\n</code></pre>"},{"location":"getting-started/applications/","title":"Applications of data valuation","text":"<p>Data valuation methods can improve various aspects of data engineering and machine learning workflows. When applied judiciously, these methods can enhance data quality, model performance, and cost-effectiveness.</p> <p>However, the results can be inconsistent. Values have a strong dependency on the training procedure and the performance metric used. For instance, accuracy is a poor metric for imbalanced sets and this has a stark effect on data values. Some models exhibit great variance in some regimes and this again has a detrimental effect on values. See Problems of data values for more on this.</p> <p>Here we quickly enumerate the most common uses of data valuation. For a comprehensive overview, along with concrete examples, please refer to the Transferlab blog post on this topic.</p>"},{"location":"getting-started/applications/#data-engineering","title":"Data engineering","text":"<p>Some of the promising applications in data engineering include:</p> <ul> <li>Removing low-value data points to increase model performance.</li> <li>Pruning redundant samples enables more efficient training of large models.</li> <li>Active learning. Points predicted to have high-value points can be prioritized   for labeling, reducing the cost of data collection.</li> <li>Analyzing high- and low-value data to guide data collection and improve   upstream data processes. Low-value points may reveal data issues to address.</li> <li>Identify irrelevant or duplicated data when evaluating offerings from data   providers.</li> </ul>"},{"location":"getting-started/applications/#model-development","title":"Model development","text":"<p>Some of the useful applications include:</p> <ul> <li>Data attribution for interpretation and debugging: Analyzing the most or least   valuable samples for a class can reveal cases where the model relies on   confounding features instead of true signal. Investigating influential points   for misclassified examples highlights limitations to address.</li> <li>Sensitivity / robustness analysis: (Broderick et al., 2021)<sup>1</sup> shows that   removing a small fraction of highly influential data can completely flip model   conclusions. This can reveal potential issues with the modeling approach, data   collection process, or intrinsic difficulties of the problem that require   further inspection.</li> <li>Continual learning: in order to avoid forgetting when training on new data,   a subset of previously seen data is presented again. Data valuation can help   in the selection of the most valuable samples to retain.</li> </ul>"},{"location":"getting-started/applications/#attacks","title":"Attacks","text":"<p>Data valuation techniques have applications in detecting data manipulation and contamination, although the feasibility of such attacks is limited.</p> <ul> <li>Watermark removal: Points with low value on a correct validation set may be   part of a watermarking mechanism.</li> <li>Poisoning attacks: Influential points can be shifted to induce large changes   in model estimators.</li> </ul>"},{"location":"getting-started/applications/#data-markets","title":"Data markets","text":"<p>Additionally, one of the motivating applications for the whole field is that of data markets, where data valuation can be the key component to determine the price of data.</p> <p>Game-theoretic valuation methods like Shapley values can help assign fair prices, but have limitations around handling duplicates or adversarial data. Model-free methods like LAVA (Just et al., 2023)<sup>2</sup> and CRAIG are particularly well suited for this, as they use the Wasserstein distance between a vendor's data and the buyer's to determine the value of the former. </p> <p>However, this is a complex problem which can face simple practical problems like data owners not willing to disclose their data for valuation, even to a broker.</p> <ol> <li> <p>Broderick, T., Giordano, R., Meager, R., 2021. An Automatic Finite-Sample Robustness Metric: When Can Dropping a Little Data Make a Big Difference? \u21a9</p> </li> <li> <p>Just, H.A., Kang, F., Wang, T., Zeng, Y., Ko, M., Jin, M., Jia, R., 2023. LAVA: Data Valuation without Pre-Specified Learning Algorithms. Presented at the The Eleventh International Conference on Learning Representations (ICLR 2023).\u00a0\u21a9</p> </li> </ol>"},{"location":"getting-started/benchmarking/","title":"Benchmarking tasks","text":"<p>Because the magnitudes of values or influences from different algorithms, or datasets, are not comparable to each other, evaluation of the methods is typically done with downstream tasks.</p>"},{"location":"getting-started/benchmarking/#benchmarking-valuation-methods","title":"Benchmarking valuation methods","text":"<p>Data valuation is particularly useful for data selection, pruning and inspection in general. For this reason, the most common benchmarks are data removal and noisy label detection.</p>"},{"location":"getting-started/benchmarking/#high-value-point-removal","title":"High-value point removal","text":"<p>After computing the values for all data in \\(T = \\{ \\mathbf{z}_i : i = 1, \\ldots, n \\}\\), the set is sorted by decreasing value. We denote by \\(T_{[i :]}\\) the sorted sequence of points \\((\\mathbf{z}_i, \\mathbf{z}_{i + 1}, \\ldots, \\mathbf{z}_n)\\) for \\(1 \\leqslant i \\leqslant n\\). Now train successively \\(f_{T [i :]}\\) and compute its accuracy \\(a_{T_{[i :]}} (D_{\\operatorname{test}})\\) on the held-out test set, then plot all numbers. By using \\(D_{\\operatorname{test}}\\) one approximates the expected accuracy drop on unseen data. Because the points removed have a high value, one expects performance to drop visibly wrt. a random baseline.</p>"},{"location":"getting-started/benchmarking/#low-value-point-removal","title":"Low-value point removal","text":"<p>The complementary experiment removes data in increasing order, with the lowest valued points first. Here one expects performance to increase relatively to randomly removing points before training. Additionally, every real dataset will include slightly out-of-distribution points, so one should also expect an absolute increase in performance when some of the lowest valued points are removed.</p>"},{"location":"getting-started/benchmarking/#value-transfer","title":"Value transfer","text":"<p>This experiment explores the extent to which data values computed with one (cheap) model can be transferred to another (potentially more complex) one. Different classifiers are used as a source to calculate data values. These values are then used in the point removal tasks described above, but using a different (target) model for evaluation of the accuracies \\(a_{T [i :]}\\). A multi-layer perceptron is added for evaluation as well.</p>"},{"location":"getting-started/benchmarking/#noisy-label-detection","title":"Noisy label detection","text":"<p>This experiment tests the ability of a method to detect mislabeled instances in the data. A fixed fraction \\(\\alpha\\) of the training data are picked at random and their labels flipped. Data values are computed, then the \\(\\alpha\\)-fraction of lowest-valued points are selected, and the overlap with the subset of flipped points is computed. This synthetic experiment is however hard to put into practical use, since the fraction \\(\\alpha\\) is of course unknown in practice.</p>"},{"location":"getting-started/benchmarking/#rank-stability","title":"Rank stability","text":"<p>Introduced in (Wang and Jia, 2023)<sup>1</sup>, one can  look at how stable the top \\(k\\)% of the values is across runs. Rank stability of a method is necessary but not sufficient for good results. Ideally one wants to identify high-value points reliably (good precision and recall) and consistently (good rank stability).</p>"},{"location":"getting-started/benchmarking/#benchmarking-influence-methods","title":"Benchmarking Influence function methods","text":"<p>Todo</p> <p>This section is basically a stub</p> <p>Although in principle one can compute the average influence over the test set and run the same tasks as above, because influences are computed for each pair of training and test sample, they typically require different experiments to compare their efficacy.</p>"},{"location":"getting-started/benchmarking/#approximation-quality","title":"Approximation quality","text":"<p>The biggest difficulty when computing influences is the approximation of the inverse Hessian-vector product. For this reason one often sees in the literature the quality of the approximation to LOO as an indicator of performance, the exact Influence Function being a first order approximation to it. However, as shown by (Bae et al., 2022)<sup>2</sup>, the different approximation errors ensuing for lack of convexity, approximate Hessian-vector products and so on, lead to this being a poor benchmark overall.</p>"},{"location":"getting-started/benchmarking/#data-re-labelling","title":"Data re-labelling","text":"<p>(Kong et al., 2022)<sup>3</sup> introduce a method using IFs to re-label harmful training samples in order to improve accuracy. One can then take the obtained improvement as a measure of the quality of the IF method.</p>"},{"location":"getting-started/benchmarking/#post-hoc-fairness-adjustment","title":"Post-hoc fairness adjustment","text":"<p>Introduced in [@...], the idea is to compute influences over a carefully selected fair set, and using them to re-weight the training data.</p> <ol> <li> <p>Wang, J.T., Jia, R., 2023. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning, in: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics, PMLR, pp. 6388--6421.\u00a0\u21a9</p> </li> <li> <p>Bae, J., Ng, N., Lo, A., Ghassemi, M., Grosse, R.B., 2022. If Influence Functions are the Answer, Then What is the Question?, in: Advances in Neural Information Processing Systems. Presented at the NeurIPS 2022, New Orleans, LA, USA, pp. 17953--17967.\u00a0\u21a9</p> </li> <li> <p>Kong, S., Shen, Y., Huang, L., 2022. Resolving Training Biases via [Influence-based Data Relabeling]{.nocase}. Presented at the International Conference on Learning Representations (ICLR 2022).\u00a0\u21a9</p> </li> </ol>"},{"location":"getting-started/first-steps/","title":"First steps","text":"<p>Info</p> <p>Make sure you have read Getting started before using the library. In particular read about which extra dependencies you may need.</p>"},{"location":"getting-started/first-steps/#main-concepts","title":"Main concepts","text":"<p>pyDVL aims to be a repository of production-ready, reference implementations of algorithms for data valuation and influence functions. Even though we only briefly introduce key concepts in the documentation, the following sections  should be enough to get you started.</p> <ul> <li> <p>Basics of data valuation</p> <p>Key objects and usage patterns for Shapley values and related methods.</p> <p> Data valuation</p> </li> <li> <p>Computing Influence Values</p> <p>Instructions on how to compute influence functions, and many approximations.</p> <p> Influence functions</p> </li> </ul>"},{"location":"getting-started/first-steps/#running-the-examples","title":"Running the examples","text":"<p>If you are somewhat familiar with the concepts of data valuation, you can start by browsing our worked-out examples illustrating pyDVL's capabilities either:</p> <ul> <li>In the examples under Basics of data valuation and Computing Influence Values.</li> <li>Using binder notebooks, deployed from each   example's page.</li> <li>Locally, by starting a jupyter server at the root of the project. You will   have to install jupyter first manually since it's not a dependency of the   library.</li> </ul>"},{"location":"getting-started/first-steps/#advanced-usage","title":"Advanced usage","text":"<p>Refer to the Advanced usage page for explanations on how to enable and use parallelization and caching.</p>"},{"location":"getting-started/glossary/","title":"Glossary","text":"<p>This glossary is meant to provide only brief explanations of each term, helping to clarify the concepts and techniques used in the library. For more detailed information, please refer to the relevant literature or resources.</p> <p>Warning</p> <p>This glossary is still a work in progress. Pull requests are welcome!</p>","boost":10},{"location":"getting-started/glossary/#data-valuation-terms","title":"Data valuation terms","text":"","boost":10},{"location":"getting-started/glossary/#glossary-beta-shapley","title":"Beta-Shapley","text":"<p>Beta-Shapley is a semi-value method that defines weights using a beta distribution, thus effectively implementing an importance sampling scheme that puts weight on marginal utilities for sets of certain sizes, as a function of the parameters of the beta distribution. Introduced in (Kwon and Zou, 2022)<sup>1</sup>.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-class-wise-shapley","title":"Class-wise Shapley","text":"<p>Class-wise Shapley is a Shapley valuation method which introduces a utility function that balances in-class, and out-of-class accuracy, with the goal of favoring points that improve the model's performance on the class they belong to. It is estimated to be particularly useful in imbalanced datasets, but more research is needed to confirm this. Introduced by (Schoch et al., 2022)<sup>2</sup>.</p> <ul> <li>Implementation    </li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-data-banzhaf","title":"Data-Banzhaf","text":"<p>Data-Banzhaf is a semi-value method that uses the Banzhaf value to determine the contribution of each data point to the model's performance. Its constant weights are a somwewhat effective importance sampling scheme that reduces the variance of the marginal utility estimates, especially for small subsets. It is most efficient when used in conjunction with the MSR sampler. Introduced by (Wang and Jia, 2023)<sup>3</sup>.</p> <ul> <li>Implementation   </li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-data-oob","title":"Data-OOB","text":"<p>Data-OOB is a method for valuing data points for a bagged model using its out-of-bag performance estimate. It overcomes the computational bottleneck of Shapley-based methods by evaluating each weak learner in an ensemble over samples it hasn't seen during training, and averaging the performance across all weak learners. Introduced in (Kwon and Zou, 2023)<sup>4</sup>.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-data-utility-learning","title":"Data Utility Learning","text":"<p>Data Utility Learning is a method that uses an ML model to learn the utility function. Essentially, it learns to predict the performance of a model when trained on a given set of indices from the dataset. The cost of training this model is quickly amortized by avoiding costly re-evaluations of the original utility. Introduced by (Wang et al., 2022)<sup>5</sup>.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-delta-shapley","title":"Delta-Shapley","text":"<p>Delta-Shapley is an approximation to Shapley value which uses a stratified sampling distribution that picks set sizes based on stability bounds for the machine learning model for which values are  estimated. An additional clipping constraint saves computation by skipping subset sizes (justified because of diminishing returns for model performance). This introduces a difference to Shapley value by a multiplicative factor that should not affect ranking. Introduced in (Watson et al., 2023)<sup>6</sup>.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-game-theoretic-methods","title":"Game-theoretic Methods","text":"<p>Game-theoretic methods for data valuation are a class of techniques that leverage concepts from cooperative game theory to assign values to data points based on their contributions to the overall performance of an ML model. Salient examples are Shapley value, Data Banzhaf, and Least Core.</p>","boost":10},{"location":"getting-started/glossary/#glossary-group-testing","title":"Group Testing","text":"<p>Group Testing is a strategy for identifying characteristics within groups of items efficiently, by testing groups rather than individuals to quickly narrow down the search for items with specific properties. The idea was introduced into data valuation by (Jia et al., 2019)<sup>7</sup>, transforming the problem of computing Shapley values into one of constraint satisfiability, with constraints given by samples of the utility, with a carefully designed sampling distribution.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-knn-shapley","title":"KNN-Shapley","text":"<p>KNN-Shapley is a Shapley value method tailored to KNN models. By exploiting the locality of KNN, it can reduce computation drastically wrt. the standard Shapley value with Monte Carlo approximation. Introduced in (Jia et al., 2019)<sup>7</sup>.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-importance-sampling","title":"Importance Sampling","text":"<p>Importance Sampling is a technique used to estimate properties of a particular distribution while only having samples generated from a different distribution. This is achieved by re-weighting the samples according to their \"sampled importance\", i.e. effectively dividing by the probability of sampling them.</p> <p>Much of the work in model-based data valuation consists of finding a good importance sampling distribution, and the weights to use for the sampled subsets. The most common choices are uniform sampling, Beta distributions, and Banzhaf indices.</p>","boost":10},{"location":"getting-started/glossary/#glossary-least-core","title":"Least Core","text":"<p>The Least Core is a solution concept in cooperative game theory, referring to the smallest set of payoffs to players that cannot be improved upon by any coalition, ensuring stability in the allocation of value. In data valuation, it implies solving a linear and a quadratic system whose constraints are determined by the evaluations of the utility function on every subset of the training data. Introduced as data valuation method by (Yan and Procaccia, 2021)<sup>8</sup>.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-loo","title":"Leave-One-Out","text":"<p>LOO in the context of data valuation refers to the process of evaluating the impact of removing individual data points on the model's performance. The value of a training point is defined as the marginal change in the model's performance when that point is removed from the training set.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-marginal-utility","title":"Marginal utility","text":"<p>In data valuation for ML, marginal utility refers to the change in performance of an ML model when a single data point is added to or removed from the training set. In our documentation it is often denoted \\(\\Delta_i(S) := U(S_{+i}) - U(S),\\) where \\(S\\) is a subset of the training set, \\(i\\) is the index of the data point to be added, and \\(U\\) is the utility function.</p> <ul> <li>Introduction to data valuation</li> </ul>","boost":10},{"location":"getting-started/glossary/#marginal-based-methods","title":"Marginal-based methods","text":"<p>Marginal-based methods are a class of data valuation techniques that define value in terms of weighted averages of the marginal  utility.</p> <ul> <li>Introduction to data valuation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-msr","title":"Maximum Sample Reuse","text":"<p>MSR is a sampling method for data valuation that updates the value of every data point in one sample. This method can achieve much faster convergence for Data Banzhaf since the sampling distribution \"coincides\" with the Banzhaf coefficients. In principle, it can be used with any semi-value by setting the sampler to MSRSampler, but it's most effective when used with the Banzhaf semi-value via MSRBanzhafValuation. Introduced by (Wang and Jia, 2023)<sup>3</sup></p> <ul> <li>Sampler</li> <li>MSRBanzhafValuation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-mclc","title":"Monte Carlo Least Core","text":"<p>MCLC is a variation of the Least Core that uses a reduced amount of constraints, sampled randomly from the powerset of the training data. Introduced by (Yan and Procaccia, 2021)<sup>8</sup>.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-monte-carlo-shapley","title":"Monte Carlo Shapley","text":"<p>MCS estimates the Shapley Value using a Monte Carlo approximation to the sum over subsets of the training set. This reduces computation to polynomial time at the cost of accuracy, but this loss is typically irrelevant for downstream applications in ML. Introduced into data valuation by (Ghorbani and Zou, 2019)<sup>9</sup>.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-point-removal-experiment","title":"Point-removal experiment","text":"<p>A point-removal experiment is a benchmarking task in data valuation where the quality of a valuation method is measured through the impact of incrementally removing data points on the model's performance. The points are removed in order of their value, and the performance is evaluated on a fixed validation set.</p> <ul> <li>Benchmarking valuation methods.</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-rank-correlation","title":"Rank correlation","text":"<p>Rank correlation is a simple way of measuring the stability of estimates for the values of a training set. It is computed as the Spearman correlation between the values of two different runs of the same method, after changing some hyperparameter like random seed, number of updates during a single run, etc.</p> <ul> <li>Benchmarking valuation methods.</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-shapley-value","title":"Shapley Value","text":"<p>Shapley Value is a concept from cooperative game theory that allocates payouts to players based on their contribution to the total payoff. In data valuation, players are data points. The method assigns a value to each data point based on a weighted average of its marginal contributions to the model's performance  when trained on each subset of the training set. This requires \\(\\mathcal{O}(2^{n-1})\\) re-trainings of the model, which is infeasible for even trivial data set sizes, so one resorts to approximations like TMCS. Introduced into data valuation by (Ghorbani and Zou, 2019)<sup>9</sup>.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-stratified-sampling","title":"Stratified sampling","text":"<p>In pyDVL, a stratified sampler is one that first chooses a subset size \\(k\\) following some distribution \\(\\mathcal{L}_k\\) over \\(\\{0,...,n-1\\},\\) then samples a subset of that size uniformly at random from the powerset of \\({N_{-i}}:\\)</p> <ol> <li>Sample \\(k \\sim \\mathcal{L}_k,\\)</li> <li>Sample \\(S \\sim \\mathcal{U}(2^{N_{-i}}).\\)</li> </ol> <p>If we denote by \\(\\mathcal{L}\\) the law for this two stage procedure, then one has that the Shapley value is the expectation over this distribution:</p> \\[v_\\text{sh}(i) = \\mathbb{E}_{S \\sim \\mathcal{L}}[\\Delta_i(S)].\\] <p>One can try to reduce variance or obtain different semi-values by choosing \\(\\mathcal{L}_k\\) differently, or combining it with any semivalue. See the links below.</p> <ul> <li>Data Shapley with a uniform stratified sampler</li> <li>Sampler implementation</li> <li>Variance-Reduced Data Shapley</li> <li>\\(\\delta\\)-Shapley </li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-tmcs","title":"Truncated Monte Carlo Shapley","text":"<p>TMCS is an efficient approach to estimating the Shapley Value using a truncated version of the Monte Carlo method with permutation sampling. Introduced by (Ghorbani and Zou, 2019)<sup>9</sup>.</p> <ul> <li>Implementation</li> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-utility-function","title":"Utility function","text":"<p>The utility function in ML data valuation is a measure of the performance of a model trained on a subset of the training data. This can be a metric like accuracy, F1 score, or any other performance measure. It is typically measured wrt. to a fixed valuation set (sometimes we use the terms test set or validation set interchangeably when no confusion is possible, but it should be a different, held-out set.</p> <p>In our documentation the utility is denoted \\(U\\), and is assumed to be a function defined over sets (hence invariant wrt. permutation of data indices): \\(U:2^{N} \\to \\mathbb{R}\\), where \\(N\\) is the index set of the training data, which we identify with the data themselves.</p> <ul> <li>Introduction to data valuation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-valuation-set","title":"Valuation set","text":"<p>The valuation set is a held-out subset of data used to evaluate the utility of a model trained on the training set. Sometimes, when there is no confusion, we use the terms test set or validation set interchangeably, but it should be a different, held-out set.</p> <p>Note that computing a score (loss) over a fixed set is typically a poor approximation to the true score of the model, i.e. to its expected score on unseen data. This problem might be alleviated with some form of cross-validation, but we haven't explored this possibility in pyDVL.</p>","boost":10},{"location":"getting-started/glossary/#glossary-vrds","title":"Variance-Reduced Data-Shapley","text":"<p>A stratified sampling-based approach to estimate Shapley values that uses a simple deterministic heuristic for sample sizes, which in particular does not depend on run-time variance estimates. A good default choice is based on the harmonic function. Introduced in (Wu et al., 2023)<sup>10</sup>.</p> <ul> <li>Implementation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-wad","title":"Weighted Accuracy Drop","text":"<p>WAD is a metric to evaluate the impact of sequentially removing data points on the performance of a machine learning model, weighted by their rank, i.e. by the time at which they were removed. Introduced by (Schoch et al., 2022)<sup>2</sup>.</p>","boost":10},{"location":"getting-started/glossary/#influence-function-terms","title":"Influence function terms","text":"","boost":10},{"location":"getting-started/glossary/#glossary-arnoldi","title":"Arnoldi Method","text":"<p>The Arnoldi method approximately computes eigenvalue, eigenvector pairs of a symmetric matrix. For influence functions, it is used to approximate the iHVP. Introduced by (Schioppa et al., 2022)<sup>11</sup> in the context of influence functions.</p> <ul> <li>Implementation (torch)     </li> <li>Documentation (torch)</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-block-cg","title":"Block Conjugate Gradient","text":"<p>A blocked version of CG, which solves several linear systems simultaneously. For Influence Functions, it is used to approximate the iHVP.</p> <ul> <li>Implementation (torch)    </li> <li>Documentation (torch)</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-cg","title":"Conjugate Gradient","text":"<p>CG is an algorithm for solving linear systems with a symmetric and positive-definite coefficient matrix. For Influence Functions, it is used to approximate the iHVP.</p> <ul> <li>Implementation    (torch)</li> <li>Documentation (torch)</li> </ul>","boost":10},{"location":"getting-started/glossary/#eigenvalue-corrected-kronecker-factored-approximate-curvature","title":"Eigenvalue-corrected Kronecker-Factored Approximate Curvature","text":"<p>EKFAC builds on K-FAC by correcting for the approximation errors in the eigenvalues of the blocks of the Kronecker-factored approximate curvature matrix. This correction aims to refine the accuracy of natural gradient approximations, thus potentially offering better training efficiency and stability in neural networks.</p> <ul> <li>Implementation    (torch)</li> <li>Documentation (torch)</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-influence-function","title":"Influence Function","text":"<p>The Influence Function measures the impact of a single data point on a statistical estimator. In machine learning, it's used to understand how much a particular data point affects the model's prediction. Introduced into data valuation by (Koh and Liang, 2017)<sup>12</sup>.</p> <ul> <li>Documentation</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-iHVP","title":"Inverse Hessian-vector product","text":"<p>iHVP is the operation of calculating the product of the inverse Hessian matrix of a function and a vector, without explicitly constructing nor inverting the full Hessian matrix first. This is essential for influence function computation.</p>","boost":10},{"location":"getting-started/glossary/#glossary-k-fac","title":"Kronecker-Factored Approximate Curvature","text":"<p>K-FAC is an optimization technique that approximates the Fisher Information matrix's inverse efficiently. It uses the Kronecker product to factor the matrix, significantly speeding up the computation of natural gradient updates and potentially improving training efficiency.</p>","boost":10},{"location":"getting-started/glossary/#glossary-lissa","title":"Linear-time Stochastic Second-order Algorithm","text":"<p>LiSSA is an efficient algorithm for approximating the inverse Hessian-vector product, enabling faster computations in large-scale machine learning problems, particularly for second-order optimization. For Influence Functions, it is used to approximate the iHVP. Introduced by (Agarwal et al., 2017)<sup>13</sup>.</p> <ul> <li>Implementation (torch)    </li> <li>Documentation (torch)    </li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-nystroem","title":"Nystr\u00f6m Low-Rank Approximation","text":"<p>The Nystr\u00f6m approximation computes a low-rank approximation to a symmetric positive-definite matrix via random projections. For influence functions,  it is used to approximate the iHVP. Introduced as sketch and solve algorithm in (Hataya and Yamada, 2023)<sup>14</sup>, and as preconditioner for PCG in (Frangella et al., 2023)<sup>15</sup>.</p> <ul> <li>Implementation Sketch-and-Solve    (torch)</li> <li>Documentation Sketch-and-Solve (torch)</li> <li>Implementation Preconditioner    (torch)</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-preconditioned-block-cg","title":"Preconditioned Block Conjugate Gradient","text":"<p>A blocked version of PCG, which solves  several linear systems simultaneously. For Influence Functions, it is used to approximate the iHVP.</p> <ul> <li>Implementation CG (torch)    </li> <li>Implementation Preconditioner    (torch)</li> <li>Documentation (torch)</li> </ul>","boost":10},{"location":"getting-started/glossary/#glossary-preconditioned-cg","title":"Preconditioned Conjugate Gradient","text":"<p>A preconditioned version of CG for improved convergence, depending on the characteristics of the matrix and the preconditioner. For Influence Functions, it is used to approximate the iHVP.</p> <ul> <li>Implementation CG (torch)    </li> <li>Implementation Preconditioner    (torch)</li> <li>Documentation (torch)</li> </ul>","boost":10},{"location":"getting-started/glossary/#other-terms","title":"Other terms","text":"","boost":10},{"location":"getting-started/glossary/#glossary-cv","title":"Coefficient of Variation","text":"<p>CV is a statistical measure of the dispersion of data points in a data series around the mean, expressed as a percentage. It's used to compare the degree of variation from one data series to another, even if the means are drastically different.</p>","boost":10},{"location":"getting-started/glossary/#glossary-csp","title":"Constraint Satisfaction Problem","text":"<p>A CSP involves finding values for variables within specified constraints or conditions, commonly used in scheduling, planning, and design problems where solutions must satisfy a set of restrictions.</p>","boost":10},{"location":"getting-started/glossary/#glossary-oob","title":"Out-of-Bag","text":"<p>OOB refers to data samples in an ensemble learning context (like random forests) that are not selected for training a specific model within the ensemble. These OOB samples are used as a validation set to estimate the model's accuracy, providing a convenient internal cross-validation mechanism.</p>","boost":10},{"location":"getting-started/glossary/#glossary-mlrc","title":"Machine Learning Reproducibility Challenge","text":"<p>The MLRC is an initiative that encourages the verification and replication of machine learning research findings, promoting transparency and reliability in the field. Papers are published in Transactions on Machine Learning Research (TMLR).</p>","boost":10},{"location":"getting-started/glossary/#glossary-point-removal-task","title":"Point removal task","text":"<p>A task in data valuation where the quality of a valuation method is measured through the impact of incrementally removing data points on the model's performance, where the points are removed in order of their value. See</p> <ul> <li>Implementation</li> <li>Benchmarking tasks</li> </ul> <ol> <li> <p>Kwon, Y., Zou, J., 2022. Beta Shapley: A Unified and [Noise-reduced Data Valuation Framework]{.nocase} for Machine Learning, in: Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022,. Presented at the AISTATS 2022, PMLR, Valencia, Spain.\u00a0\u21a9</p> </li> <li> <p>Schoch, S., Xu, H., Ji, Y., 2022. CS-Shapley: [Class-wise Shapley Values]{.nocase} for Data Valuation in Classification, in: Proc. Of the Thirty-Sixth Conference on Neural Information Processing Systems (NeurIPS). Presented at the Advances in Neural Information Processing Systems (NeurIPS 2022), New Orleans, Louisiana, USA.\u00a0\u21a9\u21a9</p> </li> <li> <p>Wang, J.T., Jia, R., 2023. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning, in: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics, PMLR, pp. 6388--6421.\u00a0\u21a9\u21a9</p> </li> <li> <p>Kwon, Y., Zou, J., 2023. Data-OOB: [Out-of-bag Estimate]{.nocase} as a Simple and Efficient Data Value, in: Proceedings of the 40th International Conference on Machine Learning. Presented at the International Conference on Machine Learning, PMLR, pp. 18135--18152.\u00a0\u21a9</p> </li> <li> <p>Wang, T., Yang, Y., Jia, R., 2022. Improving [Cooperative Game Theory-based Data Valuation]{.nocase} via Data Utility Learning. Presented at the International Conference on Learning Representations (ICLR 2022). Workshop on Socially Responsible Machine Learning, arXiv. https://doi.org/10.48550/arXiv.2107.06336 \u21a9</p> </li> <li> <p>Watson, L., Kujawa, Z., Andreeva, R., Yang, H.-T., Elahi, T., Sarkar, R., 2023. Accelerated Shapley Value Approximation for Data Evaluation [WWW Document]. https://doi.org/10.48550/arXiv.2311.05346 \u21a9</p> </li> <li> <p>Jia, R., Dao, D., Wang, B., Hubis, F.A., Gurel, N.M., Li, B., Zhang, C., Spanos, C., Song, D., 2019. Efficient task-specific data valuation for nearest neighbor algorithms. Proc. VLDB Endow. 12, 1610--1623. https://doi.org/10.14778/3342263.3342637 \u21a9\u21a9</p> </li> <li> <p>Yan, T., Procaccia, A.D., 2021. If You Like Shapley Then You'll Love the Core, in: Proceedings of the 35th AAAI Conference on Artificial Intelligence. Presented at the AAAI Conference on Artificial Intelligence, Association for the Advancement of Artificial Intelligence, Virtual conference, pp. 5751--5759. https://doi.org/10.1609/aaai.v35i6.16721 \u21a9\u21a9</p> </li> <li> <p>Ghorbani, A., Zou, J., 2019. Data Shapley: Equitable Valuation of Data for Machine Learning, in: Proceedings of the 36th International Conference on Machine Learning, PMLR. Presented at the International Conference on Machine Learning (ICML 2019), PMLR, pp. 2242--2251.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Wu, M., Jia, R., Lin, C., Huang, W., Chang, X., 2023. Variance reduced Shapley value estimation for trustworthy data valuation. Computers\\ &amp; Operations Research 159, 106305. https://doi.org/10.1016/j.cor.2023.106305 \u21a9</p> </li> <li> <p>Schioppa, A., Zablotskaia, P., Vilar, D., Sokolov, A., 2022. Scaling Up Influence Functions. Proc. AAAI Conf. Artif. Intell. 36, 8179--8186. https://doi.org/10.1609/aaai.v36i8.20791 \u21a9</p> </li> <li> <p>Koh, P.W., Liang, P., 2017. Understanding [Black-box Predictions]{.nocase} via Influence Functions, in: Proceedings of the 34th International Conference on Machine Learning. Presented at the International Conference on Machine Learning, PMLR, pp. 1885--1894.\u00a0\u21a9</p> </li> <li> <p>Agarwal, N., Bullins, B., Hazan, E., 2017. Second-Order Stochastic Optimization for Machine Learning in Linear Time. JMLR 18, 1--40.\u00a0\u21a9</p> </li> <li> <p>Hataya, R., Yamada, M., 2023. Nystr\u00f6m Method for Accurate and Scalable Implicit Differentiation, in: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics, PMLR, pp. 4643--4654.\u00a0\u21a9</p> </li> <li> <p>Frangella, Z., Tropp, J.A., Udell, M., 2023. Randomized Nystr\u00f6m Preconditioning. SIAM J. Matrix Anal. Appl. 44, 718--752. https://doi.org/10.1137/21M1466244 \u21a9</p> </li> </ol>","boost":10},{"location":"getting-started/methods/","title":"Methods","text":"<p>We currently implement the following methods:</p>"},{"location":"getting-started/methods/#implemented-methods-data-valuation","title":"Data valuation","text":"<ul> <li> <p>\\(\\delta\\)-Shapley   (Watson et al., 2023)<sup>1</sup></p> </li> <li> <p>Beta Shapley   (Kwon and Zou, 2022)<sup>2</sup>.</p> </li> <li> <p>Class-Wise Shapley   (Schoch et al., 2022)<sup>3</sup>.</p> </li> <li> <p>Data Banzhaf and MSR sampling   (Wang and Jia, 2023)<sup>4</sup>.</p> </li> <li> <p>Data Utility Learning   (Wang et al., 2022)<sup>5</sup>.</p> </li> <li> <p>Data-OOB   (Kwon and Zou, 2023)<sup>6</sup>.</p> </li> <li> <p>Group Testing Shapley   (Jia et al., 2019)<sup>7</sup></p> </li> <li> <p>kNN-Shapley, exact only   (Jia et al., 2019)<sup>8</sup>.</p> </li> <li> <p>Least Core   (Yan and Procaccia, 2021)<sup>9</sup>.</p> </li> <li> <p>Leave-One-Out values.</p> </li> <li> <p>Owen Shapley   (Okhrati and Lipani, 2021)<sup>10</sup>.</p> </li> <li> <p>Permutation Shapley, also called ApproxShapley   (Castro et al., 2009)<sup>11</sup>.</p> </li> <li> <p>Truncated Monte Carlo Shapley   (Ghorbani and Zou, 2019)<sup>12</sup>.</p> </li> <li> <p>Variance-Reduced Data Shapley   (Wu et al., 2023)<sup>13</sup>.</p> </li> </ul>"},{"location":"getting-started/methods/#implemented-methods-influence-functions","title":"Influence functions","text":"<ul> <li> <p>CG Influence   (Koh and Liang, 2017)<sup>14</sup>.</p> </li> <li> <p>Direct Influence   (Koh and Liang, 2017)<sup>14</sup>.</p> </li> <li> <p>LiSSA   (Agarwal et al., 2017)<sup>15</sup>.</p> </li> <li> <p>Arnoldi Influence   (Schioppa et al., 2022)<sup>16</sup>.</p> </li> <li> <p>EKFAC Influence   (George et al., 2018; Martens and Grosse, 2015)<sup>17</sup> <sup>18</sup>.</p> </li> <li> <p>Nystr\u00f6m Influence, based   on the ideas in (Hataya and Yamada, 2023)<sup>19</sup> for bi-level optimization.</p> </li> <li> <p>Inverse-harmonic-mean   Influence   (Kwon et al., 2023)<sup>20</sup>.</p> </li> </ul> <ol> <li> <p>Watson, L., Kujawa, Z., Andreeva, R., Yang, H.-T., Elahi, T., Sarkar, R., 2023. Accelerated Shapley Value Approximation for Data Evaluation [WWW Document]. https://doi.org/10.48550/arXiv.2311.05346 \u21a9</p> </li> <li> <p>Kwon, Y., Zou, J., 2022. Beta Shapley: A Unified and [Noise-reduced Data Valuation Framework]{.nocase} for Machine Learning, in: Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022,. Presented at the AISTATS 2022, PMLR, Valencia, Spain.\u00a0\u21a9</p> </li> <li> <p>Schoch, S., Xu, H., Ji, Y., 2022. CS-Shapley: [Class-wise Shapley Values]{.nocase} for Data Valuation in Classification, in: Proc. Of the Thirty-Sixth Conference on Neural Information Processing Systems (NeurIPS). Presented at the Advances in Neural Information Processing Systems (NeurIPS 2022), New Orleans, Louisiana, USA.\u00a0\u21a9</p> </li> <li> <p>Wang, J.T., Jia, R., 2023. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning, in: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics, PMLR, pp. 6388--6421.\u00a0\u21a9</p> </li> <li> <p>Wang, T., Yang, Y., Jia, R., 2022. Improving [Cooperative Game Theory-based Data Valuation]{.nocase} via Data Utility Learning. Presented at the International Conference on Learning Representations (ICLR 2022). Workshop on Socially Responsible Machine Learning, arXiv. https://doi.org/10.48550/arXiv.2107.06336 \u21a9</p> </li> <li> <p>Kwon, Y., Zou, J., 2023. Data-OOB: [Out-of-bag Estimate]{.nocase} as a Simple and Efficient Data Value, in: Proceedings of the 40th International Conference on Machine Learning. Presented at the International Conference on Machine Learning, PMLR, pp. 18135--18152.\u00a0\u21a9</p> </li> <li> <p>Jia, R., Dao, D., Wang, B., Hubis, F.A., Hynes, N., G\u00fcrel, N.M., Li, B., Zhang, C., Song, D., Spanos, C.J., 2019. Towards Efficient Data Valuation Based on the Shapley Value, in: Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics (AISTATS), PMLR, pp. 1167--1176.\u00a0\u21a9</p> </li> <li> <p>Jia, R., Dao, D., Wang, B., Hubis, F.A., Gurel, N.M., Li, B., Zhang, C., Spanos, C., Song, D., 2019. Efficient task-specific data valuation for nearest neighbor algorithms. Proc. VLDB Endow. 12, 1610--1623. https://doi.org/10.14778/3342263.3342637 \u21a9</p> </li> <li> <p>Yan, T., Procaccia, A.D., 2021. If You Like Shapley Then You'll Love the Core, in: Proceedings of the 35th AAAI Conference on Artificial Intelligence. Presented at the AAAI Conference on Artificial Intelligence, Association for the Advancement of Artificial Intelligence, Virtual conference, pp. 5751--5759. https://doi.org/10.1609/aaai.v35i6.16721 \u21a9</p> </li> <li> <p>Okhrati, R., Lipani, A., 2021. A Multilinear Sampling Algorithm to Estimate Shapley Values, in: 2020 25th International Conference on Pattern Recognition (ICPR). Presented at the 2020 25th International Conference on Pattern Recognition (ICPR), IEEE, pp. 7992--7999. https://doi.org/10.1109/ICPR48806.2021.9412511 \u21a9</p> </li> <li> <p>Castro, J., G\u00f3mez, D., Tejada, J., 2009. Polynomial calculation of the Shapley value based on sampling. Computers\\ &amp; Operations Research, Selected papers presented at the Tenth International Symposium on Locational Decisions (ISOLDE X) 36, 1726--1730. https://doi.org/10.1016/j.cor.2008.04.004 \u21a9</p> </li> <li> <p>Ghorbani, A., Zou, J., 2019. Data Shapley: Equitable Valuation of Data for Machine Learning, in: Proceedings of the 36th International Conference on Machine Learning, PMLR. Presented at the International Conference on Machine Learning (ICML 2019), PMLR, pp. 2242--2251.\u00a0\u21a9</p> </li> <li> <p>Wu, M., Jia, R., Lin, C., Huang, W., Chang, X., 2023. Variance reduced Shapley value estimation for trustworthy data valuation. Computers\\ &amp; Operations Research 159, 106305. https://doi.org/10.1016/j.cor.2023.106305 \u21a9</p> </li> <li> <p>Koh, P.W., Liang, P., 2017. Understanding [Black-box Predictions]{.nocase} via Influence Functions, in: Proceedings of the 34th International Conference on Machine Learning. Presented at the International Conference on Machine Learning, PMLR, pp. 1885--1894.\u00a0\u21a9\u21a9</p> </li> <li> <p>Agarwal, N., Bullins, B., Hazan, E., 2017. Second-Order Stochastic Optimization for Machine Learning in Linear Time. JMLR 18, 1--40.\u00a0\u21a9</p> </li> <li> <p>Schioppa, A., Zablotskaia, P., Vilar, D., Sokolov, A., 2022. Scaling Up Influence Functions. Proc. AAAI Conf. Artif. Intell. 36, 8179--8186. https://doi.org/10.1609/aaai.v36i8.20791 \u21a9</p> </li> <li> <p>George, T., Laurent, C., Bouthillier, X., Ballas, N., Vincent, P., 2018. Fast Approximate Natural Gradient Descent in a Kronecker Factored Eigenbasis, in: Advances in Neural Information Processing Systems. Curran Associates, Inc.\u00a0\u21a9</p> </li> <li> <p>Martens, J., Grosse, R., 2015. Optimizing Neural Networks with [Kronecker-factored Approximate Curvature]{.nocase}, in: Proceedings of the 32nd International Conference on Machine Learning. Presented at the International Conference on Machine Learning, PMLR, pp. 2408--2417.\u00a0\u21a9</p> </li> <li> <p>Hataya, R., Yamada, M., 2023. Nystr\u00f6m Method for Accurate and Scalable Implicit Differentiation, in: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics, PMLR, pp. 4643--4654.\u00a0\u21a9</p> </li> <li> <p>Kwon, Y., Wu, E., Wu, K., Zou, J., 2023. DataInf: Efficiently Estimating Data Influence in [LoRA-tuned LLMs]{.nocase} and Diffusion Models. Presented at the The Twelfth International Conference on Learning Representations. https://doi.org/10.48550/arXiv.2310.00902 \u21a9</p> </li> </ol>"},{"location":"influence/","title":"The influence function","text":""},{"location":"influence/#influence-function","title":"The influence function","text":"<p>Warning</p> <p>The code in the package pydvl.influence is experimental. Package structure and basic API are bound to change before v1.0.0</p> <p>The influence function (IF) is a method to quantify the effect (influence) that each training point has on the parameters of a model, and by extension on any function thereof. In particular, it allows to estimate how much each training sample affects the error on a test point, making the IF useful for understanding and debugging models.</p> <p>Alas, the influence function relies on some assumptions that can make their application difficult. Yet another drawback is that they require the computation of the inverse of the Hessian of the model wrt. its parameters, which is intractable for large models like deep neural networks. Much of the recent research tackles this issue using approximations, like a Neuman series (Agarwal et al., 2017)<sup>1</sup>, with the most successful solution using a low-rank approximation that iteratively finds increasing eigenspaces of the Hessian (Schioppa et al., 2022)<sup>2</sup>.</p> <p>pyDVL implements several methods for the efficient computation of the IF for machine learning. In the examples we document some of the difficulties that can arise when using the IF.</p>"},{"location":"influence/#construction","title":"Construction","text":"<p>First introduced in the context of robust statistics in (Hampel, 1974)<sup>3</sup>, the IF was popularized in the context of machine learning in (Koh and Liang, 2017)<sup>4</sup>.</p> <p>Following their formulation, consider an input space \\(\\mathcal{X}\\) (e.g. images) and an output space \\(\\mathcal{Y}\\) (e.g. labels). Let's take \\(z_i = (x_i, y_i)\\), for \\(i \\in  \\{1,...,n\\}\\) to be the \\(i\\)-th training point, and \\(\\theta\\) to be the (potentially highly) multi-dimensional parameters of a model (e.g. \\(\\theta\\) is a big array with all of a neural network's parameters, including biases and/or dropout rates). We will denote with \\(L(z, \\theta)\\) the loss of the model for point \\(z\\) when the parameters are \\(\\theta.\\)</p> <p>To train a model, we typically minimize the loss over all \\(z_i\\), i.e. the optimal parameters are</p> \\[\\hat{\\theta} = \\arg \\min_\\theta \\sum_{i=1}^n L(z_i, \\theta).\\] <p>In practice, lack of convexity means that one doesn't really obtain the minimizer of the loss, and the training is stopped when the validation loss stops decreasing.</p> <p>For notational convenience, let's define</p> \\[\\hat{\\theta}_{-z} = \\arg \\min_\\theta \\sum_{z_i \\ne z} L(z_i, \\theta), \\] <p>i.e. \\(\\hat{\\theta}_{-z}\\) are the model parameters that minimize the total loss when \\(z\\) is not in the training dataset.</p> <p>In order to compute the impact of each training point on the model, we would need to calculate \\(\\hat{\\theta}_{-z}\\) for each \\(z\\) in the training dataset, thus re-training the model at least ~\\(n\\) times (more if model training is stochastic). This is computationally very expensive, especially for big neural networks. To circumvent this problem, we can just calculate a first order approximation of \\(\\hat{\\theta}\\). This can be done through single backpropagation and without re-training the full model.</p> <p>pyDVL supports two ways of computing the empirical influence function, namely up-weighting of samples and perturbation influences.</p>"},{"location":"influence/#influence-of-a-point","title":"Approximating the influence of a point","text":"<p>Let's define</p> \\[\\hat{\\theta}_{\\epsilon, z} = \\arg \\min_\\theta \\frac{1}{n}\\sum_{i=1}^n L(z_i, \\theta) + \\epsilon L(z, \\theta), \\] <p>which is the optimal \\(\\hat{\\theta}\\) when we up-weight \\(z\\) by an amount \\(\\epsilon \\gt 0\\).</p> <p>From a classical result (a simple derivation is available in Appendix A of (Koh and Liang, 2017)<sup>4</sup>), we know that:</p> \\[\\frac{d \\ \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon} \\Big|_{\\epsilon=0} = -H_{\\hat{\\theta}}^{-1} \\nabla_\\theta L(z, \\hat{\\theta}), \\] <p>where \\(H_{\\hat{\\theta}} = \\frac{1}{n} \\sum_{i=1}^n \\nabla_\\theta^2 L(z_i, \\hat{\\theta})\\) is the Hessian of \\(L\\). These quantities are also known as influence factors.</p> <p>Importantly, notice that this expression is only valid when \\(\\hat{\\theta}\\) is a minimum of \\(L\\), or otherwise \\(H_{\\hat{\\theta}}\\) cannot be inverted! At the same time, in machine learning full convergence is rarely achieved, so direct Hessian inversion is not possible. Approximations need to be developed that circumvent the problem of inverting the Hessian of the model in all those (frequent) cases where it is not positive definite.</p> <p>The influence of training point \\(z\\) on test point \\(z_{\\text{test}}\\) is defined as:</p> \\[\\mathcal{I}(z, z_{\\text{test}}) =  L(z_{\\text{test}}, \\hat{\\theta}_{-z}) - L(z_{\\text{test}}, \\hat{\\theta}). \\] <p>Notice that \\(\\mathcal{I}\\) is higher for points \\(z\\) which positively impact the model score, since the loss is higher when they are excluded from training. In practice, one needs to rely on the following infinitesimal approximation:</p> \\[\\mathcal{I}_{up}(z, z_{\\text{test}}) = - \\frac{d L(z_{\\text{test}}, \\hat{\\theta}_{\\epsilon, z})}{d \\epsilon} \\Big|_{\\epsilon=0} \\] <p>Using the chain rule and the results calculated above, we get:</p> \\[\\mathcal{I}_{up}(z, z_{\\text{test}}) = - \\nabla_\\theta L(z_{\\text{test}}, \\hat{\\theta})^\\top \\ \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon} \\Big|_{\\epsilon=0} = \\nabla_\\theta L(z_{\\text{test}}, \\hat{\\theta})^\\top \\ H_{\\hat{\\theta}}^{-1} \\ \\nabla_\\theta L(z, \\hat{\\theta}) \\] <p>All the resulting factors are gradients of the loss wrt. the model parameters \\(\\hat{\\theta}\\). This can be easily computed through one or more backpropagation passes.</p>"},{"location":"influence/#perturbation-definition-of-the-influence-score","title":"Perturbation definition of the influence score","text":"<p>How would the loss of the model change if, instead of up-weighting an individual point \\(z\\), we were to up-weight only a single feature of that point? Given \\(z = (x, y)\\), we can define \\(z_{\\delta} = (x+\\delta, y)\\), where \\(\\delta\\) is a vector of zeros except for a 1 in the position of the feature we want to up-weight. In order to approximate the effect of modifying a single feature of a single point on the model score we can define</p> \\[\\hat{\\theta}_{\\epsilon, z_{\\delta} ,-z} = \\arg \\min_\\theta \\frac{1}{n}\\sum_{i=1}^n L(z_{i}, \\theta) + \\epsilon L(z_{\\delta}, \\theta) - \\epsilon L(z, \\theta), \\] <p>Similarly to what was done above, we up-weight point \\(z_{\\delta}\\), but then we also remove the up-weighting for all the features that are not modified by \\(\\delta\\). From the calculations in the previous section it is then easy to see that</p> \\[\\frac{d \\ \\hat{\\theta}_{\\epsilon, z_{\\delta} ,-z}}{d \\epsilon} \\Big|_{\\epsilon=0} = -H_{\\hat{\\theta}}^{-1} \\nabla_\\theta \\Big( L(z_{\\delta}, \\hat{\\theta}) - L(z, \\hat{\\theta}) \\Big) \\] <p>and if the feature space is continuous and as \\(\\delta \\to 0\\) we can write</p> \\[\\frac{d \\ \\hat{\\theta}_{\\epsilon, z_{\\delta} ,-z}}{d \\epsilon} \\Big|_{\\epsilon=0} = -H_{\\hat{\\theta}}^{-1} \\ \\nabla_x \\nabla_\\theta L(z, \\hat{\\theta}) \\delta + \\mathcal{o}(\\delta) \\] <p>The influence of each feature of \\(z\\) on the loss of the model can therefore be estimated through the following quantity:</p> \\[\\mathcal{I}_{pert}(z, z_{\\text{test}}) = - \\lim_{\\delta \\to 0} \\ \\frac{1}{\\delta} \\frac{d L(z_{\\text{test}}, \\hat{\\theta}_{\\epsilon, \\ z_{\\delta}, \\ -z})}{d \\epsilon} \\Big|_{\\epsilon=0} \\] <p>which, using the chain rule and the results calculated above, is equal to</p> \\[\\mathcal{I}_{pert}(z, z_{\\text{test}}) = - \\nabla_\\theta L(z_{\\text{test}}, \\hat{\\theta})^\\top \\ \\frac{d \\hat{\\theta}_{\\epsilon, z_{\\delta} ,-z}}{d \\epsilon} \\Big|_{\\epsilon=0} = \\nabla_\\theta L(z_{\\text{test}}, \\hat{\\theta})^\\top \\ H_{\\hat{\\theta}}^{-1} \\ \\nabla_x \\nabla_\\theta L(z, \\hat{\\theta}) \\] <p>The perturbation definition of the influence score is not straightforward to understand, but it has a simple interpretation: it tells how much the loss of the model changes when a certain feature of point z is up-weighted. A positive perturbation influence score indicates that the feature might have a positive effect on the accuracy of the model.</p> <p>It is worth noting that the perturbation influence score is a very rough estimate of the impact of a point on the models loss and it is subject to large approximation errors. It can nonetheless be used to build training-set attacks, as done in (Koh and Liang, 2017)<sup>4</sup>.</p>"},{"location":"influence/#computation","title":"Computation","text":"<p>The main abstraction of the library for influence calculation is InfluenceFunctionModel.  On implementations of this abstraction, you can call the method <code>influences</code> to compute influences. </p> <p>pyDVL provides implementations to use with pytorch model in pydvl.influence.torch. For detailed information  on available implementations see the documentation in InfluenceFunctionModel.</p> <p>Given a pre-trained pytorch model and a loss, a basic example would look like</p> <p><pre><code>from torch.utils.data import DataLoader\nfrom pydvl.influence.torch import DirectInfluence\n\ntraining_data_loader = DataLoader(...)\ninfl_model = DirectInfluence(model, loss)\ninfl_model = infl_model.fit(training_data_loader)\n\ninfluences = infl_model.influences(x_test, y_test, x, y)\n</code></pre> for batches \\(z_{\\text{test}} = (x_{\\text{test}}, y_{\\text{test}})\\) and \\(z = (x, y)\\) of data. The result is a tensor with one row per test point in  \\(z_{\\text{test}}\\) and one column per point in \\(z\\).  Thus, each entry \\((i, j)\\) represents the influence of training point \\(z[j]\\) on test point \\(z_{\\text{test}}[i]\\).</p> <p>Warning</p> <p>Compared to the mathematical definitions above, we switch the ordering of \\(z\\) and \\(z_{\\text{test}}\\), in order to make the input ordering consistent with the dimensions of the resulting tensor. More precisely, if the first dimension of \\(z_{\\text{test}}\\) is \\(N\\) and that of \\(z\\) is \\(M\\), then the resulting tensor is of shape \\(N \\times M\\)</p> <p>A large positive influence indicates that training point \\(j\\) tends to improve the performance of the model on test point \\(i\\), and vice versa, a large negative influence indicates that training point \\(j\\) tends to worsen the performance of the model on test point \\(i\\).</p>"},{"location":"influence/#hessian-regularization","title":"Hessian regularization","text":"<p>Additionally, and as discussed in the introduction, in machine learning training rarely converges to a global minimum of the loss. Despite good apparent convergence, \\(\\hat{\\theta}\\) might be located in a region with flat curvature or close to a saddle point. In particular, the Hessian might have vanishing eigenvalues making its direct inversion impossible. Certain methods, such as the Arnoldi method are robust against these problems, but most are not.</p> <p>To circumvent this problem, many approximate methods can be implemented. The simplest adds a small hessian perturbation term, i.e. \\(H_{\\hat{\\theta}} + \\lambda \\mathbb{I}\\), with \\(\\mathbb{I}\\) being the identity matrix. </p> <pre><code>from torch.utils.data import DataLoader\nfrom pydvl.influence.torch import DirectInfluence\n\ntraining_data_loader = DataLoader(...)\ninfl_model = DirectInfluence(model, loss, regularization=0.01)\ninfl_model = infl_model.fit(training_data_loader)\n</code></pre> <p>This standard trick ensures that the eigenvalues of \\(H_{\\hat{\\theta}}\\) are bounded away from zero and therefore the matrix is invertible. In order for this regularization not to corrupt the outcome too much, the parameter \\(\\lambda\\) should be as small as possible while still allowing a reliable inversion of \\(H_{\\hat{\\theta}} + \\lambda \\mathbb{I}\\).</p>"},{"location":"influence/#block-diagonal-approximation","title":"Block-diagonal approximation","text":"<p>This implementation is capable of using a block-diagonal approximation. The full matrix is approximated by a block-diagonal version, which reduces both the time and memory consumption. The blocking structure can be specified via the <code>block_structure</code> parameter. The <code>block_structure</code> parameter can either be a BlockMode enum (which provides layer-wise or parameter-wise blocking) or a custom block structure defined by an ordered dictionary with the keys being the block identifiers (arbitrary strings) and the values being lists of parameter names contained in the block. <pre><code>from torch.utils.data import DataLoader\nfrom pydvl.influence.torch import DirectInfluence, BlockMode, SecondOrderMode\n\ntraining_data_loader = DataLoader(...)\n# layer-wise block-diagonal approximation\ninfl_model = DirectInfluence(model, loss,\n                             regularization=0.1,\n                             block_structure=BlockMode.LAYER_WISE)\n\nblock_structure = OrderedDict((\n    (\"custom_block1\", [\"0.weight\", \"1.bias\"]), \n    (\"custom_block2\", [\"1.weight\", \"0.bias\"]),\n))\n# custom block-diagonal structure\ninfl_model = DirectInfluence(model, loss,\n                             regularization=0.1,\n                             block_structure=block_structure)\ninfl_model = infl_model.fit(training_data_loader)\n</code></pre> If you would like to apply a block-specific regularization, you can provide a dictionary with the block names as keys and the regularization values as values. If no value is provided for a specific key, no regularization is applied for the corresponding block.</p> <p><pre><code>regularization =  {\n\"custom_block1\": 0.1,\n\"custom_block2\": 0.2,\n}\ninfl_model = DirectInfluence(model, loss,\n                             regularization=regularization,\n                             block_structure=block_structure)\ninfl_model = infl_model.fit(training_data_loader)\n</code></pre> Accordingly, if you choose a layer-wise or parameter-wise structure (by providing <code>BlockMode.LAYER_WISE</code> or <code>BlockMode.PARAMETER_WISE</code> for <code>block_structure</code>) the keys must be the layer names or parameter names, respectively. You can retrieve the block-wise influence information from the methods with suffix <code>_by_block</code>. By default, <code>block_structure</code> is set to <code>BlockMode.FULL</code> and in this case these methods will return a dictionary with the empty string being the only key.</p>"},{"location":"influence/#gauss-newton-approximation","title":"Gauss-Newton approximation","text":"<p>In the computation of the influence values, the inversion of the Hessian can be replaced by the inversion of the Gauss-Newton matrix</p> \\[ G_{\\hat{\\theta}}=n^{-1} \\sum_{i=1}^n \\nabla_{\\theta}L(z_i, \\hat{\\theta})     \\nabla_{\\theta}L(z_i, \\hat{\\theta})^T \\] <p>so the computed values are of the form</p> \\[\\nabla_\\theta L(z_{\\text{test}}, \\hat{\\theta})^\\top \\ G_{\\hat{\\theta}}^{-1} \\ \\nabla_\\theta L(z, \\hat{\\theta}). \\] <p>The parameter <code>second_orer_mode</code> is used to configure this approximation. <pre><code>from torch.utils.data import DataLoader\nfrom pydvl.influence.torch import DirectInfluence, BlockMode, SecondOrderMode\n\ntraining_data_loader = DataLoader(...)\ninfl_model = DirectInfluence(model, loss,\n                             regularization={\"layer_1\": 0.1, \"layer_2\": 0.2},\n                             block_structure=BlockMode.LAYER_WISE,\n                             second_order_mode=SecondOrderMode.GAUSS_NEWTON)\ninfl_model = infl_model.fit(training_data_loader)\n</code></pre></p>"},{"location":"influence/#perturbation-influences","title":"Perturbation influences","text":"<p>The method of empirical influence computation can be selected with the parameter <code>mode</code>:</p> <p><pre><code>from pydvl.influence import InfluenceMode\n\ninfluences = infl_model.influences(x_test, y_test, x, y,\n                                   mode=InfluenceMode.Perturbation)\n</code></pre> The result is a tensor with at least three dimensions. The first two dimensions are the same as in the case of <code>mode=InfluenceMode.Up</code> case, i.e. one row per test point and one column per training point. The remaining dimensions are the same as the number of input features in the data. Therefore, each entry in the tensor represents the influence of each feature of each training point on each test point.</p>"},{"location":"influence/#influence-factors","title":"Influence factors","text":"<p>The influence factors(refer to the previous section for a definition) are typically the most computationally demanding part of influence calculation. They can be obtained via calling the <code>influence_factors</code> method, saved, and later used  for influence calculation on different subsets of the training dataset.</p> <pre><code>influence_factors = infl_model.influence_factors(x_test, y_test)\ninfluences = infl_model.influences_from_factors(influence_factors, x, y)\n</code></pre> <ol> <li> <p>Agarwal, N., Bullins, B., Hazan, E., 2017. Second-Order Stochastic Optimization for Machine Learning in Linear Time. JMLR 18, 1--40.\u00a0\u21a9</p> </li> <li> <p>Schioppa, A., Zablotskaia, P., Vilar, D., Sokolov, A., 2022. Scaling Up Influence Functions. Proc. AAAI Conf. Artif. Intell. 36, 8179--8186. https://doi.org/10.1609/aaai.v36i8.20791 \u21a9</p> </li> <li> <p>Hampel, F.R., 1974. The Influence Curve and Its Role in Robust Estimation. J. Am. Stat. Assoc. 69, 383--393. https://doi.org/10.2307/2285666 \u21a9</p> </li> <li> <p>Koh, P.W., Liang, P., 2017. Understanding [Black-box Predictions]{.nocase} via Influence Functions, in: Proceedings of the 34th International Conference on Machine Learning. Presented at the International Conference on Machine Learning, PMLR, pp. 1885--1894.\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"influence/influence_function_model/","title":"Influence Function Model","text":"<p>In almost every practical application it is not possible to construct, even less invert the complete Hessian in memory. pyDVL offers several implementations of  the interface InfluenceFunctionModel ,  which do not compute the full Hessian (in contrast to DirectInfluence ).</p>"},{"location":"influence/influence_function_model/#conjugate-gradient","title":"Conjugate Gradient","text":"<p>This classical procedure for solving linear systems of equations is an iterative method that does not require the explicit inversion of the Hessian. Instead, it only requires the calculation of Hessian-vector products, making it a good choice for large datasets or models with many parameters. It is nevertheless much slower to converge than the direct inversion method and not as accurate.</p> <p>More info on the theory of conjugate gradient can be found on Wikipedia, or in text books such as (Trefethen and Bau, 1997, Lecture 38)<sup>1</sup>.</p> <p>pyDVL also implements a stable block variant of the conjugate  gradient method, defined in (Ji and Li, 2017)<sup>2</sup>, which solves several right hand sides simultaneously.</p> <p>Optionally, the user can provide a pre-conditioner to improve convergence, such  as a Jacobi preconditioner , which is a simple diagonal pre-conditioner  based on Hutchinson's diagonal estimator (Bekas et al., 2007)<sup>3</sup>, or a Nystr\u00f6m approximation based preconditioner ,  described in (Frangella et al., 2023)<sup>4</sup>.</p> Using Conjugate Gradient <pre><code>from pydvl.influence.torch import CgInfluence, BlockMode, SecondOrderMode\nfrom pydvl.influence.torch.preconditioner import NystroemPreconditioner\n\nif_model = CgInfluence(\n    model,\n    loss,\n    regularization=0.0,\n    rtol=1e-7,\n    atol=1e-7,\n    maxiter=None,\n    solve_simultaneously=True,\n    preconditioner=NystroemPreconditioner(rank=10),\n    block_structure=BlockMode.FULL,\n    second_order_mode=SecondOrderMode.HESSIAN\n)\nif_model.fit(train_loader)\n</code></pre> <p>The additional optional parameters <code>rtol</code>, <code>atol</code>, <code>maxiter</code>,  <code>solve_simultaneously</code> and <code>preconditioner</code> are respectively, the relative tolerance, the absolute tolerance, the maximum number of iterations,  a flag indicating whether to use a variant of cg to simultaneously solving the system for several right hand sides and an optional preconditioner.</p> <p>This implementation is capable of using a block-diagonal approximation, see Block-diagonal approximation, and can handle Gauss-Newton approximation.</p>"},{"location":"influence/influence_function_model/#linear-time-stochastic-second-order-approximation-lissa","title":"Linear time Stochastic Second-Order Approximation (LiSSA)","text":"<p>The LiSSA method is a stochastic approximation of the inverse Hessian vector product. Compared to conjugate gradient it is faster but less accurate and typically suffers from instability.</p> <p>In order to find the solution of the HVP, LiSSA iteratively approximates the inverse of the Hessian matrix with the following update:</p> \\[H^{-1}_{j+1} b = b + (I - d) \\ H - \\frac{H^{-1}_j b}{s},\\] <p>where \\(d\\) and \\(s\\) are a dampening and a scaling factor, which are essential for the convergence of the method and they need to be chosen carefully, and I is the identity matrix. More info on the theory of LiSSA can be found in the original paper (Agarwal et al., 2017)<sup>5</sup>.</p> Using LiSSA <pre><code>from pydvl.influence.torch import LissaInfluence, BlockMode, SecondOrderMode\nif_model = LissaInfluence(\n   model,\n   loss,\n   regularization=0.0, \n   maxiter=1000,\n   dampen=0.0,\n   scale=10.0,\n   rtol=1e-4,\n   block_structure=BlockMode.FULL,\n   second_order_mode=SecondOrderMode.GAUSS_NEWTON\n)\nif_model.fit(train_loader)\n</code></pre> <p>with the additional optional parameters <code>maxiter</code>, <code>dampen</code>, <code>scale</code>, and <code>rtol</code>, being the maximum number of iterations, the dampening factor, the scaling factor and the relative tolerance, respectively. This implementation is capable of using a block-matrix  approximation, see  Block-diagonal approximation, and can handle Gauss-Newton approximation.</p>"},{"location":"influence/influence_function_model/#arnoldi-method","title":"Arnoldi","text":"<p>The Arnoldi method is a Krylov subspace method for approximating dominating eigenvalues and eigenvectors. Under a low rank assumption on the Hessian at a minimizer (which is typically observed for deep neural networks), this approximation captures the essential action of the Hessian. More concretely, for \\(Hx=b\\) the solution is approximated by</p> \\[x \\approx V D^{-1} V^T b\\] <p>where \\(D\\) is a diagonal matrix with the top (in absolute value) eigenvalues of the Hessian and \\(V\\) contains the corresponding eigenvectors. See also (Schioppa et al., 2022)<sup>6</sup>.</p> Using Arnoldi <pre><code>from pydvl.influence.torch import ArnoldiInfluence, BlockMode, SecondOrderMode\nif_model = ArnoldiInfluence(\n    model,\n    loss,\n    regularization=0.0,\n    rank=10,\n    tol=1e-6,\n    block_structure=BlockMode.FULL,\n    second_order_mode=SecondOrderMode.HESSIAN\n)\nif_model.fit(train_loader)\n</code></pre> <p>This implementation is capable of using a block-matrix approximation, see Block-diagonal approximation, and can handle Gauss-Newton approximation.</p>"},{"location":"influence/influence_function_model/#eigenvalue-corrected-k-fac","title":"Eigenvalue Corrected K-FAC","text":"<p>K-FAC, short for Kronecker-Factored Approximate Curvature, is a method that  approximates the Fisher information matrix FIM of a model.  It is possible to show that for classification models with appropriate loss  functions the FIM is equal to the Hessian of the model\u2019s loss over the dataset.  In this restricted but nonetheless important context K-FAC offers an efficient  way to approximate the Hessian and hence the influence scores.  For more info and details refer to the original paper (Martens and Grosse, 2015)<sup>7</sup>.</p> <p>The K-FAC method is implemented in the class EkfacInfluence .  The following code snippet shows how to use the K-FAC method to calculate the  influence function of a model. Note that, in contrast to the other methods for  influence function calculation, K-FAC does not require the loss function as an  input. This is because the current implementation is only applicable to  classification models with a cross entropy loss function. </p>  Using K-FAC <pre><code>from pydvl.influence.torch import EkfacInfluence\nif_model = EkfacInfluence(\n    model,\n    hessian_regularization=0.0,\n)\nif_model.fit(train_loader)\n</code></pre> <p>Upon initialization, the K-FAC method will parse the model and extract which  layers require grad and which do not. Then it will only calculate the influence  scores for the layers that require grad. The current implementation of the  K-FAC method is only available for linear layers, and therefore if the model  contains non-linear layers that require gradient the K-FAC method will raise a  NotImplementedLayerRepresentationException.</p> <p>A further improvement of the K-FAC method is the Eigenvalue Corrected  K-FAC (EKFAC) method (George et al., 2018)<sup>8</sup>, which allows to further re-fit the  eigenvalues of the Hessian, thus providing a more accurate approximation.  On top of the K-FAC method, the EKFAC method is implemented by setting  <code>update_diagonal=True</code> when initialising EkfacInfluence . </p> Using EKFAC <pre><code>from pydvl.influence.torch import EkfacInfluence\nif_model = EkfacInfluence(\n    model,\n    update_diagonal=True,\n    hessian_regularization=0.0,\n)\nif_model.fit(train_loader)\n</code></pre>"},{"location":"influence/influence_function_model/#nystrom-sketch-and-solve","title":"Nystr\u00f6m Sketch-and-Solve","text":"<p>This approximation is based on a Nystr\u00f6m low-rank approximation of the form</p> \\[\\begin{align*} H_{\\text{nys}} &amp;= (H\\Omega)(\\Omega^TH\\Omega)^{\\dagger}(H\\Omega)^T \\\\\\ &amp;= U \\Lambda U^T, \\end{align*}\\] <p>where \\((\\cdot)^{\\dagger}\\) denotes the Moore-Penrose inverse, in combination with the Sherman\u2013Morrison\u2013Woodbury formula to calculate the action of its inverse:</p> \\[\\begin{equation*}  (H_{\\text{nys}} + \\lambda I)^{-1}x = U(\\Lambda+\\lambda I)U^Tx + \\frac{1}{\\lambda}(I\u2212UU^T)x, \\end{equation*}\\] <p>see also (Hataya and Yamada, 2023)<sup>9</sup> and (Frangella et al., 2023)<sup>4</sup>. The essential  parameter is the rank of the approximation.</p> Using Nystr\u00f6m Sketch-and-Solve <pre><code>from pydvl.influence.torch import NystroemSketchInfluence, BlockMode, SecondOrderMode\nif_model = NystroemSketchInfluence(\n    model,\n    loss,\n    rank=10,\n    regularization=0.0,\n    block_structure=BlockMode.FULL,\n    second_order_mode=SecondOrderMode.HESSIAN\n)\nif_model.fit(train_loader)\n</code></pre> <p>This implementation is capable of using a block-matrix approximation, see Block-diagonal approximation, and can handle Gauss-Newton approximation.</p>"},{"location":"influence/influence_function_model/#inverse-harmonic-mean","title":"Inverse Harmonic Mean","text":"<p>This implementation replaces the inverse Hessian matrix in the influence computation with an approximation of the inverse Gauss-Newton vector product and was proposed in (Kwon et al., 2023)<sup>10</sup>.</p> <p>The approximation method comprises the following steps:</p> <ol> <li> <p>Replace the Hessian \\(H(\\theta)\\) with the Gauss-Newton matrix      \\(G(\\theta)\\):</p> \\[\\begin{equation*}     G(\\theta)=n^{-1} \\sum_{i=1}^n \\nabla_{\\theta}\\ell_i\\nabla_{\\theta}\\ell_i^T \\end{equation*}\\] <p>which results in</p> \\[\\begin{equation*}     \\mathcal{I}(z_{t}, z) \\approx \\nabla_{\\theta} \\ell(z_{t}, \\theta)^T                       (G(\\theta) + \\lambda I_d)^{-1}                       \\nabla_{\\theta} \\ell(z, \\theta)  \\end{equation*}\\] </li> <li> <p>Simplify the problem by breaking it down into a block diagonal structure,      where each block \\(G_l(\\theta)\\) corresponds to the l-th block:   </p> \\[\\begin{equation*}     G_{l}(\\theta) = n^{-1} \\sum_{i=1}^n \\nabla_{\\theta_l} \\ell_i                     \\nabla_{\\theta_l} \\ell_i^{T} + \\lambda_l I_{d_l}, \\end{equation*}\\] <p>which leads to</p> \\[\\begin{equation*}    \\mathcal{I}(z_{t}, z) \\approx \\nabla_{\\theta} \\ell(z_{t}, \\theta)^T                                   \\operatorname{diag}(G_1(\\theta)^{-1},                                   \\dots, G_L(\\theta)^{-1})                                   \\nabla_{\\theta} \\ell(z, \\theta) \\end{equation*}\\] </li> <li> <p>Substitute the arithmetic mean of the rank-\\(1\\) updates in         \\(G_l(\\theta)\\), with the inverse harmonic mean \\(R_l(\\theta)\\) of the rank-1      updates:</p> \\[\\begin{align*}     G_l(\\theta)^{-1} &amp;= \\left(  n^{-1} \\sum_{i=1}^n \\nabla_{\\theta_l}                         \\ell(z_i, \\theta) \\nabla_{\\theta_l}                         \\ell(z_i, \\theta)^{T} +                         \\lambda_l I_{d_l}\\right)^{-1} \\\\\\     R_{l}(\\theta)&amp;= n^{-1} \\sum_{i=1}^n \\left( \\nabla_{\\theta_l}                     \\ell(z_i, \\theta) \\nabla_{\\theta_l} \\ell(z_i, \\theta)^{T}                     + \\lambda_l I_{d_l} \\right)^{-1} \\end{align*}\\] </li> <li> <p>Use the           Sherman\u2013Morrison formula          to get an explicit representation of the inverses in the definition of      \\(R_l(\\theta):\\)</p> \\[\\begin{align*}     R_l(\\theta) &amp;= n^{-1} \\sum_{i=1}^n \\left( \\nabla_{\\theta_l} \\ell_i     \\nabla_{\\theta_l} \\ell_i^{T}     + \\lambda_l I_{d_l}\\right)^{-1} \\\\\\     &amp;= n^{-1} \\sum_{i=1}^n \\lambda_l^{-1} \\left(I_{d_l}     - \\frac{\\nabla_{\\theta_l} \\ell_i \\nabla_{\\theta_l}     \\ell_i^{T}}{\\lambda_l     + \\\\|\\nabla_{\\theta_l} \\ell_i\\\\|_2^2}\\right)     , \\end{align*}\\] <p>which means application of \\(R_l(\\theta)\\) boils down to computing \\(n\\) rank-\\(1\\) updates.</p> </li> </ol> <p><pre><code>from pydvl.influence.torch import InverseHarmonicMeanInfluence, BlockMode\n\nif_model = InverseHarmonicMeanInfluence(\n    model,\n    loss,\n    regularization=1e-1,\n    block_structure=BlockMode.LAYER_WISE\n)\nif_model.fit(train_loader)\n</code></pre> This implementation is capable of using a block-matrix approximation, see Block-diagonal approximation.</p> <p>These implementations represent the calculation logic on in memory tensors.  To scale up to large collection of data, we map these influence function models  over these collections. For a detailed discussion see the documentation page Scaling Computation.</p> <ol> <li> <p>Trefethen, L.N., Bau, D., Iii, 1997. Numerical Linear Algebra. Society for Industrial and Applied Mathematics, Philadelphia, PA. https://doi.org/10.1137/1.9780898719574 \u21a9</p> </li> <li> <p>Ji, H., Li, Y., 2017. A breakdown-free block conjugate gradient method. Bit Numer Math 57, 379--403. https://doi.org/10.1007/s10543-016-0631-z \u21a9</p> </li> <li> <p>Bekas, C., Kokiopoulou, E., Saad, Y., 2007. An estimator for the diagonal of a matrix. Applied Numerical Mathematics, Numerical Algorithms, Parallelism and Applications (2) 57, 1214--1229. https://doi.org/10.1016/j.apnum.2007.01.003 \u21a9</p> </li> <li> <p>Frangella, Z., Tropp, J.A., Udell, M., 2023. Randomized Nystr\u00f6m Preconditioning. SIAM J. Matrix Anal. Appl. 44, 718--752. https://doi.org/10.1137/21M1466244 \u21a9\u21a9</p> </li> <li> <p>Agarwal, N., Bullins, B., Hazan, E., 2017. Second-Order Stochastic Optimization for Machine Learning in Linear Time. JMLR 18, 1--40.\u00a0\u21a9</p> </li> <li> <p>Schioppa, A., Zablotskaia, P., Vilar, D., Sokolov, A., 2022. Scaling Up Influence Functions. Proc. AAAI Conf. Artif. Intell. 36, 8179--8186. https://doi.org/10.1609/aaai.v36i8.20791 \u21a9</p> </li> <li> <p>Martens, J., Grosse, R., 2015. Optimizing Neural Networks with [Kronecker-factored Approximate Curvature]{.nocase}, in: Proceedings of the 32nd International Conference on Machine Learning. Presented at the International Conference on Machine Learning, PMLR, pp. 2408--2417.\u00a0\u21a9</p> </li> <li> <p>George, T., Laurent, C., Bouthillier, X., Ballas, N., Vincent, P., 2018. Fast Approximate Natural Gradient Descent in a Kronecker Factored Eigenbasis, in: Advances in Neural Information Processing Systems. Curran Associates, Inc.\u00a0\u21a9</p> </li> <li> <p>Hataya, R., Yamada, M., 2023. Nystr\u00f6m Method for Accurate and Scalable Implicit Differentiation, in: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics, PMLR, pp. 4643--4654.\u00a0\u21a9</p> </li> <li> <p>Kwon, Y., Wu, E., Wu, K., Zou, J., 2023. DataInf: Efficiently Estimating Data Influence in [LoRA-tuned LLMs]{.nocase} and Diffusion Models. Presented at the The Twelfth International Conference on Learning Representations. https://doi.org/10.48550/arXiv.2310.00902 \u21a9</p> </li> </ol>"},{"location":"influence/scaling_computation/","title":"Scaling Influence Computation","text":"<p>The implementations of InfluenceFunctionModel provide a convenient way to calculate influences for in memory tensors. </p> <p>Nevertheless, there is a need for computing the influences on batches of data. This might happen, if your input data does not fit into memory (e.g. it is very high-dimensional) or for large models the derivative computations exceed your memory or any combinations of these. For this scenario, we want to map our influence function model over collections of batches (or chunks) of data.</p>"},{"location":"influence/scaling_computation/#sequential","title":"Sequential","text":"<p>The simplest way is to use a double for-loop to iterate over the batches sequentially and collect them. pyDVL provides the simple convenience class SequentialInfluenceCalculator to do this. The batch size should be chosen as large as possible, such that the corresponding batches fit into memory.</p> <p><pre><code>from pydvl.influence import SequentialInfluenceCalculator\nfrom pydvl.influence.torch.util import (\n    TorchNumpyConverter, NestedTorchCatAggregator,\n)\nfrom pydvl.influence.torch import CgInfluence\n\nbatch_size = 10\ntrain_dataloader = DataLoader(..., batch_size=batch_size)\ntest_dataloader = DataLoader(..., batch_size=batch_size)\n\ninfl_model = CgInfluence(model, loss, hessian_regularization=0.01)\ninfl_model = infl_model.fit(train_dataloader)\n\ninfl_calc = SequentialInfluenceCalculator(infl_model)\n\n# this does not trigger the computation\nlazy_influences = infl_calc.influences(test_dataloader, train_dataloader)\n\n# trigger computation and pull the result into main memory, \n# result is the full tensor for all combinations of the two loaders\ninfluences = lazy_influences.compute(aggregator=NestedTorchCatAggregator())\n# or\n# trigger computation and write results chunk-wise to disk using zarr \n# in a sequential manner\nlazy_influences.to_zarr(\"local_path/or/url\", TorchNumpyConverter())\n</code></pre> When invoking the <code>compute</code> method, you have the option to specify a custom aggregator  by implementing NestedSequenceAggregator.  This allows for the aggregation of computed chunks.  Such an approach is particularly beneficial for straightforward aggregation tasks,  commonly seen in sequential computation models.  Examples include operations like concatenation, as implemented in  NestedTorchCatAggregator,  or basic min and max operations. </p> <p>For more intricate aggregations, such as an argmax operation,  it's advisable to use the  DaskInfluenceCalculator  (refer to Parallel for more details). This is because it returns data structures in the  form of dask.array.Array objects, which offer an API almost fully  compatible with NumPy arrays.</p>"},{"location":"influence/scaling_computation/#parallel","title":"Parallel","text":"<p>While the sequential calculation helps in the case the resulting tensors are too large to fit into memory,  the batches are computed one after another. Because the influence computation itself is completely data parallel, you may want to use a parallel processing framework. </p> <p>pyDVL provides an implementation of a parallel computation model using dask. The wrapper class DaskInfluenceCalculator has convenience methods to map the influence function computation over chunks of data in a parallel manner.</p> <p>Again, choosing an appropriate chunk size can be crucial. For a better understanding see the official  dask best practice documentation and the following blog entry.</p> <p>Warning</p> <p>Make sure to set <code>threads_per_worker=1</code>, when using the distributed scheduler for computing, if your implementation of InfluenceFunctionModel is not thread-safe. <pre><code>client = Client(threads_per_worker=1)\n</code></pre> For details on dask schedulers see the official documentation.</p> <p><pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom pydvl.influence import DaskInfluenceCalculator\nfrom pydvl.influence.torch import CgInfluence\nfrom pydvl.influence.torch.util import (\n    torch_dataset_to_dask_array,\n    TorchNumpyConverter,\n)\nfrom distributed import Client\n\ntrain_data_set: Dataset = LargeDataSet(\n    ...)  # Possible some out of memory large Dataset\ntest_data_set: Dataset = LargeDataSet(\n    ...)  # Possible some out of memory large Dataset\n\ntrain_dataloader = DataLoader(train_data_set)\ninfl_model = CgInfluence(model, loss, hessian_regularization=0.01)\ninfl_model = infl_model.fit(train_dataloader)\n\n# wrap your input data into dask arrays\nchunk_size = 10\nda_x, da_y = torch_dataset_to_dask_array(train_data_set, chunk_size=chunk_size)\nda_x_test, da_y_test = torch_dataset_to_dask_array(test_data_set,\n                                                   chunk_size=chunk_size)\n\n# use only one thread for scheduling, \n# due to non-thread safety of some torch operations\nclient = Client(n_workers=4, threads_per_worker=1)\n\ninfl_calc = DaskInfluenceCalculator(infl_model, \n                                  converter=TorchNumpyConverter(\n                                      device=torch.device(\"cpu\")\n                                  ),\n                                  client=client)\nda_influences = infl_calc.influences(da_x_test, da_y_test, da_x, da_y)\n# da_influences is a dask.array.Array\n# trigger computation and write chunks to disk in parallel\nda_influences.to_zarr(\"path/or/url\")\n</code></pre> During initialization of the  DaskInfluenceCalculator,  the system verifies if all workers are operating in single-threaded mode when the provided influence_function_model is designated as not thread-safe (indicated by the <code>is_thread_safe</code> property). If this condition is not met, the initialization will raise a specific error, signaling a potential thread-safety conflict.</p> <p>To intentionally skip this safety check (e.g., for debugging purposes using the single machine synchronous scheduler), you can supply the DisableClientSingleThreadCheck type.</p> <pre><code>from pydvl.influence import DisableClientSingleThreadCheck\n\ninfl_calc = DaskInfluenceCalculator(infl_model,\n                                    TorchNumpyConverter(device=torch.device(\"cpu\")),\n                                    DisableClientSingleThreadCheck)\nda_influences = infl_calc.influences(da_x_test, da_y_test, da_x, da_y)\nda_influences.compute(scheduler=\"synchronous\")\n</code></pre>"},{"location":"value/","title":"Data valuation","text":"<p>Info</p> <p>If you want to jump right into it, skip ahead to Computing data values. If you want a quick list of applications, see Applications of data valuation. For a list of all algorithms implemented in pyDVL, see Methods.</p> <p>Data valuation is the task of assigning a number to each element of a training set which reflects its contribution to the final performance of some model trained on it. Some methods attempt to be model-agnostic, but in most cases the model is an integral part of the method. In these cases, this number is not an intrinsic property of the element of interest, but typically a function of three factors:</p> <ol> <li> <p>The dataset \\(D\\), or more generally, the distribution it was sampled from: In    some cases one only cares about values wrt. a given data set, in others    value would ideally be the (expected) contribution of a data point to any    random set \\(D\\) sampled from the same distribution. pyDVL implements methods    of the first kind.</p> </li> <li> <p>The algorithm \\(\\mathcal{A}\\) mapping the data \\(D\\) to some estimator \\(f\\) in a    model class \\(\\mathcal{F}\\). E.g. MSE minimization to find the parameters of a    linear model.</p> </li> <li> <p>The performance metric of interest \\(u\\) for the problem. When value depends on    a model, it must be measured in some way which uses it. E.g. the \\(R^2\\) score    or the negative MSE over a test set. This metric will be computed over a    held-out valuation set.</p> </li> </ol> <p>pyDVL collects algorithms for the computation of data values in this sense, mostly those derived from cooperative game theory. The methods can be found in the package pydvl.valuation.methods, with support from modules like pydvl.valuation.samplers or and pydvl.valuation.dataset, as detailed below.</p> <p>Warning</p> <p>Be sure to read the section on the difficulties using data values.</p> <p>There are three main families of methods for data valuation: model-based, influence-based and model-free. As of v0.10.0 pyDVL supports the first two. Here, we focus on model-based (and in particular game-theoretic) concepts and refer to the main documentation on the influence function for the second.</p>"},{"location":"value/#game-theoretical-methods","title":"Game theoretical methods and semi-values","text":"<p>The main contenders in game-theoretic approaches are Shapley values (Ghorbani and Zou, 2019)<sup>1</sup>, (Kwon et al., 2021)<sup>2</sup>, (Schoch et al., 2022)<sup>3</sup>, their generalization to so-called semi-values with some examples being (Kwon and Zou, 2022)<sup>4</sup> and (Wang and Jia, 2023)<sup>5</sup>, and the Core (Yan and Procaccia, 2021)<sup>6</sup>. All of these are implemented in pyDVL. For a full list see Methods.</p> <p>In these methods, data points are considered players in a cooperative game  whose outcome is the performance of the model when trained on subsets  (coalitions) of the data, measured on a held-out valuation set. This  outcome, or utility, must typically be computed for every subset of  the training set, so that an exact computation is \\(\\mathcal{O} (2^n)\\) in the  number of samples \\(n\\), with each iteration requiring a full re-fitting of the  model using a coalition as training set. Consequently, most methods involve  Monte Carlo approximations, and sometimes approximate utilities which are  faster to compute, e.g. proxy models (Wang et al., 2022)<sup>7</sup> or constant-cost approximations like Neural Tangent Kernels (Wu et al., 2022)<sup>8</sup>.</p> <p>Info</p> <p>Here is the full list of valuation methods implemented in  pyDVL.</p> <p>The reasoning behind using game theory is that, in order to be useful, an assignment of value, dubbed valuation function, is usually required to fulfil certain requirements of consistency and \"fairness\". For instance, in some applications value should not depend on the order in which data are considered, or it should be equal for samples that contribute equally to any subset of the data (of equal size). When considering aggregated value for (sub-)sets of data there are additional desiderata, like having a value function that does not increase with repeated samples. Game-theoretic methods are all rooted in axioms that by construction ensure different desiderata, but despite their practical usefulness, none of them are either necessary or sufficient for all applications. For instance, SV methods try to equitably distribute all value among all samples, failing to identify repeated ones as unnecessary, with e.g. a zero value.</p>"},{"location":"value/#computing-data-values","title":"Computing data values","text":"<p>Using pyDVL to compute data values is a flexible process that can be broken down into several steps. This degree of flexibility allows for a wide range of applications, but it can also be a bit overwhelming. The following steps are necessary:</p> <ol> <li>Creating two Datasets object from your data: one    for training and one for evaluating the utility. The quality of this latter set    is crucial for the quality of the values.</li> <li>Choosing a scoring function,typically something like accuracy or \\(R^2\\), but    it can be any function that takes a model and a dataset and returns a number.    The test dataset is attached to the scorer. This is done by instantiating a    SupervisedScorer object. Other    types, and subclassing is possible for non-supervised problems.</li> <li>Creating a utility object that ties your model to the scoring function. This    is done by instantiating a ModelUtility.</li> <li>Computing values with a valuation method of your choice, e.g. via    BanzhafValuation    or ShapleyValuation. For    semi-value methods, you will also need to choose a subset    sampling scheme, e.g. a    PermutationSampler    or a simple UniformSampler.</li> <li>For methods that require it, in particular those using infinite subset    sampling schemes, one must choose a stopping criterion, that is a    stopping condition that interrupts the    computation e.g. when the change in estimates is low, or the number of    iterations or time elapsed exceed some threshold.</li> </ol>"},{"location":"value/#creating-a-dataset","title":"Creating a Dataset","text":"<p>The first item in the tuple \\((D, \\mathcal{A}, u)\\) characterising data value is the dataset. The class Dataset is a simple convenience wrapper for use across the library. Some class methods allow for the convenient creation of train / test splits, e.g. with from_arrays:</p> Constructing a synthetic classification dataset <pre><code>from pydvl.valuation.dataset import Dataset\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=20, n_classes=3)\ntrain, test = Dataset.from_arrays(X, y, stratify_by_target=True)\n</code></pre> <p>With the class method from_sklearn it is possible to construct a dataset from any of the toy datasets in sklearn.datasets. </p> Loading a scikit-learn dataset <pre><code>from pydvl.valuation.dataset import Dataset\nfrom sklearn.datasets import load_iris\ntrain, test = Dataset.from_sklearn(\n    load_iris(), train_size=0.8, stratify_by_target=True\n)\n</code></pre>"},{"location":"value/#grouping-data","title":"Grouping data","text":"<p>Be it because data valuation methods are computationally very expensive, or because we are interested in the groups themselves, it can be often useful or necessary to group samples to valuate them together. GroupedDataset provides an alternative to Dataset with the same interface which allows this.</p> <p>You can see an example in action in the Spotify notebook, but here's a simple example grouping a pre-existing <code>Dataset</code>. First we construct an array mapping each index in the dataset to a group, then use from_dataset:</p> Grouping a dataset <p>This is of course silly, but it serves to illustrate the functionality:</p> <pre><code>import numpy as np\nfrom pydvl.valuation.dataset import Dataset, GroupedDataset\n\ndataset = Dataset.from_sklearn(sk.datasets.fetch_covtype())\nn_groups = 5800\n# Randomly assign elements to any one of n_groups:\ndata_groups = np.random.randint(0, n_groups, len(dataset))\ndummy_group_names = [f\"Group {i}\" for i in range(n_groups)]\ngrouped_dataset = GroupedDataset.from_dataset(\n  dataset, data_groups, dummy_group_names\n)\n</code></pre>"},{"location":"value/#creating-a-utility","title":"Creating a utility","text":"<p>In pyDVL we have slightly overloaded the name \"utility\" and use it to refer to an object that keeps track of both the method and its evaluation. For model-based methods like all semi-values including Shapley, the utility will be an instance of ModelUtility which, as mentioned, is a convenient wrapper for the model and scoring function.</p> Creating a <code>ModelUtility</code> <pre><code>import sklearn as sk\nfrom pydvl.valuation import Dataset, ModelUtility, SupervisedScorer\n\ntrain, test = Dataset.from_sklearn(sk.datasets.load_iris(), train_size=0.6)\nmodel = sk.svm.SVC()\n# Uses the model.score() method by default\nscorer = SupervisedScorer(model, test)\nutility = ModelUtility(model, scorer)\n</code></pre> <p>Note how we pass the test set to the scorer. Importantly, the object provides information about the range of the score, which is used by some methods to estimate the number of samples necessary, and about what default value to use when the model fails to train.</p> <p>If we pass a model to SupervisedScorer, it will use the model's <code>score()</code> method by default, but it is possible to use any scoring function (greater values must be better). In particular, the constructor accepts the same types of arguments as those of sklearn.model_selection.cross_validate: a string, a scorer callable or None for the default.</p> <pre><code>scorer = SupervisedScorer(\"explained_variance\", default=0.0, range=(-np.inf, 1))\n</code></pre> <p>The object <code>utility</code> is a callable and is used by data valuation methods to train the model on various subsets of the data and evaluate its performance.  <code>ModelUtility</code> wraps the <code>fit()</code> method of the model to cache its results. In some (rare) cases this reduces computation times of Monte Carlo methods. Because of how caching is implemented, it is important not to reuse <code>ModelUtility</code> objects for different datasets. You can read more about setting up the cache in the installation guide, and in the documentation of the caching module.</p> <p>Errors are hidden by default</p> <p>During semi-value computations, the utility can be evaluated on subsets that break the fitting process. For instance, a classifier might require at least two classes to fit, but the utility is sometimes evaluated on subsets with only one class. This will raise an error with most classifiers. To avoid this, we set by default <code>catch_errors=True</code> upon instantiation, which will catch the error and return the scorer's default value instead. While we show a warning to signal that something went wrong, this suppression can lead to unexpected results, so it is important to be aware of this setting and to set it to <code>False</code> when testing, or if you are sure that the utility will not be evaluated on problematic subsets.</p>"},{"location":"value/#computing-some-values","title":"Computing some values","text":"<p>By far the most popular concept of value is the Shapley value, a particular case of semi-value. In order to compute them for a training set, all we need to do after the previous steps is to instantiate a ShapleyValuation object and call its <code>fit()</code> method.</p> Computing Shapley values <pre><code>import sklearn as sk\nfrom joblib import parallel_config\nfrom pydvl.valuation import Dataset, ModelUtility, ShapleyValuation, SupervisedScorer\n\ntrain, test = Dataset.from_sklearn(sk.datasets.load_iris(), train_size=0.6)\nmodel = sk.svm.SVC()\nscorer = SupervisedScorer(\"accuracy\", test, default=0.0, range=(0, 1))\nutility = ModelUtility(model, scorer)\nsampler = PermutationSampler()  # Just one of many examples\nstopping = MaxUpdates(100)  # A trivial criterion\nshapley = ShapleyValuation(utility, sampler, stopping)\nwith parallel_config(n_jobs=-1):\n    shapley.fit(train)\n\nresults = shapley.values()\n</code></pre> <p>Note our use of joblib.parallel_config in the example in order to parallelize the computation of the values. Most valuation methods support this.</p> <p>The result type of all valuations is an object of type ValuationResult. This can be iterated over, sliced, sorted, as well as converted to a pandas.DataFrame using to_dataframe.</p>"},{"location":"value/#learning-the-utility","title":"Learning the utility","text":"<p>Since each evaluation of the utility entails a full retraining of the model on a new subset of the training data, it is natural to try to learn this mapping from subsets to scores. This is the idea behind Data Utility Learning (DUL) (Wang et al., 2022)<sup>7</sup> and in pyDVL it's as simple as wrapping the ModelUtility inside a DataUtilityLearning object:</p> <pre><code>from pydvl.valuation import *\nfrom pydvl.valuation.types import Sample\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.datasets import load_iris\n\ntrain, test = Dataset.from_sklearn(load_iris())\nscorer = SupervisedScorer(\"accuracy\", test, default=0.0, range=(0, 1))\nu = ModelUtility(LogisticRegression(), scorer)\ntraining_budget = 3\nutility_model = IndicatorUtilityModel(\n    predictor=LinearRegression(), n_data=len(train)\n)\nwrapped_u = DataUtilityLearning(u, training_budget, utility_model)\n\n# First 3 calls will be computed normally\nfor i in range(training_budget):\n    _ = wrapped_u(Sample(None, train.indices[:i]))\n# Subsequent calls will be computed using the learned model for DUL\nwrapped_u(Sample(None, train.indices))\n</code></pre>"},{"location":"value/#problems-of-data-values","title":"Problems of data values","text":"<p>There are a number of factors that affect how useful values can be for your project. In particular, regression can be especially tricky, but the particular nature of every (non-trivial) ML problem can have an effect:</p> <ul> <li> <p>Variance of the utility: Classical applications of game theoretic value   concepts operate with deterministic utilities, as do many of the bounds in the   literature. But in ML we use an evaluation of the model on a validation set as a   proxy for the true risk. Even if the utility is bounded, its variance will   affect final values, and even more so any Monte Carlo estimates.   Several works have tried to cope with variance. (Wang and Jia, 2023)<sup>5</sup> prove that by   relaxing one of the Shapley axioms and considering the general class of   semi-values, of which Shapley is an instance, one can prove that a choice of   constant weights is the best one can do in a utility-agnostic setting. This   method, dubbed Data Banzhaf, is available in pyDVL as   BanzhafValuation.</p> Averaging repeated utility evaluations <p>One workaround in pyDVL is to configure the caching system to allow multiple evaluations of the utility for every index set. A moving average is  computed and returned once the standard error is small, see CachedFuncConfig. Note however that in practice, the likelihood of cache hits is low, so one would have to force recomputation manually somehow.</p> </li> <li> <p>Unbounded utility: Choosing a scorer for a classifier is simple: accuracy   or some F-score provides a bounded number with a clear interpretation. However,   in regression problems most scores, like \\(R^2\\), are not bounded because   regressors can be arbitrarily bad. This leads to great variability in the   utility for low sample sizes, and hence unreliable Monte Carlo approximations   to the values. Nevertheless, in practice it is only the ranking of samples   that matters, and this tends to be accurate (wrt. to the true ranking) despite   inaccurate values.</p> Squashing scores <p>pyDVL offers a dedicated function composition for scorer functions which can be used to squash a score. The following is defined in the module scorers: <pre><code>import numpy as np\nfrom pydvl.valuation.score import compose_score\n\ndef sigmoid(x: float) -&gt; float:\n  return float(1 / (1 + np.exp(-x)))\n\nsquashed_r2 = compose_score(\"r2\", sigmoid, \"squashed r2\")\n\nsquashed_variance = compose_score(\n  \"explained_variance\", sigmoid, \"squashed explained variance\"\n)\n</code></pre> These squashed scores can prove useful in regression problems, but they can also introduce issues in the low-value regime.</p> </li> <li> <p>Data set size: Computing exact Shapley values is NP-hard, and Monte Carlo   approximations can converge slowly. Massive datasets are thus impractical, at   least with game-theoretical methods. A workaround is to group samples and investigate their value together. You can do this using   GroupedDataset. There is a fully   worked-out example here. Some algorithms   also provide different sampling strategies to reduce the variance, but due to a   no-free-lunch-type theorem, no single strategy can be optimal for all utilities.   Finally, model specific methods like   kNN-Shapley (Jia et al., 2019)<sup>9</sup>, or   altogether different and typically faster approaches like   Data-OOB (Kwon and Zou, 2023)<sup>10</sup> can also be   used. </p> </li> <li> <p>Model size: Since every evaluation of the utility entails retraining the   whole model on a subset of the data, large models require great amounts of   computation. But also, they will effortlessly interpolate small to medium   datasets, leading to great variance in the evaluation of performance on the   dedicated validation set. One mitigation for this problem is cross-validation,   but this would incur massive computational cost. As of v0.8.1 there are no   facilities in pyDVL for cross-validating the utility (note that this would   require cross-validating the whole value computation).</p> </li> </ul> <ol> <li> <p>Ghorbani, A., Zou, J., 2019. Data Shapley: Equitable Valuation of Data for Machine Learning, in: Proceedings of the 36th International Conference on Machine Learning, PMLR. Presented at the International Conference on Machine Learning (ICML 2019), PMLR, pp. 2242--2251.\u00a0\u21a9</p> </li> <li> <p>Kwon, Y., Rivas, M.A., Zou, J., 2021. Efficient Computation and Analysis of Distributional Shapley Values, in: Proceedings of the 24th International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics, PMLR, pp. 793--801.\u00a0\u21a9</p> </li> <li> <p>Schoch, S., Xu, H., Ji, Y., 2022. CS-Shapley: [Class-wise Shapley Values]{.nocase} for Data Valuation in Classification, in: Proc. Of the Thirty-Sixth Conference on Neural Information Processing Systems (NeurIPS). Presented at the Advances in Neural Information Processing Systems (NeurIPS 2022), New Orleans, Louisiana, USA.\u00a0\u21a9</p> </li> <li> <p>Kwon, Y., Zou, J., 2022. Beta Shapley: A Unified and [Noise-reduced Data Valuation Framework]{.nocase} for Machine Learning, in: Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022,. Presented at the AISTATS 2022, PMLR, Valencia, Spain.\u00a0\u21a9</p> </li> <li> <p>Wang, J.T., Jia, R., 2023. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning, in: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics, PMLR, pp. 6388--6421.\u00a0\u21a9\u21a9</p> </li> <li> <p>Yan, T., Procaccia, A.D., 2021. If You Like Shapley Then You'll Love the Core, in: Proceedings of the 35th AAAI Conference on Artificial Intelligence. Presented at the AAAI Conference on Artificial Intelligence, Association for the Advancement of Artificial Intelligence, Virtual conference, pp. 5751--5759. https://doi.org/10.1609/aaai.v35i6.16721 \u21a9</p> </li> <li> <p>Wang, T., Yang, Y., Jia, R., 2022. Improving [Cooperative Game Theory-based Data Valuation]{.nocase} via Data Utility Learning. Presented at the International Conference on Learning Representations (ICLR 2022). Workshop on Socially Responsible Machine Learning, arXiv. https://doi.org/10.48550/arXiv.2107.06336 \u21a9\u21a9</p> </li> <li> <p>Wu, Z., Shu, Y., Low, B.K.H., 2022. DAVINZ: Data Valuation using Deep Neural Networks at Initialization, in: Proceedings of the 39th International Conference on Machine Learning. Presented at the International Conference on Machine Learning, PMLR, pp. 24150--24176.\u00a0\u21a9</p> </li> <li> <p>Jia, R., Dao, D., Wang, B., Hubis, F.A., Gurel, N.M., Li, B., Zhang, C., Spanos, C., Song, D., 2019. Efficient task-specific data valuation for nearest neighbor algorithms. Proc. VLDB Endow. 12, 1610--1623. https://doi.org/10.14778/3342263.3342637 \u21a9</p> </li> <li> <p>Kwon, Y., Zou, J., 2023. Data-OOB: [Out-of-bag Estimate]{.nocase} as a Simple and Efficient Data Value, in: Proceedings of the 40th International Conference on Machine Learning. Presented at the International Conference on Machine Learning, PMLR, pp. 18135--18152.\u00a0\u21a9</p> </li> </ol>"},{"location":"value/beta-shapley/","title":"Beta Shapley","text":"<p>In ML applications, where the utility is the performance when trained on a set \\(S \\subset D\\), diminishing returns are often observed when computing the marginal utility of adding a new data point.<sup>1</sup></p> <p>Beta Shapley is a weighting scheme that uses the Beta function to place more weight on subsets deemed to be more informative. The weights are defined as:</p> \\[ w(k) := \\frac{B(k+\\beta, n-k+1+\\alpha)}{B(\\alpha, \\beta)}, \\] <p>where \\(B\\) is the Beta function, and \\(\\alpha\\) and \\(\\beta\\) are parameters that control the weighting of the subsets. Setting both to 1 recovers Shapley values, and setting \\(\\alpha = 1\\), and \\(\\beta = 16\\) is reported in (Kwon and Zou, 2022)<sup>2</sup> to be a good choice for some applications. Beta Shapley values are available in pyDVL through BetaShapleyValuation:</p> Beta Shapley values <pre><code>from joblib import parallel_config\nfrom pydvl.valuation import *\n\nmodel = ...\ntrain, test = Dataset.from_arrays(...)\nscorer = SupervisedScorer(model, test, default=0.0)\nutility = ModelUtility(model, scorer)\nsampler = PermutationSampler()\nstopping = RankCorrelation(rtol=1e-5, burn_in=100) | MaxUpdates(2000)\nvaluation = BetaShapleyValuation(\n    utility, sampler, stopping, alpha=1, beta=16\n)\nwith parallel_config(n_jobs=16):\n    valuation.fit(train)\n</code></pre> <p>See, however Banzhaf indices, for an alternative choice of weights which is reported to work better in cases of high variance in the utility function.</p> <ol> <li> <p>This observation is made somewhat formal for some  model classes in (Watson et al., 2023)<sup>3</sup>, motivating a complete truncation of the sampling space, see \\(\\delta\\)-Shapley.\u00a0\u21a9</p> </li> <li> <p>Kwon, Y., Zou, J., 2022. Beta Shapley: A Unified and [Noise-reduced Data Valuation Framework]{.nocase} for Machine Learning, in: Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022,. Presented at the AISTATS 2022, PMLR, Valencia, Spain.\u00a0\u21a9</p> </li> <li> <p>Watson, L., Kujawa, Z., Andreeva, R., Yang, H.-T., Elahi, T., Sarkar, R., 2023. Accelerated Shapley Value Approximation for Data Evaluation [WWW Document]. https://doi.org/10.48550/arXiv.2311.05346 \u21a9</p> </li> </ol>"},{"location":"value/classwise-shapley/","title":"Class-wise Shapley","text":"<p>Class-wise Shapley (CWS) (Schoch et al., 2022)<sup>1</sup> offers a Shapley framework tailored for classification problems.  Given a sample \\(x_i\\) with label \\(y_i \\in \\mathbb{N}\\), let \\(D_{y_i}\\) be the subset of \\(D\\) with labels \\(y_i\\), and \\(D_{-y_i}\\) be the complement of \\(D_{y_i}\\) in \\(D\\). The key idea is that the sample \\((x_i, y_i)\\) might improve the overall model performance on \\(D\\), while being detrimental for the performance on \\(D_{y_i},\\) e.g. because of a wrong label. To address this issue, the authors introduced</p> \\[ v_u(i) = \\frac{1}{2^{|D_{-y_i}|}} \\sum_{S_{-y_i}} \\frac{1}{|D_{y_i}|}\\sum_{S_{y_i}} \\binom{|D_{y_i}|-1}{|S_{y_i}|}^{-1} \\delta(S_{y_i} | S_{-y_i}), \\] <p>where \\(S_{y_i} \\subseteq D_{y_i} \\setminus \\{i\\}\\) and \\(S_{-y_i} \\subseteq D_{-y_i}\\) is arbitrary (in particular, not the complement of \\(S_{y_i}\\)). The function \\(\\delta\\) is called set-conditional marginal Shapley value and is defined as</p> \\[ \\delta(S | C) = u( S_{+i} | C ) \u2212 u(S | C), \\] <p>for any set \\(S\\) such that \\(i \\notin S, C\\) and \\(S \\cap C = \\emptyset\\).</p> <p>In practical applications, estimating this quantity is done both with Monte Carlo sampling of the powerset, and the set of index permutations (Castro et al., 2009)<sup>2</sup>. Typically, this requires fewer samples than the original Shapley value, although the actual speed-up depends on the model and the dataset.</p> Computing classwise Shapley values <p>CWS is implemented in ClasswiseShapleyValuation. To construct this object the model is passed inside a ClasswiseModelUtility together with a ClasswiseSupervisedScorer The two samplers required by the method are wrapped by a ClasswiseSampler.</p> <p>The following example illustrates how to replicate the algorithm in Appendix A of (Schoch et al., 2022)<sup>1</sup>. <pre><code>from pydvl.valuation import *\n\nseed = 42\nmodel = ...\ntrain, test = Dataset.from_arrays(X, y, train_size=0.6, random_state=seed)\nn_labels = len(get_unique_labels(train.data().y))\nscorer = ClasswiseSupervisedScorer(\"accuracy\", test)\nutility = ClasswiseModelUtility(model, scorer)\nsampler = ClasswiseSampler(\n    in_class=PermutationSampler(\n        truncation=RelativeTruncation(rtol=0.01, burn_in_fraction=0.3), seed=seed\n    ),\n    out_of_class=UniformSampler(index_iteration=NoIndexIteration),\n    max_in_class_samples=1,\n)\n# 500 permutations per label as in the paper\nstopping = MaxSamples(sampler, 500*n_labels)\n# Save the history in valuation.stopping.criteria[1]\nstopping |= History(n_steps=5000),\nvaluation = ClasswiseShapleyValuation(\n    utility=utility, sampler=sampler, is_done=stopping, normalize_values=True\n)\n</code></pre></p>"},{"location":"value/classwise-shapley/#the-class-wise-scorer","title":"The class-wise scorer","text":"<p>In order to use the class-wise Shapley value, one needs to instantiate a ClasswiseSupervisedScorer. This scorer is defined as</p> \\[ u(S) = f(a_S(D_{y_i})) \\ g(a_S(D_{-y_i})), \\] <p>where \\(f\\) and \\(g\\) are monotonically increasing functions, \\(a_S(D_{y_i})\\) is the in-class accuracy, and \\(a_S(D_{-y_i})\\) is the out-of-class accuracy (the names originate from a choice by the authors to use accuracy, but in principle any other score, like \\(F_1\\) can be used). </p> <p>The authors show that \\(f(x)=x\\) and \\(g(x)=e^x\\) have favorable properties and are therefore the defaults, but we leave the option to set different functions \\(f\\) and \\(g\\) for an exploration with different base scores. </p> The default class-wise scorer <p>The CWS scorer requires choosing a metric and the functions \\(f\\) and \\(g,\\) which by default are set to the values in the paper:</p> <pre><code>import numpy as np\nfrom pydvl.valuation.scorers.classwise import ClasswiseSupervisedScorer\n\n_, test = Dataset.from_sklearn(...)\nidentity = lambda x: x\nscorer = ClasswiseSupervisedScorer(\n    \"accuracy\",\n    default=0.0,\n    range=(0.0, 1.0),\n    test_data=test,\n    in_class_discount_fn=identity,\n    out_of_class_discount_fn=np.exp\n)\n</code></pre> Surface of the discounted utility function <p>The level curves for \\(f(x)=x\\) and \\(g(x)=e^x\\) are depicted below. The lines illustrate the contour lines, annotated with their respective gradients. Level curves of the class-wise utility </p>"},{"location":"value/classwise-shapley/#evaluation","title":"Evaluation","text":"<p>We illustrate the method with two experiments: point removal and noise removal, as well as an analysis of the distribution of the values. For this we employ the nine datasets used in (Schoch et al., 2022)<sup>1</sup>, using the same pre-processing. For images, PCA is used to reduce down to 32 the features found by a pre-trained <code>Resnet18</code> model. Standard loc-scale normalization is performed for all models except gradient boosting, since the latter is not sensitive to the scale of the features.</p> Datasets used for evaluation Dataset Data Type Classes Input Dims OpenML ID Diabetes Tabular 2 8 37 Click Tabular 2 11 1216 CPU Tabular 2 21 197 Covertype Tabular 7 54 1596 Phoneme Tabular 2 5 1489 FMNIST Image 2 32 40996 CIFAR10 Image 2 32 40927 MNIST (binary) Image 2 32 554 MNIST (multi) Image 10 32 554 <p>We show mean and coefficient of variation (CV) \\(\\frac{\\sigma}{\\mu}\\) of an \"inner metric\". The former shows the performance of the method, whereas the latter displays its stability: we normalize by the mean to see the relative effect of the standard deviation. Ideally the mean value is maximal and CV minimal. </p> <p>Finally, we note that for all sampling-based valuation methods the same number of evaluations of the marginal utility was used. This is important to make the algorithms comparable, but in practice one should consider using a more sophisticated stopping criterion.</p>"},{"location":"value/classwise-shapley/#dataset-pruning-for-logistic-regression-point-removal","title":"Dataset pruning for logistic regression (point removal)","text":"<p>In (best-)point removal, one first computes values for the training set and then removes in sequence the points with the highest values. After each removal, the remaining points are used to train the model from scratch and performance is measured on a test set. This produces a curve of performance vs. number of points removed which we show below.</p> <p>As a scalar summary of this curve, (Schoch et al., 2022)<sup>1</sup> define Weighted Accuracy Drop (WAD) as:</p> \\[ \\begin{aligned} \\text{wad} &amp;= \\sum_{j=1}^{n} \\frac{1}{j} \\sum_{i=1}^{j}   \\left( a_{T_{-\\{1 : i-1 \\}}}(D) - a_{T_{-\\{1 : i \\}}}(D) \\right) \\\\    &amp;= a_T(D) - \\sum_{j=1}^{n} \\frac{a_{T_{-\\{1 : j \\}}}(D)}{j} , \\end{aligned} \\] <p>where \\(a_T(D)\\) is the accuracy of the model (trained on \\(T\\)) evaluated on \\(D\\) and \\(T_{-\\{1 : j \\}}\\) is the set \\(T\\) without elements from \\(\\{1, \\dots , j \\}\\).</p> <p>We run the point removal experiment for a logistic regression model five times and compute WAD for each run, then report the mean \\(\\mu_\\text{wad}\\) and standard deviation \\(\\sigma_\\text{wad}\\).</p> <p> Mean WAD for best-point removal on logistic regression. Values computed using LOO, CWS, Beta Shapley, and TMCS </p> <p>We see that CWS is competitive with all three other methods. In all problems except <code>MNIST (multi)</code> it outperforms TMCS, while in that case TMCS has a slight advantage.</p> <p>In order to understand the variability of WAD we look at its coefficient of variation (lower is better):</p> <p> Coefficient of Variation of WAD for best-point removal on logistic regression. Values computed using LOO, CWS, Beta Shapley, and TMCS </p> <p>CWS is not the best method in terms of CV. For <code>CIFAR10</code>, <code>Click</code>, <code>CPU</code> and <code>MNIST (binary)</code> Beta Shapley has the lowest CV. For <code>Diabetes</code>, <code>MNIST (multi)</code> and <code>Phoneme</code> CWS is the winner and for <code>FMNIST</code> and <code>Covertype</code> TMCS takes the lead. Besides LOO, TMCS has the highest relative standard deviation.</p> <p>The following plot shows accuracy vs number of samples removed. Random values serve as a baseline. The shaded area represents the 95% bootstrap confidence interval of the mean across 5 runs.</p> <p> Accuracy after best-sample removal using values from logistic  regression </p> <p>Because samples are removed from high to low valuation order, we expect a steep decrease in the curve.</p> <p>Overall we conclude that in terms of mean WAD, CWS and TMCS perform best, with CWS's CV on par with Beta Shapley's, making CWS a competitive method.</p>"},{"location":"value/classwise-shapley/#dataset-pruning-for-a-neural-network-by-value-transfer","title":"Dataset pruning for a neural network by value transfer","text":"<p>Transfer of values from one model to another is probably of greater practical relevance: values are computed using a cheap model and used to prune the dataset before training a more expensive one.</p> <p>The following plot shows accuracy vs number of samples removed for transfer from logistic regression to a neural network. The shaded area represents the 95% bootstrap confidence interval of the mean across 5 runs.</p> <p> Accuracy after sample removal using values transferred from logistic regression to an MLP </p> <p>As in the previous experiment samples are removed from high to low valuation order and hence we expect a steep decrease in the curve. CWS is competitive with the other methods, especially in very unbalanced datasets like <code>Click</code>. In other datasets, like <code>Covertype</code>, <code>Diabetes</code> and <code>MNIST (multi)</code> the performance is on par with TMCS.</p>"},{"location":"value/classwise-shapley/#detection-of-mis-labeled-data-points","title":"Detection of mis-labeled data points","text":"<p>The next experiment tries to detect mis-labeled data points in binary classification tasks. 20% of the indices is flipped at random (we don't consider multi-class datasets because there isn't a unique flipping strategy). The following table shows the mean of the area under the curve (AUC) for five runs.</p> <p> Mean AUC for mis-labeled data point detection. Values computed using LOO, CWS, Beta Shapley, and  TMCS </p> <p>In the majority of cases TMCS has a slight advantage over CWS, except for <code>Click</code>, where CWS has a slight edge, most probably due to the unbalanced nature of the dataset. The following plot shows the CV for the AUC of the five runs.</p> <p> Coefficient of variation of AUC for mis-labeled data point detection. Values computed using LOO, CWS, Beta Shapley, and TMCS </p> <p>In terms of CV, CWS has a clear edge over TMCS and Beta Shapley.</p> <p>Finally, we look at the ROC curves training the classifier on the \\(n\\) first samples in increasing order of valuation (i.e. starting with the worst):</p> <p> Mean ROC across 5 runs with 95% bootstrap CI </p> <p>Although at first sight TMCS seems to be the winner, CWS stays competitive after factoring in running time. For a perfectly balanced dataset, CWS needs on average fewer samples than TCMS.</p>"},{"location":"value/classwise-shapley/#value-distribution","title":"Value distribution","text":"<p>For illustration, we compare the distribution of values computed by TMCS and CWS.</p> <p> Histogram and estimated density of the values computed by TMCS and CWS on all nine datasets </p> <p>For <code>Click</code> TMCS has a multi-modal distribution of values. We hypothesize that this is due to the highly unbalanced nature of the dataset, and notice that CWS has a single mode, leading to its greater performance on this dataset.</p>"},{"location":"value/classwise-shapley/#conclusion","title":"Conclusion","text":"<p>CWS is an effective way to handle classification problems, in particular for unbalanced datasets. It reduces the computing requirements by considering in-class and out-of-class points separately.</p> <ol> <li> <p>Schoch, S., Xu, H., Ji, Y., 2022. CS-Shapley: [Class-wise Shapley Values]{.nocase} for Data Valuation in Classification, in: Proc. Of the Thirty-Sixth Conference on Neural Information Processing Systems (NeurIPS). Presented at the Advances in Neural Information Processing Systems (NeurIPS 2022), New Orleans, Louisiana, USA.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Castro, J., G\u00f3mez, D., Tejada, J., 2009. Polynomial calculation of the Shapley value based on sampling. Computers\\ &amp; Operations Research, Selected papers presented at the Tenth International Symposium on Locational Decisions (ISOLDE X) 36, 1726--1730. https://doi.org/10.1016/j.cor.2008.04.004 \u21a9</p> </li> </ol>"},{"location":"value/data-banzhaf/","title":"Data Banzhaf semi-values","text":"<p>As noted in the section Problems of Data Values, the Shapley value can be very sensitive to variance in the utility function. For machine learning applications, where the utility is typically the performance when trained on a set \\(S \\subset D\\), this variance is often largest for smaller subsets \\(S\\). It is therefore reasonable to try reducing the relative contribution of these subsets with adequate weights.</p> <p>One such choice of weights is the Banzhaf index, which assigns a constant weight:</p> \\[ w(k) := 2^{-(n-1)}, \\] <p>for all set sizes \\(k\\). The intuition for picking a constant weight is that for any choice of weight function \\(w\\), one can always construct a utility with higher variance where \\(w\\) is greater. Therefore, in a worst-case sense, the best one can do is to pick a constant weight.</p> <p>The authors of (Wang and Jia, 2023)<sup>1</sup> show that Banzhaf indices are more robust to variance in the utility function than Shapley and Beta Shapley values. They are available in pyDVL through BanzhafValuation:</p> <pre><code>from joblib import parallel_config\nfrom pydvl.valuation import (\n    ModelUtility, Dataset, SupervisedScorer, PermutationSampler\n)\nfrom pydvl.valuation.methods.banzhaf import BanzhafValuation\nfrom pydvl.valuation.stopping import MinUpdates\n\ntrain, test = Dataset.from_arrays(...)\nmodel = ...\nscorer =  SupervisedScorer(model, test, default=0.0)\nutility = ModelUtility(model, scorer)\nsampler = PermutationSampler()\nvaluation = BanzhafValuation(utility, sampler, MinUpdates(1000))\nwith parallel_config(n_jobs=16):\n    valuation.fit(train)\n</code></pre>"},{"location":"value/data-banzhaf/#msr-banzhaf-intro","title":"Data Banzhaf with MSR sampling","text":"<p>Wang et al. propose a more sample-efficient method for computing Banzhaf  semi-values in their paper Data Banzhaf: A Robust Data Valuation Framework  for Machine Learning (Wang and Jia, 2023)<sup>1</sup>. This method updates all semi-values per each evaluation of the utility (i.e. per model training) based on whether a  specific data point was included in the data subset or not. The expression  for computing the semi-values is</p> \\[ \\hat{\\phi}_{MSR}(i) = \\frac{1}{|\\mathbf{S}_{\\ni i}|} \\sum_{S \\in  \\mathbf{S}_{\\ni i}} U(S) - \\frac{1}{|\\mathbf{S}_{\\not{\\ni} i}|}  \\sum_{S \\in \\mathbf{S}_{\\not{\\ni} i}} U(S) \\] <p>where \\(\\mathbf{S}_{\\ni i}\\) are the subsets that contain the index \\(i\\) and  \\(\\mathbf{S}_{\\not{\\ni} i}\\) are the subsets not containing the index \\(i\\).</p> <p>pyDVL provides a sampler for this method, called MSRSampler, which can be combined with any valuation method, including BanzhafValuation. However, because the sampling probabilities induced by MSR coincide with Banzhaf indices, it is preferred to use the dedicated class MSRBanzhafValuation. For more on this subject see Sampling strategies for semi-values.</p> MSR Banzhaf values <p><pre><code>from joblib import parallel_config\nfrom pydvl.valuation import ModelUtility, Dataset, SupervisedScorer\nfrom pydvl.valuation.methods.banzhaf import MSRBanzhafValuation\nfrom pydvl.valuation.stopping import MaxSamples\n\ntrain, test = Dataset.from_arrays(...)\nmodel = ...\nscorer = SupervisedScorer(model, test, default=0.0)\nutility = ModelUtility(model, scorer)\nvaluation = MSRBanzhafValuation(utility, MaxSamples(1000), batch_size=64)\nwith parallel_config(n_jobs=16):\n    valuation.fit(train)\n</code></pre> Note how we pass batch size directly to the valuation method, which does not take a sampler since it uses MSR sampling internally.</p> <ol> <li> <p>Wang, J.T., Jia, R., 2023. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning, in: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics, PMLR, pp. 6388--6421.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"value/data-oob/","title":"Data valuation for bagged models with Data-OOB","text":"<p>Data-OOB (Kwon and Zou, 2023)<sup>1</sup> is a method for valuing data used to train bagged models. It defines value as the out-of-bag (OOB) performance estimate for the model, overcoming the computational bottleneck of Shapley-based data valuation methods: Instead of fitting a large number of models to accurately estimate marginal contributions like Shapley-based methods, Data-OOB evaluates each weak learner in an ensemble over samples it hasn't seen during training, and averages the performance across all weak learners.</p> <p>More precisely, for a bagging model with \\(B\\) estimators \\(\\hat{f}_b, b \\in [B]\\), we define \\(w_{bj}\\) as the number of times that the \\(j\\)-th sample is in the training set of the \\(b\\)-th estimator. For a fixed choice of bootstrapped training sets, the Data-OOB value of sample \\((x_i, y_i)\\) is defined as:</p> \\[ \\psi_i := \\frac{\\sum_{b=1}^{B}\\mathbb{1}(w_{bi}=0)T(y_i,     \\hat{f}_b(x_i))}{\\sum_{b=1}^{B} \\mathbb{1} (w_{bi}=0)}, \\] <p>where \\(T: Y \\times Y \\rightarrow \\mathbb{R}\\) is a score function that represents the goodness of weak learner \\(\\hat{f}_b\\) at the \\(i\\)-th datum \\((x_i, y_i)\\).</p> <p>\\(\\psi\\) can therefore be interpreted as a per-sample partition of the standard OOB error estimate for a bagging model, which is: \\(\\frac{1}{n} \\sum_{i=1}^n \\psi_i\\).</p>"},{"location":"value/data-oob/#computing-values","title":"Computing values","text":"<p>The main class is DataOOBValuation. It takes a fitted bagged model and uses data precomputed during training to calculate the values. It is therefore very fast, and can be used to value large datasets.</p> Using Data-OOB with a RandomForest <p>This is how you would use it with a RandomForestClassifier.  <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom pydvl.valuation import DataOOBValuation, Dataset\n\ntrain, test = Dataset(...), Dataset(...)\nmodel = RandomForestClassifier(...)\nmodel.fit(*train.data())\nvaluation = DataOOBValuation(model)\nvaluation.fit(train)\nvalues = valuation.values()\n</code></pre></p> <p>The object returned by <code>values()</code> is a ValuationResult to be used for data inspection, cleaning, etc.</p> <p>Data-OOB is not limited to sklearn's <code>RandomForest</code>, but can be used with any bagging model that defines the attribute <code>estimators_</code>  after fitting and makes the list of bootstrapped samples available in some way. This includes <code>BaggingRegressor</code>, <code>BaggingClassifier</code>, <code>ExtraTreesClassifier</code>, <code>ExtraTreesRegressor</code> and <code>IsolationForest</code>.</p>"},{"location":"value/data-oob/#bagging-arbitrary-models","title":"Bagging arbitrary models","text":"<p>Through <code>BaggingClassifier</code> and <code>BaggingRegressor</code>, one can compute values for any model that can be bagged. Bagging in itself is not necessarily always beneficial, and there are cases where it can be detrimental. However, for data valuation we are not interested in the performance of the bagged model, but in the valuation coming out of it, which can then be used to work on the original model and data.</p> Bagging a k-NN classifier <pre><code>from sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom pydvl.valuation import DataOOBValuation, Dataset\n\ntrain, test = Dataset(...), Dataset(...)\nmodel = BaggingClassifier(\n    estimator=KNeighborsClassifier(n_neighbors=10),\n    n_estimators=20)\nmodel.fit(*train.data())\nvaluation = DataOOBValuation(model)\nvaluation.fit(train)\nvalues = valuation.values(sort=True)\nlow_values = values[:int(0.05*len(train))]  # select lowest 5%\n...\n</code></pre>"},{"location":"value/data-oob/#off-topic-when-not-to-bag-models","title":"Off-topic: When not to bag models","text":"<p>Here are some guidelines for when bagging might unnecessarily increase computational cost, or even be detrimental:</p> <ol> <li> <p>Low-variance models: Models like linear regression, support vector    machines, or other inherently stable algorithms typically have low variance.    However, even these models can benefit from bagging in certain scenarios,    particularly with noisy data or when there are influential outliers.</p> </li> <li> <p>When the model is already highly regularized: If a model is regularized    (e.g., Lasso, Ridge, or Elastic Net), it is already tuned to avoid    overfitting and reduce variance, so bagging might not provide much of a    benefit for its high computational cost.</p> </li> <li> <p>When data is limited: Bagging works by creating multiple subsets of the    data via bootstrapping. If the dataset is too small, the bootstrap samples    might overlap significantly or exclude important patterns, reducing the    effectiveness of bagging.</p> </li> <li> <p>When features are highly correlated: If features are highly correlated,    the individual models trained on different bootstrap samples may end up being    too similar.</p> </li> <li> <p>For inherently stable models: Models that are naturally resistant to    changes in the training data (like nearest neighbors) may not benefit    significantly from bagging's variance reduction properties.</p> </li> <li> <p>When interpretability is critical: Bagging produces an ensemble of    models, which makes the overall model less interpretable compared to a single    model. There are however manye techniques to maintain interpretability, like     partial dependence plots.</p> </li> <li> <p>When the bias-variance trade-off favors bias reduction: If the model's     error is primarily due to bias rather than variance, techniques that address    bias (like boosting) might be more appropriate than bagging.</p> </li> </ol>"},{"location":"value/data-oob/#transferring-values","title":"Transferring values","text":"<p>As with any other valuation method, you can transfer the values to a different model, and given the efficiency of Data-OOB, this can be done very quickly. A simple workflow is to compute values using a random forest, then use them to inspect the data and clean it, and finally train a more complex model on the cleaned data. Whether this is a valid idea or not will depend on the specific dataset.</p>"},{"location":"value/data-oob/#a-comment-about-sampling","title":"A comment about sampling","text":"<p>One might fear that there is a problem because the computation of the value \\(\\psi_i\\) requires at least some bootstrap samples not to include the \\(i\\)-th sample. But we can see that this is rarely an issue, and its probability of happening can be easily computed: For a training set of size \\(n\\) and bootstrapping sample size \\(m \\le n\\), the probability that index \\(i\\) is not included in a bootstrap sample is \\(\\prod_{j=1}^m \\mathbb{P}(i \\text{ is not drawn at pos. } j) = (1 - 1/n)^m\\), i.e. for each of the \\(m\\) draws, the number is not picked (for \\(m=n\\) this converges to \\(1/e \\approx 0.368\\)). The probability that across \\(B\\) bootstrapped samples a point is not included is therefore \\((1  - 1/n)^{mB}\\), which is typically extremely low.</p> <p>Incidentally, this allows us to estimate the estimated number of unique indices in a bootstrap sample of size \\(m\\) as \\(m(1 - 1/n)^m\\).</p> <ol> <li> <p>Kwon, Y., Zou, J., 2023. Data-OOB: [Out-of-bag Estimate]{.nocase} as a Simple and Efficient Data Value, in: Proceedings of the 40th International Conference on Machine Learning. Presented at the International Conference on Machine Learning, PMLR, pp. 18135--18152.\u00a0\u21a9</p> </li> </ol>"},{"location":"value/delta-shapley/","title":"\\(\\delta\\)-Shapley","text":"<p>\\(\\delta\\)-Shapley is a semi-value that approximates the average marginal contribution of a data point per subset size, truncated for sizes beyond a certain range. It was introduced in (Watson et al., 2023)<sup>3</sup>, and is available in pyDVL as DeltaShapleyValuation, which is built around a stratified sampler. When its clipping feature (discussed below) is not used, it is an approximation to Shapley value.</p> <p>Practical application</p> <p>While we provide an implementation of the \\(\\delta\\)-Shapley value for the sake of completeness, in practice, properly adjusting the constants required is often difficult, making it hard to use. If one still wishes to use stratified sampling, it is possible to use subset size sampling strategies  that don't require these constants, such as PowerLawSampleSize, or VRDSSampler.</p>"},{"location":"value/delta-shapley/#a-layer-wise-approximation","title":"A layer-wise approximation","text":"<p>As discussed in Stratified Shapley, it is possible to compute Shapley values by averaging the \"layer-wise\" (i.e. per subset size) marginal contributions \\(\\phi_i^{k} := \\sum_{S_j \\subseteq N_{-i}^{k}} \\Delta_i(S_j).\\)</p> <p>Therefore, one can estimate \\(v_\\text{shap}(i)\\) by approximating the \\(\\phi_i^{k}\\) and then averaging those. This approximation consists of sampling only a fraction \\(m_k\\) of all the sets of size \\(k\\) to average the \\(\\Delta_i(S_j),\\ j=1, ..., m_k.\\) The main contributions of the paper is a careful choice of \\(m_k\\) (see below) which depends on the ML model for which the values are computed.</p>"},{"location":"value/delta-shapley/#clipping-the-sample-sizes","title":"Clipping the sample sizes","text":"<p>Additionally, the authors argue that, for certain model classes, small coalitions are good estimators of the data value, because adding more data tends to have diminishing returns. This motivates clipping \\(k\\) outside a given range, to come to the final definition of the \\(\\delta\\)-Shapley value:<sup>2</sup></p> \\[ \\hat{v}_\\delta(i) := \\frac{1}{u - l + 1} \\sum_{k=l}^{u} \\frac{1}{m_k} \\sum_{j=1}^{m_k} \\Delta_i(S_j), \\] <p>where \\(l\\) and \\(u\\) are lower and upper bounds for \\(k\\), the sets \\(S_j\\) are sampled uniformly at random from \\(N_{-i}^{k}\\), and \\(m_k\\) is the number of samples at size \\(k.\\)</p>"},{"location":"value/delta-shapley/#delta-shapley-as-importance-sampling","title":"Delta Shapley as importance sampling","text":"<p>Above, we have departed from the paper and use the notation \\(\\hat{v}_\\delta\\) to indicate that this is an approximation to Shapley value using importance sampling and a certain stratified sampling distribution.</p> <p>Recall from Stratified Shapley that we if we define a distribution \\(\\mathcal{L}\\) over sets by first sampling a size \\(k\\) uniformly between \\(0\\) and \\(n-1,\\) then a subset of \\(N_{-i}\\) and size \\(k\\) uniformly from the powerset, then</p> \\[v_\\text{shap}(i) = \\mathbb{E}_{\\mathcal{L}}[\\Delta_i(S)].\\] <p>This holds because the probability under \\(\\mathcal{L}\\) of drawing any set \\(S\\) is, letting \\(k=|S|:\\)</p> \\[p_{\\mathcal{L}}(S) = p_{\\mathcal{L}}(k)\\ p_{\\mathcal{L}}(S|k) = \\frac{1}{n} \\binom{n-1}{k}^{-1}.\\] <p>Now, \\(\\delta\\)-Shapley introduces a new stratified distribution \\(\\mathcal{L}_k,\\) such that:</p> \\[p_{\\mathcal{L}_k}(S) = p_{\\mathcal{L}_k}(k)\\ p_{\\mathcal{L}_k}(S|k) =  \\frac{m_k}{\\sum_j m_j} \\binom{n-1}{k}^{-1}.\\] <p>And computes the approximation</p> \\[ \\hat{v}_\\delta(i) := \\mathbb{E}_{\\mathcal{L}_k}[\\Delta_i(S)\\ \\frac{p_{\\mathcal{L}}(S)}{p_{\\mathcal{L}_k}(S)}].\\] <p>Also see</p> <p>Read Sampling strategies for semi-values for more on the interaction between coefficients and sampler weights in Monte Carlo estimation of semi-values.</p>"},{"location":"value/delta-shapley/#setting-mk-delta-shapley","title":"Choosing the number of sets","text":"<p>As discussed, subset sizes are sampled according to the probability \\(m_k/sum_j m_j\\) (even though the exact procedure can vary, e.g. iterate through all \\(k\\) deterministically or at random). This means that the probability of sampling a set of size \\(k\\) is</p> <p>The choice of \\(m_k\\) is guided by theoretical bounds derived from the uniform stability properties of the learning algorithm. The authors of \\(\\delta\\)-Shapley derive bounds for different classes of loss functions using concentration inequalities, with Theorems 6 and 7 of the paper providing the choice of \\(m_k\\) for the case of non-convex, smooth Lipschitz models trained with SGD.<sup>1</sup> This is the case that we implement in DeltaShapleyNCSGDSampleSize, but we will discuss below how to implement any choice of \\(m_k\\).</p>"},{"location":"value/delta-shapley/#delta-shapley-for-non-convex-sgd","title":"Delta-Shapley for non-convex SGD","text":"<p>Setting \\(m_k\\) for a general model trained with SGD requires several parameters: the number of SGD iterations, the range of the loss, the Lipschitz constant of the model, and the learning rate, which is assumed to decay as \\(\\alpha_t = c / t.\\) For the exact expressions see equations (8) and (9) of the paper.</p> <p>All of these parameters must be set when instantiating the \\(\\delta\\)-Shapley class.</p> Inconsistencies between the paper and the code <p>There are several inconsistencies between the paper and the code that we could find online, which we couldn't resolve to our satisfaction. These include: 1. There are minor discrepancies in the definition of \\(m_k,\\) most notably    the introduction of a factor \\(n_\\text{test}\\). 2. The code uses a certain, rather arbitrary, number of SGD iterations \\(T\\)    to compute \\(m_k\\) which is never actually used to train the model. 3. Most constants are set to arbitrary values, seemingly without any    justification, potentially invalidating the application of the    theoretical bounds. For these reasons, we provide two modes of operation for the sample size strategy implementing these bounds to either follow those in the paper or those in the code, for reproducibility purposes. See DeltaShapleyNCSGDSampleSize.</p>"},{"location":"value/delta-shapley/#powerset-and-permutation-sampling","title":"Powerset and permutation sampling","text":"<p>The original paper uses a standard powerset sampling approach, where the sets \\(S^j\\) are sampled uniformly at random from the powerset of \\(D_{-i}^{k}.\\) We provide this sampling method via StratifiedSampler, which can be configured with any of the classes inheriting SampleSizeStrategy. These implement the \\(m_k,\\) and the lower and upper bounds truncating \\(k.\\)</p> <p>Alternatively, we provide an experimental and approximate permutation-based approach which clips permutations and keeps track of the sizes of sets returned. This reduces computation by at least a factor of 2, since the evaluation strategy can reuse the previously computed utility for the marginal contribution. It is implemented in StratifiedPermutationSampler. Note that it does not guarantee sampling the exact number of set sizes \\(m_k.\\)</p> <ol> <li> <p>A technical detail is the assumption that the order in which batches are sampled from a coalition \\(S\\) when computing \\(u(s)\\) is not random (i.e. it is constant across epochs).\u00a0\u21a9</p> </li> <li> <p>We believe Definition 9 in the paper to be a bit misleading since it lacks the averaging of the marginals over the sampled sets. As it stands, it iss an unweighted average of the marginals, which would explode in magnitude for large \\(k.\\) This seems substantiated by the fact that the code we found online does not implement this definition, but rather the one we provide here.\u00a0\u21a9</p> </li> <li> <p>Watson, L., Kujawa, Z., Andreeva, R., Yang, H.-T., Elahi, T., Sarkar, R., 2023. Accelerated Shapley Value Approximation for Data Evaluation [WWW Document]. https://doi.org/10.48550/arXiv.2311.05346 \u21a9</p> </li> </ol>"},{"location":"value/dul/","title":"Data Utility Learning","text":"<p>DUL (Wang et al., 2022)<sup>1</sup> uses an ML model \\(\\hat{u}\\) to learn the utility function \\(u:2^N \\to \\matbb{R}\\) during the fitting phase of any valuation method. This utility model is trained with tuples \\((S, U(S))\\) for a certain warm-up period. Then it is used instead of \\(u\\) in the valuation method. The cost of training \\(\\hat{u}\\) is quickly amortized by avoiding costly re-evaluations of the original utility.</p>"},{"location":"value/dul/#process","title":"Process","text":"<p>In other words, DUL accelerates data valuation by learning the utility function from a small number of subsets. The process is as follows:</p> <ol> <li>Collect a given_budget_ of so-called utility samples (subsets and their    utility values) during the normal course of data valuation.</li> <li>Fit a model \\(\\hat{u}\\) to the utility samples. The model is trained to predict    the utility of new subsets.</li> <li>Continue the valuation process, sampling subsets, but instead of evaluating the    original utility function, use the learned model to predict it.</li> </ol>"},{"location":"value/dul/#usage","title":"Usage","text":"<p>There are three components (sorry for the confusing naming!):</p> <ol> <li>The original utility object to learn, typically (but not necessarily) a    ModelUtility object which will be    expensive to evaluate. Any subclass of    UtilityBase should work. Let's call    it <code>utility</code>.</li> <li>A UtilityModel which will be    trained to predict the utility of subsets.</li> <li>The DataUtilityLearning    object.</li> </ol> <p>Assuming you have some data valuation algorithm and your <code>utility</code> object:</p> <ol> <li>Pick the actual machine learning model to use to learn the utility. In most    cases the utility takes continuous values, so this should be any regression    model, such as a linear regression or a neural network. The input to it will    be sets of indices, so one has to encode the data accordingly. For example,    an indicator vector of the set as done in (Wang et al., 2022)<sup>1</sup>, with    IndicatorUtilityModel.    This wrapper accepts any machine learning model for the actual fitting.</li> </ol> <p>An alternative way to encode the data is to use a permutation-invariant model,    such as DeepSet (Zaheer et al., 2017)<sup>2</sup>,    which is a simple architecture to learn embeddings for sets of points. 2. Wrap both your <code>utility</code> object and the utility model just constructed within    a DataUtilityLearning. 3. Use this last object in your data valuation algorithm instead of the original    <code>utility</code>.</p>"},{"location":"value/dul/#dul-indicator-encoding-intro","title":"Indicator encoding","text":"<p>The authors of DUL propose to use an indicator function to encode the sets of indices: a vector of length <code>len(training_data)</code> with a 1 at index \\(i\\) if sample \\(x_i\\) is in the set and 0 otherwise. This encoding can then be fed to any regression model.</p> <p>While this can work under some circumstances, note that one is effectively learning a regression function on the corners of an \\(n\\)-dimensional hypercube, a problem well known to be difficult. For this reason, we offer a (naive) implementation of a permutation-invariant model called Deep Sets which can serve as guidance for a more complex architecture.</p> <p>DUL with a linear regression model</p> Example <pre><code>from pydvl.valuation import Dataset, DataUtilityLearning, ModelUtility, \\\n    Sample, SupervisedScorer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.datasets import load_iris\n\ntrain, test = Dataset.from_sklearn(load_iris())\nscorer = SupervisedScorer(\"accuracy\", test, 0, (0,1))\nutility = ModelUtility(LinearRegression(), scorer)\nutility_model = IndicatorUtilityModel(LinearRegression(), len(train))\ndul = DataUtilityLearning(utility, 300, utility_model)\nvaluation = ShapleyValuation(\n    utility=dul,\n    sampler=PermutationSampler(),\n    stopping=MaxUpdates(6000)\n)\n# Note: DUL does not support parallel training yet\nvaluation.fit(train)\n</code></pre>"},{"location":"value/dul/#deep-sets-intro","title":"Deep Sets","text":"<p>Given a set \\(S= \\{x_1, x_2, ..., x_n\\},\\) Deep Sets (Zaheer et al., 2017)<sup>2</sup> learn a representation of the set which is invariant to the order of elements in the set. The model consists of two networks:</p> \\[ \\Phi(S) = \\sum_{x_i \\in S} \\phi(x_i), \\] <p>where \\(\\phi(x_i)\\) is a learned embedding for data point \\(x_i,\\) and a second network \\(\\rho\\) that predicts the output \\(y\\) from the aggregated representation:</p> \\[ y = \\rho(\\Phi(S)). \\] <p>DUL with DeepSets</p> Example <p>This example requires pytorch installed. <pre><code>from pydvl.valuation import Dataset, DataUtilityLearning, ModelUtility, \\\n    Sample, SupervisedScorer\nfrom pydvl.valuation.utility.deepset import DeepSet\nfrom sklearn.datasets import load_iris\n\ntrain, test = Dataset.from_sklearn(load_iris())\nscorer = SupervisedScorer(\"accuracy\", test, 0, (0,1))\nutility = ModelUtility(LinearRegression(), scorer)\nutility_model = DeepSet(\n    input_dim=len(train),\n    phi_hidden_dim=10,\n    phi_output_dim=20,\n    rho_hidden_dim=10\n)\ndul = DataUtilityLearning(utility, 3000, utility_model)\n\nvaluation = ShapleyValuation(\n    utility=dul,\n    sampler=PermutationSampler(),\n    stopping=MaxUpdates(10000)\n)\n# Note: DUL does not support parallel training yet\nvaluation.fit(train)\n</code></pre></p> <ol> <li> <p>Wang, T., Yang, Y., Jia, R., 2022. Improving [Cooperative Game Theory-based Data Valuation]{.nocase} via Data Utility Learning. Presented at the International Conference on Learning Representations (ICLR 2022). Workshop on Socially Responsible Machine Learning, arXiv. https://doi.org/10.48550/arXiv.2107.06336 \u21a9\u21a9</p> </li> <li> <p>Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R.R., Smola, A.J., 2017. Deep Sets, in: Advances in Neural Information Processing Systems. Curran Associates, Inc.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"value/group-testing-shapley/","title":"Group-testing Shapley","text":"<p>Group-testing Shapley draws inspiration from group testing ideas and reformulates Shapley value estimation as a feasibility problem, whose constraints are given by samples of the utility function. By carefully constructing the sampling distribution, the solution converges to the true Shapley value.(Jia et al., 2019)<sup>1</sup></p> <p>Section incomplete</p> <ol> <li> <p>Jia, R., Dao, D., Wang, B., Hubis, F.A., Hynes, N., G\u00fcrel, N.M., Li, B., Zhang, C., Song, D., Spanos, C.J., 2019. Towards Efficient Data Valuation Based on the Shapley Value, in: Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics (AISTATS), PMLR, pp. 1167--1176.\u00a0\u21a9</p> </li> </ol>"},{"location":"value/knn-shapley/","title":"KNN-Shapley","text":"<p>Using the generic class ShapleyValuation together with a PermutationSampler to compute Shapley values for a KNN model has a time complexity of  \\(O(n^2 \\log^2 n).\\)<sup>1</sup></p> <p>However, it is possible to exploit the local structure of K-Nearest Neighbours to drastically reduce the amount of utility evaluations and bring complexity down to \\(O(n \\log n)\\) per test point. The key insight is that  because no sample besides the \\(K\\) closest affects the score, most can be ignored, allowing for a recursive formula of the complexity mentioned. This method was introduced by (Jia et al., 2019)<sup>3</sup> in two forms, an exact (available with KNNShapleyValuation) and an approximate one.</p>"},{"location":"value/knn-shapley/#the-exact-computation","title":"The exact computation","text":"<p>Define the utility by the likelihood of the right label:</p> \\[ U (S, x_{\\text{test}}) = \\frac{1}{K}  \\sum_{k = 1}^{\\min K, |S|}    \\mathbb{I}[y_{\\alpha_{k} (S)} = y_{\\text{test}}] \\] <p>where \\(\\alpha_{k} (S)\\) is the \\(k\\)-th closest point \\(x_{i} \\in S\\) to  \\(x_{\\text{test}}\\). The SV of each \\(x_{i} \\in  D_{\\text{train}}\\) is computed exactly with the following recursion:</p> \\[ s_{\\alpha_{N}} = \\frac{\\mathbb{I}[y_{\\alpha_{N}} =    y_{\\text{test}}]}{N}, \\] \\[ s_{\\alpha_{i}} = s_{\\alpha_{i + 1}} + \\frac{\\mathbb{I}[y_{\\alpha    _{i}} = y_{\\text{test}}] -\\mathbb{I}[y_{\\alpha_{i + 1}} =    y_{\\text{test}}]}{K}  \\frac{\\min \\lbrace K, i \\rbrace}{i}.  \\] <p>The utilities are then averaged over all \\(x_{\\text{test}} \\in  D_{\\text{test}}\\) to compute the total utility \\(U (S).\\)</p> <p>pyDVL implements the exact method in KNNShapleyValuation. Internally it iterates through each training point with a LOOSampler and computes the recursive formula.<sup>2</sup></p> Computing exact KNN-Shapley values <pre><code>from joblib import parallel_config\nfrom pydvl.valuation import Dataset, KNNShapleyValuation\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=5)\ntrain, test = Dataset.from_arrays(...)\nvaluation = KNNShapleyValuation(model, test, progress=True)\n\nwith parallel_config(n_jobs=16):\n    valuation.fit(train)\n</code></pre>"},{"location":"value/knn-shapley/#approximate-computation","title":"Approximate computation","text":"<p>By using approximate KNN-based methods, it is possible to shave off a factor \\(n\\) in the complexity of the exact method. As of v0.10.0 there is no implementation in pyDVL for this technique.</p> <ol> <li> <p>This is based on a Hoeffding bound and the time complexity of sorting \\(n\\)   points, which is the asymptotic complexity of one utility evaluation of   KNN (Jia et al., 2019)<sup>3</sup>.\u00a0\u21a9</p> </li> <li> <p>As already noted, it is possible to use the standard infrastructure instead   although this is only useful for testing purposes To this end, we provide   an implementation of the formula above in   KNNClassifierUtility.\u00a0\u21a9</p> </li> <li> <p>Jia, R., Dao, D., Wang, B., Hubis, F.A., Gurel, N.M., Li, B., Zhang, C., Spanos, C., Song, D., 2019. Efficient task-specific data valuation for nearest neighbor algorithms. Proc. VLDB Endow. 12, 1610--1623. https://doi.org/10.14778/3342263.3342637 \u21a9\u21a9</p> </li> </ol>"},{"location":"value/loo/","title":"Leave-One-Out values","text":"<p>LOO is the simplest approach to valuation. Let \\(D\\) be the training set, and \\(D_{-i}\\) be the training set without the sample \\(x_i\\). Assume some utility function \\(u(S)\\) that measures the performance of a model trained on \\(S \\subseteq D\\).</p> <p>LOO assigns to each sample its marginal utility as value:</p> \\[v_\\text{loo}(i) = u(D) - u(D_{-i}),\\] <p>and as such is the simplest example of marginal contribution-based valuation method. In pyDVL it is available as LOOValuation.</p> <p>For the purposes of data valuation, this is rarely useful beyond serving as a baseline for benchmarking (although it can perform astonishingly well on occasion).</p> <p>One particular weakness is that it does not necessarily correlate with an intrinsic value of a sample: since it is a marginal utility, it is affected by diminishing returns. Often, the training set is large enough for a single sample not to have any significant effect on training performance, despite any qualities it may possess. Whether this is indicative of low value or not depends on one's goals and definitions, but other methods are typically preferable.</p> Leave-One-Out values <pre><code>from joblib import parallel_config\nfrom pydvl.valuation import Dataset, LOOValuation, ModelUtility\n\ntrain, test = Dataset.from_arrays(...)\nmodel = ...\nutility = ModelUtility(model)\nval_loo = LOOValuation(utility, progress=True)\nwith parallel_config(n_jobs=12):\n    val_loo.fit(train)\nresult = val_loo.values(sort=True)\n</code></pre> <p>Strictly speaking, LOO can be seen as a semivalue where all the coefficients are zero except for \\(k=|D|-1.\\)</p> <p>Connection to the influence function</p> <p>With a slight change of perspective, the influence function can be seen as a first order approximation to the Leave-One-Out values. See Approximating the influence of a point.</p>"},{"location":"value/owen/","title":"Owen sampling for Shapley values","text":"<p>Owen sampling schemes are a family of sampling schemes that are used to estimate Shapley values. They are based on a multilinear extension technique from game theory, and were introduced in (Okhrati and Lipani, 2021)<sup>1</sup>. The core idea is to use different probabilities of including indices into samples.</p> <p>By choosing these probabilities at uniform, the expected value of the marginal utility over the sampling probability of \\(p\\) is equal to the Shapley value:</p> \\[v_{sh}(i) = \\int_0^1 \\mathbb{E}_{D^p_{-i}} \\left[ u_(S_{+i}) - u(S) \\right] dp,\\] <p>where \\(D^p_{-i}\\) is the distribution over the subsets of the training set, not containing \\(i\\), whose elements are included with probability \\(p\\).</p> <p>There is an outer loop that picks sampling probabilities between 0 and 1, and an inner loop that samples indices from the dataset using that probability. Depending on the properties of each choice the samplers can be finite or infinite. The original method introduced in the paper sampled a fixed number of values for \\(p \\in (0,1)\\) and for each one of those sampled just a few (2) sets of indices, where the probability of including an index is \\(p\\).</p> <p>In order to compute values it is enough to use any of the Owen samplers together with a ShapleyValuation object.</p>"},{"location":"value/owen/#finite-owen-sampler","title":"Finite Owen Sampler","text":"<p>OwenSampler with a FiniteSequentialIndexIteration for the outer loop and a GridOwenStrategy for the sampling probabilities is the most basic Owen sampler. It uses a deterministic grid of probability values between 0 and 1 for the inner sampling. It follows the idea of the original paper and should be instantiated with NoStopping as stopping criterion.</p> Example <pre><code>from pydvl.valuation.methods import ShapleyValuation\nfrom pydvl.valuation.samplers import OwenSampler\nfrom pydvl.valuation.stopping import RankCorrelation\n\n...\n\nsampler = OwenSampler(\n    outer_sampling_strategy=GridOwenStrategy(n_samples_outer=200),\n    n_samples_inner=2,\n    index_iteration=FiniteSequentialIndexIteration,\n)\nstopping = NoStopping(sampler)  # Pass the sampler for progress updates\nvaluation = ShapleyValuation(utility, sampler, stopping, progress=True)\nwith parallel_config(n_jobs=8):\n    valuation.fit(train)\nresult = valuation.values()\n</code></pre>"},{"location":"value/owen/#infinite-uniform-owen-sampler","title":"Infinite (uniform) Owen Sampler","text":"<p>OwenSampler follows the same principle as OwenSampler, but samples probability values between 0 and 1 at random indefinitely. It requires a stopping criterion to be used with the valuation method, and thus follows more closely the general pattern of the valuation methods. This makes it more adequate for actual use since it is no longer required to estimate a number of outer samples required. Because we sample uniformly the statistical properties of the method are conserved, in particular the correction coefficient for semi-value computation remains the same.</p> Owen Sampler <pre><code>from pydvl.valuation.methods import ShapleyValuation\nfrom pydvl.valuation.samplers import OwenSampler, GridOwenStrategy\nfrom pydvl.valuation.stopping import RankCorrelation\n\nutility = ModelUtility(...)\nsampler = OwenSampler(outer_sampling_strategy=GridOwenStrategy(n_samples_outer=200))\nstopping = RankCorrelation(rtol=1e-3, burn_int=100)\nvaluation = ShapleyValuation(utility, sampler, stopping,  progress=True)\nwith parallel_config(n_jobs=-1)\n    valuation.fit(dataset)\nresult = valuation.values()\n</code></pre>"},{"location":"value/owen/#antithetic-owen-sampler","title":"Antithetic Owen Sampler","text":"<p>AntitheticOwenSampler is a variant of the OwenSampler that draws probability values \\(q\\) between 0 and 0.5 at random and then generates two samples for each index, one using the probability \\(q\\) for index draws, and another with probability \\(1-q\\).</p> <p>Example</p> <pre><code>from pydvl.valuation import AntitheticOwenSampler, ShapleyValuation, RankCorrelation\n...\n\nsampler = AntitheticOwenSampler()\nvaluation = ShapleyValuation(utility, sampler, RankCorrelation(rtol=1e-3))\nvaluation.fit(dataset)\n</code></pre> <ol> <li> <p>Okhrati, R., Lipani, A., 2021. A Multilinear Sampling Algorithm to Estimate Shapley Values, in: 2020 25th International Conference on Pattern Recognition (ICPR). Presented at the 2020 25th International Conference on Pattern Recognition (ICPR), IEEE, pp. 7992--7999. https://doi.org/10.1109/ICPR48806.2021.9412511 \u21a9</p> </li> </ol>"},{"location":"value/sampling-weights/","title":"Sampling strategies for semi-values","text":"<p>Info</p> <p>This page explains the interaction between sampler weights (probability of sampling sets of a given size), semi-value coefficients and Monte Carlo sampling.</p> <p>Valuation methods based on semi-values involve computing averages of marginal utilities over all possible subsets of the training data. As explained in the introduction with uniform sampling, we use Monte Carlo approximations to compute these averages. Below we show that this introduces additional terms in the results due to the sampling probabilities, yielding effective coefficients that are the product of the semi-value coefficient with a sampling probability. To correct for this, all samplers provide a method which is just \\(p(S),\\) the probability of sampling a set \\(S.\\) This can be seen either as a form of importance sampling to reduce variance, or as a mechanism to allow mix-and-matching of sampling strategies and semi-value coefficients.</p> <p>However, the correction an unnecessary step when the sampling distribution yields exactly the semi-value coefficient, a situation which is the basis for several methods proposed in the literature.</p> The core semi-value computation for powerset sampling <p>This is the core of the marginal update computation in PowersetEvaluationStrategy:* <pre><code>for sample in batch:\n    u_i = self.utility(sample.with_idx_in_subset())\n    u = self.utility(sample)\n    marginal = u_i - u\n    sign = np.sign(marginal)\n    log_marginal = -np.inf if marginal == 0 else np.log(marginal * sign)\n\n    # Here's the coefficient, as defined by the valuation method,\n    # potentially with a correction.\n    log_marginal += self.valuation_coefficient(\n        self.n_indices, len(sample.subset)\n    )\n\n    updates.append(ValueUpdate(sample.idx, log_marginal, sign))\n    ...\n</code></pre></p> <p>The <code>valuation_coefficient(n, k)</code> is in effect defined by the valuation method, and allows for almost arbitrary combinations of semi-value and sampler. By subclassing one can also switch off the coefficients when indicated. We provide dedicated classes that do so for the most common combinations, like TMCShapleyValuation or MSRBanzhafValuation. If you check the code you will see that they are in fact little more than thin wrappers.</p>"},{"location":"value/sampling-weights/#uniform-sampling","title":"Uniform sampling","text":"<p>We begin by rewriting the combinatorial definition of Shapley value:</p> \\[ \\begin{eqnarray*}   v_{\\operatorname{sh}} (i) &amp; = &amp; \\sum_{S \\subseteq D_{- i}} \\frac{1}{n}   \\binom{n - 1}{| S |}^{- 1}  [U (S_{+ i}) - U (S)],\\\\\\   &amp; = &amp; \\sum_{S \\subseteq D_{- i}} w_{\\operatorname{sh}} (| S |) \\Delta_i (S), \\end{eqnarray*} \\] <p>where \\(w_{\\operatorname{sh}} (| S |) = \\frac{1}{n}  \\binom{n - 1}{| S |}^{-  1}\\) is the Shapley weight and \\(\\Delta_i (S) := U (S_{+ i}) - U (S)\\) the  marginal utility. The naive Monte Carlo approximation is then to sample \\(S_{j}  \\sim \\mathcal{U} (D_{- i})\\) and let</p> <p></p> \\[ \\begin{equation}   \\hat{v}_{\\operatorname{sh}, \\operatorname{unif}} (i) = \\frac{1}{M}  \\sum_{j   = 1}^M w_{\\operatorname{sh}} (| S_{j} |) \\Delta_i (S_{j})   \\label{mc-shapley}\\tag{1} \\end{equation} \\] <p>However, as the number of samples \\(M\\) grows:</p> \\[ \\begin{eqnarray*}   \\hat{v}_{\\operatorname{sh}, \\operatorname{unif}} (i) &amp; \\underset{M   \\rightarrow \\infty}{\\longrightarrow} &amp; \\underset{S \\sim \\mathcal{U} (D_{-   i})}{\\mathbb{E}} [w_{\\operatorname{sh}} (| S |) \\Delta_i (S)]\\\\\\   &amp; = &amp; \\sum_{S \\subseteq D_{- i}} w_{\\operatorname{sh}} (k) \\Delta_i   (S) p_{\\mathcal{U}} (S), \\end{eqnarray*} \\] <p>where \\(p_{\\mathcal{U}} (S) = \\frac{1}{2^{n - 1}}\\) is the probability of  sampling a set \\(S\\) under the uniform distribution \\(\\mathcal{U} (D_{- i}).\\)  Now, note how this is not exactly what we need, so we must account for it by  introducing an additional coefficient in the Monte Carlo sums. We can call this  coefficient</p> \\[ w_{\\operatorname{unif}} (S) \\equiv 2^{n - 1} . \\] <p>The product \\(w_{\\operatorname{unif}}  (S) w_{\\operatorname{sh}} (S)\\) is the <code>valuation_coefficient(n, k)</code> in the code above. Because of how samplers work the coefficients only depend on the size \\(k = | S |\\) of the subsets, and it will always be the inverse of the probability of a set \\(S,\\) given that it has size \\(k.\\)</p> <p>At every step of the MC algorithm we do the following:</p> <p>Monte Carlo Shapley Update</p> <ol> <li>sample \\(S_{j} \\sim \\mathcal{U} (D_{- i}),\\) let \\(k = | S_{j} |\\)</li> <li>compute the marginal \\(\\Delta_i (S_{j})\\)</li> <li>compute the product of coefficients for the sampler and the method:     \\(w_{\\operatorname{unif}} (k) w_{\\operatorname{sh}} (k)\\)</li> <li>update the running average for \\(\\hat{v}_{\\operatorname{unif},     \\operatorname{sh}}\\)</li> </ol>"},{"location":"value/sampling-weights/#picking-a-different-distribution","title":"Picking a different distribution","text":"<p>In our correction above, we compute</p> \\[ w_{\\operatorname{unif}} (k) w_{\\operatorname{sh}} (k) =  \\frac{2^{n - 1}}{n}  \\binom{n - 1}{k}^{- 1}, \\] <p>for every set size \\(k.\\) Even for moderately large values of \\(n\\) and \\(k\\) these  are huge numbers that will introduce errors in the computation. One way of  alleviating the problem that is employed in pyDVL is to perform all  computations in log space and use the log-sum-exp trick for numerical  stability.</p> <p>However, while this greatly increases numerical range and accuracy, it remains suboptimal. What if one chose instead the sampling distribution \\(\\mathcal{L}\\) such that \\(p_\\mathcal{L} (S) = w_{\\operatorname{sh}} (S)\\)? This is the main contribution of several works in the area, like TMCS, AME or Owen-Shapley.</p>"},{"location":"value/sampling-weights/#an-introductory-example","title":"An introductory example","text":"<p>A simple idea is to include indices \\(j\\) in \\(S\\) following an i.i.d. Bernoulli  sampling procedure. Sample \\(n\\) r.v.s. \\(X_{j} \\sim \\operatorname{Ber} \\left( q  = 1 / 2 \\right)\\) and let \\(S := \\lbrace j : X_{j} = 1, j \\neq i \\rbrace.\\)  Because of the independent sampling:</p> \\[  p (S) = \\left( \\tfrac{1}{2}  \\right)^k  \\left( 1 - \\tfrac{1}{2}  \\right)^{m -    k} = \\tfrac{1}{2^m}. \\] <p>where \\(k = | S |\\) and \\(m = n - 1.\\) We see that for our constant case \\(q = 1 /  2,\\) one recovers exactly the uniform distribution \\(\\mathcal{U} (D_{- i}),\\) so  we haven't gained much.</p>"},{"location":"value/sampling-weights/#a-better-approach","title":"A better approach","text":"<p>The generalization of the previous idea is dubbed AME. This is a two-stage  sampling procedure which first samples \\(q \\in (0, 1)\\) according to some  distribution \\(\\mathcal{Q}\\) and then samples sets \\(S \\subseteq D_{- i}\\) as  above, with an i.i.d. process using a Bernoulli of parameter \\(q\\): \\(X_{1},  \\ldots, X_{n} \\sim \\operatorname{Ber} (q)\\) and \\(S := \\lbrace j : X_{j} = 1, j  \\neq i \\rbrace.\\) For each \\(q,\\) we have:</p> \\[ p (S|q) = q^k  (1 - q)^{m - k}, \\] <p>and if we pick a uniform \\(q \\sim \\mathcal{U} ((0, 1))\\) and marginalize over  \\(q,\\) a Beta function appears, and:</p> \\[ \\begin{eqnarray*}   p (S) &amp; = &amp; \\int_{0}^1 q^k  (1 - q)^{m - k} \\mathrm{d} q\\\\\\   &amp; = &amp; \\frac{\\Gamma (k + 1) \\Gamma (m - k + 1)}{\\Gamma (m + 2)}\\\\\\   &amp; = &amp; \\frac{k! (m - k) !}{(m + 1) !}\\\\\\   &amp; = &amp; \\frac{1}{n}  \\binom{n - 1}{k}^{- 1}\\\\\\   &amp; = &amp; w_{\\operatorname{shap}} (k) . \\end{eqnarray*} \\] <p>If we sample following this scheme, and define a Monte Carlo approximation \\(\\hat{v}_{\\operatorname{ame} (\\mathcal{U})}\\) like that of (1), it will converge exactly to \\(v_{\\operatorname{shap}}\\) without any correcting factors.</p> <p>Formally, AME is defined as the expected marginal utility over the joint  distribution. Let \\(f\\) be the density of \\(\\mathcal{Q},\\) and let \\(\\mathcal{L} (q,  D_{- i})\\) be the law of the i.i..d. Bernoulli sampling process described above  for fixed \\(q,\\) and \\(\\mathcal{L}_{Q} (D_{- i})\\) that of the whole procedure.  By total expectation:</p> \\[ \\begin{eqnarray*}   v_{\\operatorname{ame}(\\mathcal{Q})} (i) &amp; := &amp; \\mathbb{E}_{S \\sim   \\mathcal{L}_{\\mathcal{Q}} (D_{- i})} [\\Delta_i (S)]\\\\\\   &amp; = &amp; \\mathbb{E}_{q \\sim \\mathcal{Q}}  [\\mathbb{E}_{S \\sim \\mathcal{L} (q,   D_{- i})} [\\Delta_i (S) |q]]\\\\\\   &amp; = &amp; \\sum_{S \\subseteq D_{- i}} \\Delta_i (S)  \\int_{0}^1 p (S|q) f   (q) \\mathrm{d} q \\end{eqnarray*} \\] <p>and</p> \\[  \\int_{0}^1 p (S|q) f (q) \\mathrm{d} q = \\int_{0}^1 q^k  (1 - q)^{m - k}  f(q) \\mathrm{d} q = p (S).  \\] <p>One can also pick a Beta distribution for \\(\\mathcal{Q}\\) and recover  Beta-Shapley. For all the choices, and for the approximate AME method using  sparse regression to estimate values from fewer utility evaluations, consult  the documentation.</p>"},{"location":"value/sampling-weights/#sampling-strategies-permutations","title":"Sampling permutations","text":"<p>Consider now the alternative choice of uniformly sampling permutations \\(\\sigma_{j} \\sim \\mathcal{U} (\\Pi (D)).\\) Recall that we let \\(S_{i}^{\\sigma}\\) be the set of indices preceding \\(i\\) in the permutation \\(\\sigma.\\) We want to determine what the correction for permutation sampling \\(w_{\\operatorname{per}}\\) and Shapley values should be:<sup>1</sup></p> \\[ \\begin{eqnarray*}   \\hat{v}_{\\operatorname{sh}, \\operatorname{per}} (i) &amp; = &amp; \\frac{1}{M}   \\sum_{j = 1}^M w_{\\operatorname{sh}} (| S_{i}^{\\sigma_{j}} |) \\delta   _{i} (S_{i}^{\\sigma_{j}})\\\\\\   &amp; \\underset{M \\rightarrow \\infty}{\\longrightarrow} &amp; \\underset{\\sigma \\sim   \\mathcal{U} (\\Pi (D))}{\\mathbb{E}} [w_{\\operatorname{sh}} (| S_{i}^{\\sigma}   |) \\Delta_i (S_{i}^{\\sigma})]\\\\\\   &amp; = &amp; \\frac{1}{n!}  \\sum_{\\sigma \\in \\Pi (D)} w_{\\operatorname{sh}} (|   S_{i}^{\\sigma} |)  [U (S_{i}^{\\sigma} \\cup \\lbrace i \\rbrace) - U   (S_{i}^{\\sigma})]\\\\\\   &amp; \\overset{(\\star)}{=} &amp; \\frac{1}{n!}  \\sum_{S \\subseteq D_{- i}}   w_{\\operatorname{sh}} (| S |)  (n - 1 - | S |) ! | S | ! [U (S_{+ i}) - U   (S)]\\\\\\   &amp; = &amp; \\sum_{S \\subseteq D_{- i}} w_{\\operatorname{sh}} (| S |)   \\frac{1}{n}  \\binom{n - 1}{| S |}^{- 1} \\Delta_i (S)\\\\\\   &amp; = &amp; \\sum_{S \\subseteq D_{- i}} w_{\\operatorname{sh}} (| S |)   w_{\\operatorname{sh}} (| S |) \\Delta_i (S) . \\end{eqnarray*} \\] <p>We have a duplicate \\(w_{\\operatorname{sh}}\\) coefficient! It's a bit hidden in  the algebra, but it is in fact the probability of sampling a set \\(S\\) given that  it has size \\(k.\\) If we denote it as \\(p (S|k)\\) with some abuse of notation, then:</p> \\[ p (S|k) = \\frac{1}{n}  \\binom{n - 1}{| S |}^{- 1}. \\] <p>To go back to our estimator, we could simply leave out the  \\(v_{\\operatorname{sh}}\\) from the Monte Carlo sums and the estimator would  naturally converge to the Shapley values. We see therefore that for permutation  sampling we can avoid correction factors and simply compute an average of  marginals, and we will recover \\(v_{\\operatorname{sh}}.\\)</p> <p>Here pyDVL allows two choices:</p> <p>The simplest one is to use TMCShapleyValuation, a class that defines its own permutation sampler and sets its valuation coefficient to <code>None</code>, thus signalling the Monte Carlo update procedure not to apply any correction. This is preferred e.g. to reproduce the paper's results.</p> <p>The alternative is to mix a generic ShapleyValuation with any PermutationSampler configuration. Then the corrections will be applied:</p> \\[ \\hat{v}_{\\operatorname{sh}, \\operatorname{per}} (i) := \\frac{1}{m} \\sum_{j = 1}^m w_{\\operatorname{per}} (k) w_{\\operatorname{sh}} (k) \\Delta_i (S_{i}^{\\sigma_{j}}), \\quad k = | S_{i}^{\\sigma_{j}} |  \\] <p>with \\(w_{\\operatorname{per}} (k) = w_{\\operatorname{sh}} (k)^{- 1},\\) in order to obtain an unbiased estimator. This also means that we will always cancel the weights coming from the permutation sampling and can multiply with other coefficients from different methods, for instance BanzhafValuation.<sup>2</sup></p> <p>These same choices apply to MSRBanzhafValuation and a few other \u201cpre-packaged\u201d methods.</p>"},{"location":"value/sampling-weights/#conclusion-the-general-case-and-importance-sampling","title":"Conclusion: The general case and importance sampling","text":"<p>Let's look now at general semi-values, which are of the form:</p> \\[ \\begin{eqnarray*}   v_{\\operatorname{semi}} (i) &amp; = &amp; \\sum_{k = 0}^{n - 1} w (k)  \\sum_{S   \\subseteq D_{- i}^{(k)}} \\Delta_i (S), \\end{eqnarray*} \\] <p>where \\(D^{(k)}_{- i} := \\lbrace S \\subseteq D_{- i} : | S | = k \\rbrace\\) and  \\(\\sum w (k) = 1.\\) Different weight choices lead to different notions of value.  For instance, with \\(w (k) = w_{\\operatorname{sh}}\\) we have Shapley values, and  so on.</p> <p>Our discussion above tried to explain that we have a general way of mixing  sampling strategies and semi-value coefficients. There are three quantities in  play:</p> <ol> <li>The semi-value coefficient \\(w_{\\operatorname{semi}}.\\)</li> <li>The sampling probability emerging in the Monte Carlo process \\(p (S|k).\\)</li> <li>Potentially, a correction factor, \\(w_{\\operatorname{sampler}} := p (S|k)^{-     1}.\\)</li> </ol> <p>When \\(p (S|k) = w_{\\operatorname{semi}}\\) we can tell pyDVL to disable  correction factors by subclassing from SemivalueValuation and overriding the property log_coefficient to return <code>None</code>.</p> <p>Alternatively, we can mix and match sampler and semi-values, effectively performing importance sampling. Let \\(\\mathcal{L}\\) be the law of a sampling procedure such that \\(p_{\\mathcal{L}} (S|k) = w_{\\operatorname{semi}}\\) for some semi-value coefficient, and let \\(\\mathcal{Q}\\) be that of any sampler we choose. Then:</p> \\[ v_{\\operatorname{semi}} (i) = \\mathbb{E}_{\\mathcal{L}} [\\Delta_i (S)]    = \\mathbb{E}_{Q} \\left[ \\frac{w_{\\operatorname{semi}} (S)}{p_{Q} (S|k)}    \\Delta_i (S) \\right] \\] <p>The drawback is that a direct implementation with that much cancelling of coefficients might be inefficient or numerically unstable. Integration issues might arise to compute \\(p_{Q} (S|k)\\) and so on. On the plus side, we can implement any sampling method, like antithetic sampling, and immediately benefit in all semi-value computations.</p> <ol> <li> <p>At step \\((\\star)\\) we have counted the number of permutations before a fixed position of index \\(i\\) and after it, because the utility does not depend on ordering. This allows us to sum over sets instead of permutations.\u00a0\u21a9</p> </li> <li> <p>From a numerical point of view, this can be suboptimal if only used to cancel out terms, since even in log-space, it can induce errors (which are nevertheless lost in the general noise of computing values, but that's another story). However, it makes the implementation very homogeneous and allows for experimentation with different importance sampling schemes.\u00a0\u21a9</p> </li> </ol>"},{"location":"value/semi-values/","title":"Semi-values","text":"<p>The well-known Shapley Value is a particular case of a more general concept called semi-value, which is a generalization to different weighting schemes. A semi-value is any valuation function with the form:</p> \\[ v_\\text{semi}(i) = \\sum_{i=1}^n w(k) \\sum_{S \\subseteq D_{-i}^{k}} [u(S_{+i}) - u(S)], \\] <p>where the coefficients \\(w(k)\\) satisfy the property:</p> \\[\\sum_{k=1}^n \\binom{n-1}{k} w(k) = 1,\\] <p>and \\(D_{-i}^{k}\\) is the set of all sets \\(S\\) of size \\(k\\) that do not include sample \\(x_i\\), \\(S_{+i}\\) is the set \\(S\\) with \\(x_i\\) added, and \\(u\\) is the utility function.</p> <p>With \\(w(k) = \\frac{1}{n} \\binom{n-1}{k}^{-1}\\), we recover the Shapley value.</p> <p>Two additional instances of semi-value are Data Banzhaf (Wang and Jia, 2023)<sup>2</sup> and Beta Shapley (Kwon and Zou, 2022)<sup>3</sup>, which offer improved numerical and rank stability in certain situations.</p> <p>All semi-values, including those two, are implemented in pyDVL by composing different sampling methods and weighting schemes. The abstract class from which they derive is SemiValueValuation, whose main abstract method is the weighting scheme \\(k \\mapsto w(k)\\).</p>"},{"location":"value/semi-values/#general-semi-values","title":"General semi-values","text":"<p>In pyDVL we provide a general method for computing general semi-values with any combination of the three ingredients that define them:</p> <ul> <li>A utility function \\(u\\).</li> <li>A sampling method.</li> <li>A weighting scheme \\(w\\).</li> </ul> <p>You can construct any combination of these three ingredients with subclasses of SemivalueValuation and any of the samplers defined in pydvl.valuation.samplers.</p> <p>Allowing any combination enables testing different importance-sampling schemes and can help when experimenting with models that are more sensitive to changes in training set size.<sup>1</sup></p> <p>For more on this topic and how Monte Carlo sampling interacts with the semi-value coefficient and the sampler probabilities, see Sampling strategies for semi-values.</p> <ol> <li> <p>Note however that Data Banzhaf has shown to be among the most robust to variance in the utility function, in the sense of rank stability, across a range of models and datasets (Wang and Jia, 2023)<sup>2</sup>.\u00a0\u21a9</p> </li> <li> <p>Wang, J.T., Jia, R., 2023. Data Banzhaf: A Robust Data Valuation Framework for Machine Learning, in: Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics, PMLR, pp. 6388--6421.\u00a0\u21a9\u21a9</p> </li> <li> <p>Kwon, Y., Zou, J., 2022. Beta Shapley: A Unified and [Noise-reduced Data Valuation Framework]{.nocase} for Machine Learning, in: Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022,. Presented at the AISTATS 2022, PMLR, Valencia, Spain.\u00a0\u21a9</p> </li> </ol>"},{"location":"value/shapley/","title":"Shapley value","text":"<p>The Shapley method is an approach to compute data values originating in cooperative game theory. Shapley values are a common way of assigning payoffs to each participant in a cooperative game (i.e. one in which players can form coalitions) in a way that ensures that certain axioms are fulfilled.</p> <p>pyDVL implements several methods for the computation and approximation of Shapley values. Empirically, one of the most useful methods is the so-called  Truncated Monte Carlo Shapley (Ghorbani and Zou, 2019)<sup>2</sup>, but several approximations exist with different convergence rates and computational costs.</p>"},{"location":"value/shapley/#combinatorial-shapley-intro","title":"Combinatorial Shapley","text":"<p>The first algorithm is just a verbatim implementation of the definition below. As such it returns as exact a value as the utility function allows (see what this means in Problems of Data Values).</p> <p>The value \\(v\\) of the \\(i\\)-th sample in dataset \\(D\\) wrt. utility \\(u\\) is computed as a weighted sum of its marginal utility wrt. every possible coalition of training samples within the training set:</p> <p></p> \\[ \\begin{equation} v_\\text{shap}(i) = \\frac{1}{n} \\sum_{S \\subseteq D_{-i}} \\binom{n-1}{ | S | }^{-1} [u(S_{+i}) \u2212 u(S)],   \\label{combinatorial-shapley}\\tag{1} \\end{equation} \\] <p>where \\(D_{-i}\\) denotes the set of samples in \\(D\\) excluding \\(x_i,\\) and \\(S_{+i}\\) denotes the set \\(S\\) with \\(x_i\\) added.<sup>1</sup></p> Computing exact Shapley values <pre><code>from joblib import parallel_config\nfrom pydvl.valuation import (\n    Dataset, ModelUtility, SupervisedScorer, ShapleyValuation\n)\n\ntrain, test = SomeVerySmallDatasets()\nmodel = ...\nscorer = SupervisedScorer(model, test, default=..)\nutility = ModelUtility(model, scorer)\nsampler = DeterministicUniformSampler()\nvaluation = ShapleyValuation(utility, sampler, NoStopping(sampler))\n\nwith parallel_config(n_jobs=-1):\n    valuation.fit(train)\nresult = valuation.values()\n</code></pre> <p>We can convert the return value to a pandas.DataFrame with the <code>to_dataframe</code> method. Please refer to the introduction to data valuation and to the documentation in ValuationResult for more information.</p>"},{"location":"value/shapley/#monte-carlo-combinatorial-shapley-intro","title":"Monte Carlo Combinatorial Shapley","text":"<p>Because the number of subsets \\(S \\subseteq D_{-i}\\) is \\(2^{ | D | - 1 },\\) one must typically resort to approximations. The simplest one is done via Monte Carlo sampling of the powerset \\(\\mathcal{P}(D).\\) In pyDVL this simple technique is called \"Monte Carlo Combinatorial\". The method has very poor converge rate and others are preferred, but if desired, usage follows the same pattern:</p> Monte Carlo Combinatorial Shapley values <pre><code>from pydvl.valuation import (\n    ShapleyValuation,\n    ModelUtility,\n    SupervisedScorer,\n    PermutationSampler,\n    MaxSamples\n)\n\nmodel = SomeSKLearnModel()\nscorer = SupervisedScorer(\"accuracy\", test_data, default=0.0)\nutility = ModelUtility(model, scorer)\nsampler = UniformSampler(seed=42)\nstopping = MaxSamples(sampler, 5000)\nvaluation = ShapleyValuation(utility, sampler, stopping)\nwith parallel_config(n_jobs=16):\n    valuation.fit(training_data)\nresult = valuation.values()\n</code></pre> <p>Note the usage of the object [MaxSamples][pydvl.value.stopping.MaxSamples] as the stopping condition, which takes the sampler as argument. This is a special instance of a StoppingCriterion. More examples which are not tied to the sampler are MaxTime (stops after a certain time), MinUpdates (looks at the number of updates to the individual values), and AbsoluteStandardError (not very reliable as a stopping criterion), among others.</p>"},{"location":"value/shapley/#stratified-shapley-value","title":"A stratified approach","text":"<p>Let's decompose definition (1) into \"layers\", one per subset size \\(k,\\) by writing it in the equivalent form:<sup>1</sup></p> \\[v_\\text{shap}(i) = \\sum_{k=0}^{n-1} \\frac{1}{n} \\binom{n-1}{k}^{-1}      \\sum_{S \\subseteq D_{-i}^{k}} \\Delta_i(S).\\] <p>Here \\(D_i^{k}\\) is the set of all subsets of size \\(k\\) in the complement  of \\(\\{i\\}.\\) Since there are \\(\\binom{n-1}{k}\\) such sets, the above is an average over all \\(n\\) set sizes \\(k\\) of the average marginal contributions of the point \\(i\\) to all sets of size \\(k.\\)</p> <p>We can now devise a sampling scheme over the powerset of \\(N_{-i}\\) that yields this expression:</p> <ol> <li>Sample \\(k\\) uniformly from \\(\\{0, ..., n-1\\}.\\)</li> <li>Sample \\(S\\) uniformly from the powerset of \\(N_{-i}^k.\\)</li> </ol> <p>Call this distribution \\(\\mathcal{L}_k.\\) Then</p> \\[ \\begin{eqnarray*}     \\mathbb{E}_{S \\sim \\mathcal{L}} [\\Delta_i (S)]              &amp; = &amp; \\sum_{k = 0}^{n - 1} \\sum_{S \\subseteq N_{- i}^k}                    \\Delta_i (S) p (S|k) p (k) \\\\             &amp; = &amp; \\sum_{k = 0}^{n - 1} \\sum_{S \\subseteq N_{- i}^k} \\Delta_i (S)                   \\binom{n - 1}{k}^{- 1} \\frac{1}{n} \\\\             &amp; = &amp; v_{\\text{sh}}(i). \\end{eqnarray*} \\] <p>The choice \\(p(k) = 1/n\\) is implemented in  StratifiedShapleyValuation but can be changed to any other distribution over \\(k.\\) (Wu et al., 2023)<sup>3</sup> introduced VRDS sampling as a way to reduce the variance of the estimator.</p> Stratified Shapley <p>The specific instance of stratified sampling described above can be directly used by instantiating a StratifiedShapleyValuation object. For more general use cases, use ShapleyValuation with a custom sampler, for instance VRDSSampler. Note the use of the [History][pydvl.value.stopping.History] object, a stopping which does not stop, but records the trace of value updates in a rolling memory. The data can then be used to check for convergence, debugging, plotting, etc.</p> <pre><code>from pydvl.valuation import StratifiedShapleyValuation, MinUpdates, History\ntraining_data, test_data = Dataset.from_arrays(...)\nmodel = ...\nscorer = SupervisedScorer(model, test_data, default=..., range=...)\nutility = ModelUtility(model, scorer)\nvaluation = StratifiedShapleyValuation(\n    utility=utility,\n    is_done=MinUpdates(min_updates) | History(n_steps=min_updates),\n    batch_size=batch_size,\n    seed=seed,\n    skip_converged=True,\n    progress=True,\n)\nwith parallel_config(n_jobs=-4):\n    valuation.fit(training_data)\nresults = valuation.values()\n</code></pre>"},{"location":"value/shapley/#permutation-shapley-intro","title":"Permutation Shapley","text":"<p>An equivalent way of computing Shapley values (<code>ApproShapley</code>) appeared in (Castro et al., 2009)<sup>4</sup> and is the basis for the method most often used in practice. It uses permutations over indices instead of subsets:</p> \\[ v_u(i) = \\frac{1}{n!} \\sum_{\\sigma \\in \\Pi(n)} [u(S_{i}^{\\sigma} \\cup \\{i\\}) \u2212 u(S_{i}^{\\sigma})], \\] <p>where \\(S_{i}^{\\sigma}\\) denotes the set of indices in permutation sigma before the position where \\(i\\) appears. To approximate this sum (which has \\(\\mathcal{O}(n!)\\) terms!) one uses Monte Carlo sampling of permutations, something which has surprisingly low sample complexity. One notable difference wrt. the combinatorial approach above is that the approximations always fulfill the efficiency axiom of Shapley, namely \\(\\sum_{i=1}^n \\hat{v}_i = u(D)\\) (see (Castro et al., 2009)<sup>4</sup>, Proposition 3.2).</p> A note about implementation <p>The definition above uses all permutations to update one datapoint \\(i\\). However, in practice, instead of searching for the position of a fixed index in every permutation, one can use a single permutation to update all datapoints, by iterating through it and updating the value for the index at the current position. This has the added benefit of allowing to use the utility for the previous index to compute the marginal utility for the current one, thus halving the number of utility calls. This strategy is implemented in PermutationEvaluationStrategy, and is automatically selected when using any of the permutation samplers.</p>"},{"location":"value/shapley/#tmcs-intro","title":"Truncated Monte Carlo Shapley","text":"<p>By adding two types of early stopping, the result is the so-called Truncated Monte Carlo Shapley (Ghorbani and Zou, 2019)<sup>2</sup>, which is efficient enough to be useful in applications. </p> <p>The first is simply a convergence criterion, of which there are several to choose from. The second is a criterion to truncate the iteration over single permutations. RelativeTruncation chooses to stop iterating over samples in a permutation when the marginal utility becomes too small. The method is available through the class TMCShapleyValuation.</p> <p>However, being a heuristic to permutation sampling, it can be \"manually\" implemented by choosing a RelativeTruncation for a PermutationSampler when configuring ShapleyValuation (note however that this introduces some correcting factors, see  Sampling strategies for semi-values).</p> <p>You can see this method in action in this example using the Spotify dataset.</p> Truncated Monte Carlo Shapley values <p>Use of this object follows the same pattern as the previous examples, except that separate instantiation of the sampler is not necessary anymore. This has the drawback that we cannot use [MaxSamples][pydvl.value.stopping.MaxSamples] as stopping criterion anymore since it requires the sampler. To work around this, use ShapleyValuation directly.</p> <pre><code>from pydvl.valuation import (\n    MinUpdates\n    ModelUtility,\n    PermutationSampler,\n    SupervisedScorer,\n    RelativeTruncation,\n    TMCShapleyValuation,\n)\n\nmodel = SomeSKLearnModel()\nscorer = SupervisedScorer(\"accuracy\", test_data, default=0)\nutility = ModelUtility(model, scorer, ...)\ntruncation = RelativeTruncation(rtol=0.05)\nstopping = MinUpdates(5000)\nvaluation = TMCShapleyValuation(utility, truncation, stopping)\nwith parallel_config(n_jobs=16):\n    valuation.fit(training_data)\nresult = valuation.values()\n</code></pre>"},{"location":"value/shapley/#other-approximation-methods","title":"Other approximation methods","text":"<p>As already mentioned, with the architecture of ShapleyValuation it is possible to try different importance-sampling schemes by swapping the sampler. Besides TMCS we also have Owen sampling (Okhrati and Lipani, 2021)<sup>5</sup>, and Beta Shapley (Kwon and Zou, 2022)<sup>6</sup> when \\(\\alpha = \\beta = 1.\\)</p> <p>A different approach is via a SAT problem, as done in Group Testing Shapley (Jia et al., 2019)<sup>7</sup>.</p> <p>Yet another, which is applicable to any utility-based valuation method, is Data Utility Learning (Wang et al., 2022)<sup>8</sup>. This method learns a model of the utility function during a warmup phase, and then uses it to speed up marginal utility computations.</p>"},{"location":"value/shapley/#model-specific-methods","title":"Model-specific methods","text":"<p>Shapley values can have a closed form expression or a simpler approximation scheme when the model class is restricted. The prime example is kNN-Shapley (Jia et al., 2019)<sup>9</sup>, which is exact for the kNN model, and is \\(O(n_\\text{test}\\  n \\log n).\\)</p> <ol> <li> <p>The quantity \\(u(S_{+i}) \u2212 u(S)\\) is called the   marginal utility of the sample \\(x_i\\) (with   respect to \\(S\\)), and we will often denote it by \\(\\Delta_i(S, u),\\) or, when no   confusion is possible, simply \\(\\Delta_i(S).\\) \u21a9\u21a9</p> </li> <li> <p>Ghorbani, A., Zou, J., 2019. Data Shapley: Equitable Valuation of Data for Machine Learning, in: Proceedings of the 36th International Conference on Machine Learning, PMLR. Presented at the International Conference on Machine Learning (ICML 2019), PMLR, pp. 2242--2251.\u00a0\u21a9\u21a9</p> </li> <li> <p>Wu, M., Jia, R., Lin, C., Huang, W., Chang, X., 2023. Variance reduced Shapley value estimation for trustworthy data valuation. Computers\\ &amp; Operations Research 159, 106305. https://doi.org/10.1016/j.cor.2023.106305 \u21a9</p> </li> <li> <p>Castro, J., G\u00f3mez, D., Tejada, J., 2009. Polynomial calculation of the Shapley value based on sampling. Computers\\ &amp; Operations Research, Selected papers presented at the Tenth International Symposium on Locational Decisions (ISOLDE X) 36, 1726--1730. https://doi.org/10.1016/j.cor.2008.04.004 \u21a9\u21a9</p> </li> <li> <p>Okhrati, R., Lipani, A., 2021. A Multilinear Sampling Algorithm to Estimate Shapley Values, in: 2020 25th International Conference on Pattern Recognition (ICPR). Presented at the 2020 25th International Conference on Pattern Recognition (ICPR), IEEE, pp. 7992--7999. https://doi.org/10.1109/ICPR48806.2021.9412511 \u21a9</p> </li> <li> <p>Kwon, Y., Zou, J., 2022. Beta Shapley: A Unified and [Noise-reduced Data Valuation Framework]{.nocase} for Machine Learning, in: Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022,. Presented at the AISTATS 2022, PMLR, Valencia, Spain.\u00a0\u21a9</p> </li> <li> <p>Jia, R., Dao, D., Wang, B., Hubis, F.A., Hynes, N., G\u00fcrel, N.M., Li, B., Zhang, C., Song, D., Spanos, C.J., 2019. Towards Efficient Data Valuation Based on the Shapley Value, in: Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics. Presented at the International Conference on Artificial Intelligence and Statistics (AISTATS), PMLR, pp. 1167--1176.\u00a0\u21a9</p> </li> <li> <p>Wang, T., Yang, Y., Jia, R., 2022. Improving [Cooperative Game Theory-based Data Valuation]{.nocase} via Data Utility Learning. Presented at the International Conference on Learning Representations (ICLR 2022). Workshop on Socially Responsible Machine Learning, arXiv. https://doi.org/10.48550/arXiv.2107.06336 \u21a9</p> </li> <li> <p>Jia, R., Dao, D., Wang, B., Hubis, F.A., Gurel, N.M., Li, B., Zhang, C., Spanos, C., Song, D., 2019. Efficient task-specific data valuation for nearest neighbor algorithms. Proc. VLDB Endow. 12, 1610--1623. https://doi.org/10.14778/3342263.3342637 \u21a9</p> </li> </ol>"},{"location":"value/the-core/","title":"Core values","text":"<p>Shapley values define a fair way to distribute payoffs amongst all participants (training points) when they form a grand coalition, i.e. when the model is trained on the whole dataset. But they do not consider the question of stability: under which conditions do all participants in a game form the grand coalition? Are the payoffs distributed in such a way that prioritizes its formation?</p> <p>The Core is another solution concept in cooperative game theory that attempts to ensure stability in the sense that it provides the set of feasible payoffs that cannot be improved upon by a sub-coalition. This can be interesting for some applications of data valuation because it yields values consistent with training on the whole dataset, avoiding the spurious selection of subsets.</p> <p>It satisfies the following 2 properties:</p> <ul> <li> <p>Efficiency:   The payoffs are distributed such that it is not possible to make any   participant better off without making another one worse off.   \\(\\sum_{i \\in D} v(i) = u(D).\\)</p> </li> <li> <p>Coalitional rationality:   The sum of payoffs to the agents in any coalition \\(S\\) is at least as large as   the amount that these agents could earn by forming a coalition on their own.   \\(\\sum_{i \\in S} v(i) \\geq u(S), \\forall S \\subset D.\\)</p> </li> </ul> <p>The Core was first introduced into data valuation by (Yan and Procaccia, 2021)<sup>1</sup>, in the following form.</p>"},{"location":"value/the-core/#least-core-values","title":"Least Core values","text":"<p>Unfortunately, for many cooperative games the Core may be empty. By relaxing the coalitional rationality property by a subsidy \\(e \\gt 0\\), we are then able to find approximate payoffs:</p> \\[ \\sum_{i\\in S} v(i) + e \\geq u(S), \\forall S \\subset D, S \\neq \\emptyset \\ ,\\] <p>The Least Core (LC) values \\(\\{v\\}\\) for utility \\(u\\) are computed by solving the following linear program:</p> \\[ \\begin{array}{lll} \\text{minimize} &amp; e &amp; \\\\ \\text{subject to} &amp; \\sum_{i\\in D} v(i) = u(D) &amp; \\\\ &amp; \\sum_{i\\in S} v(i) + e \\geq u(S) &amp;, \\forall S \\subset D, S \\neq \\emptyset  \\\\ \\end{array} \\] <p>Note that solving this program yields a set of solutions \\(\\{v_j:N \\rightarrow \\mathbb{R}\\}\\), whereas the Shapley value is a single function \\(v\\). In order to obtain a single valuation to use, one breaks ties by solving a quadratic program to select the \\(v\\) in the LC with the smallest \\(\\ell_2\\) norm. This is called the egalitarian least core.</p>"},{"location":"value/the-core/#exact-least-core","title":"Exact Least Core","text":"<p>This first algorithm is just a verbatim implementation of the definition above. It is available through ExactLeastCoreValuation. It computes all constraints for the linear problem by evaluating the utility on every subset of the training data, and returns as exact a value as the utility function allows (see what this means in Problems of Data Values).</p> Computing exact Least-Core values <pre><code>from joblib import parallel_config\nfrom pydvl.valuation import (\n    Dataset, ExactLeastCoreValuation, ModelUtility, SupervisedScorer\n)\n\ntrain, test = Dataset.from_arrays(...)\nmodel = ...\nscorer = SupervisedScorer(model, test, default=..)\nutility = ModelUtility(model, scorer)\nvaluation = ExactLeastCoreValuation(utility, subsidy)\n\nwith parallel_config(n_jobs=12):\n    valuation.fit(data)\n</code></pre>"},{"location":"value/the-core/#monte-carlo-least-core","title":"Monte Carlo Least Core","text":"<p>Because the number of subsets \\(S \\subseteq D \\setminus \\{i\\}\\) is \\(2^{ | D | - 1 }\\), one typically must resort to approximations.</p> <p>The simplest one consists in using a fraction of all subsets for the constraints. (Yan and Procaccia, 2021)<sup>1</sup> show that a quantity of order \\(\\mathcal{O}((n - \\log \\Delta ) / \\delta^2)\\) is enough to obtain a so-called \\(\\delta\\)-approximate least core with high probability. I.e. the following property holds with probability \\(1-\\Delta\\) over the choice of subsets:</p> \\[ \\mathbb{P}_{S\\sim D}\\left[\\sum_{i\\in S} v(i) + e^{*} \\geq u(S)\\right] \\geq 1 - \\delta, \\] <p>where \\(e^{*}\\) is the optimal least core subsidy. This approximation is implemented in MonteCarloLeastCoreValuation and it's usage follows the same pattern as above:</p> Computing approximate Least-Core values <pre><code>from joblib import parallel_config\nfrom pydvl.valuation import (\n    Dataset, MonteCarloLeastCoreValuation, ModelUtility, SupervisedScorer\n)\n\ntrain, test = Dataset.from_arrays(...)\nmodel = ...\nscorer = SupervisedScorer(model, test, default=..)\nutility = ModelUtility(model, scorer)\nvaluation = MonteCarloLeastCoreValuation(utility, subsidy)\n\nwith parallel_config(n_jobs=12):\n    valuation.fit(data)\n</code></pre> <p>Note</p> <p>Although any number is supported, it is best to choose <code>n_iterations</code> to be at least equal to the number of data points.</p>"},{"location":"value/the-core/#least-core-comparison","title":"Method comparison","text":"<p>The TransferLab team reproduced the results of the original paper in a publication for the 2022 MLRC (Benmerzoug and Benito Delgado, 2023)<sup>2</sup>.</p> <p> Best sample removal on binary image classification </p> <p>Roughly speaking, MCLC performs better in identifying high value points, as measured by best-sample removal tasks. In all other aspects, it performs worse or similarly to TMCS at comparable sample budgets. But using an equal number of subsets is more computationally expensive because of the need to solve large linear and quadratic optimization problems.</p> <p> Worst sample removal on binary image classification </p> <p>For these reasons we recommend some variation of SV like TMCS for outlier detection, data cleaning and pruning, and perhaps MCLC for the selection of interesting points to be inspected for the improvement of data collection or model design.</p> <ol> <li> <p>Yan, T., Procaccia, A.D., 2021. If You Like Shapley Then You'll Love the Core, in: Proceedings of the 35th AAAI Conference on Artificial Intelligence. Presented at the AAAI Conference on Artificial Intelligence, Association for the Advancement of Artificial Intelligence, Virtual conference, pp. 5751--5759. https://doi.org/10.1609/aaai.v35i6.16721 \u21a9\u21a9</p> </li> <li> <p>Benmerzoug, A., Benito Delgado, M. de, 2023. [Re] If you like Shapley, then you'll love the core. ReScience C 9. https://doi.org/10.5281/zenodo.8173733 \u21a9</p> </li> </ol>"}]}

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pydvl.org/stable/api/pydvl/influence/torch/influence_function_model/">
      
      
        <link rel="prev" href="../functional/">
      
      
        <link rel="next" href="../pre_conditioner/">
      
      
      <link rel="icon" href="../../../../../assets/signet.svg">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.16">
    
    
      
        <title>Influence function model - pyDVL</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.bcfcd587.min.css">
      
        
        <link rel="stylesheet" href="../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../../css/extra.css">
    
      <link rel="stylesheet" href="../../../../../css/grid-cards.css">
    
      <link rel="stylesheet" href="../../../../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../../../../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  



    
    
    
    
    <script async defer
            src="https://scripts.simpleanalyticscdn.com/latest.js"
            data-collect-dnt="true"
            data-hostname="pydvl.org"></script>
    <noscript>
        <img src="https://queue.simpleanalyticscdn.com/noscript.gif?collect-dnt=True&hostname=pydvl.org&path=api/pydvl/influence/torch/influence_function_model/"
             alt=""
             referrerpolicy="no-referrer-when-downgrade">
    </noscript>

  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Influence function model - pyDVL" >
      
        <meta  property="og:description"  content="None" >
      
        <meta  property="og:image"  content="https://pydvl.org/stable/assets/images/social/api/pydvl/influence/torch/influence_function_model.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://pydvl.org/stable/api/pydvl/influence/torch/influence_function_model/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Influence function model - pyDVL" >
      
        <meta  name="twitter:description"  content="None" >
      
        <meta  name="twitter:image"  content="https://pydvl.org/stable/assets/images/social/api/pydvl/influence/torch/influence_function_model.png" >
      
    
    
   <link href="../../../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#pydvl.influence.torch.influence_function_model" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
<div class="announcement">
    <aside class="announcement-content">
        pyDVL is in an early stage of development. Expect changes to functionality and the API until version 1.0.0.
    </aside>
</div>

          </div>
          
        </aside>
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="pyDVL" class="md-header__button md-logo" aria-label="pyDVL" data-md-component="logo">
      
  <img src="../../../../../assets/signet.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            pyDVL
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Influence function model
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/aai-institute/pyDVL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    aai-institute/pyDVL
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../getting-started/" class="md-tabs__link">
        
  
    
  
  Getting Started

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../value/" class="md-tabs__link">
        
  
    
  
  Data Valuation

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../influence/" class="md-tabs__link">
        
  
    
  
  The Influence Function

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../examples/" class="md-tabs__link">
        
  
    
  
  Examples

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../../" class="md-tabs__link">
          
  
  Code

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="pyDVL" class="md-nav__button md-logo" aria-label="pyDVL" data-md-component="logo">
      
  <img src="../../../../../assets/signet.svg" alt="logo">

    </a>
    pyDVL
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/aai-institute/pyDVL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    aai-institute/pyDVL
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../getting-started/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Getting Started
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Getting Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../getting-started/first-steps/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    First steps
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../getting-started/applications/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Applications
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../getting-started/benchmarking/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Benchmarking
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../getting-started/methods/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Methods
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../getting-started/advanced-usage/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Advanced usage
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../getting-started/glossary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Glossary
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../value/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Data Valuation
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Data Valuation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../value/shapley/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Shapley values
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../value/semi-values/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Semi-values
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../value/the-core/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Core
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../value/classwise-shapley/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Class-wise Shapley
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../influence/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    The Influence Function
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            The Influence Function
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../influence/influence_function_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Influence Function Model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../influence/scaling_computation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Scaling Computation
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../examples/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Examples
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1" >
        
          
          <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Data Valuation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_1">
            <span class="md-nav__icon md-icon"></span>
            Data Valuation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/shapley_basic_spotify/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Shapley values
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/shapley_knn_flowers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KNN Shapley
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/shapley_utility_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data utility learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/least_core_basic/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Least Core
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/data_oob/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data OOB
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/msr_banzhaf_digits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Banzhaf Semivalues
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
        
          
          <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Influence Function
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            Influence Function
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/influence_imagenet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    For CNNs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/influence_synthetic/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    For mislabeled data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/influence_wine/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    For outlier detection
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../examples/influence_sentiment_analysis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    For language models
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Code
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Code
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1" id="__nav_6_1_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_6_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6_1">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_1" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Influence
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_1" id="__nav_6_1_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_1_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6_1_1">
            <span class="md-nav__icon md-icon"></span>
            Influence
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../array/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Array
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../base_influence_function_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base influence function model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../influence_calculator/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Influence calculator
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_1_4" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Torch
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_1_4" id="__nav_6_1_1_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_1_1_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6_1_1_4">
            <span class="md-nav__icon md-icon"></span>
            Torch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../functional/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Functional
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Influence function model
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Influence function model
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;TorchInfluenceFunctionModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" TorchInfluenceFunctionModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.is_fitted" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;is_fitted
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.fit" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.DirectInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;DirectInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" DirectInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.fit" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.CgInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;CgInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" CgInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.CgInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.CgInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.CgInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.LissaInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LissaInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" LissaInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.LissaInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.LissaInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.LissaInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;ArnoldiInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" ArnoldiInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.fit" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;EkfacInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" EkfacInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.fit" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_by_layer" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_by_layer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influence_factors_by_layer" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors_by_layer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_from_factors_by_layer" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors_by_layer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.explore_hessian_regularization" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;explore_hessian_regularization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NystroemSketchInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NystroemSketchInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pre_conditioner/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pre conditioner
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../util/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Util
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../parallel/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Parallel
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_2" id="__nav_6_1_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_2">
            <span class="md-nav__icon md-icon"></span>
            Parallel
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/backend/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Backend
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/map_reduce/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Map reduce
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_2_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../parallel/backends/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Backends
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_2_4" id="__nav_6_1_2_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_1_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_2_4">
            <span class="md-nav__icon md-icon"></span>
            Backends
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/backends/joblib/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Joblib
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/backends/ray/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ray
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_2_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../parallel/futures/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Futures
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_2_5" id="__nav_6_1_2_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_1_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_2_5">
            <span class="md-nav__icon md-icon"></span>
            Futures
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../parallel/futures/ray/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ray
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../reporting/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Reporting
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_3" id="__nav_6_1_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_3">
            <span class="md-nav__icon md-icon"></span>
            Reporting
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reporting/plots/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Plots
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../reporting/scores/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Scores
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../utils/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_4" id="__nav_6_1_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_1_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_4">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataset
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/exceptions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exceptions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/functional/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Functional
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/numeric/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Numeric
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/progress/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Progress
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/score/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Score
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/status/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Status
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Types
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/utility/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utility
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_4_11" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../utils/caching/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Caching
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_4_11" id="__nav_6_1_4_11_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_1_4_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_4_11">
            <span class="md-nav__icon md-icon"></span>
            Caching
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/caching/base/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/caching/config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/caching/disk/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Disk
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/caching/memcached/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memcached
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../utils/caching/memory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memory
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../value/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Value
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_5" id="__nav_6_1_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_6_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_5">
            <span class="md-nav__icon md-icon"></span>
            Value
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/games/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Games
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/result/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Result
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/sampler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sampler
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/semivalues/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Semivalues
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/stopping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Stopping
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_5_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../value/least_core/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Least core
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_5_6" id="__nav_6_1_5_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_1_5_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_5_6">
            <span class="md-nav__icon md-icon"></span>
            Least core
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/least_core/common/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Common
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/least_core/montecarlo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Montecarlo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/least_core/naive/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Naive
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_5_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../value/loo/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Loo
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_5_7" id="__nav_6_1_5_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_1_5_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_5_7">
            <span class="md-nav__icon md-icon"></span>
            Loo
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/loo/loo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Loo
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_5_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../value/oob/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Oob
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_5_8" id="__nav_6_1_5_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_1_5_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_5_8">
            <span class="md-nav__icon md-icon"></span>
            Oob
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/oob/oob/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Oob
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6_1_5_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../value/shapley/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Shapley
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6_1_5_9" id="__nav_6_1_5_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_6_1_5_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6_1_5_9">
            <span class="md-nav__icon md-icon"></span>
            Shapley
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/shapley/classwise/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Classwise
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/shapley/common/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Common
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/shapley/gt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gt
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/shapley/knn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Knn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/shapley/montecarlo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Montecarlo
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/shapley/naive/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Naive
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/shapley/owen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Owen
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/shapley/truncated/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Truncated
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../value/shapley/types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Types
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../CHANGELOG/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Changelog
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../CONTRIBUTING/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Development Guidelines
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;TorchInfluenceFunctionModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" TorchInfluenceFunctionModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.is_fitted" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;is_fitted
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.fit" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.DirectInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;DirectInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" DirectInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.fit" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.CgInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;CgInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" CgInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.CgInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.CgInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.CgInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.LissaInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;LissaInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" LissaInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.LissaInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.LissaInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.LissaInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;ArnoldiInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" ArnoldiInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.fit" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;EkfacInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" EkfacInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.fit" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_by_layer" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_by_layer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influence_factors_by_layer" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors_by_layer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_from_factors_by_layer" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors_by_layer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.explore_hessian_regularization" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;explore_hessian_regularization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;NystroemSketchInfluence
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" NystroemSketchInfluence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influence_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influence_factors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences_from_factors" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;influences_from_factors
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<div class="doc doc-object doc-module">



<h1 id="pydvl.influence.torch.influence_function_model" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-module"></code>          <span class="doc doc-object-name doc-module-name">pydvl.influence.torch.influence_function_model</span>


<a href="#pydvl.influence.torch.influence_function_model" class="headerlink" title="Permanent link">&para;</a></h1>

  <div class="doc doc-contents first">
  
      <p>This module implements several implementations of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel">InfluenceFunctionModel</a>
utilizing PyTorch.</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <span class="doc doc-object-name doc-class-name">TorchInfluenceFunctionModel</span>


<a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">TorchInfluenceFunctionModel</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">model</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">[[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="p">)</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceFunctionModel" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel">InfluenceFunctionModel</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>, <a class="autorefs autorefs-external" title="torch.utils.data.DataLoader" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a>]</code>, <code><a class="autorefs autorefs-external" title="abc.ABC" href="https://docs.python.org/3/library/abc.html#abc.ABC">ABC</a></code></p>

  
      <p>Abstract base class for influence computation related to torch models</p>

                <details class="quote">
                  <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
                  <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-64">64</a></span>
<span class="normal"><a href="#__codelineno-0-65">65</a></span>
<span class="normal"><a href="#__codelineno-0-66">66</a></span>
<span class="normal"><a href="#__codelineno-0-67">67</a></span>
<span class="normal"><a href="#__codelineno-0-68">68</a></span>
<span class="normal"><a href="#__codelineno-0-69">69</a></span>
<span class="normal"><a href="#__codelineno-0-70">70</a></span>
<span class="normal"><a href="#__codelineno-0-71">71</a></span>
<span class="normal"><a href="#__codelineno-0-72">72</a></span>
<span class="normal"><a href="#__codelineno-0-73">73</a></span>
<span class="normal"><a href="#__codelineno-0-74">74</a></span>
<span class="normal"><a href="#__codelineno-0-75">75</a></span>
<span class="normal"><a href="#__codelineno-0-76">76</a></span>
<span class="normal"><a href="#__codelineno-0-77">77</a></span>
<span class="normal"><a href="#__codelineno-0-78">78</a></span>
<span class="normal"><a href="#__codelineno-0-79">79</a></span>
<span class="normal"><a href="#__codelineno-0-80">80</a></span>
<span class="normal"><a href="#__codelineno-0-81">81</a></span>
<span class="normal"><a href="#__codelineno-0-82">82</a></span>
<span class="normal"><a href="#__codelineno-0-83">83</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-64"><a id="__codelineno-0-64" name="__codelineno-0-64"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="__span-0-65"><a id="__codelineno-0-65" name="__codelineno-0-65"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-66"><a id="__codelineno-0-66" name="__codelineno-0-66"></a>    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="__span-0-67"><a id="__codelineno-0-67" name="__codelineno-0-67"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span id="__span-0-68"><a id="__codelineno-0-68" name="__codelineno-0-68"></a><span class="p">):</span>
</span><span id="__span-0-69"><a id="__codelineno-0-69" name="__codelineno-0-69"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
</span><span id="__span-0-70"><a id="__codelineno-0-70" name="__codelineno-0-70"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
</span><span id="__span-0-71"><a id="__codelineno-0-71" name="__codelineno-0-71"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_n_parameters</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
</span><span id="__span-0-72"><a id="__codelineno-0-72" name="__codelineno-0-72"></a>        <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">]</span>
</span><span id="__span-0-73"><a id="__codelineno-0-73" name="__codelineno-0-73"></a>    <span class="p">)</span>
</span><span id="__span-0-74"><a id="__codelineno-0-74" name="__codelineno-0-74"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_model_device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span>
</span><span id="__span-0-75"><a id="__codelineno-0-75" name="__codelineno-0-75"></a>        <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</span><span id="__span-0-76"><a id="__codelineno-0-76" name="__codelineno-0-76"></a>    <span class="p">)</span>
</span><span id="__span-0-77"><a id="__codelineno-0-77" name="__codelineno-0-77"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_model_params</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="__span-0-78"><a id="__codelineno-0-78" name="__codelineno-0-78"></a>        <span class="n">k</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span>
</span><span id="__span-0-79"><a id="__codelineno-0-79" name="__codelineno-0-79"></a>    <span class="p">}</span>
</span><span id="__span-0-80"><a id="__codelineno-0-80" name="__codelineno-0-80"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_model_dtype</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span>
</span><span id="__span-0-81"><a id="__codelineno-0-81" name="__codelineno-0-81"></a>        <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</span><span id="__span-0-82"><a id="__codelineno-0-82" name="__codelineno-0-82"></a>    <span class="p">)</span>
</span><span id="__span-0-83"><a id="__codelineno-0-83" name="__codelineno-0-83"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.is_fitted" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>          <span class="doc doc-object-name doc-attribute-name">is_fitted</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

<a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.is_fitted" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">is_fitted</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Override this, to expose the fitting status of the instance.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.fit" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">fit</span>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-abstractmethod"><code>abstractmethod</code></small>
  </span>

<a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.fit" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n"><span title="pydvl.influence.base_influence_function_model.DataLoaderType">DataLoaderType</span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceFunctionModel" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel">InfluenceFunctionModel</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Override this method to fit the influence function model to training data,
e.g. pre-compute hessian matrix or matrix decompositions</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>data</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><span title="pydvl.influence.base_influence_function_model.DataLoaderType">DataLoaderType</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceFunctionModel" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceFunctionModel">InfluenceFunctionModel</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>The fitted instance</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/base_influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-76">76</a></span>
<span class="normal"><a href="#__codelineno-0-77">77</a></span>
<span class="normal"><a href="#__codelineno-0-78">78</a></span>
<span class="normal"><a href="#__codelineno-0-79">79</a></span>
<span class="normal"><a href="#__codelineno-0-80">80</a></span>
<span class="normal"><a href="#__codelineno-0-81">81</a></span>
<span class="normal"><a href="#__codelineno-0-82">82</a></span>
<span class="normal"><a href="#__codelineno-0-83">83</a></span>
<span class="normal"><a href="#__codelineno-0-84">84</a></span>
<span class="normal"><a href="#__codelineno-0-85">85</a></span>
<span class="normal"><a href="#__codelineno-0-86">86</a></span>
<span class="normal"><a href="#__codelineno-0-87">87</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-76"><a id="__codelineno-0-76" name="__codelineno-0-76"></a><span class="nd">@abstractmethod</span>
</span><span id="__span-0-77"><a id="__codelineno-0-77" name="__codelineno-0-77"></a><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">DataLoaderType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InfluenceFunctionModel</span><span class="p">:</span>
</span><span id="__span-0-78"><a id="__codelineno-0-78" name="__codelineno-0-78"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-79"><a id="__codelineno-0-79" name="__codelineno-0-79"></a><span class="sd">    Override this method to fit the influence function model to training data,</span>
</span><span id="__span-0-80"><a id="__codelineno-0-80" name="__codelineno-0-80"></a><span class="sd">    e.g. pre-compute hessian matrix or matrix decompositions</span>
</span><span id="__span-0-81"><a id="__codelineno-0-81" name="__codelineno-0-81"></a>
</span><span id="__span-0-82"><a id="__codelineno-0-82" name="__codelineno-0-82"></a><span class="sd">    Args:</span>
</span><span id="__span-0-83"><a id="__codelineno-0-83" name="__codelineno-0-83"></a><span class="sd">        data:</span>
</span><span id="__span-0-84"><a id="__codelineno-0-84" name="__codelineno-0-84"></a>
</span><span id="__span-0-85"><a id="__codelineno-0-85" name="__codelineno-0-85"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-86"><a id="__codelineno-0-86" name="__codelineno-0-86"></a><span class="sd">        The fitted instance</span>
</span><span id="__span-0-87"><a id="__codelineno-0-87" name="__codelineno-0-87"></a><span class="sd">    &quot;&quot;&quot;</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influences" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences</span>


<a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influences" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute the approximation of</p>
<div class="arithmatex">\[
\langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}})), \nabla_{\theta} \ell(y, f_{\theta}(x))\rangle
\]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[
\langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}}, f_{\theta}(x_{\text{test}})),
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle
\]</div>
<p>for the perturbation type influence case. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
of <span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
if None, use <span class="arithmatex">\(x=x_{\text{test}}\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-121"><a id="__codelineno-0-121" name="__codelineno-0-121"></a><span class="k">def</span> <span class="nf">influences</span><span class="p">(</span>
</span><span id="__span-0-122"><a id="__codelineno-0-122" name="__codelineno-0-122"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-123"><a id="__codelineno-0-123" name="__codelineno-0-123"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-124"><a id="__codelineno-0-124" name="__codelineno-0-124"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-125"><a id="__codelineno-0-125" name="__codelineno-0-125"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-126"><a id="__codelineno-0-126" name="__codelineno-0-126"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-127"><a id="__codelineno-0-127" name="__codelineno-0-127"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-128"><a id="__codelineno-0-128" name="__codelineno-0-128"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-129"><a id="__codelineno-0-129" name="__codelineno-0-129"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-130"><a id="__codelineno-0-130" name="__codelineno-0-130"></a><span class="sd">    Compute the approximation of</span>
</span><span id="__span-0-131"><a id="__codelineno-0-131" name="__codelineno-0-131"></a>
</span><span id="__span-0-132"><a id="__codelineno-0-132" name="__codelineno-0-132"></a><span class="sd">    \[</span>
</span><span id="__span-0-133"><a id="__codelineno-0-133" name="__codelineno-0-133"></a><span class="sd">    \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-134"><a id="__codelineno-0-134" name="__codelineno-0-134"></a><span class="sd">    f_{\theta}(x_{\text{test}})), \nabla_{\theta} \ell(y, f_{\theta}(x))\rangle</span>
</span><span id="__span-0-135"><a id="__codelineno-0-135" name="__codelineno-0-135"></a><span class="sd">    \]</span>
</span><span id="__span-0-136"><a id="__codelineno-0-136" name="__codelineno-0-136"></a>
</span><span id="__span-0-137"><a id="__codelineno-0-137" name="__codelineno-0-137"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-138"><a id="__codelineno-0-138" name="__codelineno-0-138"></a>
</span><span id="__span-0-139"><a id="__codelineno-0-139" name="__codelineno-0-139"></a><span class="sd">    \[</span>
</span><span id="__span-0-140"><a id="__codelineno-0-140" name="__codelineno-0-140"></a><span class="sd">    \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}}, f_{\theta}(x_{\text{test}})),</span>
</span><span id="__span-0-141"><a id="__codelineno-0-141" name="__codelineno-0-141"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle</span>
</span><span id="__span-0-142"><a id="__codelineno-0-142" name="__codelineno-0-142"></a><span class="sd">    \]</span>
</span><span id="__span-0-143"><a id="__codelineno-0-143" name="__codelineno-0-143"></a>
</span><span id="__span-0-144"><a id="__codelineno-0-144" name="__codelineno-0-144"></a><span class="sd">    for the perturbation type influence case. For all input tensors it is assumed,</span>
</span><span id="__span-0-145"><a id="__codelineno-0-145" name="__codelineno-0-145"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-146"><a id="__codelineno-0-146" name="__codelineno-0-146"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-147"><a id="__codelineno-0-147" name="__codelineno-0-147"></a>
</span><span id="__span-0-148"><a id="__codelineno-0-148" name="__codelineno-0-148"></a><span class="sd">    Args:</span>
</span><span id="__span-0-149"><a id="__codelineno-0-149" name="__codelineno-0-149"></a><span class="sd">        x_test: model input to use in the gradient computations</span>
</span><span id="__span-0-150"><a id="__codelineno-0-150" name="__codelineno-0-150"></a><span class="sd">            of $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-151"><a id="__codelineno-0-151" name="__codelineno-0-151"></a><span class="sd">                f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-152"><a id="__codelineno-0-152" name="__codelineno-0-152"></a><span class="sd">        y_test: label tensor to compute gradients</span>
</span><span id="__span-0-153"><a id="__codelineno-0-153" name="__codelineno-0-153"></a><span class="sd">        x: optional model input to use in the gradient computations</span>
</span><span id="__span-0-154"><a id="__codelineno-0-154" name="__codelineno-0-154"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-155"><a id="__codelineno-0-155" name="__codelineno-0-155"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-156"><a id="__codelineno-0-156" name="__codelineno-0-156"></a><span class="sd">            if None, use $x=x_{\text{test}}$</span>
</span><span id="__span-0-157"><a id="__codelineno-0-157" name="__codelineno-0-157"></a><span class="sd">        y: optional label tensor to compute gradients</span>
</span><span id="__span-0-158"><a id="__codelineno-0-158" name="__codelineno-0-158"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-159"><a id="__codelineno-0-159" name="__codelineno-0-159"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-160"><a id="__codelineno-0-160" name="__codelineno-0-160"></a>
</span><span id="__span-0-161"><a id="__codelineno-0-161" name="__codelineno-0-161"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-162"><a id="__codelineno-0-162" name="__codelineno-0-162"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-163"><a id="__codelineno-0-163" name="__codelineno-0-163"></a>
</span><span id="__span-0-164"><a id="__codelineno-0-164" name="__codelineno-0-164"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-165"><a id="__codelineno-0-165" name="__codelineno-0-165"></a>    <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
</span><span id="__span-0-166"><a id="__codelineno-0-166" name="__codelineno-0-166"></a>    <span class="k">return</span> <span class="n">t</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influence_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influence_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influence_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute approximation of</p>
<div class="arithmatex">\[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</div>
<p>where the gradient is meant to be per sample of the batch <span class="arithmatex">\((x, y)\)</span>.
For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise inverse Hessian matrix vector products</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-245"><a id="__codelineno-0-245" name="__codelineno-0-245"></a><span class="k">def</span> <span class="nf">influence_factors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-246"><a id="__codelineno-0-246" name="__codelineno-0-246"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-247"><a id="__codelineno-0-247" name="__codelineno-0-247"></a><span class="sd">    Compute approximation of</span>
</span><span id="__span-0-248"><a id="__codelineno-0-248" name="__codelineno-0-248"></a>
</span><span id="__span-0-249"><a id="__codelineno-0-249" name="__codelineno-0-249"></a><span class="sd">    \[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</span>
</span><span id="__span-0-250"><a id="__codelineno-0-250" name="__codelineno-0-250"></a>
</span><span id="__span-0-251"><a id="__codelineno-0-251" name="__codelineno-0-251"></a><span class="sd">    where the gradient is meant to be per sample of the batch $(x, y)$.</span>
</span><span id="__span-0-252"><a id="__codelineno-0-252" name="__codelineno-0-252"></a><span class="sd">    For all input tensors it is assumed,</span>
</span><span id="__span-0-253"><a id="__codelineno-0-253" name="__codelineno-0-253"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-254"><a id="__codelineno-0-254" name="__codelineno-0-254"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-255"><a id="__codelineno-0-255" name="__codelineno-0-255"></a>
</span><span id="__span-0-256"><a id="__codelineno-0-256" name="__codelineno-0-256"></a><span class="sd">    Args:</span>
</span><span id="__span-0-257"><a id="__codelineno-0-257" name="__codelineno-0-257"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-258"><a id="__codelineno-0-258" name="__codelineno-0-258"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-259"><a id="__codelineno-0-259" name="__codelineno-0-259"></a>
</span><span id="__span-0-260"><a id="__codelineno-0-260" name="__codelineno-0-260"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-261"><a id="__codelineno-0-261" name="__codelineno-0-261"></a><span class="sd">        Tensor representing the element-wise inverse Hessian matrix vector products</span>
</span><span id="__span-0-262"><a id="__codelineno-0-262" name="__codelineno-0-262"></a>
</span><span id="__span-0-263"><a id="__codelineno-0-263" name="__codelineno-0-263"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-264"><a id="__codelineno-0-264" name="__codelineno-0-264"></a>    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influences_from_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences_from_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel.influences_from_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Computation of</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the perturbation type influence case. The gradient is meant to be per sample
of the batch <span class="arithmatex">\((x, y)\)</span>. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>z_test_factors</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>pre-computed tensor, approximating
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span>
<span class="normal"><a href="#__codelineno-0-320">320</a></span>
<span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-276"><a id="__codelineno-0-276" name="__codelineno-0-276"></a><span class="k">def</span> <span class="nf">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-277"><a id="__codelineno-0-277" name="__codelineno-0-277"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-278"><a id="__codelineno-0-278" name="__codelineno-0-278"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-279"><a id="__codelineno-0-279" name="__codelineno-0-279"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-280"><a id="__codelineno-0-280" name="__codelineno-0-280"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-281"><a id="__codelineno-0-281" name="__codelineno-0-281"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-282"><a id="__codelineno-0-282" name="__codelineno-0-282"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-283"><a id="__codelineno-0-283" name="__codelineno-0-283"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-284"><a id="__codelineno-0-284" name="__codelineno-0-284"></a><span class="sd">    Computation of</span>
</span><span id="__span-0-285"><a id="__codelineno-0-285" name="__codelineno-0-285"></a>
</span><span id="__span-0-286"><a id="__codelineno-0-286" name="__codelineno-0-286"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-287"><a id="__codelineno-0-287" name="__codelineno-0-287"></a><span class="sd">        \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-288"><a id="__codelineno-0-288" name="__codelineno-0-288"></a>
</span><span id="__span-0-289"><a id="__codelineno-0-289" name="__codelineno-0-289"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-290"><a id="__codelineno-0-290" name="__codelineno-0-290"></a>
</span><span id="__span-0-291"><a id="__codelineno-0-291" name="__codelineno-0-291"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-292"><a id="__codelineno-0-292" name="__codelineno-0-292"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-293"><a id="__codelineno-0-293" name="__codelineno-0-293"></a>
</span><span id="__span-0-294"><a id="__codelineno-0-294" name="__codelineno-0-294"></a><span class="sd">    for the perturbation type influence case. The gradient is meant to be per sample</span>
</span><span id="__span-0-295"><a id="__codelineno-0-295" name="__codelineno-0-295"></a><span class="sd">    of the batch $(x, y)$. For all input tensors it is assumed,</span>
</span><span id="__span-0-296"><a id="__codelineno-0-296" name="__codelineno-0-296"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-297"><a id="__codelineno-0-297" name="__codelineno-0-297"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-298"><a id="__codelineno-0-298" name="__codelineno-0-298"></a>
</span><span id="__span-0-299"><a id="__codelineno-0-299" name="__codelineno-0-299"></a><span class="sd">    Args:</span>
</span><span id="__span-0-300"><a id="__codelineno-0-300" name="__codelineno-0-300"></a><span class="sd">        z_test_factors: pre-computed tensor, approximating</span>
</span><span id="__span-0-301"><a id="__codelineno-0-301" name="__codelineno-0-301"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-302"><a id="__codelineno-0-302" name="__codelineno-0-302"></a><span class="sd">            f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-303"><a id="__codelineno-0-303" name="__codelineno-0-303"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-304"><a id="__codelineno-0-304" name="__codelineno-0-304"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-305"><a id="__codelineno-0-305" name="__codelineno-0-305"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$</span>
</span><span id="__span-0-306"><a id="__codelineno-0-306" name="__codelineno-0-306"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-307"><a id="__codelineno-0-307" name="__codelineno-0-307"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-308"><a id="__codelineno-0-308" name="__codelineno-0-308"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-309"><a id="__codelineno-0-309" name="__codelineno-0-309"></a>
</span><span id="__span-0-310"><a id="__codelineno-0-310" name="__codelineno-0-310"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-311"><a id="__codelineno-0-311" name="__codelineno-0-311"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-312"><a id="__codelineno-0-312" name="__codelineno-0-312"></a>
</span><span id="__span-0-313"><a id="__codelineno-0-313" name="__codelineno-0-313"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-314"><a id="__codelineno-0-314" name="__codelineno-0-314"></a>    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">:</span>
</span><span id="__span-0-315"><a id="__codelineno-0-315" name="__codelineno-0-315"></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="__span-0-316"><a id="__codelineno-0-316" name="__codelineno-0-316"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-317"><a id="__codelineno-0-317" name="__codelineno-0-317"></a>            <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</span><span id="__span-0-318"><a id="__codelineno-0-318" name="__codelineno-0-318"></a>        <span class="p">)</span>
</span><span id="__span-0-319"><a id="__codelineno-0-319" name="__codelineno-0-319"></a>    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">:</span>
</span><span id="__span-0-320"><a id="__codelineno-0-320" name="__codelineno-0-320"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
</span><span id="__span-0-321"><a id="__codelineno-0-321" name="__codelineno-0-321"></a>            <span class="s2">&quot;ia,j...a-&gt;ij...&quot;</span><span class="p">,</span>
</span><span id="__span-0-322"><a id="__codelineno-0-322" name="__codelineno-0-322"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-323"><a id="__codelineno-0-323" name="__codelineno-0-323"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flat_loss_mixed_grad</span><span class="p">(</span>
</span><span id="__span-0-324"><a id="__codelineno-0-324" name="__codelineno-0-324"></a>                <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-325"><a id="__codelineno-0-325" name="__codelineno-0-325"></a>            <span class="p">),</span>
</span><span id="__span-0-326"><a id="__codelineno-0-326" name="__codelineno-0-326"></a>        <span class="p">)</span>
</span><span id="__span-0-327"><a id="__codelineno-0-327" name="__codelineno-0-327"></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-328"><a id="__codelineno-0-328" name="__codelineno-0-328"></a>        <span class="k">raise</span> <span class="n">UnsupportedInfluenceModeException</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pydvl.influence.torch.influence_function_model.DirectInfluence" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <span class="doc doc-object-name doc-class-name">DirectInfluence</span>


<a href="#pydvl.influence.torch.influence_function_model.DirectInfluence" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">DirectInfluence</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">model</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">[[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="p">)</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel" href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel">TorchInfluenceFunctionModel</a></code></p>

  
      <p>Given a model and training data, it finds x such that <span class="arithmatex">\(Hx = b\)</span>,
with <span class="arithmatex">\(H\)</span> being the model hessian.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A PyTorch model. The Hessian will be calculated with respect to
this model's parameters.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>loss</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A callable that takes the model's output and target as input and returns
  the scalar loss.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a>[[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>], <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>hessian_regularization</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Regularization of the hessian.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>0.0</code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>

                <details class="quote">
                  <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
                  <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-358">358</a></span>
<span class="normal"><a href="#__codelineno-0-359">359</a></span>
<span class="normal"><a href="#__codelineno-0-360">360</a></span>
<span class="normal"><a href="#__codelineno-0-361">361</a></span>
<span class="normal"><a href="#__codelineno-0-362">362</a></span>
<span class="normal"><a href="#__codelineno-0-363">363</a></span>
<span class="normal"><a href="#__codelineno-0-364">364</a></span>
<span class="normal"><a href="#__codelineno-0-365">365</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-358"><a id="__codelineno-0-358" name="__codelineno-0-358"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="__span-0-359"><a id="__codelineno-0-359" name="__codelineno-0-359"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-360"><a id="__codelineno-0-360" name="__codelineno-0-360"></a>    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="__span-0-361"><a id="__codelineno-0-361" name="__codelineno-0-361"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span id="__span-0-362"><a id="__codelineno-0-362" name="__codelineno-0-362"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-363"><a id="__codelineno-0-363" name="__codelineno-0-363"></a><span class="p">):</span>
</span><span id="__span-0-364"><a id="__codelineno-0-364" name="__codelineno-0-364"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</span><span id="__span-0-365"><a id="__codelineno-0-365" name="__codelineno-0-365"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">hessian_regularization</span> <span class="o">=</span> <span class="n">hessian_regularization</span>
</span></code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.DirectInfluence.influence_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influence_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.influence_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute approximation of</p>
<div class="arithmatex">\[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</div>
<p>where the gradient is meant to be per sample of the batch <span class="arithmatex">\((x, y)\)</span>.
For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise inverse Hessian matrix vector products</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-245"><a id="__codelineno-0-245" name="__codelineno-0-245"></a><span class="k">def</span> <span class="nf">influence_factors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-246"><a id="__codelineno-0-246" name="__codelineno-0-246"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-247"><a id="__codelineno-0-247" name="__codelineno-0-247"></a><span class="sd">    Compute approximation of</span>
</span><span id="__span-0-248"><a id="__codelineno-0-248" name="__codelineno-0-248"></a>
</span><span id="__span-0-249"><a id="__codelineno-0-249" name="__codelineno-0-249"></a><span class="sd">    \[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</span>
</span><span id="__span-0-250"><a id="__codelineno-0-250" name="__codelineno-0-250"></a>
</span><span id="__span-0-251"><a id="__codelineno-0-251" name="__codelineno-0-251"></a><span class="sd">    where the gradient is meant to be per sample of the batch $(x, y)$.</span>
</span><span id="__span-0-252"><a id="__codelineno-0-252" name="__codelineno-0-252"></a><span class="sd">    For all input tensors it is assumed,</span>
</span><span id="__span-0-253"><a id="__codelineno-0-253" name="__codelineno-0-253"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-254"><a id="__codelineno-0-254" name="__codelineno-0-254"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-255"><a id="__codelineno-0-255" name="__codelineno-0-255"></a>
</span><span id="__span-0-256"><a id="__codelineno-0-256" name="__codelineno-0-256"></a><span class="sd">    Args:</span>
</span><span id="__span-0-257"><a id="__codelineno-0-257" name="__codelineno-0-257"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-258"><a id="__codelineno-0-258" name="__codelineno-0-258"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-259"><a id="__codelineno-0-259" name="__codelineno-0-259"></a>
</span><span id="__span-0-260"><a id="__codelineno-0-260" name="__codelineno-0-260"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-261"><a id="__codelineno-0-261" name="__codelineno-0-261"></a><span class="sd">        Tensor representing the element-wise inverse Hessian matrix vector products</span>
</span><span id="__span-0-262"><a id="__codelineno-0-262" name="__codelineno-0-262"></a>
</span><span id="__span-0-263"><a id="__codelineno-0-263" name="__codelineno-0-263"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-264"><a id="__codelineno-0-264" name="__codelineno-0-264"></a>    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.DirectInfluence.influences_from_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences_from_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.influences_from_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Computation of</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the perturbation type influence case. The gradient is meant to be per sample
of the batch <span class="arithmatex">\((x, y)\)</span>. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>z_test_factors</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>pre-computed tensor, approximating
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span>
<span class="normal"><a href="#__codelineno-0-320">320</a></span>
<span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-276"><a id="__codelineno-0-276" name="__codelineno-0-276"></a><span class="k">def</span> <span class="nf">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-277"><a id="__codelineno-0-277" name="__codelineno-0-277"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-278"><a id="__codelineno-0-278" name="__codelineno-0-278"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-279"><a id="__codelineno-0-279" name="__codelineno-0-279"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-280"><a id="__codelineno-0-280" name="__codelineno-0-280"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-281"><a id="__codelineno-0-281" name="__codelineno-0-281"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-282"><a id="__codelineno-0-282" name="__codelineno-0-282"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-283"><a id="__codelineno-0-283" name="__codelineno-0-283"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-284"><a id="__codelineno-0-284" name="__codelineno-0-284"></a><span class="sd">    Computation of</span>
</span><span id="__span-0-285"><a id="__codelineno-0-285" name="__codelineno-0-285"></a>
</span><span id="__span-0-286"><a id="__codelineno-0-286" name="__codelineno-0-286"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-287"><a id="__codelineno-0-287" name="__codelineno-0-287"></a><span class="sd">        \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-288"><a id="__codelineno-0-288" name="__codelineno-0-288"></a>
</span><span id="__span-0-289"><a id="__codelineno-0-289" name="__codelineno-0-289"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-290"><a id="__codelineno-0-290" name="__codelineno-0-290"></a>
</span><span id="__span-0-291"><a id="__codelineno-0-291" name="__codelineno-0-291"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-292"><a id="__codelineno-0-292" name="__codelineno-0-292"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-293"><a id="__codelineno-0-293" name="__codelineno-0-293"></a>
</span><span id="__span-0-294"><a id="__codelineno-0-294" name="__codelineno-0-294"></a><span class="sd">    for the perturbation type influence case. The gradient is meant to be per sample</span>
</span><span id="__span-0-295"><a id="__codelineno-0-295" name="__codelineno-0-295"></a><span class="sd">    of the batch $(x, y)$. For all input tensors it is assumed,</span>
</span><span id="__span-0-296"><a id="__codelineno-0-296" name="__codelineno-0-296"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-297"><a id="__codelineno-0-297" name="__codelineno-0-297"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-298"><a id="__codelineno-0-298" name="__codelineno-0-298"></a>
</span><span id="__span-0-299"><a id="__codelineno-0-299" name="__codelineno-0-299"></a><span class="sd">    Args:</span>
</span><span id="__span-0-300"><a id="__codelineno-0-300" name="__codelineno-0-300"></a><span class="sd">        z_test_factors: pre-computed tensor, approximating</span>
</span><span id="__span-0-301"><a id="__codelineno-0-301" name="__codelineno-0-301"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-302"><a id="__codelineno-0-302" name="__codelineno-0-302"></a><span class="sd">            f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-303"><a id="__codelineno-0-303" name="__codelineno-0-303"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-304"><a id="__codelineno-0-304" name="__codelineno-0-304"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-305"><a id="__codelineno-0-305" name="__codelineno-0-305"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$</span>
</span><span id="__span-0-306"><a id="__codelineno-0-306" name="__codelineno-0-306"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-307"><a id="__codelineno-0-307" name="__codelineno-0-307"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-308"><a id="__codelineno-0-308" name="__codelineno-0-308"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-309"><a id="__codelineno-0-309" name="__codelineno-0-309"></a>
</span><span id="__span-0-310"><a id="__codelineno-0-310" name="__codelineno-0-310"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-311"><a id="__codelineno-0-311" name="__codelineno-0-311"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-312"><a id="__codelineno-0-312" name="__codelineno-0-312"></a>
</span><span id="__span-0-313"><a id="__codelineno-0-313" name="__codelineno-0-313"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-314"><a id="__codelineno-0-314" name="__codelineno-0-314"></a>    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">:</span>
</span><span id="__span-0-315"><a id="__codelineno-0-315" name="__codelineno-0-315"></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="__span-0-316"><a id="__codelineno-0-316" name="__codelineno-0-316"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-317"><a id="__codelineno-0-317" name="__codelineno-0-317"></a>            <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</span><span id="__span-0-318"><a id="__codelineno-0-318" name="__codelineno-0-318"></a>        <span class="p">)</span>
</span><span id="__span-0-319"><a id="__codelineno-0-319" name="__codelineno-0-319"></a>    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">:</span>
</span><span id="__span-0-320"><a id="__codelineno-0-320" name="__codelineno-0-320"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
</span><span id="__span-0-321"><a id="__codelineno-0-321" name="__codelineno-0-321"></a>            <span class="s2">&quot;ia,j...a-&gt;ij...&quot;</span><span class="p">,</span>
</span><span id="__span-0-322"><a id="__codelineno-0-322" name="__codelineno-0-322"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-323"><a id="__codelineno-0-323" name="__codelineno-0-323"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flat_loss_mixed_grad</span><span class="p">(</span>
</span><span id="__span-0-324"><a id="__codelineno-0-324" name="__codelineno-0-324"></a>                <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-325"><a id="__codelineno-0-325" name="__codelineno-0-325"></a>            <span class="p">),</span>
</span><span id="__span-0-326"><a id="__codelineno-0-326" name="__codelineno-0-326"></a>        <span class="p">)</span>
</span><span id="__span-0-327"><a id="__codelineno-0-327" name="__codelineno-0-327"></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-328"><a id="__codelineno-0-328" name="__codelineno-0-328"></a>        <span class="k">raise</span> <span class="n">UnsupportedInfluenceModeException</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.DirectInfluence.fit" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">fit</span>


<a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.fit" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.utils.data.DataLoader" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.DirectInfluence" href="#pydvl.influence.torch.influence_function_model.DirectInfluence">DirectInfluence</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute the hessian matrix based on a provided dataloader.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>data</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>The data to compute the Hessian with.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.utils.data.DataLoader" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.DirectInfluence" href="#pydvl.influence.torch.influence_function_model.DirectInfluence">DirectInfluence</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>The fitted instance.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-376">376</a></span>
<span class="normal"><a href="#__codelineno-0-377">377</a></span>
<span class="normal"><a href="#__codelineno-0-378">378</a></span>
<span class="normal"><a href="#__codelineno-0-379">379</a></span>
<span class="normal"><a href="#__codelineno-0-380">380</a></span>
<span class="normal"><a href="#__codelineno-0-381">381</a></span>
<span class="normal"><a href="#__codelineno-0-382">382</a></span>
<span class="normal"><a href="#__codelineno-0-383">383</a></span>
<span class="normal"><a href="#__codelineno-0-384">384</a></span>
<span class="normal"><a href="#__codelineno-0-385">385</a></span>
<span class="normal"><a href="#__codelineno-0-386">386</a></span>
<span class="normal"><a href="#__codelineno-0-387">387</a></span>
<span class="normal"><a href="#__codelineno-0-388">388</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-376"><a id="__codelineno-0-376" name="__codelineno-0-376"></a><span class="nd">@log_duration</span><span class="p">(</span><span class="n">log_level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
</span><span id="__span-0-377"><a id="__codelineno-0-377" name="__codelineno-0-377"></a><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DirectInfluence</span><span class="p">:</span>
</span><span id="__span-0-378"><a id="__codelineno-0-378" name="__codelineno-0-378"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-379"><a id="__codelineno-0-379" name="__codelineno-0-379"></a><span class="sd">    Compute the hessian matrix based on a provided dataloader.</span>
</span><span id="__span-0-380"><a id="__codelineno-0-380" name="__codelineno-0-380"></a>
</span><span id="__span-0-381"><a id="__codelineno-0-381" name="__codelineno-0-381"></a><span class="sd">    Args:</span>
</span><span id="__span-0-382"><a id="__codelineno-0-382" name="__codelineno-0-382"></a><span class="sd">        data: The data to compute the Hessian with.</span>
</span><span id="__span-0-383"><a id="__codelineno-0-383" name="__codelineno-0-383"></a>
</span><span id="__span-0-384"><a id="__codelineno-0-384" name="__codelineno-0-384"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-385"><a id="__codelineno-0-385" name="__codelineno-0-385"></a><span class="sd">        The fitted instance.</span>
</span><span id="__span-0-386"><a id="__codelineno-0-386" name="__codelineno-0-386"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-387"><a id="__codelineno-0-387" name="__codelineno-0-387"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">hessian</span> <span class="o">=</span> <span class="n">hessian</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</span><span id="__span-0-388"><a id="__codelineno-0-388" name="__codelineno-0-388"></a>    <span class="k">return</span> <span class="bp">self</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.DirectInfluence.influences" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences</span>


<a href="#pydvl.influence.torch.influence_function_model.DirectInfluence.influences" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute approximation of</p>
<div class="arithmatex">\[ \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}})),
    \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle, \]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[ \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}})),
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the perturbation type influence case. The action of <span class="arithmatex">\(H^{-1}\)</span> is achieved
via a direct solver using <a class="autorefs autorefs-external" href="https://pytorch.org/docs/stable/generated/torch.linalg.solve.html#torch.linalg.solve">torch.linalg.solve</a>.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations of
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
if None, use <span class="arithmatex">\(x=x_{\text{test}}\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>A tensor representing the element-wise scalar products for the
provided batch.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-390">390</a></span>
<span class="normal"><a href="#__codelineno-0-391">391</a></span>
<span class="normal"><a href="#__codelineno-0-392">392</a></span>
<span class="normal"><a href="#__codelineno-0-393">393</a></span>
<span class="normal"><a href="#__codelineno-0-394">394</a></span>
<span class="normal"><a href="#__codelineno-0-395">395</a></span>
<span class="normal"><a href="#__codelineno-0-396">396</a></span>
<span class="normal"><a href="#__codelineno-0-397">397</a></span>
<span class="normal"><a href="#__codelineno-0-398">398</a></span>
<span class="normal"><a href="#__codelineno-0-399">399</a></span>
<span class="normal"><a href="#__codelineno-0-400">400</a></span>
<span class="normal"><a href="#__codelineno-0-401">401</a></span>
<span class="normal"><a href="#__codelineno-0-402">402</a></span>
<span class="normal"><a href="#__codelineno-0-403">403</a></span>
<span class="normal"><a href="#__codelineno-0-404">404</a></span>
<span class="normal"><a href="#__codelineno-0-405">405</a></span>
<span class="normal"><a href="#__codelineno-0-406">406</a></span>
<span class="normal"><a href="#__codelineno-0-407">407</a></span>
<span class="normal"><a href="#__codelineno-0-408">408</a></span>
<span class="normal"><a href="#__codelineno-0-409">409</a></span>
<span class="normal"><a href="#__codelineno-0-410">410</a></span>
<span class="normal"><a href="#__codelineno-0-411">411</a></span>
<span class="normal"><a href="#__codelineno-0-412">412</a></span>
<span class="normal"><a href="#__codelineno-0-413">413</a></span>
<span class="normal"><a href="#__codelineno-0-414">414</a></span>
<span class="normal"><a href="#__codelineno-0-415">415</a></span>
<span class="normal"><a href="#__codelineno-0-416">416</a></span>
<span class="normal"><a href="#__codelineno-0-417">417</a></span>
<span class="normal"><a href="#__codelineno-0-418">418</a></span>
<span class="normal"><a href="#__codelineno-0-419">419</a></span>
<span class="normal"><a href="#__codelineno-0-420">420</a></span>
<span class="normal"><a href="#__codelineno-0-421">421</a></span>
<span class="normal"><a href="#__codelineno-0-422">422</a></span>
<span class="normal"><a href="#__codelineno-0-423">423</a></span>
<span class="normal"><a href="#__codelineno-0-424">424</a></span>
<span class="normal"><a href="#__codelineno-0-425">425</a></span>
<span class="normal"><a href="#__codelineno-0-426">426</a></span>
<span class="normal"><a href="#__codelineno-0-427">427</a></span>
<span class="normal"><a href="#__codelineno-0-428">428</a></span>
<span class="normal"><a href="#__codelineno-0-429">429</a></span>
<span class="normal"><a href="#__codelineno-0-430">430</a></span>
<span class="normal"><a href="#__codelineno-0-431">431</a></span>
<span class="normal"><a href="#__codelineno-0-432">432</a></span>
<span class="normal"><a href="#__codelineno-0-433">433</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-390"><a id="__codelineno-0-390" name="__codelineno-0-390"></a><span class="nd">@log_duration</span>
</span><span id="__span-0-391"><a id="__codelineno-0-391" name="__codelineno-0-391"></a><span class="k">def</span> <span class="nf">influences</span><span class="p">(</span>
</span><span id="__span-0-392"><a id="__codelineno-0-392" name="__codelineno-0-392"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-393"><a id="__codelineno-0-393" name="__codelineno-0-393"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-394"><a id="__codelineno-0-394" name="__codelineno-0-394"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-395"><a id="__codelineno-0-395" name="__codelineno-0-395"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-396"><a id="__codelineno-0-396" name="__codelineno-0-396"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-397"><a id="__codelineno-0-397" name="__codelineno-0-397"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-398"><a id="__codelineno-0-398" name="__codelineno-0-398"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-399"><a id="__codelineno-0-399" name="__codelineno-0-399"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-400"><a id="__codelineno-0-400" name="__codelineno-0-400"></a><span class="sd">    Compute approximation of</span>
</span><span id="__span-0-401"><a id="__codelineno-0-401" name="__codelineno-0-401"></a>
</span><span id="__span-0-402"><a id="__codelineno-0-402" name="__codelineno-0-402"></a><span class="sd">    \[ \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-403"><a id="__codelineno-0-403" name="__codelineno-0-403"></a><span class="sd">        f_{\theta}(x_{\text{test}})),</span>
</span><span id="__span-0-404"><a id="__codelineno-0-404" name="__codelineno-0-404"></a><span class="sd">        \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle, \]</span>
</span><span id="__span-0-405"><a id="__codelineno-0-405" name="__codelineno-0-405"></a>
</span><span id="__span-0-406"><a id="__codelineno-0-406" name="__codelineno-0-406"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-407"><a id="__codelineno-0-407" name="__codelineno-0-407"></a>
</span><span id="__span-0-408"><a id="__codelineno-0-408" name="__codelineno-0-408"></a><span class="sd">    \[ \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-409"><a id="__codelineno-0-409" name="__codelineno-0-409"></a><span class="sd">        f_{\theta}(x_{\text{test}})),</span>
</span><span id="__span-0-410"><a id="__codelineno-0-410" name="__codelineno-0-410"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-411"><a id="__codelineno-0-411" name="__codelineno-0-411"></a>
</span><span id="__span-0-412"><a id="__codelineno-0-412" name="__codelineno-0-412"></a><span class="sd">    for the perturbation type influence case. The action of $H^{-1}$ is achieved</span>
</span><span id="__span-0-413"><a id="__codelineno-0-413" name="__codelineno-0-413"></a><span class="sd">    via a direct solver using [torch.linalg.solve][torch.linalg.solve].</span>
</span><span id="__span-0-414"><a id="__codelineno-0-414" name="__codelineno-0-414"></a>
</span><span id="__span-0-415"><a id="__codelineno-0-415" name="__codelineno-0-415"></a><span class="sd">    Args:</span>
</span><span id="__span-0-416"><a id="__codelineno-0-416" name="__codelineno-0-416"></a><span class="sd">        x_test: model input to use in the gradient computations of</span>
</span><span id="__span-0-417"><a id="__codelineno-0-417" name="__codelineno-0-417"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-418"><a id="__codelineno-0-418" name="__codelineno-0-418"></a><span class="sd">                f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-419"><a id="__codelineno-0-419" name="__codelineno-0-419"></a><span class="sd">        y_test: label tensor to compute gradients</span>
</span><span id="__span-0-420"><a id="__codelineno-0-420" name="__codelineno-0-420"></a><span class="sd">        x: optional model input to use in the gradient computations</span>
</span><span id="__span-0-421"><a id="__codelineno-0-421" name="__codelineno-0-421"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-422"><a id="__codelineno-0-422" name="__codelineno-0-422"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-423"><a id="__codelineno-0-423" name="__codelineno-0-423"></a><span class="sd">            if None, use $x=x_{\text{test}}$</span>
</span><span id="__span-0-424"><a id="__codelineno-0-424" name="__codelineno-0-424"></a><span class="sd">        y: optional label tensor to compute gradients</span>
</span><span id="__span-0-425"><a id="__codelineno-0-425" name="__codelineno-0-425"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-426"><a id="__codelineno-0-426" name="__codelineno-0-426"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-427"><a id="__codelineno-0-427" name="__codelineno-0-427"></a>
</span><span id="__span-0-428"><a id="__codelineno-0-428" name="__codelineno-0-428"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-429"><a id="__codelineno-0-429" name="__codelineno-0-429"></a><span class="sd">        A tensor representing the element-wise scalar products for the</span>
</span><span id="__span-0-430"><a id="__codelineno-0-430" name="__codelineno-0-430"></a><span class="sd">            provided batch.</span>
</span><span id="__span-0-431"><a id="__codelineno-0-431" name="__codelineno-0-431"></a>
</span><span id="__span-0-432"><a id="__codelineno-0-432" name="__codelineno-0-432"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-433"><a id="__codelineno-0-433" name="__codelineno-0-433"></a>    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pydvl.influence.torch.influence_function_model.CgInfluence" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <span class="doc doc-object-name doc-class-name">CgInfluence</span>


<a href="#pydvl.influence.torch.influence_function_model.CgInfluence" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">CgInfluence</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">model</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">[[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">x0</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">rtol</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">1e-07</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">atol</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">1e-07</span><span class="p">,</span>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">maxiter</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="n">progress</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="n">precompute_grad</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    <span class="n">pre_conditioner</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.torch.pre_conditioner.PreConditioner" href="../pre_conditioner/#pydvl.influence.torch.pre_conditioner.PreConditioner">PreConditioner</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    <span class="n">use_block_cg</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-13"><a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    <span class="n">warn_on_max_iteration</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="__span-0-14"><a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a><span class="p">)</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel" href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel">TorchInfluenceFunctionModel</a></code></p>

  
      <p>Given a model and training data, it uses conjugate gradient to calculate the
inverse of the Hessian Vector Product. More precisely, it finds x such that <span class="arithmatex">\(Hx =
b\)</span>, with <span class="arithmatex">\(H\)</span> being the model hessian. For more info, see
<a class="autorefs autorefs-internal" href="../../../../../influence/influence_function_model/#conjugate-gradient">Conjugate Gradient</a>.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A PyTorch model. The Hessian will be calculated with respect to
this model's parameters.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>loss</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A callable that takes the model's output and target as input and returns
  the scalar loss.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a>[[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>], <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>hessian_regularization</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Optional regularization parameter added
to the Hessian-vector product for numerical stability.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>0.0</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x0</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Initial guess for hvp. If None, defaults to b.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>rtol</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Maximum relative tolerance of result.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>1e-07</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>atol</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Absolute tolerance of result.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>1e-07</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>maxiter</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Maximum number of iterations. If None, defaults to 10*len(b).</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>progress</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>If True, display progress bars for computing in the non-block mode
(use_block_cg=False).</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>False</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>precompute_grad</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>If True, the full data gradient is precomputed and kept
in memory, which can speed up the hessian vector product computation.
Set this to False, if you can't afford to keep the full computation graph
in memory.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>False</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>pre_conditioner</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Optional pre-conditioner to improve convergence of conjugate
gradient method</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-internal" title="pydvl.influence.torch.pre_conditioner.PreConditioner" href="../pre_conditioner/#pydvl.influence.torch.pre_conditioner.PreConditioner">PreConditioner</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>use_block_cg</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>If True, use block variant of conjugate gradient method, which
solves several right hand sides simultaneously</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>False</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>warn_on_max_iteration</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>If True, logs a warning, if the desired tolerance is not
achieved within <code>maxiter</code> iterations. If False, the log level for this
information is <code>logging.DEBUG</code></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>True</code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>

                <details class="quote">
                  <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
                  <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-484">484</a></span>
<span class="normal"><a href="#__codelineno-0-485">485</a></span>
<span class="normal"><a href="#__codelineno-0-486">486</a></span>
<span class="normal"><a href="#__codelineno-0-487">487</a></span>
<span class="normal"><a href="#__codelineno-0-488">488</a></span>
<span class="normal"><a href="#__codelineno-0-489">489</a></span>
<span class="normal"><a href="#__codelineno-0-490">490</a></span>
<span class="normal"><a href="#__codelineno-0-491">491</a></span>
<span class="normal"><a href="#__codelineno-0-492">492</a></span>
<span class="normal"><a href="#__codelineno-0-493">493</a></span>
<span class="normal"><a href="#__codelineno-0-494">494</a></span>
<span class="normal"><a href="#__codelineno-0-495">495</a></span>
<span class="normal"><a href="#__codelineno-0-496">496</a></span>
<span class="normal"><a href="#__codelineno-0-497">497</a></span>
<span class="normal"><a href="#__codelineno-0-498">498</a></span>
<span class="normal"><a href="#__codelineno-0-499">499</a></span>
<span class="normal"><a href="#__codelineno-0-500">500</a></span>
<span class="normal"><a href="#__codelineno-0-501">501</a></span>
<span class="normal"><a href="#__codelineno-0-502">502</a></span>
<span class="normal"><a href="#__codelineno-0-503">503</a></span>
<span class="normal"><a href="#__codelineno-0-504">504</a></span>
<span class="normal"><a href="#__codelineno-0-505">505</a></span>
<span class="normal"><a href="#__codelineno-0-506">506</a></span>
<span class="normal"><a href="#__codelineno-0-507">507</a></span>
<span class="normal"><a href="#__codelineno-0-508">508</a></span>
<span class="normal"><a href="#__codelineno-0-509">509</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-484"><a id="__codelineno-0-484" name="__codelineno-0-484"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="__span-0-485"><a id="__codelineno-0-485" name="__codelineno-0-485"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-486"><a id="__codelineno-0-486" name="__codelineno-0-486"></a>    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="__span-0-487"><a id="__codelineno-0-487" name="__codelineno-0-487"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span id="__span-0-488"><a id="__codelineno-0-488" name="__codelineno-0-488"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-489"><a id="__codelineno-0-489" name="__codelineno-0-489"></a>    <span class="n">x0</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-490"><a id="__codelineno-0-490" name="__codelineno-0-490"></a>    <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-7</span><span class="p">,</span>
</span><span id="__span-0-491"><a id="__codelineno-0-491" name="__codelineno-0-491"></a>    <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-7</span><span class="p">,</span>
</span><span id="__span-0-492"><a id="__codelineno-0-492" name="__codelineno-0-492"></a>    <span class="n">maxiter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-493"><a id="__codelineno-0-493" name="__codelineno-0-493"></a>    <span class="n">progress</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-494"><a id="__codelineno-0-494" name="__codelineno-0-494"></a>    <span class="n">precompute_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-495"><a id="__codelineno-0-495" name="__codelineno-0-495"></a>    <span class="n">pre_conditioner</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PreConditioner</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-496"><a id="__codelineno-0-496" name="__codelineno-0-496"></a>    <span class="n">use_block_cg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-497"><a id="__codelineno-0-497" name="__codelineno-0-497"></a>    <span class="n">warn_on_max_iteration</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="__span-0-498"><a id="__codelineno-0-498" name="__codelineno-0-498"></a><span class="p">):</span>
</span><span id="__span-0-499"><a id="__codelineno-0-499" name="__codelineno-0-499"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</span><span id="__span-0-500"><a id="__codelineno-0-500" name="__codelineno-0-500"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">warn_on_max_iteration</span> <span class="o">=</span> <span class="n">warn_on_max_iteration</span>
</span><span id="__span-0-501"><a id="__codelineno-0-501" name="__codelineno-0-501"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">use_block_cg</span> <span class="o">=</span> <span class="n">use_block_cg</span>
</span><span id="__span-0-502"><a id="__codelineno-0-502" name="__codelineno-0-502"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">pre_conditioner</span> <span class="o">=</span> <span class="n">pre_conditioner</span>
</span><span id="__span-0-503"><a id="__codelineno-0-503" name="__codelineno-0-503"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">precompute_grad</span> <span class="o">=</span> <span class="n">precompute_grad</span>
</span><span id="__span-0-504"><a id="__codelineno-0-504" name="__codelineno-0-504"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">progress</span> <span class="o">=</span> <span class="n">progress</span>
</span><span id="__span-0-505"><a id="__codelineno-0-505" name="__codelineno-0-505"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">maxiter</span> <span class="o">=</span> <span class="n">maxiter</span>
</span><span id="__span-0-506"><a id="__codelineno-0-506" name="__codelineno-0-506"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">atol</span> <span class="o">=</span> <span class="n">atol</span>
</span><span id="__span-0-507"><a id="__codelineno-0-507" name="__codelineno-0-507"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rtol</span> <span class="o">=</span> <span class="n">rtol</span>
</span><span id="__span-0-508"><a id="__codelineno-0-508" name="__codelineno-0-508"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">x0</span> <span class="o">=</span> <span class="n">x0</span>
</span><span id="__span-0-509"><a id="__codelineno-0-509" name="__codelineno-0-509"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">hessian_regularization</span> <span class="o">=</span> <span class="n">hessian_regularization</span>
</span></code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.CgInfluence.influence_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influence_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.CgInfluence.influence_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute approximation of</p>
<div class="arithmatex">\[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</div>
<p>where the gradient is meant to be per sample of the batch <span class="arithmatex">\((x, y)\)</span>.
For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise inverse Hessian matrix vector products</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-245"><a id="__codelineno-0-245" name="__codelineno-0-245"></a><span class="k">def</span> <span class="nf">influence_factors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-246"><a id="__codelineno-0-246" name="__codelineno-0-246"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-247"><a id="__codelineno-0-247" name="__codelineno-0-247"></a><span class="sd">    Compute approximation of</span>
</span><span id="__span-0-248"><a id="__codelineno-0-248" name="__codelineno-0-248"></a>
</span><span id="__span-0-249"><a id="__codelineno-0-249" name="__codelineno-0-249"></a><span class="sd">    \[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</span>
</span><span id="__span-0-250"><a id="__codelineno-0-250" name="__codelineno-0-250"></a>
</span><span id="__span-0-251"><a id="__codelineno-0-251" name="__codelineno-0-251"></a><span class="sd">    where the gradient is meant to be per sample of the batch $(x, y)$.</span>
</span><span id="__span-0-252"><a id="__codelineno-0-252" name="__codelineno-0-252"></a><span class="sd">    For all input tensors it is assumed,</span>
</span><span id="__span-0-253"><a id="__codelineno-0-253" name="__codelineno-0-253"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-254"><a id="__codelineno-0-254" name="__codelineno-0-254"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-255"><a id="__codelineno-0-255" name="__codelineno-0-255"></a>
</span><span id="__span-0-256"><a id="__codelineno-0-256" name="__codelineno-0-256"></a><span class="sd">    Args:</span>
</span><span id="__span-0-257"><a id="__codelineno-0-257" name="__codelineno-0-257"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-258"><a id="__codelineno-0-258" name="__codelineno-0-258"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-259"><a id="__codelineno-0-259" name="__codelineno-0-259"></a>
</span><span id="__span-0-260"><a id="__codelineno-0-260" name="__codelineno-0-260"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-261"><a id="__codelineno-0-261" name="__codelineno-0-261"></a><span class="sd">        Tensor representing the element-wise inverse Hessian matrix vector products</span>
</span><span id="__span-0-262"><a id="__codelineno-0-262" name="__codelineno-0-262"></a>
</span><span id="__span-0-263"><a id="__codelineno-0-263" name="__codelineno-0-263"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-264"><a id="__codelineno-0-264" name="__codelineno-0-264"></a>    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.CgInfluence.influences_from_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences_from_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.CgInfluence.influences_from_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Computation of</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the perturbation type influence case. The gradient is meant to be per sample
of the batch <span class="arithmatex">\((x, y)\)</span>. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>z_test_factors</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>pre-computed tensor, approximating
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span>
<span class="normal"><a href="#__codelineno-0-320">320</a></span>
<span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-276"><a id="__codelineno-0-276" name="__codelineno-0-276"></a><span class="k">def</span> <span class="nf">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-277"><a id="__codelineno-0-277" name="__codelineno-0-277"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-278"><a id="__codelineno-0-278" name="__codelineno-0-278"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-279"><a id="__codelineno-0-279" name="__codelineno-0-279"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-280"><a id="__codelineno-0-280" name="__codelineno-0-280"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-281"><a id="__codelineno-0-281" name="__codelineno-0-281"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-282"><a id="__codelineno-0-282" name="__codelineno-0-282"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-283"><a id="__codelineno-0-283" name="__codelineno-0-283"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-284"><a id="__codelineno-0-284" name="__codelineno-0-284"></a><span class="sd">    Computation of</span>
</span><span id="__span-0-285"><a id="__codelineno-0-285" name="__codelineno-0-285"></a>
</span><span id="__span-0-286"><a id="__codelineno-0-286" name="__codelineno-0-286"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-287"><a id="__codelineno-0-287" name="__codelineno-0-287"></a><span class="sd">        \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-288"><a id="__codelineno-0-288" name="__codelineno-0-288"></a>
</span><span id="__span-0-289"><a id="__codelineno-0-289" name="__codelineno-0-289"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-290"><a id="__codelineno-0-290" name="__codelineno-0-290"></a>
</span><span id="__span-0-291"><a id="__codelineno-0-291" name="__codelineno-0-291"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-292"><a id="__codelineno-0-292" name="__codelineno-0-292"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-293"><a id="__codelineno-0-293" name="__codelineno-0-293"></a>
</span><span id="__span-0-294"><a id="__codelineno-0-294" name="__codelineno-0-294"></a><span class="sd">    for the perturbation type influence case. The gradient is meant to be per sample</span>
</span><span id="__span-0-295"><a id="__codelineno-0-295" name="__codelineno-0-295"></a><span class="sd">    of the batch $(x, y)$. For all input tensors it is assumed,</span>
</span><span id="__span-0-296"><a id="__codelineno-0-296" name="__codelineno-0-296"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-297"><a id="__codelineno-0-297" name="__codelineno-0-297"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-298"><a id="__codelineno-0-298" name="__codelineno-0-298"></a>
</span><span id="__span-0-299"><a id="__codelineno-0-299" name="__codelineno-0-299"></a><span class="sd">    Args:</span>
</span><span id="__span-0-300"><a id="__codelineno-0-300" name="__codelineno-0-300"></a><span class="sd">        z_test_factors: pre-computed tensor, approximating</span>
</span><span id="__span-0-301"><a id="__codelineno-0-301" name="__codelineno-0-301"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-302"><a id="__codelineno-0-302" name="__codelineno-0-302"></a><span class="sd">            f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-303"><a id="__codelineno-0-303" name="__codelineno-0-303"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-304"><a id="__codelineno-0-304" name="__codelineno-0-304"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-305"><a id="__codelineno-0-305" name="__codelineno-0-305"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$</span>
</span><span id="__span-0-306"><a id="__codelineno-0-306" name="__codelineno-0-306"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-307"><a id="__codelineno-0-307" name="__codelineno-0-307"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-308"><a id="__codelineno-0-308" name="__codelineno-0-308"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-309"><a id="__codelineno-0-309" name="__codelineno-0-309"></a>
</span><span id="__span-0-310"><a id="__codelineno-0-310" name="__codelineno-0-310"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-311"><a id="__codelineno-0-311" name="__codelineno-0-311"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-312"><a id="__codelineno-0-312" name="__codelineno-0-312"></a>
</span><span id="__span-0-313"><a id="__codelineno-0-313" name="__codelineno-0-313"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-314"><a id="__codelineno-0-314" name="__codelineno-0-314"></a>    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">:</span>
</span><span id="__span-0-315"><a id="__codelineno-0-315" name="__codelineno-0-315"></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="__span-0-316"><a id="__codelineno-0-316" name="__codelineno-0-316"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-317"><a id="__codelineno-0-317" name="__codelineno-0-317"></a>            <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</span><span id="__span-0-318"><a id="__codelineno-0-318" name="__codelineno-0-318"></a>        <span class="p">)</span>
</span><span id="__span-0-319"><a id="__codelineno-0-319" name="__codelineno-0-319"></a>    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">:</span>
</span><span id="__span-0-320"><a id="__codelineno-0-320" name="__codelineno-0-320"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
</span><span id="__span-0-321"><a id="__codelineno-0-321" name="__codelineno-0-321"></a>            <span class="s2">&quot;ia,j...a-&gt;ij...&quot;</span><span class="p">,</span>
</span><span id="__span-0-322"><a id="__codelineno-0-322" name="__codelineno-0-322"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-323"><a id="__codelineno-0-323" name="__codelineno-0-323"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flat_loss_mixed_grad</span><span class="p">(</span>
</span><span id="__span-0-324"><a id="__codelineno-0-324" name="__codelineno-0-324"></a>                <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-325"><a id="__codelineno-0-325" name="__codelineno-0-325"></a>            <span class="p">),</span>
</span><span id="__span-0-326"><a id="__codelineno-0-326" name="__codelineno-0-326"></a>        <span class="p">)</span>
</span><span id="__span-0-327"><a id="__codelineno-0-327" name="__codelineno-0-327"></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-328"><a id="__codelineno-0-328" name="__codelineno-0-328"></a>        <span class="k">raise</span> <span class="n">UnsupportedInfluenceModeException</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.CgInfluence.influences" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences</span>


<a href="#pydvl.influence.torch.influence_function_model.CgInfluence.influences" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute an approximation of</p>
<div class="arithmatex">\[ \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}})),
    \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle, \]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[ \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}})),
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the case of perturbation-type influence. The approximate action of
<span class="arithmatex">\(H^{-1}\)</span> is achieved via the <a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">conjugate gradient
method</a>.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations of
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
if None, use <span class="arithmatex">\(x=x_{\text{test}}\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>A tensor representing the element-wise scalar products for the
provided batch.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-543">543</a></span>
<span class="normal"><a href="#__codelineno-0-544">544</a></span>
<span class="normal"><a href="#__codelineno-0-545">545</a></span>
<span class="normal"><a href="#__codelineno-0-546">546</a></span>
<span class="normal"><a href="#__codelineno-0-547">547</a></span>
<span class="normal"><a href="#__codelineno-0-548">548</a></span>
<span class="normal"><a href="#__codelineno-0-549">549</a></span>
<span class="normal"><a href="#__codelineno-0-550">550</a></span>
<span class="normal"><a href="#__codelineno-0-551">551</a></span>
<span class="normal"><a href="#__codelineno-0-552">552</a></span>
<span class="normal"><a href="#__codelineno-0-553">553</a></span>
<span class="normal"><a href="#__codelineno-0-554">554</a></span>
<span class="normal"><a href="#__codelineno-0-555">555</a></span>
<span class="normal"><a href="#__codelineno-0-556">556</a></span>
<span class="normal"><a href="#__codelineno-0-557">557</a></span>
<span class="normal"><a href="#__codelineno-0-558">558</a></span>
<span class="normal"><a href="#__codelineno-0-559">559</a></span>
<span class="normal"><a href="#__codelineno-0-560">560</a></span>
<span class="normal"><a href="#__codelineno-0-561">561</a></span>
<span class="normal"><a href="#__codelineno-0-562">562</a></span>
<span class="normal"><a href="#__codelineno-0-563">563</a></span>
<span class="normal"><a href="#__codelineno-0-564">564</a></span>
<span class="normal"><a href="#__codelineno-0-565">565</a></span>
<span class="normal"><a href="#__codelineno-0-566">566</a></span>
<span class="normal"><a href="#__codelineno-0-567">567</a></span>
<span class="normal"><a href="#__codelineno-0-568">568</a></span>
<span class="normal"><a href="#__codelineno-0-569">569</a></span>
<span class="normal"><a href="#__codelineno-0-570">570</a></span>
<span class="normal"><a href="#__codelineno-0-571">571</a></span>
<span class="normal"><a href="#__codelineno-0-572">572</a></span>
<span class="normal"><a href="#__codelineno-0-573">573</a></span>
<span class="normal"><a href="#__codelineno-0-574">574</a></span>
<span class="normal"><a href="#__codelineno-0-575">575</a></span>
<span class="normal"><a href="#__codelineno-0-576">576</a></span>
<span class="normal"><a href="#__codelineno-0-577">577</a></span>
<span class="normal"><a href="#__codelineno-0-578">578</a></span>
<span class="normal"><a href="#__codelineno-0-579">579</a></span>
<span class="normal"><a href="#__codelineno-0-580">580</a></span>
<span class="normal"><a href="#__codelineno-0-581">581</a></span>
<span class="normal"><a href="#__codelineno-0-582">582</a></span>
<span class="normal"><a href="#__codelineno-0-583">583</a></span>
<span class="normal"><a href="#__codelineno-0-584">584</a></span>
<span class="normal"><a href="#__codelineno-0-585">585</a></span>
<span class="normal"><a href="#__codelineno-0-586">586</a></span>
<span class="normal"><a href="#__codelineno-0-587">587</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-543"><a id="__codelineno-0-543" name="__codelineno-0-543"></a><span class="nd">@log_duration</span>
</span><span id="__span-0-544"><a id="__codelineno-0-544" name="__codelineno-0-544"></a><span class="k">def</span> <span class="nf">influences</span><span class="p">(</span>
</span><span id="__span-0-545"><a id="__codelineno-0-545" name="__codelineno-0-545"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-546"><a id="__codelineno-0-546" name="__codelineno-0-546"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-547"><a id="__codelineno-0-547" name="__codelineno-0-547"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-548"><a id="__codelineno-0-548" name="__codelineno-0-548"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-549"><a id="__codelineno-0-549" name="__codelineno-0-549"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-550"><a id="__codelineno-0-550" name="__codelineno-0-550"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-551"><a id="__codelineno-0-551" name="__codelineno-0-551"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-552"><a id="__codelineno-0-552" name="__codelineno-0-552"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-553"><a id="__codelineno-0-553" name="__codelineno-0-553"></a><span class="sd">    Compute an approximation of</span>
</span><span id="__span-0-554"><a id="__codelineno-0-554" name="__codelineno-0-554"></a>
</span><span id="__span-0-555"><a id="__codelineno-0-555" name="__codelineno-0-555"></a><span class="sd">    \[ \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-556"><a id="__codelineno-0-556" name="__codelineno-0-556"></a><span class="sd">        f_{\theta}(x_{\text{test}})),</span>
</span><span id="__span-0-557"><a id="__codelineno-0-557" name="__codelineno-0-557"></a><span class="sd">        \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle, \]</span>
</span><span id="__span-0-558"><a id="__codelineno-0-558" name="__codelineno-0-558"></a>
</span><span id="__span-0-559"><a id="__codelineno-0-559" name="__codelineno-0-559"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-560"><a id="__codelineno-0-560" name="__codelineno-0-560"></a>
</span><span id="__span-0-561"><a id="__codelineno-0-561" name="__codelineno-0-561"></a><span class="sd">    \[ \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-562"><a id="__codelineno-0-562" name="__codelineno-0-562"></a><span class="sd">        f_{\theta}(x_{\text{test}})),</span>
</span><span id="__span-0-563"><a id="__codelineno-0-563" name="__codelineno-0-563"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-564"><a id="__codelineno-0-564" name="__codelineno-0-564"></a>
</span><span id="__span-0-565"><a id="__codelineno-0-565" name="__codelineno-0-565"></a><span class="sd">    for the case of perturbation-type influence. The approximate action of</span>
</span><span id="__span-0-566"><a id="__codelineno-0-566" name="__codelineno-0-566"></a><span class="sd">    $H^{-1}$ is achieved via the [conjugate gradient</span>
</span><span id="__span-0-567"><a id="__codelineno-0-567" name="__codelineno-0-567"></a><span class="sd">    method](https://en.wikipedia.org/wiki/Conjugate_gradient_method).</span>
</span><span id="__span-0-568"><a id="__codelineno-0-568" name="__codelineno-0-568"></a>
</span><span id="__span-0-569"><a id="__codelineno-0-569" name="__codelineno-0-569"></a><span class="sd">    Args:</span>
</span><span id="__span-0-570"><a id="__codelineno-0-570" name="__codelineno-0-570"></a><span class="sd">        x_test: model input to use in the gradient computations of</span>
</span><span id="__span-0-571"><a id="__codelineno-0-571" name="__codelineno-0-571"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-572"><a id="__codelineno-0-572" name="__codelineno-0-572"></a><span class="sd">                f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-573"><a id="__codelineno-0-573" name="__codelineno-0-573"></a><span class="sd">        y_test: label tensor to compute gradients</span>
</span><span id="__span-0-574"><a id="__codelineno-0-574" name="__codelineno-0-574"></a><span class="sd">        x: optional model input to use in the gradient computations</span>
</span><span id="__span-0-575"><a id="__codelineno-0-575" name="__codelineno-0-575"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-576"><a id="__codelineno-0-576" name="__codelineno-0-576"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-577"><a id="__codelineno-0-577" name="__codelineno-0-577"></a><span class="sd">            if None, use $x=x_{\text{test}}$</span>
</span><span id="__span-0-578"><a id="__codelineno-0-578" name="__codelineno-0-578"></a><span class="sd">        y: optional label tensor to compute gradients</span>
</span><span id="__span-0-579"><a id="__codelineno-0-579" name="__codelineno-0-579"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-580"><a id="__codelineno-0-580" name="__codelineno-0-580"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-581"><a id="__codelineno-0-581" name="__codelineno-0-581"></a>
</span><span id="__span-0-582"><a id="__codelineno-0-582" name="__codelineno-0-582"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-583"><a id="__codelineno-0-583" name="__codelineno-0-583"></a><span class="sd">        A tensor representing the element-wise scalar products for the</span>
</span><span id="__span-0-584"><a id="__codelineno-0-584" name="__codelineno-0-584"></a><span class="sd">            provided batch.</span>
</span><span id="__span-0-585"><a id="__codelineno-0-585" name="__codelineno-0-585"></a>
</span><span id="__span-0-586"><a id="__codelineno-0-586" name="__codelineno-0-586"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-587"><a id="__codelineno-0-587" name="__codelineno-0-587"></a>    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pydvl.influence.torch.influence_function_model.LissaInfluence" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <span class="doc doc-object-name doc-class-name">LissaInfluence</span>


<a href="#pydvl.influence.torch.influence_function_model.LissaInfluence" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">LissaInfluence</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">model</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">[[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">maxiter</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">dampen</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">scale</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">h0</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="n">rtol</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">,</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="n">progress</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    <span class="n">warn_on_max_iteration</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="p">)</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel" href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel">TorchInfluenceFunctionModel</a></code></p>

  
      <p>Uses LISSA, Linear time Stochastic Second-Order Algorithm, to iteratively
approximate the inverse Hessian. More precisely, it finds x s.t. <span class="arithmatex">\(Hx = b\)</span>,
with <span class="arithmatex">\(H\)</span> being the model's second derivative wrt. the parameters.
This is done with the update</p>
<div class="arithmatex">\[H^{-1}_{j+1} b = b + (I - d) \ H - \frac{H^{-1}_j b}{s},\]</div>
<p>where <span class="arithmatex">\(I\)</span> is the identity matrix, <span class="arithmatex">\(d\)</span> is a dampening term and <span class="arithmatex">\(s\)</span> a scaling
factor that are applied to help convergence. For details,
see <a class="autorefs autorefs-internal" href="../../../../../influence/influence_function_model/#linear-time-stochastic-second-order-approximation-lissa">Linear time Stochastic Second-Order Approximation (<abbr title="Linear-time Stochastic Second-order Algorithm">LiSSA</abbr>)</a></p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A PyTorch model. The Hessian will be calculated with respect to
this model's parameters.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>loss</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A callable that takes the model's output and target as input and returns
  the scalar loss.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a>[[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>], <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>hessian_regularization</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Optional regularization parameter added
to the Hessian-vector product for numerical stability.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>0.0</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>maxiter</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Maximum number of iterations.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>1000</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>dampen</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Dampening factor, defaults to 0 for no dampening.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>0.0</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>scale</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Scaling factor, defaults to 10.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>10.0</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>h0</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Initial guess for hvp.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>rtol</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>tolerance to use for early stopping</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>0.0001</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>progress</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>If True, display progress bars.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>False</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>warn_on_max_iteration</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>If True, logs a warning, if the desired tolerance is not
achieved within <code>maxiter</code> iterations. If False, the log level for this
information is <code>logging.DEBUG</code></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>True</code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>

                <details class="quote">
                  <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
                  <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-843">843</a></span>
<span class="normal"><a href="#__codelineno-0-844">844</a></span>
<span class="normal"><a href="#__codelineno-0-845">845</a></span>
<span class="normal"><a href="#__codelineno-0-846">846</a></span>
<span class="normal"><a href="#__codelineno-0-847">847</a></span>
<span class="normal"><a href="#__codelineno-0-848">848</a></span>
<span class="normal"><a href="#__codelineno-0-849">849</a></span>
<span class="normal"><a href="#__codelineno-0-850">850</a></span>
<span class="normal"><a href="#__codelineno-0-851">851</a></span>
<span class="normal"><a href="#__codelineno-0-852">852</a></span>
<span class="normal"><a href="#__codelineno-0-853">853</a></span>
<span class="normal"><a href="#__codelineno-0-854">854</a></span>
<span class="normal"><a href="#__codelineno-0-855">855</a></span>
<span class="normal"><a href="#__codelineno-0-856">856</a></span>
<span class="normal"><a href="#__codelineno-0-857">857</a></span>
<span class="normal"><a href="#__codelineno-0-858">858</a></span>
<span class="normal"><a href="#__codelineno-0-859">859</a></span>
<span class="normal"><a href="#__codelineno-0-860">860</a></span>
<span class="normal"><a href="#__codelineno-0-861">861</a></span>
<span class="normal"><a href="#__codelineno-0-862">862</a></span>
<span class="normal"><a href="#__codelineno-0-863">863</a></span>
<span class="normal"><a href="#__codelineno-0-864">864</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-843"><a id="__codelineno-0-843" name="__codelineno-0-843"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="__span-0-844"><a id="__codelineno-0-844" name="__codelineno-0-844"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-845"><a id="__codelineno-0-845" name="__codelineno-0-845"></a>    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="__span-0-846"><a id="__codelineno-0-846" name="__codelineno-0-846"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span id="__span-0-847"><a id="__codelineno-0-847" name="__codelineno-0-847"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-848"><a id="__codelineno-0-848" name="__codelineno-0-848"></a>    <span class="n">maxiter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
</span><span id="__span-0-849"><a id="__codelineno-0-849" name="__codelineno-0-849"></a>    <span class="n">dampen</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-850"><a id="__codelineno-0-850" name="__codelineno-0-850"></a>    <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
</span><span id="__span-0-851"><a id="__codelineno-0-851" name="__codelineno-0-851"></a>    <span class="n">h0</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-852"><a id="__codelineno-0-852" name="__codelineno-0-852"></a>    <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span>
</span><span id="__span-0-853"><a id="__codelineno-0-853" name="__codelineno-0-853"></a>    <span class="n">progress</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-854"><a id="__codelineno-0-854" name="__codelineno-0-854"></a>    <span class="n">warn_on_max_iteration</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="__span-0-855"><a id="__codelineno-0-855" name="__codelineno-0-855"></a><span class="p">):</span>
</span><span id="__span-0-856"><a id="__codelineno-0-856" name="__codelineno-0-856"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</span><span id="__span-0-857"><a id="__codelineno-0-857" name="__codelineno-0-857"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">warn_on_max_iteration</span> <span class="o">=</span> <span class="n">warn_on_max_iteration</span>
</span><span id="__span-0-858"><a id="__codelineno-0-858" name="__codelineno-0-858"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">maxiter</span> <span class="o">=</span> <span class="n">maxiter</span>
</span><span id="__span-0-859"><a id="__codelineno-0-859" name="__codelineno-0-859"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">hessian_regularization</span> <span class="o">=</span> <span class="n">hessian_regularization</span>
</span><span id="__span-0-860"><a id="__codelineno-0-860" name="__codelineno-0-860"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">progress</span> <span class="o">=</span> <span class="n">progress</span>
</span><span id="__span-0-861"><a id="__codelineno-0-861" name="__codelineno-0-861"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rtol</span> <span class="o">=</span> <span class="n">rtol</span>
</span><span id="__span-0-862"><a id="__codelineno-0-862" name="__codelineno-0-862"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">h0</span> <span class="o">=</span> <span class="n">h0</span>
</span><span id="__span-0-863"><a id="__codelineno-0-863" name="__codelineno-0-863"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
</span><span id="__span-0-864"><a id="__codelineno-0-864" name="__codelineno-0-864"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">dampen</span> <span class="o">=</span> <span class="n">dampen</span>
</span></code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.LissaInfluence.influence_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influence_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.LissaInfluence.influence_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute approximation of</p>
<div class="arithmatex">\[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</div>
<p>where the gradient is meant to be per sample of the batch <span class="arithmatex">\((x, y)\)</span>.
For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise inverse Hessian matrix vector products</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-245"><a id="__codelineno-0-245" name="__codelineno-0-245"></a><span class="k">def</span> <span class="nf">influence_factors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-246"><a id="__codelineno-0-246" name="__codelineno-0-246"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-247"><a id="__codelineno-0-247" name="__codelineno-0-247"></a><span class="sd">    Compute approximation of</span>
</span><span id="__span-0-248"><a id="__codelineno-0-248" name="__codelineno-0-248"></a>
</span><span id="__span-0-249"><a id="__codelineno-0-249" name="__codelineno-0-249"></a><span class="sd">    \[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</span>
</span><span id="__span-0-250"><a id="__codelineno-0-250" name="__codelineno-0-250"></a>
</span><span id="__span-0-251"><a id="__codelineno-0-251" name="__codelineno-0-251"></a><span class="sd">    where the gradient is meant to be per sample of the batch $(x, y)$.</span>
</span><span id="__span-0-252"><a id="__codelineno-0-252" name="__codelineno-0-252"></a><span class="sd">    For all input tensors it is assumed,</span>
</span><span id="__span-0-253"><a id="__codelineno-0-253" name="__codelineno-0-253"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-254"><a id="__codelineno-0-254" name="__codelineno-0-254"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-255"><a id="__codelineno-0-255" name="__codelineno-0-255"></a>
</span><span id="__span-0-256"><a id="__codelineno-0-256" name="__codelineno-0-256"></a><span class="sd">    Args:</span>
</span><span id="__span-0-257"><a id="__codelineno-0-257" name="__codelineno-0-257"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-258"><a id="__codelineno-0-258" name="__codelineno-0-258"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-259"><a id="__codelineno-0-259" name="__codelineno-0-259"></a>
</span><span id="__span-0-260"><a id="__codelineno-0-260" name="__codelineno-0-260"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-261"><a id="__codelineno-0-261" name="__codelineno-0-261"></a><span class="sd">        Tensor representing the element-wise inverse Hessian matrix vector products</span>
</span><span id="__span-0-262"><a id="__codelineno-0-262" name="__codelineno-0-262"></a>
</span><span id="__span-0-263"><a id="__codelineno-0-263" name="__codelineno-0-263"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-264"><a id="__codelineno-0-264" name="__codelineno-0-264"></a>    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.LissaInfluence.influences" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences</span>


<a href="#pydvl.influence.torch.influence_function_model.LissaInfluence.influences" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute the approximation of</p>
<div class="arithmatex">\[
\langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}})), \nabla_{\theta} \ell(y, f_{\theta}(x))\rangle
\]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[
\langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}}, f_{\theta}(x_{\text{test}})),
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle
\]</div>
<p>for the perturbation type influence case. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
of <span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
if None, use <span class="arithmatex">\(x=x_{\text{test}}\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-121"><a id="__codelineno-0-121" name="__codelineno-0-121"></a><span class="k">def</span> <span class="nf">influences</span><span class="p">(</span>
</span><span id="__span-0-122"><a id="__codelineno-0-122" name="__codelineno-0-122"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-123"><a id="__codelineno-0-123" name="__codelineno-0-123"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-124"><a id="__codelineno-0-124" name="__codelineno-0-124"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-125"><a id="__codelineno-0-125" name="__codelineno-0-125"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-126"><a id="__codelineno-0-126" name="__codelineno-0-126"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-127"><a id="__codelineno-0-127" name="__codelineno-0-127"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-128"><a id="__codelineno-0-128" name="__codelineno-0-128"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-129"><a id="__codelineno-0-129" name="__codelineno-0-129"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-130"><a id="__codelineno-0-130" name="__codelineno-0-130"></a><span class="sd">    Compute the approximation of</span>
</span><span id="__span-0-131"><a id="__codelineno-0-131" name="__codelineno-0-131"></a>
</span><span id="__span-0-132"><a id="__codelineno-0-132" name="__codelineno-0-132"></a><span class="sd">    \[</span>
</span><span id="__span-0-133"><a id="__codelineno-0-133" name="__codelineno-0-133"></a><span class="sd">    \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-134"><a id="__codelineno-0-134" name="__codelineno-0-134"></a><span class="sd">    f_{\theta}(x_{\text{test}})), \nabla_{\theta} \ell(y, f_{\theta}(x))\rangle</span>
</span><span id="__span-0-135"><a id="__codelineno-0-135" name="__codelineno-0-135"></a><span class="sd">    \]</span>
</span><span id="__span-0-136"><a id="__codelineno-0-136" name="__codelineno-0-136"></a>
</span><span id="__span-0-137"><a id="__codelineno-0-137" name="__codelineno-0-137"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-138"><a id="__codelineno-0-138" name="__codelineno-0-138"></a>
</span><span id="__span-0-139"><a id="__codelineno-0-139" name="__codelineno-0-139"></a><span class="sd">    \[</span>
</span><span id="__span-0-140"><a id="__codelineno-0-140" name="__codelineno-0-140"></a><span class="sd">    \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}}, f_{\theta}(x_{\text{test}})),</span>
</span><span id="__span-0-141"><a id="__codelineno-0-141" name="__codelineno-0-141"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle</span>
</span><span id="__span-0-142"><a id="__codelineno-0-142" name="__codelineno-0-142"></a><span class="sd">    \]</span>
</span><span id="__span-0-143"><a id="__codelineno-0-143" name="__codelineno-0-143"></a>
</span><span id="__span-0-144"><a id="__codelineno-0-144" name="__codelineno-0-144"></a><span class="sd">    for the perturbation type influence case. For all input tensors it is assumed,</span>
</span><span id="__span-0-145"><a id="__codelineno-0-145" name="__codelineno-0-145"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-146"><a id="__codelineno-0-146" name="__codelineno-0-146"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-147"><a id="__codelineno-0-147" name="__codelineno-0-147"></a>
</span><span id="__span-0-148"><a id="__codelineno-0-148" name="__codelineno-0-148"></a><span class="sd">    Args:</span>
</span><span id="__span-0-149"><a id="__codelineno-0-149" name="__codelineno-0-149"></a><span class="sd">        x_test: model input to use in the gradient computations</span>
</span><span id="__span-0-150"><a id="__codelineno-0-150" name="__codelineno-0-150"></a><span class="sd">            of $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-151"><a id="__codelineno-0-151" name="__codelineno-0-151"></a><span class="sd">                f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-152"><a id="__codelineno-0-152" name="__codelineno-0-152"></a><span class="sd">        y_test: label tensor to compute gradients</span>
</span><span id="__span-0-153"><a id="__codelineno-0-153" name="__codelineno-0-153"></a><span class="sd">        x: optional model input to use in the gradient computations</span>
</span><span id="__span-0-154"><a id="__codelineno-0-154" name="__codelineno-0-154"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-155"><a id="__codelineno-0-155" name="__codelineno-0-155"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-156"><a id="__codelineno-0-156" name="__codelineno-0-156"></a><span class="sd">            if None, use $x=x_{\text{test}}$</span>
</span><span id="__span-0-157"><a id="__codelineno-0-157" name="__codelineno-0-157"></a><span class="sd">        y: optional label tensor to compute gradients</span>
</span><span id="__span-0-158"><a id="__codelineno-0-158" name="__codelineno-0-158"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-159"><a id="__codelineno-0-159" name="__codelineno-0-159"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-160"><a id="__codelineno-0-160" name="__codelineno-0-160"></a>
</span><span id="__span-0-161"><a id="__codelineno-0-161" name="__codelineno-0-161"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-162"><a id="__codelineno-0-162" name="__codelineno-0-162"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-163"><a id="__codelineno-0-163" name="__codelineno-0-163"></a>
</span><span id="__span-0-164"><a id="__codelineno-0-164" name="__codelineno-0-164"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-165"><a id="__codelineno-0-165" name="__codelineno-0-165"></a>    <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
</span><span id="__span-0-166"><a id="__codelineno-0-166" name="__codelineno-0-166"></a>    <span class="k">return</span> <span class="n">t</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.LissaInfluence.influences_from_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences_from_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.LissaInfluence.influences_from_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Computation of</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the perturbation type influence case. The gradient is meant to be per sample
of the batch <span class="arithmatex">\((x, y)\)</span>. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>z_test_factors</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>pre-computed tensor, approximating
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span>
<span class="normal"><a href="#__codelineno-0-320">320</a></span>
<span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-276"><a id="__codelineno-0-276" name="__codelineno-0-276"></a><span class="k">def</span> <span class="nf">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-277"><a id="__codelineno-0-277" name="__codelineno-0-277"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-278"><a id="__codelineno-0-278" name="__codelineno-0-278"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-279"><a id="__codelineno-0-279" name="__codelineno-0-279"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-280"><a id="__codelineno-0-280" name="__codelineno-0-280"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-281"><a id="__codelineno-0-281" name="__codelineno-0-281"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-282"><a id="__codelineno-0-282" name="__codelineno-0-282"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-283"><a id="__codelineno-0-283" name="__codelineno-0-283"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-284"><a id="__codelineno-0-284" name="__codelineno-0-284"></a><span class="sd">    Computation of</span>
</span><span id="__span-0-285"><a id="__codelineno-0-285" name="__codelineno-0-285"></a>
</span><span id="__span-0-286"><a id="__codelineno-0-286" name="__codelineno-0-286"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-287"><a id="__codelineno-0-287" name="__codelineno-0-287"></a><span class="sd">        \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-288"><a id="__codelineno-0-288" name="__codelineno-0-288"></a>
</span><span id="__span-0-289"><a id="__codelineno-0-289" name="__codelineno-0-289"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-290"><a id="__codelineno-0-290" name="__codelineno-0-290"></a>
</span><span id="__span-0-291"><a id="__codelineno-0-291" name="__codelineno-0-291"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-292"><a id="__codelineno-0-292" name="__codelineno-0-292"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-293"><a id="__codelineno-0-293" name="__codelineno-0-293"></a>
</span><span id="__span-0-294"><a id="__codelineno-0-294" name="__codelineno-0-294"></a><span class="sd">    for the perturbation type influence case. The gradient is meant to be per sample</span>
</span><span id="__span-0-295"><a id="__codelineno-0-295" name="__codelineno-0-295"></a><span class="sd">    of the batch $(x, y)$. For all input tensors it is assumed,</span>
</span><span id="__span-0-296"><a id="__codelineno-0-296" name="__codelineno-0-296"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-297"><a id="__codelineno-0-297" name="__codelineno-0-297"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-298"><a id="__codelineno-0-298" name="__codelineno-0-298"></a>
</span><span id="__span-0-299"><a id="__codelineno-0-299" name="__codelineno-0-299"></a><span class="sd">    Args:</span>
</span><span id="__span-0-300"><a id="__codelineno-0-300" name="__codelineno-0-300"></a><span class="sd">        z_test_factors: pre-computed tensor, approximating</span>
</span><span id="__span-0-301"><a id="__codelineno-0-301" name="__codelineno-0-301"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-302"><a id="__codelineno-0-302" name="__codelineno-0-302"></a><span class="sd">            f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-303"><a id="__codelineno-0-303" name="__codelineno-0-303"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-304"><a id="__codelineno-0-304" name="__codelineno-0-304"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-305"><a id="__codelineno-0-305" name="__codelineno-0-305"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$</span>
</span><span id="__span-0-306"><a id="__codelineno-0-306" name="__codelineno-0-306"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-307"><a id="__codelineno-0-307" name="__codelineno-0-307"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-308"><a id="__codelineno-0-308" name="__codelineno-0-308"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-309"><a id="__codelineno-0-309" name="__codelineno-0-309"></a>
</span><span id="__span-0-310"><a id="__codelineno-0-310" name="__codelineno-0-310"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-311"><a id="__codelineno-0-311" name="__codelineno-0-311"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-312"><a id="__codelineno-0-312" name="__codelineno-0-312"></a>
</span><span id="__span-0-313"><a id="__codelineno-0-313" name="__codelineno-0-313"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-314"><a id="__codelineno-0-314" name="__codelineno-0-314"></a>    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">:</span>
</span><span id="__span-0-315"><a id="__codelineno-0-315" name="__codelineno-0-315"></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="__span-0-316"><a id="__codelineno-0-316" name="__codelineno-0-316"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-317"><a id="__codelineno-0-317" name="__codelineno-0-317"></a>            <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</span><span id="__span-0-318"><a id="__codelineno-0-318" name="__codelineno-0-318"></a>        <span class="p">)</span>
</span><span id="__span-0-319"><a id="__codelineno-0-319" name="__codelineno-0-319"></a>    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">:</span>
</span><span id="__span-0-320"><a id="__codelineno-0-320" name="__codelineno-0-320"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
</span><span id="__span-0-321"><a id="__codelineno-0-321" name="__codelineno-0-321"></a>            <span class="s2">&quot;ia,j...a-&gt;ij...&quot;</span><span class="p">,</span>
</span><span id="__span-0-322"><a id="__codelineno-0-322" name="__codelineno-0-322"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-323"><a id="__codelineno-0-323" name="__codelineno-0-323"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flat_loss_mixed_grad</span><span class="p">(</span>
</span><span id="__span-0-324"><a id="__codelineno-0-324" name="__codelineno-0-324"></a>                <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-325"><a id="__codelineno-0-325" name="__codelineno-0-325"></a>            <span class="p">),</span>
</span><span id="__span-0-326"><a id="__codelineno-0-326" name="__codelineno-0-326"></a>        <span class="p">)</span>
</span><span id="__span-0-327"><a id="__codelineno-0-327" name="__codelineno-0-327"></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-328"><a id="__codelineno-0-328" name="__codelineno-0-328"></a>        <span class="k">raise</span> <span class="n">UnsupportedInfluenceModeException</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pydvl.influence.torch.influence_function_model.ArnoldiInfluence" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <span class="doc doc-object-name doc-class-name">ArnoldiInfluence</span>


<a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">ArnoldiInfluence</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">model</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">[[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">rank_estimate</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">krylov_dimension</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">tol</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">1e-06</span><span class="p">,</span>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">max_iter</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="n">eigen_computation_on_gpu</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="n">precompute_grad</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="p">)</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel" href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel">TorchInfluenceFunctionModel</a></code></p>

  
      <p>Solves the linear system Hx = b, where H is the Hessian of the model's loss function
and b is the given right-hand side vector.
It employs the [implicitly restarted Arnoldi method]
(https://en.wikipedia.org/wiki/Arnoldi_iteration) for
computing a partial eigen decomposition, which is used fo the inversion i.e.</p>
<div class="arithmatex">\[x = V D^{-1} V^T b\]</div>
<p>where <span class="arithmatex">\(D\)</span> is a diagonal matrix with the top (in absolute value) <code>rank_estimate</code>
eigenvalues of the Hessian
and <span class="arithmatex">\(V\)</span> contains the corresponding eigenvectors.
For more information, see <a class="autorefs autorefs-internal" href="../../../../../influence/influence_function_model/#arnoldi">Arnoldi</a>.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A PyTorch model. The Hessian will be calculated with respect to
this model's parameters.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>loss</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A callable that takes the model's output and target as input and returns
  the scalar loss.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a>[[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>], <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>hessian_regularization</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Optional regularization parameter added
to the Hessian-vector product for numerical stability.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>0.0</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>rank_estimate</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>The number of eigenvalues and corresponding eigenvectors
to compute. Represents the desired rank of the Hessian approximation.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>10</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>krylov_dimension</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>The number of Krylov vectors to use for the Lanczos method.
Defaults to min(model's number of parameters,
max(2 times rank_estimate + 1, 20)).</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>tol</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>The stopping criteria for the Lanczos algorithm.
Ignored if <code>low_rank_representation</code> is provided.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>1e-06</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>max_iter</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>The maximum number of iterations for the Lanczos method.
Ignored if <code>low_rank_representation</code> is provided.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>eigen_computation_on_gpu</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>If True, tries to execute the eigen pair approximation
on the model's device
via a cupy implementation. Ensure the model size or rank_estimate
is appropriate for device memory.
If False, the eigen pair approximation is executed on the CPU by the scipy
wrapper to ARPACK.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>False</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>precompute_grad</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>If True, the full data gradient is precomputed and kept
in memory, which can speed up the hessian vector product computation.
Set this to False, if you can't afford to keep the full computation graph
in memory.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>False</code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>

                <details class="quote">
                  <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
                  <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-991"> 991</a></span>
<span class="normal"><a href="#__codelineno-0-992"> 992</a></span>
<span class="normal"><a href="#__codelineno-0-993"> 993</a></span>
<span class="normal"><a href="#__codelineno-0-994"> 994</a></span>
<span class="normal"><a href="#__codelineno-0-995"> 995</a></span>
<span class="normal"><a href="#__codelineno-0-996"> 996</a></span>
<span class="normal"><a href="#__codelineno-0-997"> 997</a></span>
<span class="normal"><a href="#__codelineno-0-998"> 998</a></span>
<span class="normal"><a href="#__codelineno-0-999"> 999</a></span>
<span class="normal"><a href="#__codelineno-0-1000">1000</a></span>
<span class="normal"><a href="#__codelineno-0-1001">1001</a></span>
<span class="normal"><a href="#__codelineno-0-1002">1002</a></span>
<span class="normal"><a href="#__codelineno-0-1003">1003</a></span>
<span class="normal"><a href="#__codelineno-0-1004">1004</a></span>
<span class="normal"><a href="#__codelineno-0-1005">1005</a></span>
<span class="normal"><a href="#__codelineno-0-1006">1006</a></span>
<span class="normal"><a href="#__codelineno-0-1007">1007</a></span>
<span class="normal"><a href="#__codelineno-0-1008">1008</a></span>
<span class="normal"><a href="#__codelineno-0-1009">1009</a></span>
<span class="normal"><a href="#__codelineno-0-1010">1010</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-991"><a id="__codelineno-0-991" name="__codelineno-0-991"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="__span-0-992"><a id="__codelineno-0-992" name="__codelineno-0-992"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-993"><a id="__codelineno-0-993" name="__codelineno-0-993"></a>    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="__span-0-994"><a id="__codelineno-0-994" name="__codelineno-0-994"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span id="__span-0-995"><a id="__codelineno-0-995" name="__codelineno-0-995"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-996"><a id="__codelineno-0-996" name="__codelineno-0-996"></a>    <span class="n">rank_estimate</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
</span><span id="__span-0-997"><a id="__codelineno-0-997" name="__codelineno-0-997"></a>    <span class="n">krylov_dimension</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-998"><a id="__codelineno-0-998" name="__codelineno-0-998"></a>    <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
</span><span id="__span-0-999"><a id="__codelineno-0-999" name="__codelineno-0-999"></a>    <span class="n">max_iter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-1000"><a id="__codelineno-0-1000" name="__codelineno-0-1000"></a>    <span class="n">eigen_computation_on_gpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-1001"><a id="__codelineno-0-1001" name="__codelineno-0-1001"></a>    <span class="n">precompute_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-1002"><a id="__codelineno-0-1002" name="__codelineno-0-1002"></a><span class="p">):</span>
</span><span id="__span-0-1003"><a id="__codelineno-0-1003" name="__codelineno-0-1003"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</span><span id="__span-0-1004"><a id="__codelineno-0-1004" name="__codelineno-0-1004"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">hessian_regularization</span> <span class="o">=</span> <span class="n">hessian_regularization</span>
</span><span id="__span-0-1005"><a id="__codelineno-0-1005" name="__codelineno-0-1005"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rank_estimate</span> <span class="o">=</span> <span class="n">rank_estimate</span>
</span><span id="__span-0-1006"><a id="__codelineno-0-1006" name="__codelineno-0-1006"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
</span><span id="__span-0-1007"><a id="__codelineno-0-1007" name="__codelineno-0-1007"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
</span><span id="__span-0-1008"><a id="__codelineno-0-1008" name="__codelineno-0-1008"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">krylov_dimension</span> <span class="o">=</span> <span class="n">krylov_dimension</span>
</span><span id="__span-0-1009"><a id="__codelineno-0-1009" name="__codelineno-0-1009"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">eigen_computation_on_gpu</span> <span class="o">=</span> <span class="n">eigen_computation_on_gpu</span>
</span><span id="__span-0-1010"><a id="__codelineno-0-1010" name="__codelineno-0-1010"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">precompute_grad</span> <span class="o">=</span> <span class="n">precompute_grad</span>
</span></code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influence_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influence_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influence_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute approximation of</p>
<div class="arithmatex">\[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</div>
<p>where the gradient is meant to be per sample of the batch <span class="arithmatex">\((x, y)\)</span>.
For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise inverse Hessian matrix vector products</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-245"><a id="__codelineno-0-245" name="__codelineno-0-245"></a><span class="k">def</span> <span class="nf">influence_factors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-246"><a id="__codelineno-0-246" name="__codelineno-0-246"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-247"><a id="__codelineno-0-247" name="__codelineno-0-247"></a><span class="sd">    Compute approximation of</span>
</span><span id="__span-0-248"><a id="__codelineno-0-248" name="__codelineno-0-248"></a>
</span><span id="__span-0-249"><a id="__codelineno-0-249" name="__codelineno-0-249"></a><span class="sd">    \[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</span>
</span><span id="__span-0-250"><a id="__codelineno-0-250" name="__codelineno-0-250"></a>
</span><span id="__span-0-251"><a id="__codelineno-0-251" name="__codelineno-0-251"></a><span class="sd">    where the gradient is meant to be per sample of the batch $(x, y)$.</span>
</span><span id="__span-0-252"><a id="__codelineno-0-252" name="__codelineno-0-252"></a><span class="sd">    For all input tensors it is assumed,</span>
</span><span id="__span-0-253"><a id="__codelineno-0-253" name="__codelineno-0-253"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-254"><a id="__codelineno-0-254" name="__codelineno-0-254"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-255"><a id="__codelineno-0-255" name="__codelineno-0-255"></a>
</span><span id="__span-0-256"><a id="__codelineno-0-256" name="__codelineno-0-256"></a><span class="sd">    Args:</span>
</span><span id="__span-0-257"><a id="__codelineno-0-257" name="__codelineno-0-257"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-258"><a id="__codelineno-0-258" name="__codelineno-0-258"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-259"><a id="__codelineno-0-259" name="__codelineno-0-259"></a>
</span><span id="__span-0-260"><a id="__codelineno-0-260" name="__codelineno-0-260"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-261"><a id="__codelineno-0-261" name="__codelineno-0-261"></a><span class="sd">        Tensor representing the element-wise inverse Hessian matrix vector products</span>
</span><span id="__span-0-262"><a id="__codelineno-0-262" name="__codelineno-0-262"></a>
</span><span id="__span-0-263"><a id="__codelineno-0-263" name="__codelineno-0-263"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-264"><a id="__codelineno-0-264" name="__codelineno-0-264"></a>    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences</span>


<a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute the approximation of</p>
<div class="arithmatex">\[
\langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}})), \nabla_{\theta} \ell(y, f_{\theta}(x))\rangle
\]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[
\langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}}, f_{\theta}(x_{\text{test}})),
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle
\]</div>
<p>for the perturbation type influence case. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
of <span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
if None, use <span class="arithmatex">\(x=x_{\text{test}}\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-121"><a id="__codelineno-0-121" name="__codelineno-0-121"></a><span class="k">def</span> <span class="nf">influences</span><span class="p">(</span>
</span><span id="__span-0-122"><a id="__codelineno-0-122" name="__codelineno-0-122"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-123"><a id="__codelineno-0-123" name="__codelineno-0-123"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-124"><a id="__codelineno-0-124" name="__codelineno-0-124"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-125"><a id="__codelineno-0-125" name="__codelineno-0-125"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-126"><a id="__codelineno-0-126" name="__codelineno-0-126"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-127"><a id="__codelineno-0-127" name="__codelineno-0-127"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-128"><a id="__codelineno-0-128" name="__codelineno-0-128"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-129"><a id="__codelineno-0-129" name="__codelineno-0-129"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-130"><a id="__codelineno-0-130" name="__codelineno-0-130"></a><span class="sd">    Compute the approximation of</span>
</span><span id="__span-0-131"><a id="__codelineno-0-131" name="__codelineno-0-131"></a>
</span><span id="__span-0-132"><a id="__codelineno-0-132" name="__codelineno-0-132"></a><span class="sd">    \[</span>
</span><span id="__span-0-133"><a id="__codelineno-0-133" name="__codelineno-0-133"></a><span class="sd">    \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-134"><a id="__codelineno-0-134" name="__codelineno-0-134"></a><span class="sd">    f_{\theta}(x_{\text{test}})), \nabla_{\theta} \ell(y, f_{\theta}(x))\rangle</span>
</span><span id="__span-0-135"><a id="__codelineno-0-135" name="__codelineno-0-135"></a><span class="sd">    \]</span>
</span><span id="__span-0-136"><a id="__codelineno-0-136" name="__codelineno-0-136"></a>
</span><span id="__span-0-137"><a id="__codelineno-0-137" name="__codelineno-0-137"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-138"><a id="__codelineno-0-138" name="__codelineno-0-138"></a>
</span><span id="__span-0-139"><a id="__codelineno-0-139" name="__codelineno-0-139"></a><span class="sd">    \[</span>
</span><span id="__span-0-140"><a id="__codelineno-0-140" name="__codelineno-0-140"></a><span class="sd">    \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}}, f_{\theta}(x_{\text{test}})),</span>
</span><span id="__span-0-141"><a id="__codelineno-0-141" name="__codelineno-0-141"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle</span>
</span><span id="__span-0-142"><a id="__codelineno-0-142" name="__codelineno-0-142"></a><span class="sd">    \]</span>
</span><span id="__span-0-143"><a id="__codelineno-0-143" name="__codelineno-0-143"></a>
</span><span id="__span-0-144"><a id="__codelineno-0-144" name="__codelineno-0-144"></a><span class="sd">    for the perturbation type influence case. For all input tensors it is assumed,</span>
</span><span id="__span-0-145"><a id="__codelineno-0-145" name="__codelineno-0-145"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-146"><a id="__codelineno-0-146" name="__codelineno-0-146"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-147"><a id="__codelineno-0-147" name="__codelineno-0-147"></a>
</span><span id="__span-0-148"><a id="__codelineno-0-148" name="__codelineno-0-148"></a><span class="sd">    Args:</span>
</span><span id="__span-0-149"><a id="__codelineno-0-149" name="__codelineno-0-149"></a><span class="sd">        x_test: model input to use in the gradient computations</span>
</span><span id="__span-0-150"><a id="__codelineno-0-150" name="__codelineno-0-150"></a><span class="sd">            of $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-151"><a id="__codelineno-0-151" name="__codelineno-0-151"></a><span class="sd">                f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-152"><a id="__codelineno-0-152" name="__codelineno-0-152"></a><span class="sd">        y_test: label tensor to compute gradients</span>
</span><span id="__span-0-153"><a id="__codelineno-0-153" name="__codelineno-0-153"></a><span class="sd">        x: optional model input to use in the gradient computations</span>
</span><span id="__span-0-154"><a id="__codelineno-0-154" name="__codelineno-0-154"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-155"><a id="__codelineno-0-155" name="__codelineno-0-155"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-156"><a id="__codelineno-0-156" name="__codelineno-0-156"></a><span class="sd">            if None, use $x=x_{\text{test}}$</span>
</span><span id="__span-0-157"><a id="__codelineno-0-157" name="__codelineno-0-157"></a><span class="sd">        y: optional label tensor to compute gradients</span>
</span><span id="__span-0-158"><a id="__codelineno-0-158" name="__codelineno-0-158"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-159"><a id="__codelineno-0-159" name="__codelineno-0-159"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-160"><a id="__codelineno-0-160" name="__codelineno-0-160"></a>
</span><span id="__span-0-161"><a id="__codelineno-0-161" name="__codelineno-0-161"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-162"><a id="__codelineno-0-162" name="__codelineno-0-162"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-163"><a id="__codelineno-0-163" name="__codelineno-0-163"></a>
</span><span id="__span-0-164"><a id="__codelineno-0-164" name="__codelineno-0-164"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-165"><a id="__codelineno-0-165" name="__codelineno-0-165"></a>    <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
</span><span id="__span-0-166"><a id="__codelineno-0-166" name="__codelineno-0-166"></a>    <span class="k">return</span> <span class="n">t</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences_from_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences_from_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.influences_from_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Computation of</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the perturbation type influence case. The gradient is meant to be per sample
of the batch <span class="arithmatex">\((x, y)\)</span>. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>z_test_factors</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>pre-computed tensor, approximating
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span>
<span class="normal"><a href="#__codelineno-0-320">320</a></span>
<span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-276"><a id="__codelineno-0-276" name="__codelineno-0-276"></a><span class="k">def</span> <span class="nf">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-277"><a id="__codelineno-0-277" name="__codelineno-0-277"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-278"><a id="__codelineno-0-278" name="__codelineno-0-278"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-279"><a id="__codelineno-0-279" name="__codelineno-0-279"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-280"><a id="__codelineno-0-280" name="__codelineno-0-280"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-281"><a id="__codelineno-0-281" name="__codelineno-0-281"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-282"><a id="__codelineno-0-282" name="__codelineno-0-282"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-283"><a id="__codelineno-0-283" name="__codelineno-0-283"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-284"><a id="__codelineno-0-284" name="__codelineno-0-284"></a><span class="sd">    Computation of</span>
</span><span id="__span-0-285"><a id="__codelineno-0-285" name="__codelineno-0-285"></a>
</span><span id="__span-0-286"><a id="__codelineno-0-286" name="__codelineno-0-286"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-287"><a id="__codelineno-0-287" name="__codelineno-0-287"></a><span class="sd">        \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-288"><a id="__codelineno-0-288" name="__codelineno-0-288"></a>
</span><span id="__span-0-289"><a id="__codelineno-0-289" name="__codelineno-0-289"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-290"><a id="__codelineno-0-290" name="__codelineno-0-290"></a>
</span><span id="__span-0-291"><a id="__codelineno-0-291" name="__codelineno-0-291"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-292"><a id="__codelineno-0-292" name="__codelineno-0-292"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-293"><a id="__codelineno-0-293" name="__codelineno-0-293"></a>
</span><span id="__span-0-294"><a id="__codelineno-0-294" name="__codelineno-0-294"></a><span class="sd">    for the perturbation type influence case. The gradient is meant to be per sample</span>
</span><span id="__span-0-295"><a id="__codelineno-0-295" name="__codelineno-0-295"></a><span class="sd">    of the batch $(x, y)$. For all input tensors it is assumed,</span>
</span><span id="__span-0-296"><a id="__codelineno-0-296" name="__codelineno-0-296"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-297"><a id="__codelineno-0-297" name="__codelineno-0-297"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-298"><a id="__codelineno-0-298" name="__codelineno-0-298"></a>
</span><span id="__span-0-299"><a id="__codelineno-0-299" name="__codelineno-0-299"></a><span class="sd">    Args:</span>
</span><span id="__span-0-300"><a id="__codelineno-0-300" name="__codelineno-0-300"></a><span class="sd">        z_test_factors: pre-computed tensor, approximating</span>
</span><span id="__span-0-301"><a id="__codelineno-0-301" name="__codelineno-0-301"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-302"><a id="__codelineno-0-302" name="__codelineno-0-302"></a><span class="sd">            f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-303"><a id="__codelineno-0-303" name="__codelineno-0-303"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-304"><a id="__codelineno-0-304" name="__codelineno-0-304"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-305"><a id="__codelineno-0-305" name="__codelineno-0-305"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$</span>
</span><span id="__span-0-306"><a id="__codelineno-0-306" name="__codelineno-0-306"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-307"><a id="__codelineno-0-307" name="__codelineno-0-307"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-308"><a id="__codelineno-0-308" name="__codelineno-0-308"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-309"><a id="__codelineno-0-309" name="__codelineno-0-309"></a>
</span><span id="__span-0-310"><a id="__codelineno-0-310" name="__codelineno-0-310"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-311"><a id="__codelineno-0-311" name="__codelineno-0-311"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-312"><a id="__codelineno-0-312" name="__codelineno-0-312"></a>
</span><span id="__span-0-313"><a id="__codelineno-0-313" name="__codelineno-0-313"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-314"><a id="__codelineno-0-314" name="__codelineno-0-314"></a>    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">:</span>
</span><span id="__span-0-315"><a id="__codelineno-0-315" name="__codelineno-0-315"></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="__span-0-316"><a id="__codelineno-0-316" name="__codelineno-0-316"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-317"><a id="__codelineno-0-317" name="__codelineno-0-317"></a>            <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</span><span id="__span-0-318"><a id="__codelineno-0-318" name="__codelineno-0-318"></a>        <span class="p">)</span>
</span><span id="__span-0-319"><a id="__codelineno-0-319" name="__codelineno-0-319"></a>    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">:</span>
</span><span id="__span-0-320"><a id="__codelineno-0-320" name="__codelineno-0-320"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
</span><span id="__span-0-321"><a id="__codelineno-0-321" name="__codelineno-0-321"></a>            <span class="s2">&quot;ia,j...a-&gt;ij...&quot;</span><span class="p">,</span>
</span><span id="__span-0-322"><a id="__codelineno-0-322" name="__codelineno-0-322"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-323"><a id="__codelineno-0-323" name="__codelineno-0-323"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flat_loss_mixed_grad</span><span class="p">(</span>
</span><span id="__span-0-324"><a id="__codelineno-0-324" name="__codelineno-0-324"></a>                <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-325"><a id="__codelineno-0-325" name="__codelineno-0-325"></a>            <span class="p">),</span>
</span><span id="__span-0-326"><a id="__codelineno-0-326" name="__codelineno-0-326"></a>        <span class="p">)</span>
</span><span id="__span-0-327"><a id="__codelineno-0-327" name="__codelineno-0-327"></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-328"><a id="__codelineno-0-328" name="__codelineno-0-328"></a>        <span class="k">raise</span> <span class="n">UnsupportedInfluenceModeException</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.ArnoldiInfluence.fit" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">fit</span>


<a href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence.fit" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.utils.data.DataLoader" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.ArnoldiInfluence" href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence">ArnoldiInfluence</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Fitting corresponds to the computation of the low rank decomposition</p>
<div class="arithmatex">\[ V D^{-1} V^T \]</div>
<p>of the Hessian defined by the provided data loader.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>data</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>The data to compute the Hessian with.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.utils.data.DataLoader" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.ArnoldiInfluence" href="#pydvl.influence.torch.influence_function_model.ArnoldiInfluence">ArnoldiInfluence</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>The fitted instance.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1019">1019</a></span>
<span class="normal"><a href="#__codelineno-0-1020">1020</a></span>
<span class="normal"><a href="#__codelineno-0-1021">1021</a></span>
<span class="normal"><a href="#__codelineno-0-1022">1022</a></span>
<span class="normal"><a href="#__codelineno-0-1023">1023</a></span>
<span class="normal"><a href="#__codelineno-0-1024">1024</a></span>
<span class="normal"><a href="#__codelineno-0-1025">1025</a></span>
<span class="normal"><a href="#__codelineno-0-1026">1026</a></span>
<span class="normal"><a href="#__codelineno-0-1027">1027</a></span>
<span class="normal"><a href="#__codelineno-0-1028">1028</a></span>
<span class="normal"><a href="#__codelineno-0-1029">1029</a></span>
<span class="normal"><a href="#__codelineno-0-1030">1030</a></span>
<span class="normal"><a href="#__codelineno-0-1031">1031</a></span>
<span class="normal"><a href="#__codelineno-0-1032">1032</a></span>
<span class="normal"><a href="#__codelineno-0-1033">1033</a></span>
<span class="normal"><a href="#__codelineno-0-1034">1034</a></span>
<span class="normal"><a href="#__codelineno-0-1035">1035</a></span>
<span class="normal"><a href="#__codelineno-0-1036">1036</a></span>
<span class="normal"><a href="#__codelineno-0-1037">1037</a></span>
<span class="normal"><a href="#__codelineno-0-1038">1038</a></span>
<span class="normal"><a href="#__codelineno-0-1039">1039</a></span>
<span class="normal"><a href="#__codelineno-0-1040">1040</a></span>
<span class="normal"><a href="#__codelineno-0-1041">1041</a></span>
<span class="normal"><a href="#__codelineno-0-1042">1042</a></span>
<span class="normal"><a href="#__codelineno-0-1043">1043</a></span>
<span class="normal"><a href="#__codelineno-0-1044">1044</a></span>
<span class="normal"><a href="#__codelineno-0-1045">1045</a></span>
<span class="normal"><a href="#__codelineno-0-1046">1046</a></span>
<span class="normal"><a href="#__codelineno-0-1047">1047</a></span>
<span class="normal"><a href="#__codelineno-0-1048">1048</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1019"><a id="__codelineno-0-1019" name="__codelineno-0-1019"></a><span class="nd">@log_duration</span><span class="p">(</span><span class="n">log_level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
</span><span id="__span-0-1020"><a id="__codelineno-0-1020" name="__codelineno-0-1020"></a><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ArnoldiInfluence</span><span class="p">:</span>
</span><span id="__span-0-1021"><a id="__codelineno-0-1021" name="__codelineno-0-1021"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-1022"><a id="__codelineno-0-1022" name="__codelineno-0-1022"></a><span class="sd">    Fitting corresponds to the computation of the low rank decomposition</span>
</span><span id="__span-0-1023"><a id="__codelineno-0-1023" name="__codelineno-0-1023"></a>
</span><span id="__span-0-1024"><a id="__codelineno-0-1024" name="__codelineno-0-1024"></a><span class="sd">    \[ V D^{-1} V^T \]</span>
</span><span id="__span-0-1025"><a id="__codelineno-0-1025" name="__codelineno-0-1025"></a>
</span><span id="__span-0-1026"><a id="__codelineno-0-1026" name="__codelineno-0-1026"></a><span class="sd">    of the Hessian defined by the provided data loader.</span>
</span><span id="__span-0-1027"><a id="__codelineno-0-1027" name="__codelineno-0-1027"></a>
</span><span id="__span-0-1028"><a id="__codelineno-0-1028" name="__codelineno-0-1028"></a><span class="sd">    Args:</span>
</span><span id="__span-0-1029"><a id="__codelineno-0-1029" name="__codelineno-0-1029"></a><span class="sd">        data: The data to compute the Hessian with.</span>
</span><span id="__span-0-1030"><a id="__codelineno-0-1030" name="__codelineno-0-1030"></a>
</span><span id="__span-0-1031"><a id="__codelineno-0-1031" name="__codelineno-0-1031"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-1032"><a id="__codelineno-0-1032" name="__codelineno-0-1032"></a><span class="sd">        The fitted instance.</span>
</span><span id="__span-0-1033"><a id="__codelineno-0-1033" name="__codelineno-0-1033"></a>
</span><span id="__span-0-1034"><a id="__codelineno-0-1034" name="__codelineno-0-1034"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-1035"><a id="__codelineno-0-1035" name="__codelineno-0-1035"></a>    <span class="n">low_rank_representation</span> <span class="o">=</span> <span class="n">model_hessian_low_rank</span><span class="p">(</span>
</span><span id="__span-0-1036"><a id="__codelineno-0-1036" name="__codelineno-0-1036"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
</span><span id="__span-0-1037"><a id="__codelineno-0-1037" name="__codelineno-0-1037"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
</span><span id="__span-0-1038"><a id="__codelineno-0-1038" name="__codelineno-0-1038"></a>        <span class="n">data</span><span class="p">,</span>
</span><span id="__span-0-1039"><a id="__codelineno-0-1039" name="__codelineno-0-1039"></a>        <span class="n">hessian_perturbation</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>  <span class="c1"># regularization is applied, when computing values</span>
</span><span id="__span-0-1040"><a id="__codelineno-0-1040" name="__codelineno-0-1040"></a>        <span class="n">rank_estimate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_estimate</span><span class="p">,</span>
</span><span id="__span-0-1041"><a id="__codelineno-0-1041" name="__codelineno-0-1041"></a>        <span class="n">krylov_dimension</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">krylov_dimension</span><span class="p">,</span>
</span><span id="__span-0-1042"><a id="__codelineno-0-1042" name="__codelineno-0-1042"></a>        <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
</span><span id="__span-0-1043"><a id="__codelineno-0-1043" name="__codelineno-0-1043"></a>        <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
</span><span id="__span-0-1044"><a id="__codelineno-0-1044" name="__codelineno-0-1044"></a>        <span class="n">eigen_computation_on_gpu</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eigen_computation_on_gpu</span><span class="p">,</span>
</span><span id="__span-0-1045"><a id="__codelineno-0-1045" name="__codelineno-0-1045"></a>        <span class="n">precompute_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precompute_grad</span><span class="p">,</span>
</span><span id="__span-0-1046"><a id="__codelineno-0-1046" name="__codelineno-0-1046"></a>    <span class="p">)</span>
</span><span id="__span-0-1047"><a id="__codelineno-0-1047" name="__codelineno-0-1047"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">low_rank_representation</span> <span class="o">=</span> <span class="n">low_rank_representation</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-1048"><a id="__codelineno-0-1048" name="__codelineno-0-1048"></a>    <span class="k">return</span> <span class="bp">self</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pydvl.influence.torch.influence_function_model.EkfacInfluence" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <span class="doc doc-object-name doc-class-name">EkfacInfluence</span>


<a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">EkfacInfluence</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">model</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">update_diagonal</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">progress</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel" href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel">TorchInfluenceFunctionModel</a></code></p>

  
      <p>Approximately solves the linear system Hx = b, where H is the Hessian of a model
with the empirical categorical cross entropy as loss function and b is the given
right-hand side vector.
It employs the EK-FAC method, which is based on the kronecker
factorization of the Hessian.</p>
<p>Contrary to the other influence function methods, this implementation can only
be used for classification tasks with a cross entropy loss function. However, it
is much faster than the other methods and can be used efficiently for very large
datasets and models. For more information,
see <a class="autorefs autorefs-internal" href="../../../../../influence/influence_function_model/#eigenvalue-corrected-k-fac">Eigenvalue Corrected <abbr title="Kronecker-Factored Approximate Curvature">K-FAC</abbr></a>.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A PyTorch model. The Hessian will be calculated with respect to
this model's parameters.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>update_diagonal</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>If True, the diagonal values in the ekfac representation are
refitted from the training data after calculating the KFAC blocks.
This provides a more accurate approximation of the Hessian, but it is
computationally more expensive.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>False</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>hessian_regularization</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Regularization of the hessian.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>0.0</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>progress</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>If True, display progress bars.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>False</code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>

                <details class="quote">
                  <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
                  <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1144">1144</a></span>
<span class="normal"><a href="#__codelineno-0-1145">1145</a></span>
<span class="normal"><a href="#__codelineno-0-1146">1146</a></span>
<span class="normal"><a href="#__codelineno-0-1147">1147</a></span>
<span class="normal"><a href="#__codelineno-0-1148">1148</a></span>
<span class="normal"><a href="#__codelineno-0-1149">1149</a></span>
<span class="normal"><a href="#__codelineno-0-1150">1150</a></span>
<span class="normal"><a href="#__codelineno-0-1151">1151</a></span>
<span class="normal"><a href="#__codelineno-0-1152">1152</a></span>
<span class="normal"><a href="#__codelineno-0-1153">1153</a></span>
<span class="normal"><a href="#__codelineno-0-1154">1154</a></span>
<span class="normal"><a href="#__codelineno-0-1155">1155</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1144"><a id="__codelineno-0-1144" name="__codelineno-0-1144"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="__span-0-1145"><a id="__codelineno-0-1145" name="__codelineno-0-1145"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-1146"><a id="__codelineno-0-1146" name="__codelineno-0-1146"></a>    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="__span-0-1147"><a id="__codelineno-0-1147" name="__codelineno-0-1147"></a>    <span class="n">update_diagonal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-1148"><a id="__codelineno-0-1148" name="__codelineno-0-1148"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="__span-0-1149"><a id="__codelineno-0-1149" name="__codelineno-0-1149"></a>    <span class="n">progress</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-0-1150"><a id="__codelineno-0-1150" name="__codelineno-0-1150"></a><span class="p">):</span>
</span><span id="__span-0-1151"><a id="__codelineno-0-1151" name="__codelineno-0-1151"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">)</span>
</span><span id="__span-0-1152"><a id="__codelineno-0-1152" name="__codelineno-0-1152"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">hessian_regularization</span> <span class="o">=</span> <span class="n">hessian_regularization</span>
</span><span id="__span-0-1153"><a id="__codelineno-0-1153" name="__codelineno-0-1153"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">update_diagonal</span> <span class="o">=</span> <span class="n">update_diagonal</span>
</span><span id="__span-0-1154"><a id="__codelineno-0-1154" name="__codelineno-0-1154"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">active_layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parse_active_layers</span><span class="p">()</span>
</span><span id="__span-0-1155"><a id="__codelineno-0-1155" name="__codelineno-0-1155"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">progress</span> <span class="o">=</span> <span class="n">progress</span>
</span></code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.EkfacInfluence.influence_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influence_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influence_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute approximation of</p>
<div class="arithmatex">\[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</div>
<p>where the gradient is meant to be per sample of the batch <span class="arithmatex">\((x, y)\)</span>.
For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise inverse Hessian matrix vector products</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-245"><a id="__codelineno-0-245" name="__codelineno-0-245"></a><span class="k">def</span> <span class="nf">influence_factors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-246"><a id="__codelineno-0-246" name="__codelineno-0-246"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-247"><a id="__codelineno-0-247" name="__codelineno-0-247"></a><span class="sd">    Compute approximation of</span>
</span><span id="__span-0-248"><a id="__codelineno-0-248" name="__codelineno-0-248"></a>
</span><span id="__span-0-249"><a id="__codelineno-0-249" name="__codelineno-0-249"></a><span class="sd">    \[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</span>
</span><span id="__span-0-250"><a id="__codelineno-0-250" name="__codelineno-0-250"></a>
</span><span id="__span-0-251"><a id="__codelineno-0-251" name="__codelineno-0-251"></a><span class="sd">    where the gradient is meant to be per sample of the batch $(x, y)$.</span>
</span><span id="__span-0-252"><a id="__codelineno-0-252" name="__codelineno-0-252"></a><span class="sd">    For all input tensors it is assumed,</span>
</span><span id="__span-0-253"><a id="__codelineno-0-253" name="__codelineno-0-253"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-254"><a id="__codelineno-0-254" name="__codelineno-0-254"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-255"><a id="__codelineno-0-255" name="__codelineno-0-255"></a>
</span><span id="__span-0-256"><a id="__codelineno-0-256" name="__codelineno-0-256"></a><span class="sd">    Args:</span>
</span><span id="__span-0-257"><a id="__codelineno-0-257" name="__codelineno-0-257"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-258"><a id="__codelineno-0-258" name="__codelineno-0-258"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-259"><a id="__codelineno-0-259" name="__codelineno-0-259"></a>
</span><span id="__span-0-260"><a id="__codelineno-0-260" name="__codelineno-0-260"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-261"><a id="__codelineno-0-261" name="__codelineno-0-261"></a><span class="sd">        Tensor representing the element-wise inverse Hessian matrix vector products</span>
</span><span id="__span-0-262"><a id="__codelineno-0-262" name="__codelineno-0-262"></a>
</span><span id="__span-0-263"><a id="__codelineno-0-263" name="__codelineno-0-263"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-264"><a id="__codelineno-0-264" name="__codelineno-0-264"></a>    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.EkfacInfluence.influences" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences</span>


<a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute the approximation of</p>
<div class="arithmatex">\[
\langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}})), \nabla_{\theta} \ell(y, f_{\theta}(x))\rangle
\]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[
\langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}}, f_{\theta}(x_{\text{test}})),
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle
\]</div>
<p>for the perturbation type influence case. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
of <span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
if None, use <span class="arithmatex">\(x=x_{\text{test}}\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-121"><a id="__codelineno-0-121" name="__codelineno-0-121"></a><span class="k">def</span> <span class="nf">influences</span><span class="p">(</span>
</span><span id="__span-0-122"><a id="__codelineno-0-122" name="__codelineno-0-122"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-123"><a id="__codelineno-0-123" name="__codelineno-0-123"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-124"><a id="__codelineno-0-124" name="__codelineno-0-124"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-125"><a id="__codelineno-0-125" name="__codelineno-0-125"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-126"><a id="__codelineno-0-126" name="__codelineno-0-126"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-127"><a id="__codelineno-0-127" name="__codelineno-0-127"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-128"><a id="__codelineno-0-128" name="__codelineno-0-128"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-129"><a id="__codelineno-0-129" name="__codelineno-0-129"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-130"><a id="__codelineno-0-130" name="__codelineno-0-130"></a><span class="sd">    Compute the approximation of</span>
</span><span id="__span-0-131"><a id="__codelineno-0-131" name="__codelineno-0-131"></a>
</span><span id="__span-0-132"><a id="__codelineno-0-132" name="__codelineno-0-132"></a><span class="sd">    \[</span>
</span><span id="__span-0-133"><a id="__codelineno-0-133" name="__codelineno-0-133"></a><span class="sd">    \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-134"><a id="__codelineno-0-134" name="__codelineno-0-134"></a><span class="sd">    f_{\theta}(x_{\text{test}})), \nabla_{\theta} \ell(y, f_{\theta}(x))\rangle</span>
</span><span id="__span-0-135"><a id="__codelineno-0-135" name="__codelineno-0-135"></a><span class="sd">    \]</span>
</span><span id="__span-0-136"><a id="__codelineno-0-136" name="__codelineno-0-136"></a>
</span><span id="__span-0-137"><a id="__codelineno-0-137" name="__codelineno-0-137"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-138"><a id="__codelineno-0-138" name="__codelineno-0-138"></a>
</span><span id="__span-0-139"><a id="__codelineno-0-139" name="__codelineno-0-139"></a><span class="sd">    \[</span>
</span><span id="__span-0-140"><a id="__codelineno-0-140" name="__codelineno-0-140"></a><span class="sd">    \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}}, f_{\theta}(x_{\text{test}})),</span>
</span><span id="__span-0-141"><a id="__codelineno-0-141" name="__codelineno-0-141"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle</span>
</span><span id="__span-0-142"><a id="__codelineno-0-142" name="__codelineno-0-142"></a><span class="sd">    \]</span>
</span><span id="__span-0-143"><a id="__codelineno-0-143" name="__codelineno-0-143"></a>
</span><span id="__span-0-144"><a id="__codelineno-0-144" name="__codelineno-0-144"></a><span class="sd">    for the perturbation type influence case. For all input tensors it is assumed,</span>
</span><span id="__span-0-145"><a id="__codelineno-0-145" name="__codelineno-0-145"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-146"><a id="__codelineno-0-146" name="__codelineno-0-146"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-147"><a id="__codelineno-0-147" name="__codelineno-0-147"></a>
</span><span id="__span-0-148"><a id="__codelineno-0-148" name="__codelineno-0-148"></a><span class="sd">    Args:</span>
</span><span id="__span-0-149"><a id="__codelineno-0-149" name="__codelineno-0-149"></a><span class="sd">        x_test: model input to use in the gradient computations</span>
</span><span id="__span-0-150"><a id="__codelineno-0-150" name="__codelineno-0-150"></a><span class="sd">            of $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-151"><a id="__codelineno-0-151" name="__codelineno-0-151"></a><span class="sd">                f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-152"><a id="__codelineno-0-152" name="__codelineno-0-152"></a><span class="sd">        y_test: label tensor to compute gradients</span>
</span><span id="__span-0-153"><a id="__codelineno-0-153" name="__codelineno-0-153"></a><span class="sd">        x: optional model input to use in the gradient computations</span>
</span><span id="__span-0-154"><a id="__codelineno-0-154" name="__codelineno-0-154"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-155"><a id="__codelineno-0-155" name="__codelineno-0-155"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-156"><a id="__codelineno-0-156" name="__codelineno-0-156"></a><span class="sd">            if None, use $x=x_{\text{test}}$</span>
</span><span id="__span-0-157"><a id="__codelineno-0-157" name="__codelineno-0-157"></a><span class="sd">        y: optional label tensor to compute gradients</span>
</span><span id="__span-0-158"><a id="__codelineno-0-158" name="__codelineno-0-158"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-159"><a id="__codelineno-0-159" name="__codelineno-0-159"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-160"><a id="__codelineno-0-160" name="__codelineno-0-160"></a>
</span><span id="__span-0-161"><a id="__codelineno-0-161" name="__codelineno-0-161"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-162"><a id="__codelineno-0-162" name="__codelineno-0-162"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-163"><a id="__codelineno-0-163" name="__codelineno-0-163"></a>
</span><span id="__span-0-164"><a id="__codelineno-0-164" name="__codelineno-0-164"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-165"><a id="__codelineno-0-165" name="__codelineno-0-165"></a>    <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
</span><span id="__span-0-166"><a id="__codelineno-0-166" name="__codelineno-0-166"></a>    <span class="k">return</span> <span class="n">t</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_from_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences_from_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_from_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Computation of</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the perturbation type influence case. The gradient is meant to be per sample
of the batch <span class="arithmatex">\((x, y)\)</span>. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>z_test_factors</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>pre-computed tensor, approximating
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span>
<span class="normal"><a href="#__codelineno-0-320">320</a></span>
<span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-276"><a id="__codelineno-0-276" name="__codelineno-0-276"></a><span class="k">def</span> <span class="nf">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-277"><a id="__codelineno-0-277" name="__codelineno-0-277"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-278"><a id="__codelineno-0-278" name="__codelineno-0-278"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-279"><a id="__codelineno-0-279" name="__codelineno-0-279"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-280"><a id="__codelineno-0-280" name="__codelineno-0-280"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-281"><a id="__codelineno-0-281" name="__codelineno-0-281"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-282"><a id="__codelineno-0-282" name="__codelineno-0-282"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-283"><a id="__codelineno-0-283" name="__codelineno-0-283"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-284"><a id="__codelineno-0-284" name="__codelineno-0-284"></a><span class="sd">    Computation of</span>
</span><span id="__span-0-285"><a id="__codelineno-0-285" name="__codelineno-0-285"></a>
</span><span id="__span-0-286"><a id="__codelineno-0-286" name="__codelineno-0-286"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-287"><a id="__codelineno-0-287" name="__codelineno-0-287"></a><span class="sd">        \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-288"><a id="__codelineno-0-288" name="__codelineno-0-288"></a>
</span><span id="__span-0-289"><a id="__codelineno-0-289" name="__codelineno-0-289"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-290"><a id="__codelineno-0-290" name="__codelineno-0-290"></a>
</span><span id="__span-0-291"><a id="__codelineno-0-291" name="__codelineno-0-291"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-292"><a id="__codelineno-0-292" name="__codelineno-0-292"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-293"><a id="__codelineno-0-293" name="__codelineno-0-293"></a>
</span><span id="__span-0-294"><a id="__codelineno-0-294" name="__codelineno-0-294"></a><span class="sd">    for the perturbation type influence case. The gradient is meant to be per sample</span>
</span><span id="__span-0-295"><a id="__codelineno-0-295" name="__codelineno-0-295"></a><span class="sd">    of the batch $(x, y)$. For all input tensors it is assumed,</span>
</span><span id="__span-0-296"><a id="__codelineno-0-296" name="__codelineno-0-296"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-297"><a id="__codelineno-0-297" name="__codelineno-0-297"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-298"><a id="__codelineno-0-298" name="__codelineno-0-298"></a>
</span><span id="__span-0-299"><a id="__codelineno-0-299" name="__codelineno-0-299"></a><span class="sd">    Args:</span>
</span><span id="__span-0-300"><a id="__codelineno-0-300" name="__codelineno-0-300"></a><span class="sd">        z_test_factors: pre-computed tensor, approximating</span>
</span><span id="__span-0-301"><a id="__codelineno-0-301" name="__codelineno-0-301"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-302"><a id="__codelineno-0-302" name="__codelineno-0-302"></a><span class="sd">            f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-303"><a id="__codelineno-0-303" name="__codelineno-0-303"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-304"><a id="__codelineno-0-304" name="__codelineno-0-304"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-305"><a id="__codelineno-0-305" name="__codelineno-0-305"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$</span>
</span><span id="__span-0-306"><a id="__codelineno-0-306" name="__codelineno-0-306"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-307"><a id="__codelineno-0-307" name="__codelineno-0-307"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-308"><a id="__codelineno-0-308" name="__codelineno-0-308"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-309"><a id="__codelineno-0-309" name="__codelineno-0-309"></a>
</span><span id="__span-0-310"><a id="__codelineno-0-310" name="__codelineno-0-310"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-311"><a id="__codelineno-0-311" name="__codelineno-0-311"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-312"><a id="__codelineno-0-312" name="__codelineno-0-312"></a>
</span><span id="__span-0-313"><a id="__codelineno-0-313" name="__codelineno-0-313"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-314"><a id="__codelineno-0-314" name="__codelineno-0-314"></a>    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">:</span>
</span><span id="__span-0-315"><a id="__codelineno-0-315" name="__codelineno-0-315"></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="__span-0-316"><a id="__codelineno-0-316" name="__codelineno-0-316"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-317"><a id="__codelineno-0-317" name="__codelineno-0-317"></a>            <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</span><span id="__span-0-318"><a id="__codelineno-0-318" name="__codelineno-0-318"></a>        <span class="p">)</span>
</span><span id="__span-0-319"><a id="__codelineno-0-319" name="__codelineno-0-319"></a>    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">:</span>
</span><span id="__span-0-320"><a id="__codelineno-0-320" name="__codelineno-0-320"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
</span><span id="__span-0-321"><a id="__codelineno-0-321" name="__codelineno-0-321"></a>            <span class="s2">&quot;ia,j...a-&gt;ij...&quot;</span><span class="p">,</span>
</span><span id="__span-0-322"><a id="__codelineno-0-322" name="__codelineno-0-322"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-323"><a id="__codelineno-0-323" name="__codelineno-0-323"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flat_loss_mixed_grad</span><span class="p">(</span>
</span><span id="__span-0-324"><a id="__codelineno-0-324" name="__codelineno-0-324"></a>                <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-325"><a id="__codelineno-0-325" name="__codelineno-0-325"></a>            <span class="p">),</span>
</span><span id="__span-0-326"><a id="__codelineno-0-326" name="__codelineno-0-326"></a>        <span class="p">)</span>
</span><span id="__span-0-327"><a id="__codelineno-0-327" name="__codelineno-0-327"></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-328"><a id="__codelineno-0-328" name="__codelineno-0-328"></a>        <span class="k">raise</span> <span class="n">UnsupportedInfluenceModeException</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.EkfacInfluence.fit" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">fit</span>


<a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.fit" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.utils.data.DataLoader" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">DataLoader</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.EkfacInfluence" href="#pydvl.influence.torch.influence_function_model.EkfacInfluence">EkfacInfluence</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute the KFAC blocks for each layer of the model, using the provided data.
It then creates an EkfacRepresentation object that stores the KFAC blocks for
each layer, their eigenvalue decomposition and diagonal values.</p>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1276">1276</a></span>
<span class="normal"><a href="#__codelineno-0-1277">1277</a></span>
<span class="normal"><a href="#__codelineno-0-1278">1278</a></span>
<span class="normal"><a href="#__codelineno-0-1279">1279</a></span>
<span class="normal"><a href="#__codelineno-0-1280">1280</a></span>
<span class="normal"><a href="#__codelineno-0-1281">1281</a></span>
<span class="normal"><a href="#__codelineno-0-1282">1282</a></span>
<span class="normal"><a href="#__codelineno-0-1283">1283</a></span>
<span class="normal"><a href="#__codelineno-0-1284">1284</a></span>
<span class="normal"><a href="#__codelineno-0-1285">1285</a></span>
<span class="normal"><a href="#__codelineno-0-1286">1286</a></span>
<span class="normal"><a href="#__codelineno-0-1287">1287</a></span>
<span class="normal"><a href="#__codelineno-0-1288">1288</a></span>
<span class="normal"><a href="#__codelineno-0-1289">1289</a></span>
<span class="normal"><a href="#__codelineno-0-1290">1290</a></span>
<span class="normal"><a href="#__codelineno-0-1291">1291</a></span>
<span class="normal"><a href="#__codelineno-0-1292">1292</a></span>
<span class="normal"><a href="#__codelineno-0-1293">1293</a></span>
<span class="normal"><a href="#__codelineno-0-1294">1294</a></span>
<span class="normal"><a href="#__codelineno-0-1295">1295</a></span>
<span class="normal"><a href="#__codelineno-0-1296">1296</a></span>
<span class="normal"><a href="#__codelineno-0-1297">1297</a></span>
<span class="normal"><a href="#__codelineno-0-1298">1298</a></span>
<span class="normal"><a href="#__codelineno-0-1299">1299</a></span>
<span class="normal"><a href="#__codelineno-0-1300">1300</a></span>
<span class="normal"><a href="#__codelineno-0-1301">1301</a></span>
<span class="normal"><a href="#__codelineno-0-1302">1302</a></span>
<span class="normal"><a href="#__codelineno-0-1303">1303</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1276"><a id="__codelineno-0-1276" name="__codelineno-0-1276"></a><span class="nd">@log_duration</span><span class="p">(</span><span class="n">log_level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
</span><span id="__span-0-1277"><a id="__codelineno-0-1277" name="__codelineno-0-1277"></a><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EkfacInfluence</span><span class="p">:</span>
</span><span id="__span-0-1278"><a id="__codelineno-0-1278" name="__codelineno-0-1278"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-1279"><a id="__codelineno-0-1279" name="__codelineno-0-1279"></a><span class="sd">    Compute the KFAC blocks for each layer of the model, using the provided data.</span>
</span><span id="__span-0-1280"><a id="__codelineno-0-1280" name="__codelineno-0-1280"></a><span class="sd">    It then creates an EkfacRepresentation object that stores the KFAC blocks for</span>
</span><span id="__span-0-1281"><a id="__codelineno-0-1281" name="__codelineno-0-1281"></a><span class="sd">    each layer, their eigenvalue decomposition and diagonal values.</span>
</span><span id="__span-0-1282"><a id="__codelineno-0-1282" name="__codelineno-0-1282"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-1283"><a id="__codelineno-0-1283" name="__codelineno-0-1283"></a>    <span class="n">forward_x</span><span class="p">,</span> <span class="n">grad_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_kfac_blocks</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-0-1284"><a id="__codelineno-0-1284" name="__codelineno-0-1284"></a>    <span class="n">layers_evecs_a</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="__span-0-1285"><a id="__codelineno-0-1285" name="__codelineno-0-1285"></a>    <span class="n">layers_evect_g</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="__span-0-1286"><a id="__codelineno-0-1286" name="__codelineno-0-1286"></a>    <span class="n">layers_diags</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="__span-0-1287"><a id="__codelineno-0-1287" name="__codelineno-0-1287"></a>    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">active_layers</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
</span><span id="__span-0-1288"><a id="__codelineno-0-1288" name="__codelineno-0-1288"></a>        <span class="n">evals_a</span><span class="p">,</span> <span class="n">evecs_a</span> <span class="o">=</span> <span class="n">safe_torch_linalg_eigh</span><span class="p">(</span><span class="n">forward_x</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
</span><span id="__span-0-1289"><a id="__codelineno-0-1289" name="__codelineno-0-1289"></a>        <span class="n">evals_g</span><span class="p">,</span> <span class="n">evecs_g</span> <span class="o">=</span> <span class="n">safe_torch_linalg_eigh</span><span class="p">(</span><span class="n">grad_y</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
</span><span id="__span-0-1290"><a id="__codelineno-0-1290" name="__codelineno-0-1290"></a>        <span class="n">layers_evecs_a</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">evecs_a</span>
</span><span id="__span-0-1291"><a id="__codelineno-0-1291" name="__codelineno-0-1291"></a>        <span class="n">layers_evect_g</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">evecs_g</span>
</span><span id="__span-0-1292"><a id="__codelineno-0-1292" name="__codelineno-0-1292"></a>        <span class="n">layers_diags</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">kron</span><span class="p">(</span><span class="n">evals_g</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">evals_a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="__span-0-1293"><a id="__codelineno-0-1293" name="__codelineno-0-1293"></a>
</span><span id="__span-0-1294"><a id="__codelineno-0-1294" name="__codelineno-0-1294"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">ekfac_representation</span> <span class="o">=</span> <span class="n">EkfacRepresentation</span><span class="p">(</span>
</span><span id="__span-0-1295"><a id="__codelineno-0-1295" name="__codelineno-0-1295"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">active_layers</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span>
</span><span id="__span-0-1296"><a id="__codelineno-0-1296" name="__codelineno-0-1296"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">active_layers</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
</span><span id="__span-0-1297"><a id="__codelineno-0-1297" name="__codelineno-0-1297"></a>        <span class="n">layers_evecs_a</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
</span><span id="__span-0-1298"><a id="__codelineno-0-1298" name="__codelineno-0-1298"></a>        <span class="n">layers_evect_g</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
</span><span id="__span-0-1299"><a id="__codelineno-0-1299" name="__codelineno-0-1299"></a>        <span class="n">layers_diags</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
</span><span id="__span-0-1300"><a id="__codelineno-0-1300" name="__codelineno-0-1300"></a>    <span class="p">)</span>
</span><span id="__span-0-1301"><a id="__codelineno-0-1301" name="__codelineno-0-1301"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_diagonal</span><span class="p">:</span>
</span><span id="__span-0-1302"><a id="__codelineno-0-1302" name="__codelineno-0-1302"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_update_diag</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-0-1303"><a id="__codelineno-0-1303" name="__codelineno-0-1303"></a>    <span class="k">return</span> <span class="bp">self</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_by_layer" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences_by_layer</span>


<a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_by_layer" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences_by_layer</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute the influence of the data on the test data for each layer of the model.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations of
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
if None, use <span class="arithmatex">\(x=x_{\text{test}}\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>A dictionary containing the influence of the data on the test data for each</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>layer of the model, with the layer name as key.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1459">1459</a></span>
<span class="normal"><a href="#__codelineno-0-1460">1460</a></span>
<span class="normal"><a href="#__codelineno-0-1461">1461</a></span>
<span class="normal"><a href="#__codelineno-0-1462">1462</a></span>
<span class="normal"><a href="#__codelineno-0-1463">1463</a></span>
<span class="normal"><a href="#__codelineno-0-1464">1464</a></span>
<span class="normal"><a href="#__codelineno-0-1465">1465</a></span>
<span class="normal"><a href="#__codelineno-0-1466">1466</a></span>
<span class="normal"><a href="#__codelineno-0-1467">1467</a></span>
<span class="normal"><a href="#__codelineno-0-1468">1468</a></span>
<span class="normal"><a href="#__codelineno-0-1469">1469</a></span>
<span class="normal"><a href="#__codelineno-0-1470">1470</a></span>
<span class="normal"><a href="#__codelineno-0-1471">1471</a></span>
<span class="normal"><a href="#__codelineno-0-1472">1472</a></span>
<span class="normal"><a href="#__codelineno-0-1473">1473</a></span>
<span class="normal"><a href="#__codelineno-0-1474">1474</a></span>
<span class="normal"><a href="#__codelineno-0-1475">1475</a></span>
<span class="normal"><a href="#__codelineno-0-1476">1476</a></span>
<span class="normal"><a href="#__codelineno-0-1477">1477</a></span>
<span class="normal"><a href="#__codelineno-0-1478">1478</a></span>
<span class="normal"><a href="#__codelineno-0-1479">1479</a></span>
<span class="normal"><a href="#__codelineno-0-1480">1480</a></span>
<span class="normal"><a href="#__codelineno-0-1481">1481</a></span>
<span class="normal"><a href="#__codelineno-0-1482">1482</a></span>
<span class="normal"><a href="#__codelineno-0-1483">1483</a></span>
<span class="normal"><a href="#__codelineno-0-1484">1484</a></span>
<span class="normal"><a href="#__codelineno-0-1485">1485</a></span>
<span class="normal"><a href="#__codelineno-0-1486">1486</a></span>
<span class="normal"><a href="#__codelineno-0-1487">1487</a></span>
<span class="normal"><a href="#__codelineno-0-1488">1488</a></span>
<span class="normal"><a href="#__codelineno-0-1489">1489</a></span>
<span class="normal"><a href="#__codelineno-0-1490">1490</a></span>
<span class="normal"><a href="#__codelineno-0-1491">1491</a></span>
<span class="normal"><a href="#__codelineno-0-1492">1492</a></span>
<span class="normal"><a href="#__codelineno-0-1493">1493</a></span>
<span class="normal"><a href="#__codelineno-0-1494">1494</a></span>
<span class="normal"><a href="#__codelineno-0-1495">1495</a></span>
<span class="normal"><a href="#__codelineno-0-1496">1496</a></span>
<span class="normal"><a href="#__codelineno-0-1497">1497</a></span>
<span class="normal"><a href="#__codelineno-0-1498">1498</a></span>
<span class="normal"><a href="#__codelineno-0-1499">1499</a></span>
<span class="normal"><a href="#__codelineno-0-1500">1500</a></span>
<span class="normal"><a href="#__codelineno-0-1501">1501</a></span>
<span class="normal"><a href="#__codelineno-0-1502">1502</a></span>
<span class="normal"><a href="#__codelineno-0-1503">1503</a></span>
<span class="normal"><a href="#__codelineno-0-1504">1504</a></span>
<span class="normal"><a href="#__codelineno-0-1505">1505</a></span>
<span class="normal"><a href="#__codelineno-0-1506">1506</a></span>
<span class="normal"><a href="#__codelineno-0-1507">1507</a></span>
<span class="normal"><a href="#__codelineno-0-1508">1508</a></span>
<span class="normal"><a href="#__codelineno-0-1509">1509</a></span>
<span class="normal"><a href="#__codelineno-0-1510">1510</a></span>
<span class="normal"><a href="#__codelineno-0-1511">1511</a></span>
<span class="normal"><a href="#__codelineno-0-1512">1512</a></span>
<span class="normal"><a href="#__codelineno-0-1513">1513</a></span>
<span class="normal"><a href="#__codelineno-0-1514">1514</a></span>
<span class="normal"><a href="#__codelineno-0-1515">1515</a></span>
<span class="normal"><a href="#__codelineno-0-1516">1516</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1459"><a id="__codelineno-0-1459" name="__codelineno-0-1459"></a><span class="k">def</span> <span class="nf">influences_by_layer</span><span class="p">(</span>
</span><span id="__span-0-1460"><a id="__codelineno-0-1460" name="__codelineno-0-1460"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-1461"><a id="__codelineno-0-1461" name="__codelineno-0-1461"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-1462"><a id="__codelineno-0-1462" name="__codelineno-0-1462"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-1463"><a id="__codelineno-0-1463" name="__codelineno-0-1463"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-1464"><a id="__codelineno-0-1464" name="__codelineno-0-1464"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-1465"><a id="__codelineno-0-1465" name="__codelineno-0-1465"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-1466"><a id="__codelineno-0-1466" name="__codelineno-0-1466"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="__span-0-1467"><a id="__codelineno-0-1467" name="__codelineno-0-1467"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-1468"><a id="__codelineno-0-1468" name="__codelineno-0-1468"></a><span class="sd">    Compute the influence of the data on the test data for each layer of the model.</span>
</span><span id="__span-0-1469"><a id="__codelineno-0-1469" name="__codelineno-0-1469"></a>
</span><span id="__span-0-1470"><a id="__codelineno-0-1470" name="__codelineno-0-1470"></a><span class="sd">    Args:</span>
</span><span id="__span-0-1471"><a id="__codelineno-0-1471" name="__codelineno-0-1471"></a><span class="sd">        x_test: model input to use in the gradient computations of</span>
</span><span id="__span-0-1472"><a id="__codelineno-0-1472" name="__codelineno-0-1472"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-1473"><a id="__codelineno-0-1473" name="__codelineno-0-1473"></a><span class="sd">                f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-1474"><a id="__codelineno-0-1474" name="__codelineno-0-1474"></a><span class="sd">        y_test: label tensor to compute gradients</span>
</span><span id="__span-0-1475"><a id="__codelineno-0-1475" name="__codelineno-0-1475"></a><span class="sd">        x: optional model input to use in the gradient computations</span>
</span><span id="__span-0-1476"><a id="__codelineno-0-1476" name="__codelineno-0-1476"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-1477"><a id="__codelineno-0-1477" name="__codelineno-0-1477"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-1478"><a id="__codelineno-0-1478" name="__codelineno-0-1478"></a><span class="sd">            if None, use $x=x_{\text{test}}$</span>
</span><span id="__span-0-1479"><a id="__codelineno-0-1479" name="__codelineno-0-1479"></a><span class="sd">        y: optional label tensor to compute gradients</span>
</span><span id="__span-0-1480"><a id="__codelineno-0-1480" name="__codelineno-0-1480"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-1481"><a id="__codelineno-0-1481" name="__codelineno-0-1481"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-1482"><a id="__codelineno-0-1482" name="__codelineno-0-1482"></a>
</span><span id="__span-0-1483"><a id="__codelineno-0-1483" name="__codelineno-0-1483"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-1484"><a id="__codelineno-0-1484" name="__codelineno-0-1484"></a><span class="sd">        A dictionary containing the influence of the data on the test data for each</span>
</span><span id="__span-0-1485"><a id="__codelineno-0-1485" name="__codelineno-0-1485"></a><span class="sd">        layer of the model, with the layer name as key.</span>
</span><span id="__span-0-1486"><a id="__codelineno-0-1486" name="__codelineno-0-1486"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-1487"><a id="__codelineno-0-1487" name="__codelineno-0-1487"></a>    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span><span class="p">:</span>
</span><span id="__span-0-1488"><a id="__codelineno-0-1488" name="__codelineno-0-1488"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="__span-0-1489"><a id="__codelineno-0-1489" name="__codelineno-0-1489"></a>            <span class="s2">&quot;Instance must be fitted before calling influence methods on it&quot;</span>
</span><span id="__span-0-1490"><a id="__codelineno-0-1490" name="__codelineno-0-1490"></a>        <span class="p">)</span>
</span><span id="__span-0-1491"><a id="__codelineno-0-1491" name="__codelineno-0-1491"></a>
</span><span id="__span-0-1492"><a id="__codelineno-0-1492" name="__codelineno-0-1492"></a>    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-0-1493"><a id="__codelineno-0-1493" name="__codelineno-0-1493"></a>        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-0-1494"><a id="__codelineno-0-1494" name="__codelineno-0-1494"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="__span-0-1495"><a id="__codelineno-0-1495" name="__codelineno-0-1495"></a>                <span class="s2">&quot;Providing labels y, without providing model input x &quot;</span>
</span><span id="__span-0-1496"><a id="__codelineno-0-1496" name="__codelineno-0-1496"></a>                <span class="s2">&quot;is not supported&quot;</span>
</span><span id="__span-0-1497"><a id="__codelineno-0-1497" name="__codelineno-0-1497"></a>            <span class="p">)</span>
</span><span id="__span-0-1498"><a id="__codelineno-0-1498" name="__codelineno-0-1498"></a>
</span><span id="__span-0-1499"><a id="__codelineno-0-1499" name="__codelineno-0-1499"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_symmetric_values_by_layer</span><span class="p">(</span>
</span><span id="__span-0-1500"><a id="__codelineno-0-1500" name="__codelineno-0-1500"></a>            <span class="n">x_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-1501"><a id="__codelineno-0-1501" name="__codelineno-0-1501"></a>            <span class="n">y_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-1502"><a id="__codelineno-0-1502" name="__codelineno-0-1502"></a>            <span class="n">mode</span><span class="p">,</span>
</span><span id="__span-0-1503"><a id="__codelineno-0-1503" name="__codelineno-0-1503"></a>        <span class="p">)</span>
</span><span id="__span-0-1504"><a id="__codelineno-0-1504" name="__codelineno-0-1504"></a>
</span><span id="__span-0-1505"><a id="__codelineno-0-1505" name="__codelineno-0-1505"></a>    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-0-1506"><a id="__codelineno-0-1506" name="__codelineno-0-1506"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="__span-0-1507"><a id="__codelineno-0-1507" name="__codelineno-0-1507"></a>            <span class="s2">&quot;Providing model input x without providing labels y is not supported&quot;</span>
</span><span id="__span-0-1508"><a id="__codelineno-0-1508" name="__codelineno-0-1508"></a>        <span class="p">)</span>
</span><span id="__span-0-1509"><a id="__codelineno-0-1509" name="__codelineno-0-1509"></a>
</span><span id="__span-0-1510"><a id="__codelineno-0-1510" name="__codelineno-0-1510"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_symmetric_values_by_layer</span><span class="p">(</span>
</span><span id="__span-0-1511"><a id="__codelineno-0-1511" name="__codelineno-0-1511"></a>        <span class="n">x_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-1512"><a id="__codelineno-0-1512" name="__codelineno-0-1512"></a>        <span class="n">y_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-1513"><a id="__codelineno-0-1513" name="__codelineno-0-1513"></a>        <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-1514"><a id="__codelineno-0-1514" name="__codelineno-0-1514"></a>        <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-1515"><a id="__codelineno-0-1515" name="__codelineno-0-1515"></a>        <span class="n">mode</span><span class="p">,</span>
</span><span id="__span-0-1516"><a id="__codelineno-0-1516" name="__codelineno-0-1516"></a>    <span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.EkfacInfluence.influence_factors_by_layer" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influence_factors_by_layer</span>


<a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influence_factors_by_layer" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influence_factors_by_layer</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Computes the approximation of</p>
<div class="arithmatex">\[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</div>
<p>for each layer of the model separately.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>A dictionary containing the influence factors for each layer of the model,</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>with the layer name as key.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1518">1518</a></span>
<span class="normal"><a href="#__codelineno-0-1519">1519</a></span>
<span class="normal"><a href="#__codelineno-0-1520">1520</a></span>
<span class="normal"><a href="#__codelineno-0-1521">1521</a></span>
<span class="normal"><a href="#__codelineno-0-1522">1522</a></span>
<span class="normal"><a href="#__codelineno-0-1523">1523</a></span>
<span class="normal"><a href="#__codelineno-0-1524">1524</a></span>
<span class="normal"><a href="#__codelineno-0-1525">1525</a></span>
<span class="normal"><a href="#__codelineno-0-1526">1526</a></span>
<span class="normal"><a href="#__codelineno-0-1527">1527</a></span>
<span class="normal"><a href="#__codelineno-0-1528">1528</a></span>
<span class="normal"><a href="#__codelineno-0-1529">1529</a></span>
<span class="normal"><a href="#__codelineno-0-1530">1530</a></span>
<span class="normal"><a href="#__codelineno-0-1531">1531</a></span>
<span class="normal"><a href="#__codelineno-0-1532">1532</a></span>
<span class="normal"><a href="#__codelineno-0-1533">1533</a></span>
<span class="normal"><a href="#__codelineno-0-1534">1534</a></span>
<span class="normal"><a href="#__codelineno-0-1535">1535</a></span>
<span class="normal"><a href="#__codelineno-0-1536">1536</a></span>
<span class="normal"><a href="#__codelineno-0-1537">1537</a></span>
<span class="normal"><a href="#__codelineno-0-1538">1538</a></span>
<span class="normal"><a href="#__codelineno-0-1539">1539</a></span>
<span class="normal"><a href="#__codelineno-0-1540">1540</a></span>
<span class="normal"><a href="#__codelineno-0-1541">1541</a></span>
<span class="normal"><a href="#__codelineno-0-1542">1542</a></span>
<span class="normal"><a href="#__codelineno-0-1543">1543</a></span>
<span class="normal"><a href="#__codelineno-0-1544">1544</a></span>
<span class="normal"><a href="#__codelineno-0-1545">1545</a></span>
<span class="normal"><a href="#__codelineno-0-1546">1546</a></span>
<span class="normal"><a href="#__codelineno-0-1547">1547</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1518"><a id="__codelineno-0-1518" name="__codelineno-0-1518"></a><span class="k">def</span> <span class="nf">influence_factors_by_layer</span><span class="p">(</span>
</span><span id="__span-0-1519"><a id="__codelineno-0-1519" name="__codelineno-0-1519"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-1520"><a id="__codelineno-0-1520" name="__codelineno-0-1520"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-1521"><a id="__codelineno-0-1521" name="__codelineno-0-1521"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-1522"><a id="__codelineno-0-1522" name="__codelineno-0-1522"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="__span-0-1523"><a id="__codelineno-0-1523" name="__codelineno-0-1523"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-1524"><a id="__codelineno-0-1524" name="__codelineno-0-1524"></a><span class="sd">    Computes the approximation of</span>
</span><span id="__span-0-1525"><a id="__codelineno-0-1525" name="__codelineno-0-1525"></a>
</span><span id="__span-0-1526"><a id="__codelineno-0-1526" name="__codelineno-0-1526"></a><span class="sd">    \[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</span>
</span><span id="__span-0-1527"><a id="__codelineno-0-1527" name="__codelineno-0-1527"></a>
</span><span id="__span-0-1528"><a id="__codelineno-0-1528" name="__codelineno-0-1528"></a><span class="sd">    for each layer of the model separately.</span>
</span><span id="__span-0-1529"><a id="__codelineno-0-1529" name="__codelineno-0-1529"></a>
</span><span id="__span-0-1530"><a id="__codelineno-0-1530" name="__codelineno-0-1530"></a><span class="sd">    Args:</span>
</span><span id="__span-0-1531"><a id="__codelineno-0-1531" name="__codelineno-0-1531"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-1532"><a id="__codelineno-0-1532" name="__codelineno-0-1532"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-1533"><a id="__codelineno-0-1533" name="__codelineno-0-1533"></a>
</span><span id="__span-0-1534"><a id="__codelineno-0-1534" name="__codelineno-0-1534"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-1535"><a id="__codelineno-0-1535" name="__codelineno-0-1535"></a><span class="sd">        A dictionary containing the influence factors for each layer of the model,</span>
</span><span id="__span-0-1536"><a id="__codelineno-0-1536" name="__codelineno-0-1536"></a><span class="sd">        with the layer name as key.</span>
</span><span id="__span-0-1537"><a id="__codelineno-0-1537" name="__codelineno-0-1537"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-1538"><a id="__codelineno-0-1538" name="__codelineno-0-1538"></a>    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_fitted</span><span class="p">:</span>
</span><span id="__span-0-1539"><a id="__codelineno-0-1539" name="__codelineno-0-1539"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="__span-0-1540"><a id="__codelineno-0-1540" name="__codelineno-0-1540"></a>            <span class="s2">&quot;Instance must be fitted before calling influence methods on it&quot;</span>
</span><span id="__span-0-1541"><a id="__codelineno-0-1541" name="__codelineno-0-1541"></a>        <span class="p">)</span>
</span><span id="__span-0-1542"><a id="__codelineno-0-1542" name="__codelineno-0-1542"></a>
</span><span id="__span-0-1543"><a id="__codelineno-0-1543" name="__codelineno-0-1543"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_hvp_by_layer</span><span class="p">(</span>
</span><span id="__span-0-1544"><a id="__codelineno-0-1544" name="__codelineno-0-1544"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)),</span>
</span><span id="__span-0-1545"><a id="__codelineno-0-1545" name="__codelineno-0-1545"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ekfac_representation</span><span class="p">,</span>
</span><span id="__span-0-1546"><a id="__codelineno-0-1546" name="__codelineno-0-1546"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hessian_regularization</span><span class="p">,</span>
</span><span id="__span-0-1547"><a id="__codelineno-0-1547" name="__codelineno-0-1547"></a>    <span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_from_factors_by_layer" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences_from_factors_by_layer</span>


<a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.influences_from_factors_by_layer" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences_from_factors_by_layer</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Computation of</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the perturbation type influence case for each layer of the model
separately. The gradients are meant to be per sample of the batch <span class="arithmatex">\((x,
y)\)</span>.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>z_test_factors</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>pre-computed tensor, approximating
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>A dictionary containing the influence of the data on the test data</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>for each layer of the model, with the layer name as key.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1549">1549</a></span>
<span class="normal"><a href="#__codelineno-0-1550">1550</a></span>
<span class="normal"><a href="#__codelineno-0-1551">1551</a></span>
<span class="normal"><a href="#__codelineno-0-1552">1552</a></span>
<span class="normal"><a href="#__codelineno-0-1553">1553</a></span>
<span class="normal"><a href="#__codelineno-0-1554">1554</a></span>
<span class="normal"><a href="#__codelineno-0-1555">1555</a></span>
<span class="normal"><a href="#__codelineno-0-1556">1556</a></span>
<span class="normal"><a href="#__codelineno-0-1557">1557</a></span>
<span class="normal"><a href="#__codelineno-0-1558">1558</a></span>
<span class="normal"><a href="#__codelineno-0-1559">1559</a></span>
<span class="normal"><a href="#__codelineno-0-1560">1560</a></span>
<span class="normal"><a href="#__codelineno-0-1561">1561</a></span>
<span class="normal"><a href="#__codelineno-0-1562">1562</a></span>
<span class="normal"><a href="#__codelineno-0-1563">1563</a></span>
<span class="normal"><a href="#__codelineno-0-1564">1564</a></span>
<span class="normal"><a href="#__codelineno-0-1565">1565</a></span>
<span class="normal"><a href="#__codelineno-0-1566">1566</a></span>
<span class="normal"><a href="#__codelineno-0-1567">1567</a></span>
<span class="normal"><a href="#__codelineno-0-1568">1568</a></span>
<span class="normal"><a href="#__codelineno-0-1569">1569</a></span>
<span class="normal"><a href="#__codelineno-0-1570">1570</a></span>
<span class="normal"><a href="#__codelineno-0-1571">1571</a></span>
<span class="normal"><a href="#__codelineno-0-1572">1572</a></span>
<span class="normal"><a href="#__codelineno-0-1573">1573</a></span>
<span class="normal"><a href="#__codelineno-0-1574">1574</a></span>
<span class="normal"><a href="#__codelineno-0-1575">1575</a></span>
<span class="normal"><a href="#__codelineno-0-1576">1576</a></span>
<span class="normal"><a href="#__codelineno-0-1577">1577</a></span>
<span class="normal"><a href="#__codelineno-0-1578">1578</a></span>
<span class="normal"><a href="#__codelineno-0-1579">1579</a></span>
<span class="normal"><a href="#__codelineno-0-1580">1580</a></span>
<span class="normal"><a href="#__codelineno-0-1581">1581</a></span>
<span class="normal"><a href="#__codelineno-0-1582">1582</a></span>
<span class="normal"><a href="#__codelineno-0-1583">1583</a></span>
<span class="normal"><a href="#__codelineno-0-1584">1584</a></span>
<span class="normal"><a href="#__codelineno-0-1585">1585</a></span>
<span class="normal"><a href="#__codelineno-0-1586">1586</a></span>
<span class="normal"><a href="#__codelineno-0-1587">1587</a></span>
<span class="normal"><a href="#__codelineno-0-1588">1588</a></span>
<span class="normal"><a href="#__codelineno-0-1589">1589</a></span>
<span class="normal"><a href="#__codelineno-0-1590">1590</a></span>
<span class="normal"><a href="#__codelineno-0-1591">1591</a></span>
<span class="normal"><a href="#__codelineno-0-1592">1592</a></span>
<span class="normal"><a href="#__codelineno-0-1593">1593</a></span>
<span class="normal"><a href="#__codelineno-0-1594">1594</a></span>
<span class="normal"><a href="#__codelineno-0-1595">1595</a></span>
<span class="normal"><a href="#__codelineno-0-1596">1596</a></span>
<span class="normal"><a href="#__codelineno-0-1597">1597</a></span>
<span class="normal"><a href="#__codelineno-0-1598">1598</a></span>
<span class="normal"><a href="#__codelineno-0-1599">1599</a></span>
<span class="normal"><a href="#__codelineno-0-1600">1600</a></span>
<span class="normal"><a href="#__codelineno-0-1601">1601</a></span>
<span class="normal"><a href="#__codelineno-0-1602">1602</a></span>
<span class="normal"><a href="#__codelineno-0-1603">1603</a></span>
<span class="normal"><a href="#__codelineno-0-1604">1604</a></span>
<span class="normal"><a href="#__codelineno-0-1605">1605</a></span>
<span class="normal"><a href="#__codelineno-0-1606">1606</a></span>
<span class="normal"><a href="#__codelineno-0-1607">1607</a></span>
<span class="normal"><a href="#__codelineno-0-1608">1608</a></span>
<span class="normal"><a href="#__codelineno-0-1609">1609</a></span>
<span class="normal"><a href="#__codelineno-0-1610">1610</a></span>
<span class="normal"><a href="#__codelineno-0-1611">1611</a></span>
<span class="normal"><a href="#__codelineno-0-1612">1612</a></span>
<span class="normal"><a href="#__codelineno-0-1613">1613</a></span>
<span class="normal"><a href="#__codelineno-0-1614">1614</a></span>
<span class="normal"><a href="#__codelineno-0-1615">1615</a></span>
<span class="normal"><a href="#__codelineno-0-1616">1616</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1549"><a id="__codelineno-0-1549" name="__codelineno-0-1549"></a><span class="k">def</span> <span class="nf">influences_from_factors_by_layer</span><span class="p">(</span>
</span><span id="__span-0-1550"><a id="__codelineno-0-1550" name="__codelineno-0-1550"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-1551"><a id="__codelineno-0-1551" name="__codelineno-0-1551"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span id="__span-0-1552"><a id="__codelineno-0-1552" name="__codelineno-0-1552"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-1553"><a id="__codelineno-0-1553" name="__codelineno-0-1553"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-1554"><a id="__codelineno-0-1554" name="__codelineno-0-1554"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-1555"><a id="__codelineno-0-1555" name="__codelineno-0-1555"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="__span-0-1556"><a id="__codelineno-0-1556" name="__codelineno-0-1556"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-1557"><a id="__codelineno-0-1557" name="__codelineno-0-1557"></a><span class="sd">    Computation of</span>
</span><span id="__span-0-1558"><a id="__codelineno-0-1558" name="__codelineno-0-1558"></a>
</span><span id="__span-0-1559"><a id="__codelineno-0-1559" name="__codelineno-0-1559"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-1560"><a id="__codelineno-0-1560" name="__codelineno-0-1560"></a><span class="sd">        \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-1561"><a id="__codelineno-0-1561" name="__codelineno-0-1561"></a>
</span><span id="__span-0-1562"><a id="__codelineno-0-1562" name="__codelineno-0-1562"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-1563"><a id="__codelineno-0-1563" name="__codelineno-0-1563"></a>
</span><span id="__span-0-1564"><a id="__codelineno-0-1564" name="__codelineno-0-1564"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-1565"><a id="__codelineno-0-1565" name="__codelineno-0-1565"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-1566"><a id="__codelineno-0-1566" name="__codelineno-0-1566"></a>
</span><span id="__span-0-1567"><a id="__codelineno-0-1567" name="__codelineno-0-1567"></a><span class="sd">    for the perturbation type influence case for each layer of the model</span>
</span><span id="__span-0-1568"><a id="__codelineno-0-1568" name="__codelineno-0-1568"></a><span class="sd">    separately. The gradients are meant to be per sample of the batch $(x,</span>
</span><span id="__span-0-1569"><a id="__codelineno-0-1569" name="__codelineno-0-1569"></a><span class="sd">    y)$.</span>
</span><span id="__span-0-1570"><a id="__codelineno-0-1570" name="__codelineno-0-1570"></a>
</span><span id="__span-0-1571"><a id="__codelineno-0-1571" name="__codelineno-0-1571"></a><span class="sd">    Args:</span>
</span><span id="__span-0-1572"><a id="__codelineno-0-1572" name="__codelineno-0-1572"></a><span class="sd">        z_test_factors: pre-computed tensor, approximating</span>
</span><span id="__span-0-1573"><a id="__codelineno-0-1573" name="__codelineno-0-1573"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-1574"><a id="__codelineno-0-1574" name="__codelineno-0-1574"></a><span class="sd">            f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-1575"><a id="__codelineno-0-1575" name="__codelineno-0-1575"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-1576"><a id="__codelineno-0-1576" name="__codelineno-0-1576"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-1577"><a id="__codelineno-0-1577" name="__codelineno-0-1577"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$</span>
</span><span id="__span-0-1578"><a id="__codelineno-0-1578" name="__codelineno-0-1578"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-1579"><a id="__codelineno-0-1579" name="__codelineno-0-1579"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-1580"><a id="__codelineno-0-1580" name="__codelineno-0-1580"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-1581"><a id="__codelineno-0-1581" name="__codelineno-0-1581"></a>
</span><span id="__span-0-1582"><a id="__codelineno-0-1582" name="__codelineno-0-1582"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-1583"><a id="__codelineno-0-1583" name="__codelineno-0-1583"></a><span class="sd">        A dictionary containing the influence of the data on the test data</span>
</span><span id="__span-0-1584"><a id="__codelineno-0-1584" name="__codelineno-0-1584"></a><span class="sd">        for each layer of the model, with the layer name as key.</span>
</span><span id="__span-0-1585"><a id="__codelineno-0-1585" name="__codelineno-0-1585"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-1586"><a id="__codelineno-0-1586" name="__codelineno-0-1586"></a>    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">:</span>
</span><span id="__span-0-1587"><a id="__codelineno-0-1587" name="__codelineno-0-1587"></a>        <span class="n">total_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_grad</span><span class="p">(</span>
</span><span id="__span-0-1588"><a id="__codelineno-0-1588" name="__codelineno-0-1588"></a>            <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-1589"><a id="__codelineno-0-1589" name="__codelineno-0-1589"></a>        <span class="p">)</span>
</span><span id="__span-0-1590"><a id="__codelineno-0-1590" name="__codelineno-0-1590"></a>        <span class="n">start_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-0-1591"><a id="__codelineno-0-1591" name="__codelineno-0-1591"></a>        <span class="n">influences</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="__span-0-1592"><a id="__codelineno-0-1592" name="__codelineno-0-1592"></a>        <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">layer_z_test</span> <span class="ow">in</span> <span class="n">z_test_factors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="__span-0-1593"><a id="__codelineno-0-1593" name="__codelineno-0-1593"></a>            <span class="n">end_idx</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">layer_z_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="__span-0-1594"><a id="__codelineno-0-1594" name="__codelineno-0-1594"></a>            <span class="n">influences</span><span class="p">[</span><span class="n">layer_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="__span-0-1595"><a id="__codelineno-0-1595" name="__codelineno-0-1595"></a>                <span class="n">layer_z_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-1596"><a id="__codelineno-0-1596" name="__codelineno-0-1596"></a>                <span class="o">@</span> <span class="n">total_grad</span><span class="p">[:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
</span><span id="__span-0-1597"><a id="__codelineno-0-1597" name="__codelineno-0-1597"></a>            <span class="p">)</span>
</span><span id="__span-0-1598"><a id="__codelineno-0-1598" name="__codelineno-0-1598"></a>            <span class="n">start_idx</span> <span class="o">=</span> <span class="n">end_idx</span>
</span><span id="__span-0-1599"><a id="__codelineno-0-1599" name="__codelineno-0-1599"></a>        <span class="k">return</span> <span class="n">influences</span>
</span><span id="__span-0-1600"><a id="__codelineno-0-1600" name="__codelineno-0-1600"></a>    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">:</span>
</span><span id="__span-0-1601"><a id="__codelineno-0-1601" name="__codelineno-0-1601"></a>        <span class="n">total_mixed_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flat_loss_mixed_grad</span><span class="p">(</span>
</span><span id="__span-0-1602"><a id="__codelineno-0-1602" name="__codelineno-0-1602"></a>            <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-1603"><a id="__codelineno-0-1603" name="__codelineno-0-1603"></a>        <span class="p">)</span>
</span><span id="__span-0-1604"><a id="__codelineno-0-1604" name="__codelineno-0-1604"></a>        <span class="n">start_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-0-1605"><a id="__codelineno-0-1605" name="__codelineno-0-1605"></a>        <span class="n">influences</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="__span-0-1606"><a id="__codelineno-0-1606" name="__codelineno-0-1606"></a>        <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">layer_z_test</span> <span class="ow">in</span> <span class="n">z_test_factors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="__span-0-1607"><a id="__codelineno-0-1607" name="__codelineno-0-1607"></a>            <span class="n">end_idx</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">layer_z_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="__span-0-1608"><a id="__codelineno-0-1608" name="__codelineno-0-1608"></a>            <span class="n">influences</span><span class="p">[</span><span class="n">layer_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
</span><span id="__span-0-1609"><a id="__codelineno-0-1609" name="__codelineno-0-1609"></a>                <span class="s2">&quot;ia,j...a-&gt;ij...&quot;</span><span class="p">,</span>
</span><span id="__span-0-1610"><a id="__codelineno-0-1610" name="__codelineno-0-1610"></a>                <span class="n">layer_z_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-1611"><a id="__codelineno-0-1611" name="__codelineno-0-1611"></a>                <span class="n">total_mixed_grad</span><span class="p">[:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">],</span>
</span><span id="__span-0-1612"><a id="__codelineno-0-1612" name="__codelineno-0-1612"></a>            <span class="p">)</span>
</span><span id="__span-0-1613"><a id="__codelineno-0-1613" name="__codelineno-0-1613"></a>            <span class="n">start_idx</span> <span class="o">=</span> <span class="n">end_idx</span>
</span><span id="__span-0-1614"><a id="__codelineno-0-1614" name="__codelineno-0-1614"></a>        <span class="k">return</span> <span class="n">influences</span>
</span><span id="__span-0-1615"><a id="__codelineno-0-1615" name="__codelineno-0-1615"></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-1616"><a id="__codelineno-0-1616" name="__codelineno-0-1616"></a>        <span class="k">raise</span> <span class="n">UnsupportedInfluenceModeException</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.EkfacInfluence.explore_hessian_regularization" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">explore_hessian_regularization</span>


<a href="#pydvl.influence.torch.influence_function_model.EkfacInfluence.explore_hessian_regularization" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">explore_hessian_regularization</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">regularization_values</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.List" href="https://docs.python.org/3/library/typing.html#typing.List">List</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span><span class="p">]</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]]</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Efficiently computes the influence for input x and label y for each layer of the
model, for different values of the hessian regularization parameter. This is done
by computing the gradient of the loss function for the input x and label y only once
and then solving the Hessian Vector Product for each regularization value. This is
useful for finding the optimal regularization value and for exploring
how robust the influence values are to changes in the regularization value.</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>regularization_values</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>list of regularization values to use</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.List" href="https://docs.python.org/3/library/typing.html#typing.List">List</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>]</code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>, <a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]]</code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>A dictionary containing with keys being the regularization values and values</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>, <a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]]</code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>being dictionaries containing the influences for each layer of the model,</p>
            </div>
          </td>
        </tr>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a>, <a class="autorefs autorefs-external" title="typing.Dict" href="https://docs.python.org/3/library/typing.html#typing.Dict">Dict</a>[<a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]]</code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>with the layer name as key.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1673">1673</a></span>
<span class="normal"><a href="#__codelineno-0-1674">1674</a></span>
<span class="normal"><a href="#__codelineno-0-1675">1675</a></span>
<span class="normal"><a href="#__codelineno-0-1676">1676</a></span>
<span class="normal"><a href="#__codelineno-0-1677">1677</a></span>
<span class="normal"><a href="#__codelineno-0-1678">1678</a></span>
<span class="normal"><a href="#__codelineno-0-1679">1679</a></span>
<span class="normal"><a href="#__codelineno-0-1680">1680</a></span>
<span class="normal"><a href="#__codelineno-0-1681">1681</a></span>
<span class="normal"><a href="#__codelineno-0-1682">1682</a></span>
<span class="normal"><a href="#__codelineno-0-1683">1683</a></span>
<span class="normal"><a href="#__codelineno-0-1684">1684</a></span>
<span class="normal"><a href="#__codelineno-0-1685">1685</a></span>
<span class="normal"><a href="#__codelineno-0-1686">1686</a></span>
<span class="normal"><a href="#__codelineno-0-1687">1687</a></span>
<span class="normal"><a href="#__codelineno-0-1688">1688</a></span>
<span class="normal"><a href="#__codelineno-0-1689">1689</a></span>
<span class="normal"><a href="#__codelineno-0-1690">1690</a></span>
<span class="normal"><a href="#__codelineno-0-1691">1691</a></span>
<span class="normal"><a href="#__codelineno-0-1692">1692</a></span>
<span class="normal"><a href="#__codelineno-0-1693">1693</a></span>
<span class="normal"><a href="#__codelineno-0-1694">1694</a></span>
<span class="normal"><a href="#__codelineno-0-1695">1695</a></span>
<span class="normal"><a href="#__codelineno-0-1696">1696</a></span>
<span class="normal"><a href="#__codelineno-0-1697">1697</a></span>
<span class="normal"><a href="#__codelineno-0-1698">1698</a></span>
<span class="normal"><a href="#__codelineno-0-1699">1699</a></span>
<span class="normal"><a href="#__codelineno-0-1700">1700</a></span>
<span class="normal"><a href="#__codelineno-0-1701">1701</a></span>
<span class="normal"><a href="#__codelineno-0-1702">1702</a></span>
<span class="normal"><a href="#__codelineno-0-1703">1703</a></span>
<span class="normal"><a href="#__codelineno-0-1704">1704</a></span>
<span class="normal"><a href="#__codelineno-0-1705">1705</a></span>
<span class="normal"><a href="#__codelineno-0-1706">1706</a></span>
<span class="normal"><a href="#__codelineno-0-1707">1707</a></span>
<span class="normal"><a href="#__codelineno-0-1708">1708</a></span>
<span class="normal"><a href="#__codelineno-0-1709">1709</a></span>
<span class="normal"><a href="#__codelineno-0-1710">1710</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1673"><a id="__codelineno-0-1673" name="__codelineno-0-1673"></a><span class="k">def</span> <span class="nf">explore_hessian_regularization</span><span class="p">(</span>
</span><span id="__span-0-1674"><a id="__codelineno-0-1674" name="__codelineno-0-1674"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-1675"><a id="__codelineno-0-1675" name="__codelineno-0-1675"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-1676"><a id="__codelineno-0-1676" name="__codelineno-0-1676"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-1677"><a id="__codelineno-0-1677" name="__codelineno-0-1677"></a>    <span class="n">regularization_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
</span><span id="__span-0-1678"><a id="__codelineno-0-1678" name="__codelineno-0-1678"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
</span><span id="__span-0-1679"><a id="__codelineno-0-1679" name="__codelineno-0-1679"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-1680"><a id="__codelineno-0-1680" name="__codelineno-0-1680"></a><span class="sd">    Efficiently computes the influence for input x and label y for each layer of the</span>
</span><span id="__span-0-1681"><a id="__codelineno-0-1681" name="__codelineno-0-1681"></a><span class="sd">    model, for different values of the hessian regularization parameter. This is done</span>
</span><span id="__span-0-1682"><a id="__codelineno-0-1682" name="__codelineno-0-1682"></a><span class="sd">    by computing the gradient of the loss function for the input x and label y only once</span>
</span><span id="__span-0-1683"><a id="__codelineno-0-1683" name="__codelineno-0-1683"></a><span class="sd">    and then solving the Hessian Vector Product for each regularization value. This is</span>
</span><span id="__span-0-1684"><a id="__codelineno-0-1684" name="__codelineno-0-1684"></a><span class="sd">    useful for finding the optimal regularization value and for exploring</span>
</span><span id="__span-0-1685"><a id="__codelineno-0-1685" name="__codelineno-0-1685"></a><span class="sd">    how robust the influence values are to changes in the regularization value.</span>
</span><span id="__span-0-1686"><a id="__codelineno-0-1686" name="__codelineno-0-1686"></a>
</span><span id="__span-0-1687"><a id="__codelineno-0-1687" name="__codelineno-0-1687"></a><span class="sd">    Args:</span>
</span><span id="__span-0-1688"><a id="__codelineno-0-1688" name="__codelineno-0-1688"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-1689"><a id="__codelineno-0-1689" name="__codelineno-0-1689"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-1690"><a id="__codelineno-0-1690" name="__codelineno-0-1690"></a><span class="sd">        regularization_values: list of regularization values to use</span>
</span><span id="__span-0-1691"><a id="__codelineno-0-1691" name="__codelineno-0-1691"></a>
</span><span id="__span-0-1692"><a id="__codelineno-0-1692" name="__codelineno-0-1692"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-1693"><a id="__codelineno-0-1693" name="__codelineno-0-1693"></a><span class="sd">        A dictionary containing with keys being the regularization values and values</span>
</span><span id="__span-0-1694"><a id="__codelineno-0-1694" name="__codelineno-0-1694"></a><span class="sd">        being dictionaries containing the influences for each layer of the model,</span>
</span><span id="__span-0-1695"><a id="__codelineno-0-1695" name="__codelineno-0-1695"></a><span class="sd">        with the layer name as key.</span>
</span><span id="__span-0-1696"><a id="__codelineno-0-1696" name="__codelineno-0-1696"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-1697"><a id="__codelineno-0-1697" name="__codelineno-0-1697"></a>    <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">))</span>
</span><span id="__span-0-1698"><a id="__codelineno-0-1698" name="__codelineno-0-1698"></a>    <span class="n">influences_by_reg_value</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="__span-0-1699"><a id="__codelineno-0-1699" name="__codelineno-0-1699"></a>    <span class="k">for</span> <span class="n">reg_value</span> <span class="ow">in</span> <span class="n">regularization_values</span><span class="p">:</span>
</span><span id="__span-0-1700"><a id="__codelineno-0-1700" name="__codelineno-0-1700"></a>        <span class="n">reg_factors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_solve_hvp_by_layer</span><span class="p">(</span>
</span><span id="__span-0-1701"><a id="__codelineno-0-1701" name="__codelineno-0-1701"></a>            <span class="n">grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ekfac_representation</span><span class="p">,</span> <span class="n">reg_value</span>
</span><span id="__span-0-1702"><a id="__codelineno-0-1702" name="__codelineno-0-1702"></a>        <span class="p">)</span>
</span><span id="__span-0-1703"><a id="__codelineno-0-1703" name="__codelineno-0-1703"></a>        <span class="n">values</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="__span-0-1704"><a id="__codelineno-0-1704" name="__codelineno-0-1704"></a>        <span class="n">start_idx</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-0-1705"><a id="__codelineno-0-1705" name="__codelineno-0-1705"></a>        <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">layer_fac</span> <span class="ow">in</span> <span class="n">reg_factors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="__span-0-1706"><a id="__codelineno-0-1706" name="__codelineno-0-1706"></a>            <span class="n">end_idx</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">layer_fac</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span id="__span-0-1707"><a id="__codelineno-0-1707" name="__codelineno-0-1707"></a>            <span class="n">values</span><span class="p">[</span><span class="n">layer_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">layer_fac</span> <span class="o">@</span> <span class="n">grad</span><span class="p">[:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
</span><span id="__span-0-1708"><a id="__codelineno-0-1708" name="__codelineno-0-1708"></a>            <span class="n">start_idx</span> <span class="o">=</span> <span class="n">end_idx</span>
</span><span id="__span-0-1709"><a id="__codelineno-0-1709" name="__codelineno-0-1709"></a>        <span class="n">influences_by_reg_value</span><span class="p">[</span><span class="n">reg_value</span><span class="p">]</span> <span class="o">=</span> <span class="n">values</span>
</span><span id="__span-0-1710"><a id="__codelineno-0-1710" name="__codelineno-0-1710"></a>    <span class="k">return</span> <span class="n">influences_by_reg_value</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pydvl.influence.torch.influence_function_model.NystroemSketchInfluence" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <span class="doc doc-object-name doc-class-name">NystroemSketchInfluence</span>


<a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence" class="headerlink" title="Permanent link">&para;</a></h2>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">NystroemSketchInfluence</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">model</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">[[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">],</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">rank</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span>
</span></code></pre></div>

  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel" href="#pydvl.influence.torch.influence_function_model.TorchInfluenceFunctionModel">TorchInfluenceFunctionModel</a></code></p>

  
      <p>Given a model and training data, it uses a low-rank approximation of the Hessian
(derived via random projection Nyström approximation) in combination with
the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Sherman–Morrison–Woodbury
formula</a> to
calculate the inverse of the Hessian Vector Product. More concrete, it
computes a low-rank approximation</p>
<div class="arithmatex">\[\begin{align*}
    H_{\text{nys}} &amp;= (H\Omega)(\Omega^TH\Omega)^{+}(H\Omega)^T \\\
                   &amp;= U \Lambda U^T
\end{align*}\]</div>
<p>in factorized form and approximates the action of the inverse Hessian via</p>
<div class="arithmatex">\[ (H_{\text{nys}} + \lambda I)^{-1} = U(\Lambda+\lambda I)U^T +
    \frac{1}{\lambda}(I−UU^T). \]</div>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A PyTorch model. The Hessian will be calculated with respect to
this model's parameters.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.nn.Module" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>loss</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>A callable that takes the model's output and target as input and returns
  the scalar loss.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a>[[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>, <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>], <a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>hessian_regularization</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>Optional regularization parameter added
to the Hessian-vector product for numerical stability.</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>rank</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>rank of the low-rank approximation</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>

                <details class="quote">
                  <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
                  <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1750">1750</a></span>
<span class="normal"><a href="#__codelineno-0-1751">1751</a></span>
<span class="normal"><a href="#__codelineno-0-1752">1752</a></span>
<span class="normal"><a href="#__codelineno-0-1753">1753</a></span>
<span class="normal"><a href="#__codelineno-0-1754">1754</a></span>
<span class="normal"><a href="#__codelineno-0-1755">1755</a></span>
<span class="normal"><a href="#__codelineno-0-1756">1756</a></span>
<span class="normal"><a href="#__codelineno-0-1757">1757</a></span>
<span class="normal"><a href="#__codelineno-0-1758">1758</a></span>
<span class="normal"><a href="#__codelineno-0-1759">1759</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1750"><a id="__codelineno-0-1750" name="__codelineno-0-1750"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span id="__span-0-1751"><a id="__codelineno-0-1751" name="__codelineno-0-1751"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-1752"><a id="__codelineno-0-1752" name="__codelineno-0-1752"></a>    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="__span-0-1753"><a id="__codelineno-0-1753" name="__codelineno-0-1753"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span><span id="__span-0-1754"><a id="__codelineno-0-1754" name="__codelineno-0-1754"></a>    <span class="n">hessian_regularization</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span><span id="__span-0-1755"><a id="__codelineno-0-1755" name="__codelineno-0-1755"></a>    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="__span-0-1756"><a id="__codelineno-0-1756" name="__codelineno-0-1756"></a><span class="p">):</span>
</span><span id="__span-0-1757"><a id="__codelineno-0-1757" name="__codelineno-0-1757"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</span><span id="__span-0-1758"><a id="__codelineno-0-1758" name="__codelineno-0-1758"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">hessian_regularization</span> <span class="o">=</span> <span class="n">hessian_regularization</span>
</span><span id="__span-0-1759"><a id="__codelineno-0-1759" name="__codelineno-0-1759"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
</span></code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influence_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influence_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influence_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute approximation of</p>
<div class="arithmatex">\[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</div>
<p>where the gradient is meant to be per sample of the batch <span class="arithmatex">\((x, y)\)</span>.
For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise inverse Hessian matrix vector products</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-245"><a id="__codelineno-0-245" name="__codelineno-0-245"></a><span class="k">def</span> <span class="nf">influence_factors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-246"><a id="__codelineno-0-246" name="__codelineno-0-246"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-247"><a id="__codelineno-0-247" name="__codelineno-0-247"></a><span class="sd">    Compute approximation of</span>
</span><span id="__span-0-248"><a id="__codelineno-0-248" name="__codelineno-0-248"></a>
</span><span id="__span-0-249"><a id="__codelineno-0-249" name="__codelineno-0-249"></a><span class="sd">    \[ H^{-1}\nabla_{\theta} \ell(y, f_{\theta}(x)) \]</span>
</span><span id="__span-0-250"><a id="__codelineno-0-250" name="__codelineno-0-250"></a>
</span><span id="__span-0-251"><a id="__codelineno-0-251" name="__codelineno-0-251"></a><span class="sd">    where the gradient is meant to be per sample of the batch $(x, y)$.</span>
</span><span id="__span-0-252"><a id="__codelineno-0-252" name="__codelineno-0-252"></a><span class="sd">    For all input tensors it is assumed,</span>
</span><span id="__span-0-253"><a id="__codelineno-0-253" name="__codelineno-0-253"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-254"><a id="__codelineno-0-254" name="__codelineno-0-254"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-255"><a id="__codelineno-0-255" name="__codelineno-0-255"></a>
</span><span id="__span-0-256"><a id="__codelineno-0-256" name="__codelineno-0-256"></a><span class="sd">    Args:</span>
</span><span id="__span-0-257"><a id="__codelineno-0-257" name="__codelineno-0-257"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-258"><a id="__codelineno-0-258" name="__codelineno-0-258"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-259"><a id="__codelineno-0-259" name="__codelineno-0-259"></a>
</span><span id="__span-0-260"><a id="__codelineno-0-260" name="__codelineno-0-260"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-261"><a id="__codelineno-0-261" name="__codelineno-0-261"></a><span class="sd">        Tensor representing the element-wise inverse Hessian matrix vector products</span>
</span><span id="__span-0-262"><a id="__codelineno-0-262" name="__codelineno-0-262"></a>
</span><span id="__span-0-263"><a id="__codelineno-0-263" name="__codelineno-0-263"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-264"><a id="__codelineno-0-264" name="__codelineno-0-264"></a>    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influence_factors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences</span>


<a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Compute the approximation of</p>
<div class="arithmatex">\[
\langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}})), \nabla_{\theta} \ell(y, f_{\theta}(x))\rangle
\]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[
\langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}}, f_{\theta}(x_{\text{test}})),
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle
\]</div>
<p>for the perturbation type influence case. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>x_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
of <span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
    f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y_test</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
if None, use <span class="arithmatex">\(x=x_{\text{test}}\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>optional label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="typing.Optional" href="https://docs.python.org/3/library/typing.html#typing.Optional">Optional</a>[<a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a>]</code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code>None</code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-121"><a id="__codelineno-0-121" name="__codelineno-0-121"></a><span class="k">def</span> <span class="nf">influences</span><span class="p">(</span>
</span><span id="__span-0-122"><a id="__codelineno-0-122" name="__codelineno-0-122"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-123"><a id="__codelineno-0-123" name="__codelineno-0-123"></a>    <span class="n">x_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-124"><a id="__codelineno-0-124" name="__codelineno-0-124"></a>    <span class="n">y_test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-125"><a id="__codelineno-0-125" name="__codelineno-0-125"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-126"><a id="__codelineno-0-126" name="__codelineno-0-126"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-0-127"><a id="__codelineno-0-127" name="__codelineno-0-127"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-128"><a id="__codelineno-0-128" name="__codelineno-0-128"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-129"><a id="__codelineno-0-129" name="__codelineno-0-129"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-130"><a id="__codelineno-0-130" name="__codelineno-0-130"></a><span class="sd">    Compute the approximation of</span>
</span><span id="__span-0-131"><a id="__codelineno-0-131" name="__codelineno-0-131"></a>
</span><span id="__span-0-132"><a id="__codelineno-0-132" name="__codelineno-0-132"></a><span class="sd">    \[</span>
</span><span id="__span-0-133"><a id="__codelineno-0-133" name="__codelineno-0-133"></a><span class="sd">    \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-134"><a id="__codelineno-0-134" name="__codelineno-0-134"></a><span class="sd">    f_{\theta}(x_{\text{test}})), \nabla_{\theta} \ell(y, f_{\theta}(x))\rangle</span>
</span><span id="__span-0-135"><a id="__codelineno-0-135" name="__codelineno-0-135"></a><span class="sd">    \]</span>
</span><span id="__span-0-136"><a id="__codelineno-0-136" name="__codelineno-0-136"></a>
</span><span id="__span-0-137"><a id="__codelineno-0-137" name="__codelineno-0-137"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-138"><a id="__codelineno-0-138" name="__codelineno-0-138"></a>
</span><span id="__span-0-139"><a id="__codelineno-0-139" name="__codelineno-0-139"></a><span class="sd">    \[</span>
</span><span id="__span-0-140"><a id="__codelineno-0-140" name="__codelineno-0-140"></a><span class="sd">    \langle H^{-1}\nabla_{\theta} \ell(y_{\text{test}}, f_{\theta}(x_{\text{test}})),</span>
</span><span id="__span-0-141"><a id="__codelineno-0-141" name="__codelineno-0-141"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle</span>
</span><span id="__span-0-142"><a id="__codelineno-0-142" name="__codelineno-0-142"></a><span class="sd">    \]</span>
</span><span id="__span-0-143"><a id="__codelineno-0-143" name="__codelineno-0-143"></a>
</span><span id="__span-0-144"><a id="__codelineno-0-144" name="__codelineno-0-144"></a><span class="sd">    for the perturbation type influence case. For all input tensors it is assumed,</span>
</span><span id="__span-0-145"><a id="__codelineno-0-145" name="__codelineno-0-145"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-146"><a id="__codelineno-0-146" name="__codelineno-0-146"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-147"><a id="__codelineno-0-147" name="__codelineno-0-147"></a>
</span><span id="__span-0-148"><a id="__codelineno-0-148" name="__codelineno-0-148"></a><span class="sd">    Args:</span>
</span><span id="__span-0-149"><a id="__codelineno-0-149" name="__codelineno-0-149"></a><span class="sd">        x_test: model input to use in the gradient computations</span>
</span><span id="__span-0-150"><a id="__codelineno-0-150" name="__codelineno-0-150"></a><span class="sd">            of $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-151"><a id="__codelineno-0-151" name="__codelineno-0-151"></a><span class="sd">                f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-152"><a id="__codelineno-0-152" name="__codelineno-0-152"></a><span class="sd">        y_test: label tensor to compute gradients</span>
</span><span id="__span-0-153"><a id="__codelineno-0-153" name="__codelineno-0-153"></a><span class="sd">        x: optional model input to use in the gradient computations</span>
</span><span id="__span-0-154"><a id="__codelineno-0-154" name="__codelineno-0-154"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-155"><a id="__codelineno-0-155" name="__codelineno-0-155"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-156"><a id="__codelineno-0-156" name="__codelineno-0-156"></a><span class="sd">            if None, use $x=x_{\text{test}}$</span>
</span><span id="__span-0-157"><a id="__codelineno-0-157" name="__codelineno-0-157"></a><span class="sd">        y: optional label tensor to compute gradients</span>
</span><span id="__span-0-158"><a id="__codelineno-0-158" name="__codelineno-0-158"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-159"><a id="__codelineno-0-159" name="__codelineno-0-159"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-160"><a id="__codelineno-0-160" name="__codelineno-0-160"></a>
</span><span id="__span-0-161"><a id="__codelineno-0-161" name="__codelineno-0-161"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-162"><a id="__codelineno-0-162" name="__codelineno-0-162"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-163"><a id="__codelineno-0-163" name="__codelineno-0-163"></a>
</span><span id="__span-0-164"><a id="__codelineno-0-164" name="__codelineno-0-164"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-165"><a id="__codelineno-0-165" name="__codelineno-0-165"></a>    <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">influences</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
</span><span id="__span-0-166"><a id="__codelineno-0-166" name="__codelineno-0-166"></a>    <span class="k">return</span> <span class="n">t</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences_from_factors" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <span class="doc doc-object-name doc-function-name">influences_from_factors</span>


<a href="#pydvl.influence.torch.influence_function_model.NystroemSketchInfluence.influences_from_factors" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="language-python doc-signature highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">x</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">y</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span><span class="p">,</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></span>
</span></code></pre></div>

  <div class="doc doc-contents ">
  
      <p>Computation of</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the case of up-weighting influence, resp.</p>
<div class="arithmatex">\[ \langle z_{\text{test_factors}},
    \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</div>
<p>for the perturbation type influence case. The gradient is meant to be per sample
of the batch <span class="arithmatex">\((x, y)\)</span>. For all input tensors it is assumed,
that the first dimension is the batch dimension (in case, you want to provide
a single sample z, call z.unsqueeze(0) if no batch dimension is present).</p>



  <table>
    <thead>
      <tr>
        <th><b>PARAMETER</b></th>
        <th><b> DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>z_test_factors</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>pre-computed tensor, approximating
<span class="arithmatex">\(H^{-1}\nabla_{\theta} \ell(y_{\text{test}},
f_{\theta}(x_{\text{test}}))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>x</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>model input to use in the gradient computations
<span class="arithmatex">\(\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span>,
resp. <span class="arithmatex">\(\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))\)</span></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>label tensor to compute gradients</p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
                </span>
            </p>
          </td>
        </tr>
        <tr>
          <td><code>mode</code></td>
          <td class="doc-param-details">
            <div class="doc-md-description">
              <p>enum value of <a class="autorefs autorefs-internal" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></p>
            </div>
            <p>
                <span class="doc-param-annotation">
                  <b>TYPE:</b>
                    <code><a class="autorefs autorefs-internal" title="pydvl.influence.base_influence_function_model.InfluenceMode" href="../../base_influence_function_model/#pydvl.influence.base_influence_function_model.InfluenceMode">InfluenceMode</a></code>
                </span>
                <span class="doc-param-default">
                  <b>DEFAULT:</b> 
                    <code><span title="pydvl.influence.base_influence_function_model.InfluenceMode.Up">Up</span></code>
                </span>
            </p>
          </td>
        </tr>
    </tbody>
  </table>



  <table>
    <thead>
      <tr>
        <th><b>RETURNS</b></th>
        <th><b>DESCRIPTION</b></th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
              <span class="doc-returns-annotation">
                  <code><a class="autorefs autorefs-external" title="torch.Tensor" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor">Tensor</a></code>
              </span>
          </td>
          <td class="doc-returns-details">
            <div class="doc-md-description">
              <p>Tensor representing the element-wise scalar products for the provided batch</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>src/pydvl/influence/torch/influence_function_model.py</code></summary>
            <div class="language-python highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span>
<span class="normal"><a href="#__codelineno-0-320">320</a></span>
<span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-276"><a id="__codelineno-0-276" name="__codelineno-0-276"></a><span class="k">def</span> <span class="nf">influences_from_factors</span><span class="p">(</span>
</span><span id="__span-0-277"><a id="__codelineno-0-277" name="__codelineno-0-277"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-0-278"><a id="__codelineno-0-278" name="__codelineno-0-278"></a>    <span class="n">z_test_factors</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-279"><a id="__codelineno-0-279" name="__codelineno-0-279"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-280"><a id="__codelineno-0-280" name="__codelineno-0-280"></a>    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-0-281"><a id="__codelineno-0-281" name="__codelineno-0-281"></a>    <span class="n">mode</span><span class="p">:</span> <span class="n">InfluenceMode</span> <span class="o">=</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">,</span>
</span><span id="__span-0-282"><a id="__codelineno-0-282" name="__codelineno-0-282"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-0-283"><a id="__codelineno-0-283" name="__codelineno-0-283"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-0-284"><a id="__codelineno-0-284" name="__codelineno-0-284"></a><span class="sd">    Computation of</span>
</span><span id="__span-0-285"><a id="__codelineno-0-285" name="__codelineno-0-285"></a>
</span><span id="__span-0-286"><a id="__codelineno-0-286" name="__codelineno-0-286"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-287"><a id="__codelineno-0-287" name="__codelineno-0-287"></a><span class="sd">        \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-288"><a id="__codelineno-0-288" name="__codelineno-0-288"></a>
</span><span id="__span-0-289"><a id="__codelineno-0-289" name="__codelineno-0-289"></a><span class="sd">    for the case of up-weighting influence, resp.</span>
</span><span id="__span-0-290"><a id="__codelineno-0-290" name="__codelineno-0-290"></a>
</span><span id="__span-0-291"><a id="__codelineno-0-291" name="__codelineno-0-291"></a><span class="sd">    \[ \langle z_{\text{test_factors}},</span>
</span><span id="__span-0-292"><a id="__codelineno-0-292" name="__codelineno-0-292"></a><span class="sd">        \nabla_{x} \nabla_{\theta} \ell(y, f_{\theta}(x)) \rangle \]</span>
</span><span id="__span-0-293"><a id="__codelineno-0-293" name="__codelineno-0-293"></a>
</span><span id="__span-0-294"><a id="__codelineno-0-294" name="__codelineno-0-294"></a><span class="sd">    for the perturbation type influence case. The gradient is meant to be per sample</span>
</span><span id="__span-0-295"><a id="__codelineno-0-295" name="__codelineno-0-295"></a><span class="sd">    of the batch $(x, y)$. For all input tensors it is assumed,</span>
</span><span id="__span-0-296"><a id="__codelineno-0-296" name="__codelineno-0-296"></a><span class="sd">    that the first dimension is the batch dimension (in case, you want to provide</span>
</span><span id="__span-0-297"><a id="__codelineno-0-297" name="__codelineno-0-297"></a><span class="sd">    a single sample z, call z.unsqueeze(0) if no batch dimension is present).</span>
</span><span id="__span-0-298"><a id="__codelineno-0-298" name="__codelineno-0-298"></a>
</span><span id="__span-0-299"><a id="__codelineno-0-299" name="__codelineno-0-299"></a><span class="sd">    Args:</span>
</span><span id="__span-0-300"><a id="__codelineno-0-300" name="__codelineno-0-300"></a><span class="sd">        z_test_factors: pre-computed tensor, approximating</span>
</span><span id="__span-0-301"><a id="__codelineno-0-301" name="__codelineno-0-301"></a><span class="sd">            $H^{-1}\nabla_{\theta} \ell(y_{\text{test}},</span>
</span><span id="__span-0-302"><a id="__codelineno-0-302" name="__codelineno-0-302"></a><span class="sd">            f_{\theta}(x_{\text{test}}))$</span>
</span><span id="__span-0-303"><a id="__codelineno-0-303" name="__codelineno-0-303"></a><span class="sd">        x: model input to use in the gradient computations</span>
</span><span id="__span-0-304"><a id="__codelineno-0-304" name="__codelineno-0-304"></a><span class="sd">            $\nabla_{\theta}\ell(y, f_{\theta}(x))$,</span>
</span><span id="__span-0-305"><a id="__codelineno-0-305" name="__codelineno-0-305"></a><span class="sd">            resp. $\nabla_{x}\nabla_{\theta}\ell(y, f_{\theta}(x))$</span>
</span><span id="__span-0-306"><a id="__codelineno-0-306" name="__codelineno-0-306"></a><span class="sd">        y: label tensor to compute gradients</span>
</span><span id="__span-0-307"><a id="__codelineno-0-307" name="__codelineno-0-307"></a><span class="sd">        mode: enum value of [InfluenceMode]</span>
</span><span id="__span-0-308"><a id="__codelineno-0-308" name="__codelineno-0-308"></a><span class="sd">            [pydvl.influence.base_influence_function_model.InfluenceMode]</span>
</span><span id="__span-0-309"><a id="__codelineno-0-309" name="__codelineno-0-309"></a>
</span><span id="__span-0-310"><a id="__codelineno-0-310" name="__codelineno-0-310"></a><span class="sd">    Returns:</span>
</span><span id="__span-0-311"><a id="__codelineno-0-311" name="__codelineno-0-311"></a><span class="sd">        Tensor representing the element-wise scalar products for the provided batch</span>
</span><span id="__span-0-312"><a id="__codelineno-0-312" name="__codelineno-0-312"></a>
</span><span id="__span-0-313"><a id="__codelineno-0-313" name="__codelineno-0-313"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-0-314"><a id="__codelineno-0-314" name="__codelineno-0-314"></a>    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Up</span><span class="p">:</span>
</span><span id="__span-0-315"><a id="__codelineno-0-315" name="__codelineno-0-315"></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="__span-0-316"><a id="__codelineno-0-316" name="__codelineno-0-316"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-317"><a id="__codelineno-0-317" name="__codelineno-0-317"></a>            <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</span><span id="__span-0-318"><a id="__codelineno-0-318" name="__codelineno-0-318"></a>        <span class="p">)</span>
</span><span id="__span-0-319"><a id="__codelineno-0-319" name="__codelineno-0-319"></a>    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">InfluenceMode</span><span class="o">.</span><span class="n">Perturbation</span><span class="p">:</span>
</span><span id="__span-0-320"><a id="__codelineno-0-320" name="__codelineno-0-320"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
</span><span id="__span-0-321"><a id="__codelineno-0-321" name="__codelineno-0-321"></a>            <span class="s2">&quot;ia,j...a-&gt;ij...&quot;</span><span class="p">,</span>
</span><span id="__span-0-322"><a id="__codelineno-0-322" name="__codelineno-0-322"></a>            <span class="n">z_test_factors</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span>
</span><span id="__span-0-323"><a id="__codelineno-0-323" name="__codelineno-0-323"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_flat_loss_mixed_grad</span><span class="p">(</span>
</span><span id="__span-0-324"><a id="__codelineno-0-324" name="__codelineno-0-324"></a>                <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_device</span><span class="p">)</span>
</span><span id="__span-0-325"><a id="__codelineno-0-325" name="__codelineno-0-325"></a>            <span class="p">),</span>
</span><span id="__span-0-326"><a id="__codelineno-0-326" name="__codelineno-0-326"></a>        <span class="p">)</span>
</span><span id="__span-0-327"><a id="__codelineno-0-327" name="__codelineno-0-327"></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="__span-0-328"><a id="__codelineno-0-328" name="__codelineno-0-328"></a>        <span class="k">raise</span> <span class="n">UnsupportedInfluenceModeException</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>




  </div>

  </div>

</div>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1-2.1-2M12.5 7v5.2l4 2.4-1 1L11 13V7h1.5M11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2v1.8Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2024-05-07</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3h-2Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2024-05-07</span>
  </span>

    
    
    
  </aside>


  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../functional/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Functional">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Functional
              </div>
            </div>
          </a>
        
        
          
          <a href="../pre_conditioner/" class="md-footer__link md-footer__link--next" aria-label="Next: Pre conditioner">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Pre conditioner
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
        
        <a href="https://appliedai-institute.de">
            Copyright &copy; AppliedAI Institute gGmbH
        </a>
        
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/aai-institute/pyDVL" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://pypi.org/project/pyDVL/" target="_blank" rel="noopener" title="pypi.org" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/aai_transferlab" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://de.linkedin.com/company/appliedai-institute-for-europe-ggmbh" target="_blank" rel="noopener" title="de.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["content.code.annotate", "content.code.copy", "navigation.footer", "content.tooltips", "navigation.instant", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.suggest", "search.highlight", "toc.follow"], "search": "../../../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": "stable", "provider": "mike"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.bd41221c.min.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  <script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body>
</html>
@article{agarwal_secondorder_2017,
  title = {Second-{{Order Stochastic Optimization}} for {{Machine Learning}} in {{Linear Time}}},
  author = {Agarwal, Naman and Bullins, Brian and Hazan, Elad},
  date = {2017},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {JMLR},
  volume = {18},
  eprint = {1602.03943},
  eprinttype = {arxiv},
  pages = {1--40},
  url = {https://www.jmlr.org/papers/v18/16-491.html},
  abstract = {First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improve upon the overall running time over popular first-order methods. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data.},
  langid = {english},
  keywords = {notion}
}

@inproceedings{bae_if_2022,
  title = {If {{Influence Functions}} Are the {{Answer}}, {{Then What}} Is the {{Question}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bae, Juhan and Ng, Nathan and Lo, Alston and Ghassemi, Marzyeh and Grosse, Roger B.},
  date = {2022-12-06},
  volume = {35},
  pages = {17953--17967},
  location = {New Orleans, LA, USA},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/7234e0c36fdbcb23e7bd56b68838999b-Abstract-Conference.html},
  urldate = {2023-11-27},
  abstract = {Influence functions efficiently estimate the effect of removing a single training data point on a model's learned parameters. While influence estimates align well with leave-one-out retraining for linear models, recent works have shown this alignment is often poor in neural networks. In this work, we investigate the specific factors that cause this discrepancy by decomposing it into five separate terms. We study the contributions of each term on a variety of architectures and datasets and how they vary with factors such as network width and training time. While practical influence function estimates may be a poor match to leave-one-out retraining for nonlinear networks, we show that they are often a good approximation to a different object we term the proximal Bregman response function (PBRF). Since the PBRF can still be used to answer many of the questions motivating influence functions, such as identifying influential or mislabeled examples, our results suggest that current algorithms for influence function estimation give more informative results than previous error analyses would suggest.},
  eventtitle = {{{NeurIPS}} 2022},
  langid = {english}
}

@article{bekas_estimator_2007,
  title = {An Estimator for the Diagonal of a Matrix},
  author = {Bekas, C. and Kokiopoulou, E. and Saad, Y.},
  date = {2007-11-01},
  journaltitle = {Applied Numerical Mathematics},
  shortjournal = {Applied Numerical Mathematics},
  series = {Numerical {{Algorithms}}, {{Parallelism}} and {{Applications}} (2)},
  volume = {57},
  number = {11},
  pages = {1214--1229},
  issn = {0168-9274},
  doi = {10.1016/j.apnum.2007.01.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0168927407000244},
  urldate = {2024-03-19},
  abstract = {A number of applications require to compute an approximation of the diagonal of a matrix when this matrix is not explicitly available but matrix–vector products with it are easy to evaluate. In some cases, it is the trace of the matrix rather than the diagonal that is needed. This paper describes methods for estimating diagonals and traces of matrices in these situations. The goal is to obtain a good estimate of the diagonal by applying only a small number of matrix–vector products, using selected vectors. We begin by considering the use of random test vectors and then explore special vectors obtained from Hadamard matrices. The methods are tested in the context of computational materials science to estimate the diagonal of the density matrix which holds the charge densities. Numerical experiments indicate that the diagonal estimator may offer an alternative method that in some cases can greatly reduce computational costs in electronic structures calculations.}
}

@article{benmerzoug_re_2023,
  title = {[{{Re}}] {{If}} You like {{Shapley}}, Then You'll Love the Core},
  author = {Benmerzoug, Anes and de Benito Delgado, Miguel},
  date = {2023-07-31},
  journaltitle = {ReScience C},
  volume = {9},
  number = {2},
  pages = {\#32},
  doi = {10.5281/zenodo.8173733},
  url = {https://zenodo.org/record/8173733},
  urldate = {2023-08-27},
  abstract = {We investigate the results of [1] in the field of data valuation. We repeat their experiments and conclude that the (Monte Carlo) Least Core is sensitive to important characteristics of the ML problem of interest, making it difficult to apply.},
  keywords = {notion}
}

@unpublished{broderick_automatic_2021,
  title = {An {{Automatic Finite-Sample Robustness Metric}}: {{When Can Dropping}} a {{Little Data Make}} a {{Big Difference}}?},
  shorttitle = {An {{Automatic Finite-Sample Robustness Metric}}},
  author = {Broderick, Tamara and Giordano, Ryan and Meager, Rachael},
  date = {2021-11-03},
  eprint = {2011.14999},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/2011.14999},
  abstract = {We propose a method to assess the sensitivity of econometric analyses to the removal of a small fraction of the data. Manually checking the influence of all possible small subsets is computationally infeasible, so we provide an approximation to find the most influential subset. Our metric, the "Approximate Maximum Influence Perturbation," is automatically computable for common methods including (but not limited to) OLS, IV, MLE, GMM, and variational Bayes. We provide finite-sample error bounds on approximation performance. At minimal extra cost, we provide an exact finite-sample lower bound on sensitivity. We find that sensitivity is driven by a signal-to-noise ratio in the inference problem, is not reflected in standard errors, does not disappear asymptotically, and is not due to misspecification. While some empirical applications are robust, results of several economics papers can be overturned by removing less than 1\% of the sample.},
  langid = {english},
  keywords = {notion},
  annotation = {video: https://youtu.be/w8OX0lK1CKo}
}

@article{castro_polynomial_2009,
  title = {Polynomial Calculation of the {{Shapley}} Value Based on Sampling},
  author = {Castro, Javier and Gómez, Daniel and Tejada, Juan},
  date = {2009-05-01},
  journaltitle = {Computers \& Operations Research},
  shortjournal = {Computers \& Operations Research},
  series = {Selected Papers Presented at the {{Tenth International Symposium}} on {{Locational Decisions}} ({{ISOLDE X}})},
  volume = {36},
  number = {5},
  pages = {1726--1730},
  issn = {0305-0548},
  doi = {10.1016/j.cor.2008.04.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0305054808000804},
  urldate = {2020-11-21},
  abstract = {In this paper we develop a polynomial method based on sampling theory that can be used to estimate the Shapley value (or any semivalue) for cooperative games. Besides analyzing the complexity problem, we examine some desirable statistical properties of the proposed approach and provide some computational results.},
  langid = {english},
  keywords = {notion}
}

@article{frangella_randomized_2023,
  title = {Randomized {{Nyström Preconditioning}}},
  author = {Frangella, Zachary and Tropp, Joel A. and Udell, Madeleine},
  date = {2023-06-30},
  journaltitle = {SIAM Journal on Matrix Analysis and Applications},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  volume = {44},
  number = {2},
  pages = {718--752},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0895-4798},
  doi = {10.1137/21M1466244},
  url = {https://epubs.siam.org/doi/abs/10.1137/21M1466244},
  urldate = {2024-03-12},
  abstract = {Randomized methods are becoming increasingly popular in numerical linear algebra. However, few attempts have been made to use them in developing preconditioners. Our interest lies in solving large-scale sparse symmetric positive definite linear systems of equations, where the system matrix is preordered to doubly bordered block diagonal form (for example, using a nested dissection ordering). We investigate the use of randomized methods to construct high-quality preconditioners. In particular, we propose a new and efficient approach that employs Nyström's method for computing low rank approximations to develop robust algebraic two-level preconditioners. Construction of the new preconditioners involves iteratively solving a smaller but denser symmetric positive definite Schur complement system with multiple right-hand sides. Numerical experiments on problems coming from a range of application areas demonstrate that this inner system can be solved cheaply using block conjugate gradients and that using a large convergence tolerance to limit the cost does not adversely affect the quality of the resulting Nyström--Schur two-level preconditioner.}
}

@inproceedings{george_fast_2018,
  title = {Fast {{Approximate Natural Gradient Descent}} in a {{Kronecker Factored Eigenbasis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {George, Thomas and Laurent, César and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal},
  date = {2018},
  volume = {31},
  eprint = {1806.03884},
  eprinttype = {arxiv},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2018/hash/48000647b315f6f00f913caa757a70b3-Abstract.html},
  urldate = {2024-01-12},
  abstract = {Optimization algorithms that leverage gradient covariance information, such as variants of natural gradient descent (Amari, 1998), offer the prospect of yielding more effective descent directions. For models with many parameters, the covari- ance matrix they are based on becomes gigantic, making them inapplicable in their original form. This has motivated research into both simple diagonal approxima- tions and more sophisticated factored approximations such as KFAC (Heskes, 2000; Martens \& Grosse, 2015; Grosse \& Martens, 2016). In the present work we draw inspiration from both to propose a novel approximation that is provably better than KFAC and amendable to cheap partial updates. It consists in tracking a diagonal variance, not in parameter coordinates, but in a Kronecker-factored eigenbasis, in which the diagonal approximation is likely to be more effective. Experiments show improvements over KFAC in optimization speed for several deep network architectures.},
  keywords = {notion}
}

@inproceedings{ghorbani_data_2019,
  title = {Data {{Shapley}}: {{Equitable Valuation}} of {{Data}} for {{Machine Learning}}},
  shorttitle = {Data {{Shapley}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}, {{PMLR}}},
  author = {Ghorbani, Amirata and Zou, James},
  date = {2019-05-24},
  eprint = {1904.02868},
  eprinttype = {arxiv},
  pages = {2242--2251},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/ghorbani19c.html},
  urldate = {2020-11-01},
  abstract = {As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on n data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.},
  eventtitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}} 2019)},
  langid = {english},
  keywords = {notion}
}

@article{hampel_influence_1974,
  title = {The {{Influence Curve}} and {{Its Role}} in {{Robust Estimation}}},
  author = {Hampel, Frank R.},
  date = {1974},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {J. Am. Stat. Assoc.},
  volume = {69},
  number = {346},
  eprint = {2285666},
  eprinttype = {jstor},
  pages = {383--393},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2285666},
  url = {https://www.jstor.org/stable/2285666},
  urldate = {2022-05-09},
  abstract = {This paper treats essentially the first derivative of an estimator viewed as functional and the ways in which it can be used to study local robustness properties. A theory of robust estimation "near" strict parametric models is briefly sketched and applied to some classical situations. Relations between von Mises functionals, the jackknife and U-statistics are indicated. A number of classical and new estimators are discussed, including trimmed and Winsorized means, Huber-estimators, and more generally maximum likelihood and M-estimators. Finally, a table with some numerical robustness properties is given.}
}

@inproceedings{hataya_nystrom_2023,
  title = {Nyström {{Method}} for {{Accurate}} and {{Scalable Implicit Differentiation}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Hataya, Ryuichiro and Yamada, Makoto},
  date = {2023-04-11},
  pages = {4643--4654},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v206/hataya23a.html},
  urldate = {2024-02-26},
  abstract = {The essential difficulty of gradient-based bilevel optimization using implicit differentiation is to estimate the inverse Hessian vector product with respect to neural network parameters. This paper proposes to tackle this problem by the Nyström method and the Woodbury matrix identity, exploiting the low-rankness of the Hessian. Compared to existing methods using iterative approximation, such as conjugate gradient and the Neumann series approximation, the proposed method avoids numerical instability and can be efficiently computed in matrix operations without iterations. As a result, the proposed method works stably in various tasks and is faster than iterative approximations. Throughout experiments including large-scale hyperparameter optimization and meta learning, we demonstrate that the Nyström method consistently achieves comparable or even superior performance to other approaches. The source code is available from https://github.com/moskomule/hypergrad.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {notion}
}

@article{ji_breakdownfree_2017,
  title = {A Breakdown-Free Block Conjugate Gradient Method},
  author = {Ji, Hao and Li, Yaohang},
  date = {2017-06},
  journaltitle = {BIT Numerical Mathematics},
  shortjournal = {Bit Numer Math},
  volume = {57},
  number = {2},
  pages = {379--403},
  issn = {0006-3835, 1572-9125},
  doi = {10.1007/s10543-016-0631-z},
  url = {https://link.springer.com/10.1007/s10543-016-0631-z},
  urldate = {2024-02-28},
  abstract = {In this paper, we analyze all possible situations of rank deficiency that cause breakdown in block conjugate gradient (BCG) solvers. A simple solution, breakdownfree block conjugate gradient (BFBCG), is designed to address the rank deficiency problem. The rationale of the BFBCG algorithm is to derive new forms of parameter matrices based on the potentially reduced search subspace to handle rank deficiency. Orthogonality properties and convergence of BFBCG in case of rank deficiency are justified accordingly with mathematical rigor. BFBCG yields faster convergence than restarting BCG when breakdown occurs. Numerical examples suffering from rank deficiency are provided to demonstrate the robustness of BFBCG.},
  langid = {english}
}

@inproceedings{jia_efficient_2019,
  title = {Towards {{Efficient Data Valuation Based}} on the {{Shapley Value}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Hynes, Nick and Gürel, Nezihe Merve and Li, Bo and Zhang, Ce and Song, Dawn and Spanos, Costas J.},
  date = {2019-04-11},
  pages = {1167--1176},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v89/jia19a.html},
  urldate = {2021-02-12},
  abstract = {“How much is my data worth?” is an increasingly common question posed by organizations and individuals alike. An answer to this question could allow, for instance, fairly distributing profits...},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  langid = {english},
  keywords = {notion}
}

@article{jia_efficient_2019a,
  title = {Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms},
  shorttitle = {{{VLDB}} 2019},
  author = {Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Gurel, Nezihe Merve and Li, Bo and Zhang, Ce and Spanos, Costas and Song, Dawn},
  date = {2019-07-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {12},
  number = {11},
  pages = {1610--1623},
  issn = {2150-8097},
  doi = {10.14778/3342263.3342637},
  url = {https://doi.org/10.14778/3342263.3342637},
  urldate = {2021-02-12},
  abstract = {Given a data set D containing millions of data points and a data consumer who is willing to pay \textbackslash\$X to train a machine learning (ML) model over D, how should we distribute this \textbackslash\$X to each data point to reflect its "value"? In this paper, we define the "relative value of data" via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all N data points, it requires O(2N) model evaluations for exact computation and O(N log N) for (ϵ, δ)-approximation. In this paper, we focus on one popular family of ML models relying on K-nearest neighbors (KNN). The most surprising result is that for unweighted KNN classifiers and regressors, the Shapley value of all N data points can be computed, exactly, in O(N log N) time - an exponential improvement on computational complexity! Moreover, for (ϵ, δ)-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity O(Nh(ϵ, K) log N) when ϵ is not too small and K is not too large. We empirically evaluate our algorithms on up to 10 million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further. We then extend our algorithm to other scenarios such as (1) weighed KNN classifiers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing computation who also requires proper valuation. Some of these extensions, although also being improved exponentially, are less practical for exact computation (e.g., O(NK) complexity for weigthed KNN). We thus propose an Monte Carlo approximation algorithm, which is O(N(log N)2/(log K)2) times more efficient than the baseline approximation algorithm.},
  langid = {english},
  keywords = {notion}
}

@inproceedings{just_lava_2023,
  title = {{{LAVA}}: {{Data Valuation}} without {{Pre-Specified Learning Algorithms}}},
  shorttitle = {{{LAVA}}},
  author = {Just, Hoang Anh and Kang, Feiyang and Wang, Tianhao and Zeng, Yi and Ko, Myeongseob and Jin, Ming and Jia, Ruoxi},
  date = {2023-02-01},
  url = {https://openreview.net/forum?id=JJuP86nBl4q},
  urldate = {2023-04-25},
  abstract = {Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden. This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning algorithm. Our main results are as follows. \$\textbackslash textbf\{(1)\}\$ We develop a proxy for the validation performance associated with a training set based on a non-conventional \$\textbackslash textit\{class-wise\}\$ \$\textbackslash textit\{Wasserstein distance\}\$ between the training and the validation set. We show that the distance characterizes the upper bound of the validation performance for any given model under certain Lipschitz conditions. \$\textbackslash textbf\{(2)\}\$ We develop a novel method to value individual data based on the sensitivity analysis of the \$\textbackslash textit\{class-wise\}\$ Wasserstein distance. Importantly, these values can be directly obtained \$\textbackslash textit\{for free\}\$ from the output of off-the-shelf optimization solvers once the Wasserstein distance is computed. \$\textbackslash textbf\{(3) \}\$We evaluate our new data valuation framework over various use cases related to detecting low-quality data and show that, surprisingly, the learning-agnostic feature of our framework enables a \$\textbackslash textit\{significant improvement\}\$ over the state-of-the-art performance while being \$\textbackslash textit\{orders of magnitude faster.\}\$},
  eventtitle = {The {{Eleventh International Conference}} on {{Learning Representations}} ({{ICLR}} 2023)},
  langid = {english},
  keywords = {notion}
}

@inproceedings{koh_understanding_2017,
  title = {Understanding {{Black-box Predictions}} via {{Influence Functions}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Koh, Pang Wei and Liang, Percy},
  date = {2017-07-17},
  eprint = {1703.04730},
  eprinttype = {arxiv},
  pages = {1885--1894},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v70/koh17a.html},
  urldate = {2022-05-09},
  abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {notion}
}

@inproceedings{kong_resolving_2022,
  title = {Resolving {{Training Biases}} via {{Influence-based Data Relabeling}}},
  author = {Kong, Shuming and Shen, Yanyan and Huang, Linpeng},
  date = {2022},
  url = {https://openreview.net/forum?id=EskfH0bwNVn},
  urldate = {2022-05-03},
  abstract = {The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a  technique that...},
  eventtitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  langid = {english},
  keywords = {notion},
  annotation = {video:https://iclr.cc/virtual/2022/oral/6492}
}

@inproceedings{kwon_beta_2022,
  title = {Beta {{Shapley}}: A {{Unified}} and {{Noise-reduced Data Valuation Framework}} for {{Machine Learning}}},
  shorttitle = {Beta {{Shapley}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}}) 2022,},
  author = {Kwon, Yongchan and Zou, James},
  date = {2022-01-18},
  volume = {151},
  eprint = {2110.14049},
  eprinttype = {arxiv},
  publisher = {PMLR},
  location = {Valencia, Spain},
  url = {https://arxiv.org/abs/2110.14049},
  urldate = {2022-04-06},
  abstract = {Data Shapley has recently been proposed as a principled framework to quantify the contribution of individual datum in machine learning. It can effectively identify helpful or harmful data points for a learning algorithm. In this paper, we propose Beta Shapley, which is a substantial generalization of Data Shapley. Beta Shapley arises naturally by relaxing the efficiency axiom of the Shapley value, which is not critical for machine learning settings. Beta Shapley unifies several popular data valuation methods and includes data Shapley as a special case. Moreover, we prove that Beta Shapley has several desirable statistical properties and propose efficient algorithms to estimate it. We demonstrate that Beta Shapley outperforms state-of-the-art data valuation methods on several downstream ML tasks such as: 1) detecting mislabeled training data; 2) learning with subsamples; and 3) identifying points whose addition or removal have the largest positive or negative impact on the model.},
  eventtitle = {{{AISTATS}} 2022},
  langid = {english},
  keywords = {notion}
}

@inproceedings{kwon_datainf_2023,
  title = {{{DataInf}}: {{Efficiently Estimating Data Influence}} in {{LoRA-tuned LLMs}} and {{Diffusion Models}}},
  shorttitle = {{{DataInf}}},
  author = {Kwon, Yongchan and Wu, Eric and Wu, Kevin and Zou, James},
  date = {2023-10-13},
  doi = {10.48550/arXiv.2310.00902},
  url = {https://openreview.net/forum?id=9m02ib92Wz},
  urldate = {2023-10-27},
  abstract = {Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods. In applications to RoBERTa-large, Llama-2-13B-chat, and stable-diffusion-v1.5 models, DataInf effectively identifies the most influential fine-tuning examples better than other approximate influence scores. Moreover, it can help to identify which data points are mislabeled.},
  eventtitle = {The {{Twelfth International Conference}} on {{Learning Representations}}}
}

@inproceedings{kwon_dataoob_2023,
  title = {Data-{{OOB}}: {{Out-of-bag Estimate}} as a {{Simple}} and {{Efficient Data Value}}},
  shorttitle = {Data-{{OOB}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Kwon, Yongchan and Zou, James},
  date = {2023-07-03},
  pages = {18135--18152},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/kwon23e.html},
  urldate = {2023-09-06},
  abstract = {Data valuation is a powerful framework for providing statistical insights into which data are beneficial or detrimental to model training. Many Shapley-based data valuation methods have shown promising results in various downstream tasks, however, they are well known to be computationally challenging as it requires training a large number of models. As a result, it has been recognized as infeasible to apply to large datasets. To address this issue, we propose Data-OOB, a new data valuation method for a bagging model that utilizes the out-of-bag estimate. The proposed method is computationally efficient and can scale to millions of data by reusing trained weak learners. Specifically, Data-OOB takes less than 2.25 hours on a single CPU processor when there are \$10\textasciicircum 6\$ samples to evaluate and the input dimension is 100. Furthermore, Data-OOB has solid theoretical interpretations in that it identifies the same important data point as the infinitesimal jackknife influence function when two different points are compared. We conduct comprehensive experiments using 12 classification datasets, each with thousands of sample sizes. We demonstrate that the proposed method significantly outperforms existing state-of-the-art data valuation methods in identifying mislabeled data and finding a set of helpful (or harmful) data points, highlighting the potential for applying data values in real-world applications.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {notion}
}

@inproceedings{kwon_efficient_2021,
  title = {Efficient {{Computation}} and {{Analysis}} of {{Distributional Shapley Values}}},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Kwon, Yongchan and Rivas, Manuel A. and Zou, James},
  date = {2021-03-18},
  eprint = {2007.01357},
  eprinttype = {arxiv},
  pages = {793--801},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v130/kwon21a.html},
  urldate = {2021-04-23},
  abstract = {Distributional data Shapley value (DShapley) has recently been proposed as a principled framework to quantify the contribution of individual datum in machine learning. DShapley develops the founda...},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english}
}

@inproceedings{li_achieving_2022,
  title = {Achieving {{Fairness}} at {{No Utility Cost}} via {{Data Reweighing}} with {{Influence}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Peizhao and Liu, Hongfu},
  date = {2022-06-28},
  pages = {12917--12930},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/li22p.html},
  urldate = {2024-03-20},
  abstract = {With the fast development of algorithmic governance, fairness has become a compulsory property for machine learning models to suppress unintentional discrimination. In this paper, we focus on the pre-processing aspect for achieving fairness, and propose a data reweighing approach that only adjusts the weight for samples in the training phase. Different from most previous reweighing methods which usually assign a uniform weight for each (sub)group, we granularly model the influence of each training sample with regard to fairness-related quantity and predictive utility, and compute individual weights based on influence under the constraints from both fairness and utility. Experimental results reveal that previous methods achieve fairness at a non-negligible cost of utility, while as a significant advantage, our approach can empirically release the tradeoff and obtain cost-free fairness for equal opportunity. We demonstrate the cost-free fairness through vanilla classifiers and standard training processes, compared to baseline methods on multiple real-world tabular datasets. Code available at https://github.com/brandeis-machine-learning/influence-fairness.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{maleki_bounding_2014,
  title = {Bounding the {{Estimation Error}} of {{Sampling-based Shapley Value Approximation}}},
  author = {Maleki, Sasan and Tran-Thanh, Long and Hines, Greg and Rahwan, Talal and Rogers, Alex},
  date = {2014-02-12},
  journaltitle = {ArXiv13064265 Cs},
  eprint = {1306.4265},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {https://arxiv.org/abs/1306.4265},
  urldate = {2020-11-16},
  abstract = {The Shapley value is arguably the most central normative solution concept in cooperative game theory. It specifies a unique way in which the reward from cooperation can be "fairly" divided among players. While it has a wide range of real world applications, its use is in many cases hampered by the hardness of its computation. A number of researchers have tackled this problem by (i) focusing on classes of games where the Shapley value can be computed efficiently, or (ii) proposing representation formalisms that facilitate such efficient computation, or (iii) approximating the Shapley value in certain classes of games. For the classical \textbackslash textit\{characteristic function\} representation, the only attempt to approximate the Shapley value for the general class of games is due to Castro \textbackslash textit\{et al.\} \textbackslash cite\{castro\}. While this algorithm provides a bound on the approximation error, this bound is \textbackslash textit\{asymptotic\}, meaning that it only holds when the number of samples increases to infinity. On the other hand, when a finite number of samples is drawn, an unquantifiable error is introduced, meaning that the bound no longer holds. With this in mind, we provide non-asymptotic bounds on the estimation error for two cases: where (i) the \textbackslash textit\{variance\}, and (ii) the \textbackslash textit\{range\}, of the players' marginal contributions is known. Furthermore, for the second case, we show that when the range is significantly large relative to the Shapley value, the bound can be improved (from \$O(\textbackslash frac\{r\}\{m\})\$ to \$O(\textbackslash sqrt\{\textbackslash frac\{r\}\{m\}\})\$). Finally, we propose, and demonstrate the effectiveness of using stratified sampling for improving the bounds further.}
}

@inproceedings{martens_optimizing_2015,
  title = {Optimizing {{Neural Networks}} with {{Kronecker-factored Approximate Curvature}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Martens, James and Grosse, Roger},
  date = {2015-06-01},
  pages = {2408--2417},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/martens15.html},
  urldate = {2022-11-26},
  abstract = {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network’s Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC’s approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@article{mitchell_sampling_2022,
  title = {Sampling {{Permutations}} for {{Shapley Value Estimation}}},
  author = {Mitchell, Rory and Cooper, Joshua and Frank, Eibe and Holmes, Geoffrey},
  date = {2022},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {23},
  number = {43},
  pages = {1--46},
  issn = {1533-7928},
  url = {https://jmlr.org/papers/v23/21-0439.html},
  urldate = {2022-10-23},
  abstract = {Game-theoretic attribution techniques based on Shapley values are used to interpret black-box machine learning models, but their exact calculation is generally NP-hard, requiring approximation methods for non-trivial models. As the computation of Shapley values can be expressed as a summation over a set of permutations, a common approach is to sample a subset of these permutations for approximation. Unfortunately, standard Monte Carlo sampling methods can exhibit slow convergence, and more sophisticated quasi-Monte Carlo methods have not yet been applied to the space of permutations. To address this, we investigate new approaches based on two classes of approximation methods and compare them empirically. First, we demonstrate quadrature techniques in a RKHS containing functions of permutations, using the Mallows kernel in combination with kernel herding and sequential Bayesian quadrature. The RKHS perspective also leads to quasi-Monte Carlo type error bounds, with a tractable discrepancy measure defined on permutations. Second, we exploit connections between the hypersphere  S d−2 Sd−2  and permutations to create practical algorithms for generating permutation samples with good properties. Experiments show the above techniques provide significant improvements for Shapley value estimates over existing methods, converging to a smaller RMSE in the same number of model evaluations.}
}

@inproceedings{okhrati_multilinear_2021,
  title = {A {{Multilinear Sampling Algorithm}} to {{Estimate Shapley Values}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Okhrati, Ramin and Lipani, Aldo},
  date = {2021-01},
  eprint = {2010.12082},
  eprinttype = {arxiv},
  pages = {7992--7999},
  publisher = {IEEE},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9412511},
  url = {https://ieeexplore.ieee.org/abstract/document/9412511},
  abstract = {Shapley values are great analytical tools in game theory to measure the importance of a player in a game. Due to their axiomatic and desirable properties such as efficiency, they have become popular for feature importance analysis in data science and machine learning. However, the time complexity to compute Shapley values based on the original formula is exponential, and as the number of features increases, this becomes infeasible. Castro et al. [1] developed a sampling algorithm, to estimate Shapley values. In this work, we propose a new sampling method based on a multilinear extension technique as applied in game theory. The aim is to provide a more efficient (sampling) method for estimating Shapley values. Our method is applicable to any machine learning model, in particular for either multiclass classifications or regression problems. We apply the method to estimate Shapley values for multilayer perceptrons (MLPs) and through experimentation on two datasets, we demonstrate that our method provides more accurate estimations of the Shapley values by reducing the variance of the sampling statistics.},
  eventtitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  langid = {english},
  keywords = {notion}
}

@article{schioppa_scaling_2022,
  title = {Scaling {{Up Influence Functions}}},
  author = {Schioppa, Andrea and Zablotskaia, Polina and Vilar, David and Sokolov, Artem},
  date = {2022-06-28},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {Proc. AAAI Conf. Artif. Intell.},
  volume = {36},
  number = {8},
  eprint = {2112.03052},
  eprinttype = {arxiv},
  pages = {8179--8186},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i8.20791},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/20791},
  urldate = {2024-03-30},
  abstract = {We address efficient calculation of influence functions for tracking predictions back to the training data. We propose and analyze a new approach to speeding up the inverse Hessian calculation based on Arnoldi iteration. With this improvement, we achieve, to the best of our knowledge, the first successful implementation of influence functions that scales to full-size (language and vision) Transformer models with several hundreds of millions of parameters. We evaluate our approach in image classification and sequence-to-sequence tasks with tens to a hundred of millions of training examples. Our code is available at https://github.com/google-research/jax-influence.},
  langid = {english},
  keywords = {notion}
}

@inproceedings{schoch_csshapley_2022,
  title = {{{CS-Shapley}}: {{Class-wise Shapley Values}} for {{Data Valuation}} in {{Classification}}},
  shorttitle = {{{CS-Shapley}}},
  booktitle = {Proc. of the Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Schoch, Stephanie and Xu, Haifeng and Ji, Yangfeng},
  date = {2022-10-31},
  location = {New Orleans, Louisiana, USA},
  url = {https://openreview.net/forum?id=KTOcrOR5mQ9},
  urldate = {2022-11-23},
  abstract = {Data valuation, or the valuation of individual datum contributions, has seen growing interest in machine learning due to its demonstrable efficacy for tasks such as noisy label detection. In particular, due to the desirable axiomatic properties, several Shapley value approximations have been proposed. In these methods, the value function is usually defined as the predictive accuracy over the entire development set. However, this limits the ability to differentiate between training instances that are helpful or harmful to their own classes. Intuitively, instances that harm their own classes may be noisy or mislabeled and should receive a lower valuation than helpful instances. In this work, we propose CS-Shapley, a Shapley value with a new value function that discriminates between training instances’ in-class and out-of-class contributions. Our theoretical analysis shows the proposed value function is (essentially) the unique function that satisfies two desirable properties for evaluating data values in classification. Further, our experiments on two benchmark evaluation tasks (data removal and noisy label detection) and four classifiers demonstrate the effectiveness of CS-Shapley over existing methods. Lastly, we evaluate the “transferability” of data values estimated from one classifier to others, and our results suggest Shapley-based data valuation is transferable for application across different models.},
  eventtitle = {Advances in {{Neural Information Processing Systems}}  ({{NeurIPS}} 2022)},
  langid = {english},
  keywords = {notion}
}

@book{trefethen_numerical_1997,
  title = {Numerical {{Linear Algebra}}},
  author = {Trefethen, Lloyd N. and Bau, Iii, David},
  date = {1997-01},
  publisher = {{Society for Industrial and Applied Mathematics}},
  location = {Philadelphia, PA},
  doi = {10.1137/1.9780898719574},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9780898719574},
  urldate = {2024-03-19},
  isbn = {978-0-89871-361-9 978-0-89871-957-4},
  langid = {english}
}

@inproceedings{wang_data_2023,
  title = {Data {{Banzhaf}}: {{A Robust Data Valuation Framework}} for {{Machine Learning}}},
  shorttitle = {Data {{Banzhaf}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Wang, Jiachen T. and Jia, Ruoxi},
  date = {2023-04-11},
  pages = {6388--6421},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v206/wang23e.html},
  urldate = {2024-02-15},
  abstract = {Data valuation has wide use cases in machine learning, including improving data quality and creating economic incentives for data sharing. This paper studies the robustness of data valuation to noisy model performance scores. Particularly, we find that the inherent randomness of the widely used stochastic gradient descent can cause existing data value notions (e.g., the Shapley value and the Leave-one-out error) to produce inconsistent data value rankings across different runs. To address this challenge, we introduce the concept of safety margin, which measures the robustness of a data value notion. We show that the Banzhaf value, a famous value notion that originated from cooperative game theory literature, achieves the largest safety margin among all semivalues (a class of value notions that satisfy crucial properties entailed by ML applications and include the famous Shapley value and Leave-one-out error). We propose an algorithm to efficiently estimate the Banzhaf value based on the Maximum Sample Reuse (MSR) principle. Our evaluation demonstrates that the Banzhaf value outperforms the existing semivalue-based data value notions on several ML tasks such as learning with weighted samples and noisy label detection. Overall, our study suggests that when the underlying ML algorithm is stochastic, the Banzhaf value is a promising alternative to the other semivalue-based data value schemes given its computational advantage and ability to robustly differentiate data quality.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english}
}

@inproceedings{wang_improving_2022,
  title = {Improving {{Cooperative Game Theory-based Data Valuation}} via {{Data Utility Learning}}},
  author = {Wang, Tianhao and Yang, Yu and Jia, Ruoxi},
  date = {2022-04-07},
  eprint = {2107.06336v2},
  eprinttype = {arxiv},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.06336},
  url = {https://arxiv.org/abs/2107.06336v2},
  urldate = {2022-05-19},
  abstract = {The Shapley value (SV) and Least core (LC) are classic methods in cooperative game theory for cost/profit sharing problems. Both methods have recently been proposed as a principled solution for data valuation tasks, i.e., quantifying the contribution of individual datum in machine learning. However, both SV and LC suffer computational challenges due to the need for retraining models on combinatorially many data subsets. In this work, we propose to boost the efficiency in computing Shapley value or Least core by learning to estimate the performance of a learning algorithm on unseen data combinations. Theoretically, we derive bounds relating the error in the predicted learning performance to the approximation error in SV and LC. Empirically, we show that the proposed method can significantly improve the accuracy of SV and LC estimation.},
  eventtitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022). {{Workshop}} on {{Socially Responsible Machine Learning}}},
  langid = {english},
  keywords = {notion}
}

@online{watson_accelerated_2023,
  title = {Accelerated {{Shapley Value Approximation}} for {{Data Evaluation}}},
  author = {Watson, Lauren and Kujawa, Zeno and Andreeva, Rayna and Yang, Hao-Tsung and Elahi, Tariq and Sarkar, Rik},
  date = {2023-11-09},
  eprint = {2311.05346},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.05346},
  url = {https://arxiv.org/abs/2311.05346},
  urldate = {2023-12-07},
  abstract = {Data valuation has found various applications in machine learning, such as data filtering, efficient learning and incentives for data sharing. The most popular current approach to data valuation is the Shapley value. While popular for its various applications, Shapley value is computationally expensive even to approximate, as it requires repeated iterations of training models on different subsets of data. In this paper we show that the Shapley value of data points can be approximated more efficiently by leveraging the structural properties of machine learning problems. We derive convergence guarantees on the accuracy of the approximate Shapley value for different learning settings including Stochastic Gradient Descent with convex and non-convex loss functions. Our analysis suggests that in fact models trained on small subsets are more important in the context of data valuation. Based on this idea, we describe \$\textbackslash delta\$-Shapley -- a strategy of only using small subsets for the approximation. Experiments show that this approach preserves approximate value and rank of data, while achieving speedup of up to 9.9x. In pre-trained networks the approach is found to bring more efficiency in terms of accurate evaluation using small subsets.},
  pubstate = {preprint}
}

@inproceedings{wu_davinz_2022,
  title = {{{DAVINZ}}: {{Data Valuation}} Using {{Deep Neural Networks}} at {{Initialization}}},
  shorttitle = {{{DAVINZ}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Wu, Zhaoxuan and Shu, Yao and Low, Bryan Kian Hsiang},
  date = {2022-06-28},
  pages = {24150--24176},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v162/wu22j.html},
  urldate = {2022-10-29},
  abstract = {Recent years have witnessed a surge of interest in developing trustworthy methods to evaluate the value of data in many real-world applications (e.g., collaborative machine learning, data marketplaces). Existing data valuation methods typically valuate data using the generalization performance of converged machine learning models after their long-term model training, hence making data valuation on large complex deep neural networks (DNNs) unaffordable. To this end, we theoretically derive a domain-aware generalization bound to estimate the generalization performance of DNNs without model training. We then exploit this theoretically derived generalization bound to develop a novel training-free data valuation method named data valuation at initialization (DAVINZ) on DNNs, which consistently achieves remarkable effectiveness and efficiency in practice. Moreover, our training-free DAVINZ, surprisingly, can even theoretically and empirically enjoy the desirable properties that training-based data valuation methods usually attain, thus making it more trustworthy in practice.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {notion}
}

@inproceedings{yan_if_2021,
  title = {If {{You Like Shapley Then You}}’ll {{Love}} the {{Core}}},
  booktitle = {Proceedings of the 35th {{AAAI Conference}} on {{Artificial Intelligence}}, 2021},
  author = {Yan, Tom and Procaccia, Ariel D.},
  date = {2021-05-18},
  volume = {6},
  pages = {5751--5759},
  publisher = {Association for the Advancement of Artificial Intelligence},
  location = {Virtual conference},
  doi = {10.1609/aaai.v35i6.16721},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/16721},
  urldate = {2021-04-23},
  abstract = {The prevalent approach to problems of credit assignment in machine learning — such as feature and data valuation— is to model the problem at hand as a cooperative game and apply the Shapley value. But cooperative game theory offers a rich menu of alternative solution concepts, which famously includes the core and its variants. Our goal is to challenge the machine learning community’s current consensus around the Shapley value, and make a case for the core as a viable alternative. To that end, we prove that arbitrarily good approximations to the least core — a core relaxation that is always feasible — can be computed efficiently (but prove an impossibility for a more refined solution concept, the nucleolus). We also perform experiments that corroborate these theoretical results and shed light on settings where the least core may be preferable to the Shapley value.},
  eventtitle = {{{AAAI Conference}} on {{Artificial Intelligence}}},
  langid = {english},
  keywords = {notion}
}
